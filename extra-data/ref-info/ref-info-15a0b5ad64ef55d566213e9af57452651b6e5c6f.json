{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47761629"
                        ],
                        "name": "J. Horn",
                        "slug": "J.-Horn",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Horn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Horn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144842759"
                        ],
                        "name": "G. Schmidt",
                        "slug": "G.-Schmidt",
                        "structuredName": {
                            "firstName": "G\u00fcnther",
                            "lastName": "Schmidt",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Schmidt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These are systems that use no explicit representation at all about the space in which navigation is to take place, but rather resort to recognizing objects found in the environment or to tracking those objects by generating motions based on visual observations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11048703,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b370e9251466727c00efbaf689ed967b230b0d4",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes the localization system of a free-navigating mobile robot. Absolute position and orientation of the vehicle are determined by matching vertical planar surfaces extracted from a 3D-laser-range-image with corresponding surfaces predicted from a 3D-environmental model. Continuous localization is achieved by fusing single-image localization and dead-reckoning data by means of a statistical uncertainty evolution technique. Extensive closed-loop experiments with the full-scale mobile robot MAC-ROBE proved robustness, accuracy and real-time capability of this localization scheme."
            },
            "slug": "Continuous-localization-for-long-range-indoor-of-Horn-Schmidt",
            "title": {
                "fragments": [],
                "text": "Continuous localization for long-range indoor navigation of mobile robots"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This article describes the localization system of a free-navigating mobile robot, which is achieved by fusing single-image localization and dead-reckoning data by means of a statistical uncertainty evolution technique."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1995 IEEE International Conference on Robotics and Automation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9438154"
                        ],
                        "name": "A. Ohya",
                        "slug": "A.-Ohya",
                        "structuredName": {
                            "firstName": "Akihisa",
                            "lastName": "Ohya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ohya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14037223"
                        ],
                        "name": "A. Kosaka",
                        "slug": "A.-Kosaka",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Kosaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kosaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703247"
                        ],
                        "name": "A. Kak",
                        "slug": "A.-Kak",
                        "structuredName": {
                            "firstName": "Avinash",
                            "lastName": "Kak",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These are systems that use no explicit representation at all about the space in which navigation is to take place, but rather resort to recognizing objects found in the environment or to tracking those objects by generating motions based on visual observations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9302454,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4ab5b4f127c8ddbe0d70fd1f543ce0ea1eb5954",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a vision-based navigation method in an indoor environment for an autonomous mobile robot which can avoid obstacles. In this method, the self-localization of the robot is done with a mode-based vision system, and a non-stop navigation is realized by a retroactive position correction system. Stationary obstacles are avoided with single-camera vision and moving obstacles are detected with ultrasonic sensors. We report on experiments in a hallway using the YAMABICO robot."
            },
            "slug": "Vision-based-navigation-of-mobile-robot-with-by-and-Ohya-Kosaka",
            "title": {
                "fragments": [],
                "text": "Vision-based navigation of mobile robot with obstacle avoidance by single camera vision and ultrasonic sensing"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This paper describes a vision-based navigation method in an indoor environment for an autonomous mobile robot which can avoid obstacles and a non-stop navigation is realized by a retroactive position correction system."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34588530"
                        ],
                        "name": "C. Thorpe",
                        "slug": "C.-Thorpe",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Thorpe",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Thorpe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12290634"
                        ],
                        "name": "S. Shafer",
                        "slug": "S.-Shafer",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Shafer",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shafer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Mobile robotics, navigation, computer vision, indoor navigation, outdoor navigation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58857990,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "ebb3d0579d0436bb17391c304ad1d22abffab702",
            "isKey": false,
            "numCitedBy": 529,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A distributed architecture articulated around the CODGER (communication database with geometric reasoning) knowledge database is described for a mobile robot system that includes both perception and navigation tools. Results are described for vision and navigation tests using a mobile testbed that integrates perception and navigation capabilities that are based on two types of vision algorithms: color vision for road following, and 3-D vision for obstacle detection and avoidance. The perception modules are integrated into a system that allows the vehicle to drive continuously in an actual outdoor environment. The resulting system is able to navigate continuously on roads while avoiding obstacles. >"
            },
            "slug": "Vision-and-Navigation-for-the-Carnegie-Mellon-Thorpe-Hebert",
            "title": {
                "fragments": [],
                "text": "Vision and Navigation for the Carnegie-Mellon Navlab"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "By reading vision and navigation the carnegie mellon navlab, you can take more advantages with limited budget."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9438154"
                        ],
                        "name": "A. Ohya",
                        "slug": "A.-Ohya",
                        "structuredName": {
                            "firstName": "Akihisa",
                            "lastName": "Ohya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ohya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14037223"
                        ],
                        "name": "A. Kosaka",
                        "slug": "A.-Kosaka",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Kosaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kosaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703247"
                        ],
                        "name": "A. Kak",
                        "slug": "A.-Kak",
                        "structuredName": {
                            "firstName": "Avinash",
                            "lastName": "Kak",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10221402,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c27995696968dfb0854637b441174a4b34bd662",
            "isKey": false,
            "numCitedBy": 243,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a vision-based navigation method in an indoor environment for an autonomous mobile robot which can avoid obstacles. In this method, the self-localization of the robot is achieved by a model-based vision system, and nonstop navigation is realized by a retroactive position correction system. Stationary obstacles are avoided with single-camera vision and moving obstacles are detected with ultrasonic sensors. We report on experiments in a hallway using the YAMABICO robot."
            },
            "slug": "Vision-based-navigation-by-a-mobile-robot-with-and-Ohya-Kosaka",
            "title": {
                "fragments": [],
                "text": "Vision-based navigation by a mobile robot with obstacle avoidance using single-camera vision and ultrasonic sensing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "In this method, the self-localization of the robot is achieved by a model-based vision system, and nonstop navigation is realized by a retroactive position correction system."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Robotics Autom."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054815080"
                        ],
                        "name": "Takayuki Ohno",
                        "slug": "Takayuki-Ohno",
                        "structuredName": {
                            "firstName": "Takayuki",
                            "lastName": "Ohno",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takayuki Ohno"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9438154"
                        ],
                        "name": "A. Ohya",
                        "slug": "A.-Ohya",
                        "structuredName": {
                            "firstName": "Akihisa",
                            "lastName": "Ohya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ohya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145985307"
                        ],
                        "name": "S. Yuta",
                        "slug": "S.-Yuta",
                        "structuredName": {
                            "firstName": "Shin'ichi",
                            "lastName": "Yuta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Yuta"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 1
                            }
                        ],
                        "text": "([115]) proposed a similar but faster implementation also using sequences of images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 23130680,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "aa100de03e7929e7e162c43cadb4b73047648c90",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The purpose of this study is to realize autonomous navigation of a mobile robot with reference pre-recorded images, without any environment maps. The strategy of navigation in this research is that the differences of position and orientation of robot between the reference one and the present one are continuously estimated from true images, and are used in the locomotion control commands. In this paper, we use only vertical lines in the image as extracted features, and propose the method to calculate the estimated differences of position and orientation from two images. We also show some results of simulations and experiments using an actual mobile robot, and examine its reliability."
            },
            "slug": "Autonomous-navigation-for-mobile-robots-referring-Ohno-Ohya",
            "title": {
                "fragments": [],
                "text": "Autonomous navigation for mobile robots referring pre-recorded image sequence"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper uses only vertical lines in the image as extracted features, and proposes the method to calculate the estimated differences of position and orientation from two images, and shows some results of simulations and experiments using an actual mobile robot, and examines its reliability."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems. IROS '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14037223"
                        ],
                        "name": "A. Kosaka",
                        "slug": "A.-Kosaka",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Kosaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kosaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703247"
                        ],
                        "name": "A. Kak",
                        "slug": "A.-Kak",
                        "structuredName": {
                            "firstName": "Avinash",
                            "lastName": "Kak",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kak"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "These are systems that use no explicit representation at all about the space in which navigation is to take place, but rather resort to recognizing objects found in the environment or to tracking those objects by generating motions based on visual observations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Mobile robotics, navigation, computer vision, indoor navigation, outdoor navigation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The three different approaches to vision-based localization\u2014Absolute (or Global) Localization, Incremental Localization, and Localization Derived from Landmark Tracking\u2014will be visited in greater detail in the next three sections."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37451312,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e3f3775c29b29277b83583d193954c1e9a6ff263",
            "isKey": false,
            "numCitedBy": 224,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The model-based vision system described in this thesis allows a mobile robot to navigate indoors at an average speed of 8 meters/minute using ordinary laboratory computing hardware of approximately 16 MIPS power. The navigation capabilities of the robot are not impaired by the presence of the stationary or moving obstacles. The vision system maintains a model of uncertainty and keeps track of the growth of uncertainty as the robot travels towards the goal position. The estimates of uncertainty are then used to predict bounds on the locations and orientations of landmarks expected to be seen in a monocular image. This greatly reduces the search for establishing correspondence between the features visible in the image and the landmarks. Given a sequence of image features and a sequence of landmarks derived from a geometric model of the environment, a special aspect of our vision system is the sequential reduction in the uncertainty as each image feature is matched successfully with a landmark, allowing subsequent features to be matched more easily, this being a natural byproduct of the manner in which we use Kalman-filter based updating. Strategies for path planning, path replanning and perception planning are introduced for the robot to navigate in the presence of obstacles. Finally, experimental results are presented."
            },
            "slug": "Fast-Vision-guided-Mobile-Robot-Navigation-Using-Of-Kosaka-Kak",
            "title": {
                "fragments": [],
                "text": "Fast Vision-guided Mobile Robot Navigation Using Model-based Reasoning And Prediction Of Uncertainties"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A special aspect of the model-based vision system is the sequential reduction in the uncertainty as each image feature is matched successfully with a landmark, allowing subsequent features to be matched more easily, this being a natural byproduct of the manner in which it uses Kalman-filter based updating."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765887"
                        ],
                        "name": "D. Kriegman",
                        "slug": "D.-Kriegman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kriegman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kriegman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2437345"
                        ],
                        "name": "E. Triendl",
                        "slug": "E.-Triendl",
                        "structuredName": {
                            "firstName": "Ernst",
                            "lastName": "Triendl",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Triendl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738591"
                        ],
                        "name": "T. Binford",
                        "slug": "T.-Binford",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Binford",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Binford"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These are systems that use no explicit representation at all about the space in which navigation is to take place, but rather resort to recognizing objects found in the environment or to tracking those objects by generating motions based on visual observations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 140578,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a27332b0e4d02e3d379bd1cc62aa9388f437087",
            "isKey": false,
            "numCitedBy": 316,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "A mobile robot that autonomously functions in a complex and previously unknown indoor environment has been developed. The omnidirectional mobile robot uses stereo vision, odometry, and contact bumpers to instantiate a symbolic world model. Finding stereo correspondences across a single epipolar line is adequate for instantiating the model. Uncertainty in sensor data is represented by a multivariate normal distribution, and uncertainty models for motion and stereo are presented. Uncertainty is reduced by extended Kalman filtering. To execute a high-level command such as 'Enter the second door on the left', a model is instantiated from sensing and motions are planned and executed. Experimental results from the fast, running system are presented. >"
            },
            "slug": "Stereo-vision-and-navigation-in-buildings-for-Kriegman-Triendl",
            "title": {
                "fragments": [],
                "text": "Stereo vision and navigation in buildings for mobile robots"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A mobile robot that autonomously functions in a complex and previously unknown indoor environment has been developed that uses stereo vision, odometry, and contact bumpers to instantiate a symbolic world model."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Robotics Autom."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14037223"
                        ],
                        "name": "A. Kosaka",
                        "slug": "A.-Kosaka",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Kosaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kosaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054282875"
                        ],
                        "name": "M. Meng",
                        "slug": "M.-Meng",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703247"
                        ],
                        "name": "A. Kak",
                        "slug": "A.-Kak",
                        "structuredName": {
                            "firstName": "Avinash",
                            "lastName": "Kak",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kak"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These are systems that use no explicit representation at all about the space in which navigation is to take place, but rather resort to recognizing objects found in the environment or to tracking those objects by generating motions based on visual observations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27997595,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "9fe44b45c71c6a87eff95abf7cb85f95be051af3",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors describe a vision-guided mobile robot navigation that allows a mobile robot to navigate indoors at speeds of approximately 10 m/min in the presence of obstacles. This system uses empirically constructed models of the dependence of robot-motion-uncertainties on commanded motions for vision-based self-location and for the planning of future motions toward the goal. The vision processes are model-based and use a Kalman filter to reduce the uncertainties in the position of the robot as the landmarks in the scene are matched with features extracted from the images. Position updating using vision is retroactive in the sense that the robot does not wait for the results of vision processing to become available. When the vision results do become available, the robot then retroactively updates the positional uncertainties. In addition to this retroactive updating feature, another major difference between the FINALE system of Kosaka and Kak (1992) and the work reported here is that now the authors are able to execute multiple tasks concurrently.<<ETX>>"
            },
            "slug": "Vision-guided-mobile-robot-navigation-using-of-Kosaka-Meng",
            "title": {
                "fragments": [],
                "text": "Vision-guided mobile robot navigation using retroactive updating of position uncertainty"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "A vision-guided mobile robot navigation that allows a mobile robot to navigate indoors at speeds of approximately 10 m/min in the presence of obstacles is described."
            },
            "venue": {
                "fragments": [],
                "text": "[1993] Proceedings IEEE International Conference on Robotics and Automation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34767446"
                        ],
                        "name": "J. Borenstein",
                        "slug": "J.-Borenstein",
                        "structuredName": {
                            "firstName": "Johann",
                            "lastName": "Borenstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Borenstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41217396"
                        ],
                        "name": "H. Everett",
                        "slug": "H.-Everett",
                        "structuredName": {
                            "firstName": "Hobart",
                            "lastName": "Everett",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Everett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48521792"
                        ],
                        "name": "Liqiang Feng",
                        "slug": "Liqiang-Feng",
                        "structuredName": {
                            "firstName": "Liqiang",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liqiang Feng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 108471076,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8cb911d2a948ae494237edca40b5c9a10833c171",
            "isKey": false,
            "numCitedBy": 727,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This is a survey of the state-of-the-art in sensors, systems, methods and technologies utilized by a mobile robot to determine its position in the environment. The many potential \"solutions\" are roughly categorized into two groups: relative and absolute position measurements. The first includes odometry and inertial navigation; the second comprises active beacons, artificial and natural landmark recognition and model matching. The authors compare and analyze these different methods based on technical publications and on commercial product and patent information. Comparison is centred around the following criteria: accuracy of position and orientation measurements; equipment needed; cost; sampling rate; effective range; computational power required; processing needs; and other special features."
            },
            "slug": "Navigating-Mobile-Robots:-Systems-and-Techniques-Borenstein-Everett",
            "title": {
                "fragments": [],
                "text": "Navigating Mobile Robots: Systems and Techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This is a survey of the state-of-the-art in sensors, systems, methods and technologies utilized by a mobile robot to determine its position in the environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158166870"
                        ],
                        "name": "J. Zheng",
                        "slug": "J.-Zheng",
                        "structuredName": {
                            "firstName": "Jiang",
                            "lastName": "Zheng",
                            "middleNames": [
                                "Yu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773036"
                        ],
                        "name": "M. Barth",
                        "slug": "M.-Barth",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Barth",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Barth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143873014"
                        ],
                        "name": "S. Tsuji",
                        "slug": "S.-Tsuji",
                        "structuredName": {
                            "firstName": "Saburo",
                            "lastName": "Tsuji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Tsuji"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[163], and Yagi et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17393322,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "94a45208de3f263716f86cd57ab68f9772f1ac76",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors introduce an approach to building a qualitative description of scenes along a route, which is used in route recognition by a mobile robot. The description consists of a series of landmarks autonomously selected by the robot from a generalized panoramic view generated as a visual memory of scenes along routes. The basic idea to bridge the quantitative panoramic view to qualitative landmarks is to examine the distinctiveness of patterns in the image and select landmarks from unique patterns that are remarkable by which to navigate.<<ETX>>"
            },
            "slug": "Autonomous-landmark-selection-for-route-recognition-Zheng-Barth",
            "title": {
                "fragments": [],
                "text": "Autonomous landmark selection for route recognition by a mobile robot"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The authors introduce an approach to building a qualitative description of scenes along a route, which is used in route recognition by a mobile robot, which consists of a series of landmarks autonomously selected by the robot from a generalized panoramic view generated as a visual memory of Scenes along routes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1991 IEEE International Conference on Robotics and Automation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144021572"
                        ],
                        "name": "Dongsung Kim",
                        "slug": "Dongsung-Kim",
                        "structuredName": {
                            "firstName": "Dongsung",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dongsung Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144862593"
                        ],
                        "name": "R. Nevatia",
                        "slug": "R.-Nevatia",
                        "structuredName": {
                            "firstName": "Ramakant",
                            "lastName": "Nevatia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nevatia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "Kim and Nevatia [72], [73] have proposed a different approach for mapless navigation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42469394,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "88b38615e5c4875097c3d362a93b160f9edbbc12",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recognition-and-localization-of-generic-objects-for-Kim-Nevatia",
            "title": {
                "fragments": [],
                "text": "Recognition and localization of generic objects for indoor navigation using functionality"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30066585"
                        ],
                        "name": "S. Atiya",
                        "slug": "S.-Atiya",
                        "structuredName": {
                            "firstName": "Sami",
                            "lastName": "Atiya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Atiya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678633"
                        ],
                        "name": "Gregory Hager",
                        "slug": "Gregory-Hager",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Hager",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gregory Hager"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 14408837,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eea5b8fec3ea05356d460536e439384babb479e0",
            "isKey": false,
            "numCitedBy": 226,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for robot localization using visual landmarks is described. This algorithm determines both the correspondence between observed landmarks (in this case vertical edges in the environment) and a preloaded map, and the location of the robot from those correspondences. The primary advantages of this algorithm are its use of a single geometric tolerance to describe observation error, its ability to recognize ambiguous sets of correspondences, its ability to compute bounds on the error in localization, and its fast execution. The current version of the algorithm has been implemented and tested on a mobile robot system. In several hundred trials the algorithm has not failed, and computes location accurate to within a centimeter in less than half a second.<<ETX>>"
            },
            "slug": "Real-time-vision-based-robot-localization-Atiya-Hager",
            "title": {
                "fragments": [],
                "text": "Real-time vision-based robot localization"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "An algorithm for robot localization using visual landmarks is described, which determines both the correspondence between observed landmarks and a preloaded map, and the location of the robot from those correspondences."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Robotics Autom."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765887"
                        ],
                        "name": "D. Kriegman",
                        "slug": "D.-Kriegman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kriegman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kriegman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738591"
                        ],
                        "name": "T. Binford",
                        "slug": "T.-Binford",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Binford",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Binford"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31686213,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf26b4f965e37ac8ac7ba89cc3f230f34dae1bbb",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors introduce the concept of an explicit generic building model. This abstract model is valid for a broad class of buildings. It uses mechanisms that have been used to build generic models for other object classes, including machine screws and generalized cylinders. These methods extend the generality of object classes which can be implemented, compared to previous implementations of object classes. Using a combination of epipolar stereo vision from the mobile robot along with monocular vision, the generic model is instantiated. This instantiation can be used for robot navigation.<<ETX>>"
            },
            "slug": "Generic-models-for-robot-navigation-Kriegman-Binford",
            "title": {
                "fragments": [],
                "text": "Generic models for robot navigation"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The authors introduce the concept of an explicit generic building model that uses a combination of epipolar stereo vision from the mobile robot along with monocular vision, and this instantiation can be used for robot navigation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1988 IEEE International Conference on Robotics and Automation"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7136913"
                        ],
                        "name": "J. Leonard",
                        "slug": "J.-Leonard",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Leonard",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Leonard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398671965"
                        ],
                        "name": "H. Durrant-Whyte",
                        "slug": "H.-Durrant-Whyte",
                        "structuredName": {
                            "firstName": "Hugh",
                            "lastName": "Durrant-Whyte",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Durrant-Whyte"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33390595,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97421e92320d15c4e3f554d46784c028523861d9",
            "isKey": false,
            "numCitedBy": 1405,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The application of the extended Kaman filter to the problem of mobile robot navigation in a known environment is presented. An algorithm for, model-based localization that relies on the concept of a geometric beacon, a naturally occurring environment feature that can be reliably observed in successive sensor measurements and can be accurately described in terms of a concise geometric parameterization, is developed. The algorithm is based on an extended Kalman filter that utilizes matches between observed geometric beacons and an a priori map of beacon locations. Two implementations of this navigation algorithm, both of which use sonar, are described. The first implementation uses a simple vehicle with point kinematics equipped with a single rotating sonar. The second implementation uses a 'Robuter' mobile robot and six static sonar transducers to provide localization information while the vehicle moves at typical speeds of 30 cm/s. >"
            },
            "slug": "Mobile-robot-localization-by-tracking-geometric-Leonard-Durrant-Whyte",
            "title": {
                "fragments": [],
                "text": "Mobile robot localization by tracking geometric beacons"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "An algorithm for, model-based localization that relies on the concept of a geometric beacon, a naturally occurring environment feature that can be reliably observed in successive sensor measurements and can be accurately described in terms of a concise geometric parameterization, is developed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Robotics Autom."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3133484"
                        ],
                        "name": "M. Hashima",
                        "slug": "M.-Hashima",
                        "structuredName": {
                            "firstName": "Masayoshi",
                            "lastName": "Hashima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hashima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064252072"
                        ],
                        "name": "Fumi Hasegawa",
                        "slug": "Fumi-Hasegawa",
                        "structuredName": {
                            "firstName": "Fumi",
                            "lastName": "Hasegawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fumi Hasegawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46369968"
                        ],
                        "name": "S. Kanda",
                        "slug": "S.-Kanda",
                        "structuredName": {
                            "firstName": "Shinji",
                            "lastName": "Kanda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kanda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2431172"
                        ],
                        "name": "T. Maruyama",
                        "slug": "T.-Maruyama",
                        "structuredName": {
                            "firstName": "Tsugito",
                            "lastName": "Maruyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Maruyama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46322703"
                        ],
                        "name": "T. Uchiyama",
                        "slug": "T.-Uchiyama",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Uchiyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Uchiyama"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 37838019,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ac251fb135924fa3df8fe53df042c36487eb0a8",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses a practical system which, when incorporated in an autonomous mobile robot moving indoors, can determine the position of the robot and detect obstacles so that the robot can navigate appropriately. We have developed new real-time, reliable techniques using correlation operations to achieve vision-based localization and obstacle detection. These techniques include a landmark detection technique that assures stable detection even under variable brightness and an obstacle measurement technique that combines obstacle region segmentation with stereo vision. Our experiments have proven that these techniques can make the necessary measurements in 100 ms, assure stable measurement in environments of varying brightness, and are sufficient for mobile robots."
            },
            "slug": "Localization-and-obstacle-detection-for-robots-for-Hashima-Hasegawa",
            "title": {
                "fragments": [],
                "text": "Localization and obstacle detection for robots for carrying food trays"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "New real-time, reliable techniques using correlation operations to achieve vision-based localization and obstacle detection are developed that can determine the position of the robot and detect obstacles so that the robot can navigate appropriately."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738538"
                        ],
                        "name": "L. Delahoche",
                        "slug": "L.-Delahoche",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Delahoche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Delahoche"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688449"
                        ],
                        "name": "C. P\u00e9gard",
                        "slug": "C.-P\u00e9gard",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "P\u00e9gard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. P\u00e9gard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2674289"
                        ],
                        "name": "B. Marhic",
                        "slug": "B.-Marhic",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Marhic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Marhic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1816586"
                        ],
                        "name": "P. Vasseur",
                        "slug": "P.-Vasseur",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vasseur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Vasseur"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28660865,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77eb257a3ff984d7c17fdae88460aa4d8155592c",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a dynamic localization system which allows a mobile robot to evolve autonomously in a structured environment. Our system is based on the use of two sensors: an odometer and an omnidirectional vision system which gives a reference in connection with a set of natural beacons. Our navigation algorithm gives a reliable position estimation due to a systematic dynamic resetting. To merge the data obtained we use the extended Kalman filter. Our proposed method allows us to treat efficiently the noise problems linked to the primitive extraction, which contributes to the robustness of our system. Thus, we have developed a reliable and quick navigation system which can deals with the constraints of moving the robots in an industrial environment. We give the experimental results obtained from a mission realized in an a priori known environment."
            },
            "slug": "A-navigation-system-based-on-an-ominidirectional-Delahoche-P\u00e9gard",
            "title": {
                "fragments": [],
                "text": "A navigation system based on an ominidirectional vision sensor"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A reliable and quick navigation system which can deals with the constraints of moving the robots in an industrial environment and gives the experimental results obtained from a mission realized in an a priori known environment."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2540531"
                        ],
                        "name": "Liana M. Lorigo",
                        "slug": "Liana-M.-Lorigo",
                        "structuredName": {
                            "firstName": "Liana",
                            "lastName": "Lorigo",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liana M. Lorigo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72419159"
                        ],
                        "name": "R. Brooks",
                        "slug": "R.-Brooks",
                        "structuredName": {
                            "firstName": "Rodney",
                            "lastName": "Brooks",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Brooks"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2172333"
                        ],
                        "name": "W. E. L. Grimsou",
                        "slug": "W.-E.-L.-Grimsou",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Grimsou",
                            "middleNames": [
                                "E.",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. E. L. Grimsou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 147
                            }
                        ],
                        "text": "Sometimes in these sorts of applications, the robot is supposed to just wander around, exploring the vicinity of the robot without a clearcut goal [91], [136]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "Another more recent example of this is in [91], where the authors have addressed the problem of vision-guided obstacle avoidance using three redundant vision modules, one for intensity (BW), one for RGB, and one for HSV."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15477703,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d70846fb45a0397cf24a871e60b7369572a4c358",
            "isKey": false,
            "numCitedBy": 189,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an autonomous vision-based obstacle avoidance system. The system consists of three independent vision modules for obstacle detection, each of which is computationally simple and uses a different criterion for detection purposes. These criteria are based on brightness gradients, RGB (red, green, blue) color, and HSV (hue, saturation, value) color, respectively. Selection of which modules are used to command the robot proceeds exclusively from the outputs of the modules themselves. The system is implemented on a small monocular mobile robot and uses very lour resolution images. It has been tested for over 200 hours in diverse environments."
            },
            "slug": "Visually-guided-obstacle-avoidance-in-unstructured-Lorigo-Brooks",
            "title": {
                "fragments": [],
                "text": "Visually-guided obstacle avoidance in unstructured environments"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper presents an autonomous vision-based obstacle avoidance system that consists of three independent vision modules for obstacle detection, each of which is computationally simple and uses a different criterion for detection purposes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751698"
                        ],
                        "name": "K. Sugihara",
                        "slug": "K.-Sugihara",
                        "structuredName": {
                            "firstName": "Kokichi",
                            "lastName": "Sugihara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sugihara"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 15
                            }
                        ],
                        "text": "In the VC (for Virtual Camera) version, virtual images are constructed from the actual camera image, the virtual images corresponding to different viewpoints at different distances from the actual camera."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 66
                            }
                        ],
                        "text": "One of the later versions is called ALVINN-\nVC, where VC stands for Virtual Camera [61], [63]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 232
                            }
                        ],
                        "text": "Four of these systems are: RALPH (Rapidly Adapting Lateral Position Handler) [128], ALVINN (Autonomous land vehicle in a neural network) [124], [125], [126], [127], AURORA (Automotive Run-OffRoad Avoidance) [21], and ALVINN-VC (for Virtual Camera) [61], [63], etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "These are systems that use no explicit representation at all about the space in which navigation is to take place, but rather resort to recognizing objects found in the environment or to tracking those objects by generating motions based on visual observations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 72
                            }
                        ],
                        "text": "Later, the idea of occupancy maps was improved by incorporating \u201dVirtual Force Fields\u201d (VFF) [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31599065,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f6e995cb59d93e153173f59aced48214857f51b3",
            "isKey": true,
            "numCitedBy": 148,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Some-location-problems-for-robot-navigation-using-a-Sugihara",
            "title": {
                "fragments": [],
                "text": "Some location problems for robot navigation using a single camera"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Graph. Image Process."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2174340"
                        ],
                        "name": "Y. Goto",
                        "slug": "Y.-Goto",
                        "structuredName": {
                            "firstName": "Yoshimasa",
                            "lastName": "Goto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Goto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722938"
                        ],
                        "name": "A. Stentz",
                        "slug": "A.-Stentz",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Stentz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stentz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "[88], Goto and Stentz [45], Kuan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 10
                            }
                        ],
                        "text": "Kelly and Stentz [70] have proposed an adaptive approach based on the active vision paradigm that can be used to increase the throughput of perception and, thus, to increase the maximum speed of a mobile robot."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 6
                            }
                        ],
                        "text": "[88], Goto and Stentz [45], Kuan et al. [83] and Kuan and Sharma [84], and others."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206532296,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50f309cc81824bab3efab40b341dce9de1fe28ed",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the current status of the Autonomous Land Vehicle research at Carnegie-Mellon University's Robotics Institute, focusing primarily on the system architecture. We begin with a discussion of the issues concerning outdoor navigation, then describe the various perception, planning, and control components of our system that address these issues. We describe the CODGER software system for integrating these components into a single system, synchronizing the data flow between them in order to maximize parallelism. Our system is able to drive a robot vehicle continuously with two sensors, a color camera and a laser rangefinder, on a network of sidewalks, up a bicycle slope, and through a curved road through an area populated with trees. Finally, we discuss the results of our experiments, as well as problems uncovered in the process and our plans for addressing them."
            },
            "slug": "The-CMU-system-for-mobile-robot-navigation-Goto-Stentz",
            "title": {
                "fragments": [],
                "text": "The CMU system for mobile robot navigation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The various perception, planning, and control components of the CODGER software system for integrating these components into a single system, synchronizing the data flow between them in order to maximize parallelism are described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1987 IEEE International Conference on Robotics and Automation"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742183"
                        ],
                        "name": "M. Matari\u0107",
                        "slug": "M.-Matari\u0107",
                        "structuredName": {
                            "firstName": "Maja",
                            "lastName": "Matari\u0107",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Matari\u0107"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 163
                            }
                        ],
                        "text": "The occupancy-grid approach to map learning is to be contrasted with the approaches that create topological representations of space [22], [23], [39], [74], [87], [97], [123], [161], [164]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60587385,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a511da7971ffd0c282409052cfac2cf1a66c1fd7",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A distributed method for mobile robot navigation, spatial learning, and path planning is presented. It is implemented on a sonar-based physical robot, Toto, consisting of three competence layers: 1) Low-level navigation: a collection of reflex-like rules resulting in emergent boundary-tracing. 2) Landmark detection: dynamically extracts landmarks from the robot''s motion. 3) Map learning: constructs a distributed map of landmarks. The parallel implementation allows for localization in constant time. {\\it Spreading of activation} computes both topological and physical shortest paths in linear time. The main issues addressed are: distributed, procedural, and qualitative representation and computation, emergent behaviors, dynamic landmarks, minimized communication."
            },
            "slug": "A-Distributed-Model-for-Mobile-Robot-and-Navigation-Matari\u0107",
            "title": {
                "fragments": [],
                "text": "A Distributed Model for Mobile Robot Environment-Learning and Navigation"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A distributed method for mobile robot navigation, spatial learning, and path planning is presented and the main issues addressed are: distributed, procedural, and qualitative representation and computation, emergent behaviors, dynamic landmarks, minimized communication."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719955"
                        ],
                        "name": "R. Simmons",
                        "slug": "R.-Simmons",
                        "structuredName": {
                            "firstName": "Reid",
                            "lastName": "Simmons",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Simmons"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145314818"
                        ],
                        "name": "Sven Koenig",
                        "slug": "Sven-Koenig",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Koenig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sven Koenig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14831277,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69969d5738f265dbc6fbf9a6a67eea02acb9e88a",
            "isKey": false,
            "numCitedBy": 616,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Autonomous mobile robots need very reliable navigation capabilities in order to operate unattended for long periods of time. This paper reports on first results of a research program that uses par tially observable Markov models to robustly track a robots location in office environments and to direct its goal-oriented actions. The approach explicitly maintains a probability distribution over the possible locations of the robot taking into account various sources of uncertainly including approximate knowledge of the environment and actuator and sensor uncertainty. A novel feature of our approach is its integration of topological map information with approximate metric information. We demonstrate the robustness of this approach in controlling an actual indoor mobile robot navigating corridors."
            },
            "slug": "Probabilistic-Robot-Navigation-in-Partially-Simmons-Koenig",
            "title": {
                "fragments": [],
                "text": "Probabilistic Robot Navigation in Partially Observable Environments"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "First results are reported on first results of a research program that uses par tially observable Markov models to robustly track a robots location in office environments and to direct its goal-oriented actions."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723059"
                        ],
                        "name": "H. Christensen",
                        "slug": "H.-Christensen",
                        "structuredName": {
                            "firstName": "Henrik",
                            "lastName": "Christensen",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Christensen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1973191"
                        ],
                        "name": "N. Kirkeby",
                        "slug": "N.-Kirkeby",
                        "structuredName": {
                            "firstName": "Niels",
                            "lastName": "Kirkeby",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kirkeby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144483558"
                        ],
                        "name": "S. Kristensen",
                        "slug": "S.-Kristensen",
                        "structuredName": {
                            "firstName": "Steen",
                            "lastName": "Kristensen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kristensen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "84186801"
                        ],
                        "name": "L. Knudsen",
                        "slug": "L.-Knudsen",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Knudsen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Knudsen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700008"
                        ],
                        "name": "E. Granum",
                        "slug": "E.-Granum",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Granum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Granum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These are systems that use no explicit representation at all about the space in which navigation is to take place, but rather resort to recognizing objects found in the environment or to tracking those objects by generating motions based on visual observations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16767296,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69f34d330fc5877dd4bc73b1fa43ff2f0196a3c5",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract For navigation in a partially known environment it is possible to provide a model that may be used for guidance in the navigation and as a basis for selective sensing. In this paper a navigation system for an autonomous mobile robot is presented. Both navigation and sensing is built around a graphics model, which enables prediction of the expected scene content. The model is used directly for prediction of line segments which, through matching, allow estimation of position and orientation. In addition, the model is used as a basis for a hierarchical stereo matching that enables dynamic updating of the model with unmodelled objects in the environment. For short-term path planning a set of reactive behaviours is used. The reactive behaviours include use of inverse perspective mapping for generation of occupancy grids, a sonar system and simple gaze holding for monitoring of dynamic obstacles. The full system and its component processes are described and initial experiments with the system are briefly outlined."
            },
            "slug": "Model-driven-vision-for-in-door-navigation-Christensen-Kirkeby",
            "title": {
                "fragments": [],
                "text": "Model-driven vision for in-door navigation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A navigation system for an autonomous mobile robot is presented, built around a graphics model, which enables prediction of the expected scene content and is used directly for prediction of line segments which allow estimation of position and orientation."
            },
            "venue": {
                "fragments": [],
                "text": "Robotics Auton. Syst."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8470691"
                        ],
                        "name": "G. Oriolo",
                        "slug": "G.-Oriolo",
                        "structuredName": {
                            "firstName": "Giuseppe",
                            "lastName": "Oriolo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Oriolo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244331"
                        ],
                        "name": "M. Vendittelli",
                        "slug": "M.-Vendittelli",
                        "structuredName": {
                            "firstName": "Marilena",
                            "lastName": "Vendittelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Vendittelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714101"
                        ],
                        "name": "G. Ulivi",
                        "slug": "G.-Ulivi",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Ulivi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ulivi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These are systems that use no explicit representation at all about the space in which navigation is to take place, but rather resort to recognizing objects found in the environment or to tracking those objects by generating motions based on visual observations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206537721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8660682bce28ee9c783f67e35949fd69629b5e5",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of sensor-based robot motion planning in unknown environments is addressed. The proposed solution approach prescribes the repeated sequence of two fundamental processes: perception and navigation. In the former, the robot collects data from its sensors, builds local maps and integrates them with the global maps so far reconstructed, using fuzzy logic operators. During the navigation process, a planner based on the A* algorithm proposes a path from the current position to the goal. The robot moves along this path until one of two termination conditions is verified namely (i) an unexpected obstructing obstacle is detected, or (ii) the robot is leaving the area in which reliable information has been gathered. Experimental results are presented for a Nomad 200 mobile robot."
            },
            "slug": "On-line-map-building-and-navigation-for-autonomous-Oriolo-Vendittelli",
            "title": {
                "fragments": [],
                "text": "On-line map building and navigation for autonomous mobile robots"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The problem of sensor-based robot motion planning in unknown environments is addressed and the proposed solution approach prescribes the repeated sequence of two fundamental processes: perception and navigation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1995 IEEE International Conference on Robotics and Automation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47761629"
                        ],
                        "name": "J. Horn",
                        "slug": "J.-Horn",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Horn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Horn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144842759"
                        ],
                        "name": "G. Schmidt",
                        "slug": "G.-Schmidt",
                        "structuredName": {
                            "firstName": "G\u00fcnther",
                            "lastName": "Schmidt",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Schmidt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These are systems that use no explicit representation at all about the space in which navigation is to take place, but rather resort to recognizing objects found in the environment or to tracking those objects by generating motions based on visual observations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 781155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba8c2008b4b8cc240c66f5747c8ad56b611dacc0",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Continuous-localization-of-a-mobile-robot-based-on-Horn-Schmidt",
            "title": {
                "fragments": [],
                "text": "Continuous localization of a mobile robot based on 3D-laser-range-data, predicted sensor images, and dead-reckoning"
            },
            "venue": {
                "fragments": [],
                "text": "Robotics Auton. Syst."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122748666"
                        ],
                        "name": "Josep Fern\u00e1ndez",
                        "slug": "Josep-Fern\u00e1ndez",
                        "structuredName": {
                            "firstName": "Josep",
                            "lastName": "Fern\u00e1ndez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josep Fern\u00e1ndez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144921705"
                        ],
                        "name": "A. Casals",
                        "slug": "A.-Casals",
                        "structuredName": {
                            "firstName": "Alicia",
                            "lastName": "Casals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Casals"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36832644,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67699311d7bb2976eb3dacf4a73e7dc1cc649326",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents a methodology for autonomous navigation in weakly structured outdoor environments such as dirt roads or mountain ways. The main problem to solve is the detection of an ill-defined structure-the way-and the obstacles in the scene, when working in variable lighting conditions. First, we discuss the road description requirements to perform autonomous navigation in this kind of environment and propose a simple sensors configuration based on vision. A simplified road description is generated from the analysis of a sequence of color images, considering the constraints imposed by the model of ill-structured roads. This environment description is done in three steps: region segmentation, obstacle detection and coherence evaluation."
            },
            "slug": "Autonomous-navigation-in-ill-structured-outdoor-Fern\u00e1ndez-Casals",
            "title": {
                "fragments": [],
                "text": "Autonomous navigation in ill-structured outdoor environment"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A simplified road description is generated from the analysis of a sequence of color images, considering the constraints imposed by the model of ill-structured roads, and a simple sensors configuration based on vision is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144827643"
                        ],
                        "name": "N. Ayache",
                        "slug": "N.-Ayache",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Ayache",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ayache"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33726225"
                        ],
                        "name": "O. Faugeras",
                        "slug": "O.-Faugeras",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Faugeras",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Faugeras"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 79
                            }
                        ],
                        "text": "Other notable contributions related to map building are by Ayache and Faugeras [7], [6] using trinocular vision and Kalman filtering, by Zhang and Faugeras [162] using 3D reconstruction from image sequences, by Giralt et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 59
                            }
                        ],
                        "text": "Other notable contributions related to map building are by Ayache and Faugeras [7], [6] using trinocular vision and Kalman filtering, by Zhang and Faugeras [162] using 3D reconstruction from image sequences, by Giralt et al. [44] using sensor fusion techniques, and by Zheng et al. [163], and Yagi et al. [159] using panoramic views."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9041044,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "6324db39054abf05dbd58ac8f719420cd9ff5ae7",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A description is given of current ideas related to the problem of building and updating three-dimensional representations of the environment of a mobile robot that uses passive vision as its main sensory modality. The authors attempt to represent both geometry and uncertainty. The authors motivate their approach by defining the problems they are trying to solve and then give some simple didactic examples. They then present a tool they think is extremely well adapted to solving most of these problems: the extended Kalman filter (EKF). The authors discuss the notions of minimal geometric representations for three-dimensional lines, planes, and rigid motions. They show how the EKF and the representations can be combined to provide solutions for some of the problems. A number of experimental results on real data are given. >"
            },
            "slug": "Maintaining-representations-of-the-environment-of-a-Ayache-Faugeras",
            "title": {
                "fragments": [],
                "text": "Maintaining representations of the environment of a mobile robot"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Robotics Autom."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691940"
                        ],
                        "name": "J. Miura",
                        "slug": "J.-Miura",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Miura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Miura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700379"
                        ],
                        "name": "Y. Shirai",
                        "slug": "Y.-Shirai",
                        "structuredName": {
                            "firstName": "Yoshiaki",
                            "lastName": "Shirai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Shirai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 80
                            }
                        ],
                        "text": "For those issues, the reader is referred to the work of Miura and Shirai [104], [105], [106]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1771181,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40fda70ba1f8c47231fd6bb1dba49d95a58f78aa",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors describe a framework for planning of vision and motion for a mobile robot. For planning in a real world, the uncertainty and the cost of visual recognition are important issues. A robot has to consider a tradeoff between the cost of visual recognition and the effect of information obtained by recognition. A problem is to generate a sequence of vision and motion operations based on sensor information which is an integration of the current information and the predicted next sensor data. The problem is solved by recursive prediction of sensor information and the recursive search of operations. As an example of sensor modeling, a model of stereo vision is described in which correspondence of wrong pairs of features as well as quantization error are considered. Using the framework, a robot can successfully generate a plan for real-world problem.<<ETX>>"
            },
            "slug": "Vision-motion-planning-with-uncertainty-Miura-Shirai",
            "title": {
                "fragments": [],
                "text": "Vision-motion planning with uncertainty"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "A framework for planning of vision and motion for a mobile robot is described in which correspondence of wrong pairs of features as well as quantization error are considered and a robot can successfully generate a plan for real-world problem."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1992 IEEE International Conference on Robotics and Automation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747999"
                        ],
                        "name": "U. Zimmer",
                        "slug": "U.-Zimmer",
                        "structuredName": {
                            "firstName": "Uwe",
                            "lastName": "Zimmer",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Zimmer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 631069,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "b1b9821c70e154c6073b862dcd6ed4c01fdfb271",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Robust-world-modelling-and-navigation-in-a-real-Zimmer",
            "title": {
                "fragments": [],
                "text": "Robust world-modelling and navigation in a real world"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1938874"
                        ],
                        "name": "X. Leb\u00e8gue",
                        "slug": "X.-Leb\u00e8gue",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Leb\u00e8gue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Leb\u00e8gue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705627"
                        ],
                        "name": "J. Aggarwal",
                        "slug": "J.-Aggarwal",
                        "structuredName": {
                            "firstName": "Jake",
                            "lastName": "Aggarwal",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Aggarwal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12246980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "583b0fa59ba7ffab974c7ff58d07f0a4e915b5d2",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "New algorithms for detecting and interpreting linear features of a real scene as imaged by a single camera on a mobile robot are described. The low-level processing stages are specifically designed to increase the usefulness and the quality of the extracted features for indoor scene understanding. In order to derive 3-D information from a 2-D image, we consider only lines with particular orientation in 3-D. The detection and interpretation processes provide a 3-D orientation hypothesis for each 2-D segment. This in turn is used to estimate the robot's orientation and relative position in the environment. Next, the orientation data is used by a motion stereo algorithm to fully estimate the 3-D structure when a sequence of images becomes available. From detection to 3-D estimation, a strong emphasis is placed on real-world applications and very fast processing with conventional hardware. Results of experimentation with a mobile robot under realistic conditions are given and discussed. >"
            },
            "slug": "Significant-line-segments-for-an-indoor-mobile-Leb\u00e8gue-Aggarwal",
            "title": {
                "fragments": [],
                "text": "Significant line segments for an indoor mobile robot"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "New algorithms for detecting and interpreting linear features of a real scene as imaged by a single camera on a mobile robot are described, with a strong emphasis on real-world applications and very fast processing with conventional hardware."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Robotics Autom."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715071"
                        ],
                        "name": "Y. Yagi",
                        "slug": "Y.-Yagi",
                        "structuredName": {
                            "firstName": "Yasushi",
                            "lastName": "Yagi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yagi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821402"
                        ],
                        "name": "S. Kawato",
                        "slug": "S.-Kawato",
                        "structuredName": {
                            "firstName": "Shinjiro",
                            "lastName": "Kawato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kawato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143873014"
                        ],
                        "name": "S. Tsuji",
                        "slug": "S.-Tsuji",
                        "structuredName": {
                            "firstName": "Saburo",
                            "lastName": "Tsuji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Tsuji"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[159] using panoramic views."
                    },
                    "intents": []
                }
            ],
            "corpusId": 32332609,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "761190a47991108a1af0a6dcd70b81f57f6dd871",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Describes a conic projection image sensor (COPIS) and its application: navigating a mobile robot in a manner that avoids collisions with objects approaching from any direction. The COPIS system acquires an omnidirectional view around the robot, in real-time, with use of a conic mirror. Based on the assumption of constant linear motion of the robot and objects, the objects moving along collision paths are detected by monitoring azimuth changes. Confronted with such objects, the robot changes velocity to avoid collision and determines locations and velocities. >"
            },
            "slug": "Real-time-omnidirectional-image-sensor-(COPIS)-for-Yagi-Kawato",
            "title": {
                "fragments": [],
                "text": "Real-time omnidirectional image sensor (COPIS) for vision-guided navigation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The COPIS system acquires an omnidirectional view around the robot, in real-time, with use of a conic mirror, based on the assumption of constant linear motion of the robot and objects."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Robotics Autom."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694310"
                        ],
                        "name": "T. Tsubouchi",
                        "slug": "T.-Tsubouchi",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Tsubouchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tsubouchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145985307"
                        ],
                        "name": "S. Yuta",
                        "slug": "S.-Yuta",
                        "structuredName": {
                            "firstName": "Shin'ichi",
                            "lastName": "Yuta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Yuta"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These are systems that use no explicit representation at all about the space in which navigation is to take place, but rather resort to recognizing objects found in the environment or to tracking those objects by generating motions based on visual observations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26703334,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "f06c6b4f7cf06bd12348c0fab654129112132227",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors have constructed a self-contained intelligent robot on which ultrasonic range finders are installed. A sensory system with sensing ability appears necessary; the authors propose a vision system for a mobile robot that meets real time requirements. In the vision system, a color image provids actual image information and map information is also utilized for reckoning in a building environment. The key point is that matching between image and map information is performed by using highly abstracted information from the image and also from the map. The constitution of the vision system and experimental results are described."
            },
            "slug": "Map-assisted-vision-system-of-mobile-robots-for-in-Tsubouchi-Yuta",
            "title": {
                "fragments": [],
                "text": "Map assisted vision system of mobile robots for reckoning in a building environment"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "In the vision system, a color image provids actual image information and map information is also utilized for reckoning in a building environment, and the key point is that matching between image and map Information is performed by using highly abstracted information from the image and also from the map."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1987 IEEE International Conference on Robotics and Automation"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144367692"
                        ],
                        "name": "E. Huber",
                        "slug": "E.-Huber",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Huber",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Huber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689716"
                        ],
                        "name": "D. Kortenkamp",
                        "slug": "D.-Kortenkamp",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kortenkamp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kortenkamp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18002686,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7050064acaf5f5598a89bbf4a005fafbbac3ed7d",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "To interact effectively with humans, mobile robots will need certain skills. One particularly important skill is the ability to pursue moving agents. To do this, the robot needs a robust visual tracking algorithm and an effective obstacle avoidance algorithm, plus a means of integrating these two behaviors in a seamless manner. In this paper, we introduce the proximity space method as a means for performing real-time, behavior-based control of visual gaze. We then show how this method is integrated with robot motion using an intelligent control architecture that can automatically reconfigure the robot's behaviors in response to environmental changes. The resulting implementation pursues people and other robots around our laboratory for extended periods of time."
            },
            "slug": "Using-stereo-vision-to-pursue-moving-agents-with-a-Huber-Kortenkamp",
            "title": {
                "fragments": [],
                "text": "Using stereo vision to pursue moving agents with a mobile robot"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper introduces the proximity space method as a means for performing real-time, behavior-based control of visual gaze and shows how this method is integrated with robot motion using an intelligent control architecture that can automatically reconfigure the robot's behaviors in response to environmental changes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1995 IEEE International Conference on Robotics and Automation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50538860"
                        ],
                        "name": "R. T. Dunlay",
                        "slug": "R.-T.-Dunlay",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Dunlay",
                            "middleNames": [
                                "Terry"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. T. Dunlay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2353298"
                        ],
                        "name": "D. Morgenthaler",
                        "slug": "D.-Morgenthaler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Morgenthaler",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Morgenthaler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "VITS is a general framework for vision for outdoor road-following together with obstacle detection and avoidance [38], [150]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 140171157,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "a579b71f2007f4667304c933d3b2a2c3df646d66",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "This report describes range data based obstacle avoidance techniques developed for use on an autonomous road-following robot vehicle. The purpose of these techniques is to detect and locate obstacles present in a road environment for navigation of a robot vehicle equipped with an active laser-based range sensor. Techniques are presented for obstacle detection, obstacle location, and coordinate transformations needed in the construction of Scene Models (symbolic structures representing the 3-D obstacle boundaries used by the vehicle's Navigator for path planning). These techniques have been successfully tested on an outdoor robotic vehicle, the Autonomous Land Vehicle (ALV), at speeds up to 3.5 km/hour."
            },
            "slug": "Obstacle-Avoidance-On-Roadways-Using-Range-Data-Dunlay-Morgenthaler",
            "title": {
                "fragments": [],
                "text": "Obstacle Avoidance On Roadways Using Range Data"
            },
            "venue": {
                "fragments": [],
                "text": "Other Conferences"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2937141"
                        ],
                        "name": "Brian Yamauchi",
                        "slug": "Brian-Yamauchi",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Yamauchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Yamauchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760775"
                        ],
                        "name": "R. Beer",
                        "slug": "R.-Beer",
                        "structuredName": {
                            "firstName": "Randall",
                            "lastName": "Beer",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Beer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 176
                            }
                        ],
                        "text": "The occupancy-grid approach to map learning is to be contrasted with the approaches that create topological representations of space [22], [23], [39], [74], [87], [97], [123], [161], [164]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14316334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "230644fbf2afea618f8c746de47372db371b4b99",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes techniques that have been developed for spatial learning in dynamic environments and a mobile robot system, ELDEN, that integrates these techniques for exploration and navigation. In this research, we introduce the concept of adaptive place networks, incrementally-constructed spatial representations that incorporate variable-confidence links to model uncertainty about topological adjacency. These networks guide the robot's navigation while constantly adapting to any topological changes that are encountered. ELDEN integrates these networks with a reactive controller that is robust to transient changes in the environment and a relocalization system that uses evidence grids to recalibrate dead reckoning."
            },
            "slug": "Spatial-learning-for-navigation-in-dynamic-Yamauchi-Beer",
            "title": {
                "fragments": [],
                "text": "Spatial learning for navigation in dynamic environments"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The concept of adaptive place networks, incrementally-constructed spatial representations that incorporate variable-confidence links to model uncertainty about topological adjacency, are introduced, which guide the robot's navigation while constantly adapting to any topological changes that are encountered."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern. Part B"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803608"
                        ],
                        "name": "G. Giralt",
                        "slug": "G.-Giralt",
                        "structuredName": {
                            "firstName": "Georges",
                            "lastName": "Giralt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Giralt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1916089"
                        ],
                        "name": "R. Sobek",
                        "slug": "R.-Sobek",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Sobek",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sobek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143907295"
                        ],
                        "name": "R. Chatila",
                        "slug": "R.-Chatila",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Chatila",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chatila"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "WRITING a survey paper on computer vision for mobilerobot navigation\u2014a subject that was in high prominence in the 1980s and the first half of the 1990s\u2014is daunting, if not actually hazardous to one\u2019s career."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29815989,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e09a594c8bfd9fb88f22573871074a0f8c049cae",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the current state of HILARE: a modular progressively-built mobile robot aimed at general robotics research. The computer organization comprises of local mini and micro processors coupled with a remote time-shared system acting as a consulting facility. A multi-level decision-making system is presented with world models, inference rules, and algorithms particular to each level. The navigation planner uses a geometric model wherein 2-space is partitioned into polygonal areas based on perceptual and/or initial information. A cost function is proposed which provides support for optimal or E-optimal path finding."
            },
            "slug": "A-Multi-level-Planning-and-Navigation-System-for-a-Giralt-Sobek",
            "title": {
                "fragments": [],
                "text": "A Multi-level Planning and Navigation System for a Mobile Robot: A First Approach to HILARE"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "The current state of HILARE: a modular progressively-built mobile robot aimed at general robotics research is described, which comprises of local mini and micro processors coupled with a remote time-shared system acting as a consulting facility."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782162"
                        ],
                        "name": "L. Matthies",
                        "slug": "L.-Matthies",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Matthies",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Matthies"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12290634"
                        ],
                        "name": "S. Shafer",
                        "slug": "S.-Shafer",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Shafer",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shafer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9357489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1656dfb725af96448089f5508fbf399c91f3d5ac",
            "isKey": false,
            "numCitedBy": 386,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In stereo navigation, a mobile robot estimates its position by tracking landmarks with on-board cameras. Previous systems for stereo navigation have suffered from poor accuracy, in part because they relied on scalar models of measurement error in triangulation. Using three-dimensional (3D) Gaussian distributions to model triangulation error is shown to lead to much better performance. How to compute the error model from image correspondences, estimate robot motion between frames, and update the global positions of the robot and the landmarks over time are discussed. Simulations show that, compared to scalar error models, the 3D Gaussian reduces the variance in robot position estimates and better distinguishes rotational from translational motion. A short indoor run with real images supported these conclusions and computed the final robot position to within two percent of distance and one degree of orientation. These results illustrate the importance of error modeling in stereo vision for this and other applications."
            },
            "slug": "Error-modeling-in-stereo-navigation-Matthies-Shafer",
            "title": {
                "fragments": [],
                "text": "Error modeling in stereo navigation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Simulations show that, compared to scalar error models, the 3D Gaussian reduces the variance in robot position estimates and better distinguishes rotational from translational motion."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE J. Robotics Autom."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100512205"
                        ],
                        "name": "C. A.",
                        "slug": "C.-A.",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "A.",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. A."
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These are systems that use no explicit representation at all about the space in which navigation is to take place, but rather resort to recognizing objects found in the environment or to tracking those objects by generating motions based on visual observations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Mobile robotics, navigation, computer vision, indoor navigation, outdoor navigation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17394639,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10d0a6742bc718dc37f80eba5bf4e3595172f646",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "arge strides recently have been made in the model-based L approach to vision-guided mobile robot navigation in indoor environments. Although the model-based method does indeed result in very robust reasoning and control architectures, the fact remains that this approach requires precise geometrical modeling of those elements of the environment that are considered visually significant a requirement that can be difficult to fulfill in some cases. The need for geometrical modeling of the environment also makes such systems \u201cnon-human-like.\u201d We have, therefore, recently developed a new kind of reasoning and control architecture for vision-guided navigation that makes a robot more \u201chuman-like.\u201d This system, called NEURO-NAV, discards the more traditional geometrical representation of the environment, and instead uses a semantically richer nonmetrical representation in which a hallway is modeled by the order of appearance of various landmarks and by adjacency relationships. With such a representation, it becomes possible for the robot to respond to humansupplied commands such as, \u201cFollow the corridor and tum right at the second T junction.\u201d This capability is achieved by an ensemble of neural networks whose activation and deactivation are controlled by a supervisory controller that is rule-based. The individual neural networks in the ensemble are trained to interpret visual information and perform primitive navigational tasks such as hallway following and landmark detection."
            },
            "slug": "Mobile-Robot-Navigation-Using-Neural-Networks-and-C.",
            "title": {
                "fragments": [],
                "text": "Mobile Robot Navigation Using Neural Networks and Nonmetrical Environment Models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new kind of reasoning and control architecture for vision-guided navigation that makes a robot more \u201chuman-like\u201d is developed, called NEURO-NAV, which discards the more traditional geometrical representation of the environment and uses a semantically richer nonmetrical representation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398909021"
                        ],
                        "name": "J. Santos-Victor",
                        "slug": "J.-Santos-Victor",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Santos-Victor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Santos-Victor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678909"
                        ],
                        "name": "G. Sandini",
                        "slug": "G.-Sandini",
                        "structuredName": {
                            "firstName": "Giulio",
                            "lastName": "Sandini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sandini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788383"
                        ],
                        "name": "Francesca Curotto",
                        "slug": "Francesca-Curotto",
                        "structuredName": {
                            "firstName": "Francesca",
                            "lastName": "Curotto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francesca Curotto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798610"
                        ],
                        "name": "Stefano Garibaldi",
                        "slug": "Stefano-Garibaldi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Garibaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefano Garibaldi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 27
                            }
                        ],
                        "text": "implementation reported in [133], if texture can be extracted"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 26
                            }
                        ],
                        "text": "In robee, as the robot in [133] is called, a divergent stereo approach was employed to mimic the centering reflex of a bee."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[133] have developed an optical-flowbased system that mimics the visual behavior of bees."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11289873,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a71fc34d04360318f510e9c1894aefd7a45ec5e8",
            "isKey": true,
            "numCitedBy": 109,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A qualitative approach to visually guided navigation based on the computation of optical flow field is presented. The approach is based on the use of two cameras mounted on a mobile robot and with the optical axis directed in opposite directions, such that the two visual fields do not overlap (divergent stereo). Range computation is based on the computation of the apparent image speed on images acquired during the robot's motion. An example of reflex-type control of motion, driven by differential estimation of the flow field measured by the two eyes, is presented. It is shown how a difficult task like navigating through a funneled corridor with obstacles is possible without the need for metric depth estimation.<<ETX>>"
            },
            "slug": "Divergent-stereo-for-robot-navigation:-learning-Santos-Victor-Sandini",
            "title": {
                "fragments": [],
                "text": "Divergent stereo for robot navigation: learning from bees"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown how a difficult task like navigating through a funneled corridor with obstacles is possible without the need for metric depth estimation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038179"
                        ],
                        "name": "A. Waxman",
                        "slug": "A.-Waxman",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Waxman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waxman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733530"
                        ],
                        "name": "J. L. Moigne",
                        "slug": "J.-L.-Moigne",
                        "structuredName": {
                            "firstName": "Jacqueline",
                            "lastName": "Moigne",
                            "middleNames": [
                                "Le"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. L. Moigne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152880830"
                        ],
                        "name": "B. Srinivasan",
                        "slug": "B.-Srinivasan",
                        "structuredName": {
                            "firstName": "Babu",
                            "lastName": "Srinivasan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Srinivasan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[154], [155],Wallaceetal."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5526568,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "066d107870023d473b21844a456abaf12aec7199",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes ongoing work at the Computer Vision Laboratory, in support of the Autonomous Land Vehicle project of the Defense Advanced Research Projects Agency's Strategic Computing Program. A visual navigation system is being developed to enable a vehicle to follow roads which, for the time being, are free of obstacles. We have developed an approach which consists of a boot-strap phase and a feed-forward phase. In each phase, visual processing is executed in three modules: an image processing module, a geometry module and a rule-based reasoning module. These modules and their interrelationships are described, with particular detail given to the image processing module."
            },
            "slug": "Visual-navigation-of-roadways-Waxman-Moigne",
            "title": {
                "fragments": [],
                "text": "Visual navigation of roadways"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A visual navigation system is being developed to enable a vehicle to follow roads which, for the time being, are free of obstacles, and an approach which consists of a boot-strap phase and a feed-forward phase is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1985 IEEE International Conference on Robotics and Automation"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145356886"
                        ],
                        "name": "R. Wallace",
                        "slug": "R.-Wallace",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Wallace"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "134258420"
                        ],
                        "name": "K. Matsuzaki",
                        "slug": "K.-Matsuzaki",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Matsuzaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Matsuzaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2174340"
                        ],
                        "name": "Y. Goto",
                        "slug": "Y.-Goto",
                        "structuredName": {
                            "firstName": "Yoshimasa",
                            "lastName": "Goto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Goto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1835606"
                        ],
                        "name": "J. Crisman",
                        "slug": "J.-Crisman",
                        "structuredName": {
                            "firstName": "Jill",
                            "lastName": "Crisman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Crisman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145787720"
                        ],
                        "name": "J. Webb",
                        "slug": "J.-Webb",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Webb",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Webb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[152]andWallace[153],Lawtonetal."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14418097,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2bada16761efd2d589c6f1f1ce61a76e7778619d",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We report progress in visual road following by autonomous robot vehicles. We present results and work in progress in the areas of system architecture, image rectification and camera calibration, oriented edge tracking, color classification and road-region segmentation, extracting geometric structure, and the use of a map. In test runs of an outdoor robot vehicle, the Terregator, under control of the Warp computer, we have demonstrated continuous motion vision-guided road-following at speeds up to 1.08 km/hour with image processing and steering servo loop times of 3 sec."
            },
            "slug": "Progress-in-robot-road-following-Wallace-Matsuzaki",
            "title": {
                "fragments": [],
                "text": "Progress in robot road-following"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "In test runs of an outdoor robot vehicle, the Terregator, under control of the Warp computer, it is demonstrated continuous motion vision-guided road-following at speeds up to 1.08 km/hour with image processing and steering servo loop times of 3 sec."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1986 IEEE International Conference on Robotics and Automation"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144021572"
                        ],
                        "name": "Dongsung Kim",
                        "slug": "Dongsung-Kim",
                        "structuredName": {
                            "firstName": "Dongsung",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dongsung Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144862593"
                        ],
                        "name": "R. Nevatia",
                        "slug": "R.-Nevatia",
                        "structuredName": {
                            "firstName": "Ramakant",
                            "lastName": "Nevatia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nevatia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "These are systems that depend on user-created geometric models or topological maps of the environment."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43329131,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55c69b5229e6f02a5c419c44f7ef8e3ceb0a3436",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a spatial representation, s-map, for an indoor navigation robot. The s-map represents the locations of obstacles in a planar domain, where obstacles are defined as any objects that can block movement of the robot. In building the s-map, the viewing triangle constraint and the stability constraint are introduced for efficient verification of vertical surfaces. These verified vertical surfaces and 3-D segments of obstacles smaller than a robot, are mapped to the s-map by simply dropping height information. Thus, the s-map is made directly from 3-D segments with simple verification, and represents obstacles in a planar domain so that it becomes a navigable map for the robot without further processing. In addition to efficient map building, the s-map represents the environment more realistically and completely. Furthermore, the s-map converts many navigation problems in 3-D, such as map fusion and path planning, into 2-D ones. We present the analysis of the s-map in terms of complexity and reliability, and discuss its pros and cons. Moreover, we show the results of the s-maps for indoor environments.<<ETX>>"
            },
            "slug": "Representation-and-computation-of-the-spatial-for-Kim-Nevatia",
            "title": {
                "fragments": [],
                "text": "Representation and computation of the spatial environment for indoor navigation"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The s-map represents the locations of obstacles in a planar domain so that it becomes a navigable map for the robot without further processing and converts many navigation problems in 3-D, such as map fusion and path planning, into 2-D ones."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038179"
                        ],
                        "name": "A. Waxman",
                        "slug": "A.-Waxman",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Waxman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waxman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733530"
                        ],
                        "name": "J. L. Moigne",
                        "slug": "J.-L.-Moigne",
                        "structuredName": {
                            "firstName": "Jacqueline",
                            "lastName": "Moigne",
                            "middleNames": [
                                "Le"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. L. Moigne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057595625"
                        ],
                        "name": "B. Srinivasan",
                        "slug": "B.-Srinivasan",
                        "structuredName": {
                            "firstName": "Balaji",
                            "lastName": "Srinivasan",
                            "middleNames": [
                                "Vasan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Srinivasan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3168280"
                        ],
                        "name": "T. Kushner",
                        "slug": "T.-Kushner",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Kushner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kushner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40601641"
                        ],
                        "name": "Eli Liang",
                        "slug": "Eli-Liang",
                        "structuredName": {
                            "firstName": "Eli",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eli Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2326292"
                        ],
                        "name": "T. Siddalingaiah",
                        "slug": "T.-Siddalingaiah",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Siddalingaiah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Siddalingaiah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[154], [155],Wallaceetal."
                    },
                    "intents": []
                }
            ],
            "corpusId": 32873799,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1f58a1455b6cbd98662548f1e6c7af7cb9daf02",
            "isKey": false,
            "numCitedBy": 243,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A modular system architecture has been developed to support visual navigation by an autonomous land vehicle. The system consists of vision modules performing image processing, three-dimensional shape recovery, and geometric reasoning, as well as modules for planning, navigating, and piloting. The system runs in two distinct modes, bootstrap and feedforward. The bootstrap mode requires analysis of entire images to find and model the objects of interest in the scene (e.g., roads). In the feedforward mode (while the vehicle is moving), attention is focused on small parts of the visual field as determined by prior views of the scene, to continue to track and model the objects of interest. General navigational tasks are decomposed into three categories, all of which contribute to planning a vehicle path. They are called long-, intermediate-, and short-range navigation, reflecting the scale to which they apply. The system has been implemented as a set of concurrent communicating modules and used to drive a camera (carried by a robot arm) over a scale model road network on a terrain board. A large subset of the system has been reimplemented on a VICOM image processor and has driven the DARPA Autonomous Land Vehicle (ALV) at Martin Marietta's test site in Denver, CO."
            },
            "slug": "A-visual-navigation-system-for-autonomous-land-Waxman-Moigne",
            "title": {
                "fragments": [],
                "text": "A visual navigation system for autonomous land vehicles"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "A modular system architecture has been developed to support visual navigation by an autonomous land vehicle that consists of vision modules performing image processing, three-dimensional shape recovery, and geometric reasoning, as well as modules for planning, navigating, and piloting."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE J. Robotics Autom."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143907295"
                        ],
                        "name": "R. Chatila",
                        "slug": "R.-Chatila",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Chatila",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chatila"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145187280"
                        ],
                        "name": "J. Laumond",
                        "slug": "J.-Laumond",
                        "structuredName": {
                            "firstName": "Jean-Paul",
                            "lastName": "Laumond",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Laumond"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These are systems that use no explicit representation at all about the space in which navigation is to take place, but rather resort to recognizing objects found in the environment or to tracking those objects by generating motions based on visual observations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 132
                            }
                        ],
                        "text": "We do realize that the latter approach is asking for trouble, especially in the light of the fact that the members of the research community hold widely differing opinions on what contributions were genuinely original and what merely derivative."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16263890,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c34a678e40a7d80cb3683f07fc837179fd9bf3ee",
            "isKey": false,
            "numCitedBy": 646,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "In order to understand its environment, a mobile robot should be able to model consistently this environment, and to locate itself correctly. One major difficulty to be solved is the inaccuracies introduced by the sensors. The approach proposed in this paper to cope with this problem relies on 1) defining general principles to deal with uncertainties : the use of a multisensory system, favo ring of the data collected by the more accurate sensor in a given situation, averaging of different but consistent measurements of the same entity weighted with their associated uncertainties, and 2) a methodology enabling a mobile robot to define its own reference landmarks while exploring its environment. These ideas are presented together with an example of their application on the mobile robot HILARE."
            },
            "slug": "Position-referencing-and-consistent-world-modeling-Chatila-Laumond",
            "title": {
                "fragments": [],
                "text": "Position referencing and consistent world modeling for mobile robots"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The approach proposed in this paper relies on the use of a multisensory system, favo ring of the data collected by the more accurate sensor in a given situation, averaging of different but consistent measurements of the same entity weighted with their associated uncertainties."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1985 IEEE International Conference on Robotics and Automation"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1384255355"
                        ],
                        "name": "Aleix M. Martinez",
                        "slug": "Aleix-M.-Martinez",
                        "structuredName": {
                            "firstName": "Aleix M.",
                            "lastName": "Martinez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aleix M. Martinez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793079"
                        ],
                        "name": "Jordi Vitri\u00e0",
                        "slug": "Jordi-Vitri\u00e0",
                        "structuredName": {
                            "firstName": "Jordi",
                            "lastName": "Vitri\u00e0",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jordi Vitri\u00e0"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Mart\u0131\u0301nez and Vitri\u00e0 [94] have also proposed an appearancebased approach using mixture models learned by the expectation maximization algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Mart\u0131\u0301nez and Vitria\u0300 [94] have also proposed an appearancebased approach using mixture models learned by the expectation maximization algorithm."
                    },
                    "intents": []
                }
            ],
            "corpusId": 515429,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f05051ed8cabb28e82390c5bbd8e5de425102fd",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "The most classical way of attempting to solve the vision-guided navigation problem for autonomous robots corresponds to the use of three-dimensional (3-D) geometrical descriptions of the scene; what is known as model-based approaches. However, these approaches do not facilitate the user's task because they require that geometrically precise models of the 3-D environment be given by the user. In this paper, we propose the use of \"annotations\" posted on some type of blackboard or \"descriptive\" map to facilitate this user-robot interaction. We show that, by using this technique, user commands can be as simple as \"go to label 5.\" To build such a mechanism, new approaches for vision-guided mobile robot navigation have to be found. We show that this can be achieved by using mixture models within an appearance-based paradigm. Mixture models are more useful in practice than other pattern recognition methods such as principal component analysis (PCA) or Fisher discriminant analysis (FDA)-also known as linear discriminant analysis (LDA), because they can represent nonlinear subspaces. However, given the fact that mixture models are usually learned using the expectation-maximization (EM) algorithm which is a gradient ascent technique, the system cannot always converge to a desired final solution, due to the local maxima problem. To resolve this, a genetic version of the EM algorithm is used. We then show the capabilities of this latest approach on a navigation task that uses the above described \"annotations.\""
            },
            "slug": "Clustering-in-image-space-for-place-recognition-and-Martinez-Vitri\u00e0",
            "title": {
                "fragments": [],
                "text": "Clustering in image space for place recognition and visual annotations for human-robot interaction"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern. Part B"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038264"
                        ],
                        "name": "F. Dellaert",
                        "slug": "F.-Dellaert",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Dellaert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Dellaert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145197953"
                        ],
                        "name": "D. Fox",
                        "slug": "D.-Fox",
                        "structuredName": {
                            "firstName": "Dieter",
                            "lastName": "Fox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725973"
                        ],
                        "name": "W. Burgard",
                        "slug": "W.-Burgard",
                        "structuredName": {
                            "firstName": "Wolfram",
                            "lastName": "Burgard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Burgard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "Such a representation is usually referred to as \u201doccupancy map\u201d and it was formally introduced in [109]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9696573,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e33f356098b577134eccce8a2cac984fac0baad2",
            "isKey": false,
            "numCitedBy": 1603,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "To navigate reliably in indoor environments, a mobile robot must know where it is. Thus, reliable position estimation is a key problem in mobile robotics. We believe that probabilistic approaches are among the most promising candidates to providing a comprehensive and real-time solution to the robot localization problem. However, current methods still face considerable hurdles. In particular the problems encountered are closely related to the type of representation used to represent probability densities over the robot's state space. Earlier work on Bayesian filtering with particle-based density representations opened up a new approach for mobile robot localization based on these principles. We introduce the Monte Carlo localization method, where we represent the probability density involved by maintaining a set of samples that are randomly drawn from it. By using a sampling-based representation we obtain a localization method that can represent arbitrary distributions. We show experimentally that the resulting method is able to efficiently localize a mobile robot without knowledge of its starting location. It is faster, more accurate and less memory-intensive than earlier grid-based methods,."
            },
            "slug": "Monte-Carlo-localization-for-mobile-robots-Dellaert-Fox",
            "title": {
                "fragments": [],
                "text": "Monte Carlo localization for mobile robots"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The Monte Carlo localization method is introduced, where the probability density is represented by maintaining a set of samples that are randomly drawn from it, and it is shown that the resulting method is able to efficiently localize a mobile robot without knowledge of its starting location."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1999 IEEE International Conference on Robotics and Automation (Cat. No.99CH36288C)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706062"
                        ],
                        "name": "R. Arkin",
                        "slug": "R.-Arkin",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Arkin",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Arkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38964885"
                        ],
                        "name": "D. MacKenzie",
                        "slug": "D.-MacKenzie",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "MacKenzie",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. MacKenzie"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6444999,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cae736abd5078f7acea377ccbceaa098ac0fa319",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A methodology for integrating multiple perceptual algorithms within a reactive robotic control system is presented. A model using finite state accepters is developed as a means for expressing perceptual processing over space and time in the context of a particular motor behavior. This model can be utilized for a wide range of perceptual sequencing problems. The feasibility of this method is demonstrated in two separate implementations. The first is in the context of mobile robot docking where the mobile robot uses four different vision and ultrasonic algorithms to position itself relative to a docking workstation over a long-range course. The second uses vision, IR beacon, and ultrasonic algorithms to park the robot next to a desired plastic pole randomly placed within an arena. >"
            },
            "slug": "Temporal-coordination-of-perceptual-algorithms-for-Arkin-MacKenzie",
            "title": {
                "fragments": [],
                "text": "Temporal coordination of perceptual algorithms for mobile robot navigation"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "A methodology for integrating multiple perceptual algorithms within a reactive robotic control system is presented and a model using finite state accepters is developed as a means for expressing perceptual processing over space and time in the context of a particular motor behavior."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Robotics Autom."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1980742"
                        ],
                        "name": "A. Dev",
                        "slug": "A.-Dev",
                        "structuredName": {
                            "firstName": "Anuj",
                            "lastName": "Dev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804676"
                        ],
                        "name": "B. Kr\u00f6se",
                        "slug": "B.-Kr\u00f6se",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Kr\u00f6se",
                            "middleNames": [
                                "J.",
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kr\u00f6se"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145069321"
                        ],
                        "name": "F. Groen",
                        "slug": "F.-Groen",
                        "structuredName": {
                            "firstName": "Frans",
                            "lastName": "Groen",
                            "middleNames": [
                                "C.",
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Groen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[33]"
                    },
                    "intents": []
                }
            ],
            "corpusId": 4175663,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "18e72194da757ba303eca05ed8954aadaed9b316",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The robot navigation task presented in this paper is to drive through the center of a corridor, based on a sequence of images from an on-board camera. Our measurements of the system state the distance to the wall and orientation of the wall, are derived from the optic flow. Whereas the structure of the environment is usually computed from the spatial derivatives of the optic flow, we used the structure contained in the temporal derivatives of the optic flow to compare the environment structure and hence the system state. The algorithm is used to control a 'remote brain' robot and results on the accuracy of the state estimates are presented."
            },
            "slug": "Navigation-of-a-mobile-robot-on-the-temporal-of-the-Dev-Kr\u00f6se",
            "title": {
                "fragments": [],
                "text": "Navigation of a mobile robot on the temporal development of the optic flow"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "The robot navigation task presented in this paper is to drive through the center of a corridor, based on a sequence of images from an on-board camera, using the structure contained in the temporal derivatives of the optic flow to compare the environment structure and hence the system state."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1453076333"
                        ],
                        "name": "Zhengyou Zhang",
                        "slug": "Zhengyou-Zhang",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Zhengyou Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhengyou Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33726225"
                        ],
                        "name": "O. Faugeras",
                        "slug": "O.-Faugeras",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Faugeras",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Faugeras"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 156
                            }
                        ],
                        "text": "Other notable contributions related to map building are by Ayache and Faugeras [7], [6] using trinocular vision and Kalman filtering, by Zhang and Faugeras [162] using 3D reconstruction from image sequences, by Giralt et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 137
                            }
                        ],
                        "text": "Other notable contributions related to map building are by Ayache and Faugeras [7], [6] using trinocular vision and Kalman filtering, by Zhang and Faugeras [162] using 3D reconstruction from image sequences, by Giralt et al. [44] using sensor fusion techniques, and by Zheng et al. [163], and Yagi et al. [159] using panoramic views."
                    },
                    "intents": []
                }
            ],
            "corpusId": 120375691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7635132491f0a536f01e2c8aff18275007a01c28",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes a system to incrementally build a world model with a mobile robot in an unknown environment. The model is, for the moment, segment based. A trinocular stereo system is used to build a local map about the environment. A global map is obtained by integrating a sequence of stereo frames taken when the robot navigates in the environment. The emphasis of this article is on the representation of the uncer tainty of 3D segments from stereo and on the integration of segments from multiple views. The proposed representation is simple and very convenient to characterize the uncertainty of segment. A Kalman filter is used to merge matched line seg ments. An important characteristic of our integration strategy is that a segment observed by the stereo system corresponds only to one part of the segment in space, so the union of the different observations gives a better estimate on the segment in space. We have succeeded in integrating 35 stereo frames taken in our robot room."
            },
            "slug": "A-3D-World-Model-Builder-with-a-Mobile-Robot-Zhang-Faugeras",
            "title": {
                "fragments": [],
                "text": "A 3D World Model Builder with a Mobile Robot"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The emphasis of this article is on the representation of the uncer tainty of 3D segments from stereo and on the integration of segments from multiple views and the proposed representation is simple and very convenient to characterize the uncertainty of segment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115302307"
                        ],
                        "name": "Stephen D. Jones",
                        "slug": "Stephen-D.-Jones",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Jones",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen D. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073059043"
                        ],
                        "name": "Claus Andresen",
                        "slug": "Claus-Andresen",
                        "structuredName": {
                            "firstName": "Claus",
                            "lastName": "Andresen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claus Andresen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145687022"
                        ],
                        "name": "J. Crowley",
                        "slug": "J.-Crowley",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Crowley",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Crowley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36077886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "186f66bd55b7788e6e4202ee252979451a26b290",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Describes the use of appearance based vision for defining visual processes for navigation. A visual processes which transform images to commands and events. A family of visual processes are defined by associating the appearance of a scene from a given viewpoint with the simple trajectories. Appearance is captured as a set of low-resolution images. Energy normalised cross correlation is used to maintain heading, to estimate confidence and to servo control a robot vehicle while following a path. Experimental results are presented which compare results with a single camera, a pair of parallel cameras and a pair of divergent cameras. The most accurate (and robust) navigation is found with a pair of cameras which are slightly divergent."
            },
            "slug": "Appearance-based-process-for-visual-navigation-Jones-Andresen",
            "title": {
                "fragments": [],
                "text": "Appearance based process for visual navigation"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Energy normalised cross correlation is used to maintain heading, to estimate confidence and to servo control a robot vehicle while following a path."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715071"
                        ],
                        "name": "Y. Yagi",
                        "slug": "Y.-Yagi",
                        "structuredName": {
                            "firstName": "Yasushi",
                            "lastName": "Yagi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yagi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072650"
                        ],
                        "name": "Y. Nishizawa",
                        "slug": "Y.-Nishizawa",
                        "structuredName": {
                            "firstName": "Yoshimitsu",
                            "lastName": "Nishizawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Nishizawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735941"
                        ],
                        "name": "M. Yachida",
                        "slug": "M.-Yachida",
                        "structuredName": {
                            "firstName": "Masahiko",
                            "lastName": "Yachida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yachida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9058230,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc2ee5e46b3f7d94dd1b2c24b97e5b589bbf0a79",
            "isKey": false,
            "numCitedBy": 190,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We designed a new omnidirectional image sensor COPIS (Conic Projection Image Sensor) to guide the navigation of a mobile robot. The feature of COPIS is passive sensing of the omnidirectional image of the environment, in real-time (at the frame rate of a TV camera), using a conic mirror. COPIS is a suitable sensor for visual navigation in a real world environment. We report here a method for navigating a robot by detecting the azimuth of each object in the omnidirectional image. The azimuth is matched with the given environmental map. The robot can precisely estimate its own location and motion (the velocity of the robot) because COPIS observes a 360/spl deg/ view around the robot, even when all edges are not extracted correctly from the omnidirectional image. The robot can avoid colliding against unknown obstacles and estimate locations by detecting azimuth changes, while moving about in the environment. Under the assumption of the known motion of the robot, an environmental map of an indoor scene is generated by monitoring azimuth change in the image. >"
            },
            "slug": "Map-based-navigation-for-a-mobile-robot-with-image-Yagi-Nishizawa",
            "title": {
                "fragments": [],
                "text": "Map-based navigation for a mobile robot with omnidirectional image sensor COPIS"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A method for navigating a robot by detecting the azimuth of each object in the omnidirectional image, in real-time (at the frame rate of a TV camera), using a conic mirror is reported here."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Robotics Autom."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34767446"
                        ],
                        "name": "J. Borenstein",
                        "slug": "J.-Borenstein",
                        "structuredName": {
                            "firstName": "Johann",
                            "lastName": "Borenstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Borenstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687718"
                        ],
                        "name": "Y. Koren",
                        "slug": "Y.-Koren",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Koren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Koren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3138372,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64fe8233f9b38d7624f6c9ccfbc301e0f9552149",
            "isKey": false,
            "numCitedBy": 611,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "A real-time obstacle avoidance approach for mobile robots has been developed and implemented. It permits the detection of unknown obstacles simultaneously with the steering of the mobile robot to avoid collisions and advance toward the target. The novelty of this approach, entitled the virtual force field method, lies in the integration of two known concepts: certainty grids for obstacle representation and potential fields for navigation. This combination is especially suitable for the accommodation of inaccurate sensor data as well as for sensor fusion and makes possible continuous motion of the robot with stopping in front of obstacles. This navigation algorithm also takes into account the dynamic behavior of a fast mobile robot and solves the local minimum trap problem. Experimental results from a mobile robot running at a maximum speed of 0.78 m/s demonstrate the power of the algorithm. >"
            },
            "slug": "Real-time-obstacle-avoidance-for-fact-mobile-robots-Borenstein-Koren",
            "title": {
                "fragments": [],
                "text": "Real-time obstacle avoidance for fact mobile robots"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A real-time obstacle avoidance approach for mobile robots that permits the detection of unknown obstacles simultaneously with the steering of the mobile robot to avoid collisions and advance toward the target."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737323"
                        ],
                        "name": "M. Daily",
                        "slug": "M.-Daily",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Daily",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Daily"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116996656"
                        ],
                        "name": "John G. Harris",
                        "slug": "John-G.-Harris",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Harris",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John G. Harris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155102"
                        ],
                        "name": "D. Keirsey",
                        "slug": "D.-Keirsey",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Keirsey",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Keirsey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39222152"
                        ],
                        "name": "K. Olin",
                        "slug": "K.-Olin",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Olin",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Olin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2941965"
                        ],
                        "name": "D. Payton",
                        "slug": "D.-Payton",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Payton",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Payton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070755555"
                        ],
                        "name": "K. Reiser",
                        "slug": "K.-Reiser",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Reiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Reiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1908267"
                        ],
                        "name": "J. Rosenblatt",
                        "slug": "J.-Rosenblatt",
                        "structuredName": {
                            "firstName": "Julio",
                            "lastName": "Rosenblatt",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rosenblatt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2430616"
                        ],
                        "name": "D. Y. Tseng",
                        "slug": "D.-Y.-Tseng",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Tseng",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Y. Tseng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068910654"
                        ],
                        "name": "V. Wong",
                        "slug": "V.-Wong",
                        "structuredName": {
                            "firstName": "V.",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Wong"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 407,
                                "start": 403
                            }
                        ],
                        "text": "When maps need to be built by a robot, they may be created in either a vehiclecentered coordinate frame vehicle [81] (and updated [82] as the vehicle moves) or with respect to some external reference, such as an external camera attached to a companion device [99] (as in Mars Pathfinder and Lander), or a virtual viewpoint with respect to which range data is projected to form a Cartesian Elevation Map [30] or a global positioning reference such as the sun [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5907790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2dd368a6d2e600a3a4a75c70b2f7699f5b638b1",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A description is given of the first cross-country map and sensor-based autonomous operation of a robotic vehicle. Experiments on the autonomous land vehicle (ALV) in natural terrain were performed. An overview of the software architecture is provided, and details of the perception and planning techniques are presented. Two key experiments in which the vehicle avoided known and unknown obstacles in its path are described.<<ETX>>"
            },
            "slug": "Autonomous-cross-country-navigation-with-the-ALV-Daily-Harris",
            "title": {
                "fragments": [],
                "text": "Autonomous cross-country navigation with the ALV"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A description is given of the first cross-country map and sensor-based autonomous operation of a robotic vehicle and two key experiments in which the vehicle avoided known and unknown obstacles in its path."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1988 IEEE International Conference on Robotics and Automation"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737088"
                        ],
                        "name": "E. Krotkov",
                        "slug": "E.-Krotkov",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Krotkov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Krotkov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These are systems that use no explicit representation at all about the space in which navigation is to take place, but rather resort to recognizing objects found in the environment or to tracking those objects by generating motions based on visual observations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "Later, the idea of occupancy maps was improved by incorporating \u201dVirtual Force Fields\u201d (VFF) [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 30986734,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "18aded1a7aab3e96e6fb5c06ce0e04e2429b5d1e",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Following and extending the approach of K. Sugihara (1988), the author assumes that a mobile robot is equipped with a single camera and a map marking the positions in its environment of landmarks. The robot moves on a flat surface, acquires one image, extracts vertical edges from it, and computes the directions to visible landmarks. The problem is to determine the robot's position and orientation (pose) by establishing the correspondence between landmark directions and points in the map. This approach capitalizes on the excellent angular resolution of standard CCD cameras, while avoiding the feature-correspondence and 3D reconstruction problems. The problem is formulated as a search in a tree of interpretations (pairings of landmark directions and landmark points), and an algorithm to search the tree efficiently to determine the solution poses(s) is developed, taking into account errors in the landmark directions extracted by image processing. Quantitative results from simulations and experiments with real imagery are presented.<<ETX>>"
            },
            "slug": "Mobile-robot-localization-using-a-single-image-Krotkov",
            "title": {
                "fragments": [],
                "text": "Mobile robot localization using a single image"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An algorithm to search a tree of interpretations efficiently to determine the solution poses(s) is developed, taking into account errors in the landmark directions extracted by image processing."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings, 1989 International Conference on Robotics and Automation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72419159"
                        ],
                        "name": "R. Brooks",
                        "slug": "R.-Brooks",
                        "structuredName": {
                            "firstName": "Rodney",
                            "lastName": "Brooks",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Brooks"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13372124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50c7e3840462a7d48a9828ed2e06756608305a9c",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Mobile robots sense their environment and receive error laden readings. They try to move a certain distance and direction, and do so only approximately. Rather than try to engineer these problems away it may be possible, and may be necessary, to develop map making and navigation algorithms which explicitly represent these uncertainties, but still provide robust performance. The key idea is to use a relational map, which is rubbery and stretchy, rather than try to place observations in a 2-d coordinate system."
            },
            "slug": "Visual-map-making-for-a-mobile-robot-Brooks",
            "title": {
                "fragments": [],
                "text": "Visual map making for a mobile robot"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The key idea is to use a relational map, which is rubbery and stretchy, rather than try to place observations in a 2-d coordinate system."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1985 IEEE International Conference on Robotics and Automation"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066894"
                        ],
                        "name": "Hans P. Moravec",
                        "slug": "Hans-P.-Moravec",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Moravec",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hans P. Moravec"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 112
                            }
                        ],
                        "text": "WRITING a survey paper on computer vision for mobilerobot navigation\u2014a subject that was in high prominence in the 1980s and the first half of the 1990s\u2014is daunting, if not actually hazardous to one\u2019s career."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 128525458,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "93b376bd451db8ed94a18c556da16f25a3e7961b",
            "isKey": false,
            "numCitedBy": 1053,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The Stanford AI Lab cart is a card-table sized mobile robot controlled remotely through a radio link, and equipped with a TV camera and transmitter. A computer has been programmed to drive the cart through cluttered indoor and outdoor spaces, gaining its knowledge of the world entirely from images broadcast by the onboard TV system. The cart uses several kinds of stereo to locate objects around it in 3D and to deduce its own motion. It plans an obstacle avoiding path to a desired destination on the basis of a model built with this information. The plan changes as the cart perceives new obstacles on its journey. The system is reliable for short runs, but slow. The cart moves one meter every ten to fifteen minutes, in lurches. After rolling a meter it stops, takes some pictures and thinks about them for a long time. Then it plans a new path, executes a little of it, and pauses again. The program has successfully driven the cart through several 20 meter indoor courses (each taking about five hours) complex enough to necessitate three or four avoiding swerves. A less successful outdoor run, in which the cart skirted two obstacles but collided with a third, was also done. Harsh lighting (very bright surfaces next to very dark shadows) giving poor pictures and movement of shadows during the cart's creeping progress were major reasons for the poorer outdoor performance. The action portions of these runs were filmed by computer controlled cameras. (Author)"
            },
            "slug": "Obstacle-avoidance-and-navigation-in-the-real-world-Moravec",
            "title": {
                "fragments": [],
                "text": "Obstacle avoidance and navigation in the real world by a seeing robot rover"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144021572"
                        ],
                        "name": "Dongsung Kim",
                        "slug": "Dongsung-Kim",
                        "structuredName": {
                            "firstName": "Dongsung",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dongsung Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144862593"
                        ],
                        "name": "R. Nevatia",
                        "slug": "R.-Nevatia",
                        "structuredName": {
                            "firstName": "Ramakant",
                            "lastName": "Nevatia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nevatia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "These are systems that depend on user-created geometric models or topological maps of the environment."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 34337911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "349f5682a73c74d14f7c900a60c943161d17c8b7",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a robot system that can navigate in indoor environments, such as office buildings and laboratories, without having a detailed map of its environment and which can accept symbolic commands such as \u201cgo through the door on the left of the first desk on your right\u201d (expressed in a formal language). Such a system can operate in different instances of similar environments and does not require the effort of constructing a detailed map of the environment. It is also not sensitive to changes in the environment such as those caused by moving furniture. It uses generic representations of the objects in the environment such as walls, desks and doors to recognize them for the purposes of landmark detection and avoids obstacles which may not be modeled explicitly."
            },
            "slug": "Symbolic-Navigation-with-a-Generic-Map-Kim-Nevatia",
            "title": {
                "fragments": [],
                "text": "Symbolic Navigation with a Generic Map"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A robot system that can navigate in indoor environments, such as office buildings and laboratories, without having a detailed map of its environment and which can accept symbolic commands such as \u201cgo through the door on the left of the first desk on your right\u201d is described."
            },
            "venue": {
                "fragments": [],
                "text": "Auton. Robots"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2221876"
                        ],
                        "name": "C. Andersen",
                        "slug": "C.-Andersen",
                        "structuredName": {
                            "firstName": "Claus",
                            "lastName": "Andersen",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Andersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115302307"
                        ],
                        "name": "Stephen D. Jones",
                        "slug": "Stephen-D.-Jones",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Jones",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen D. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145687022"
                        ],
                        "name": "J. Crowley",
                        "slug": "J.-Crowley",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Crowley",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Crowley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[64]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7000484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d39d20cf715409b4390e3d29de8aec9ee6223a77",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the use of appearance based vision for defining visual processes for navigation. A visual processes which transform images to commands and events. A family of visual processes are defined by associating the appearance of a scene from a given viewpoint with the simple trajectories. Appearance is captured as a set of low-resolution images. Energy normalised cross correlation is used to maintain heading, to estimated confidence and to servo control a robot vehicle while following a path. Experimental results are presented which compare results with a single camera, a pair of parallel cameras and a pair of divergent cameras. The most accurate (and robust) navigation is found with a pair of cameras which are slightly divergent."
            },
            "slug": "Appearance-based-processes-for-visual-navigation-Andersen-Jones",
            "title": {
                "fragments": [],
                "text": "Appearance based processes for visual navigation"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The use of appearance based vision for defining visual processes for navigation by associating the appearance of a scene from a given viewpoint with the simple trajectories is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778544"
                        ],
                        "name": "Y. Matsumoto",
                        "slug": "Y.-Matsumoto",
                        "structuredName": {
                            "firstName": "Yoshio",
                            "lastName": "Matsumoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Matsumoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749935"
                        ],
                        "name": "M. Inaba",
                        "slug": "M.-Inaba",
                        "structuredName": {
                            "firstName": "Masayuki",
                            "lastName": "Inaba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Inaba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738023"
                        ],
                        "name": "H. Inoue",
                        "slug": "H.-Inoue",
                        "structuredName": {
                            "firstName": "Hirochika",
                            "lastName": "Inoue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Inoue"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Reprinted with permission from [98]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[98]"
                    },
                    "intents": []
                }
            ],
            "corpusId": 26558912,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "4f665ad537f238f97ccc90ac87b496f6da5ecfb3",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work in vision-based mobile robotics have lacked models of the route which can be utilized for (1) localization, (2) steering angle determination, and (3) obstacle detection, simultaneously. In this paper, the authors propose a new visual representation of the route, the \"view-sequenced route representation (VSRR).\" The VSRR is a non-metrical model of the route, which contains a sequence of front view images along a route memorized in the recording run. In the autonomous run, the three types of recognition described above are achieved in real-time by matching between the current view image and the memorized view sequence using a correlation technique. the authors also developed an easy procedure for acquiring VSRRs, and a quick control procedure using VSRRs. VSRRs are especially useful for representing routes in corridors. Results of autonomous navigation using a two-wheeled robot in a real corridor are also presented."
            },
            "slug": "Visual-navigation-using-view-sequenced-route-Matsumoto-Inaba",
            "title": {
                "fragments": [],
                "text": "Visual navigation using view-sequenced route representation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new visual representation of the route, the \"view-sequenced route representation (VSRR),\" which contains a sequence of front view images along a route memorized in the recording run, and an easy procedure for acquiring and a quick control procedure using VSRRs."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Robotics and Automation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14978055,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "843a29e397864b4bd148356bb580b62ad1314283",
            "isKey": false,
            "numCitedBy": 581,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "An efficient probabilistic algorithm for the concurrent mapping and localization problem that arises in mobile robotics is presented. The algorithm addresses the problem in which a team of robots builds a map on-line while simultaneously accommodating errors in the robots\u2019 odometry. At the core of the algorithm is a technique that combines fast maximum likelihood map growing with a Monte Carlo localizer that uses particle representations. The combination of both yields an on-line algorithm that can cope with large odometric errors typically found when mapping environments with cycles. The algorithm can be implemented in a distributed manner on multiple robot platforms, enabling a team of robots to cooperatively generate a single map of their environment. Finally, an extension is described for acquiring three-dimensional maps, which capture the structure and visual appearance of indoor environments in three dimensions."
            },
            "slug": "A-Probabilistic-On-Line-Mapping-Algorithm-for-Teams-Thrun",
            "title": {
                "fragments": [],
                "text": "A Probabilistic On-Line Mapping Algorithm for Teams of Mobile Robots"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "An efficient probabilistic algorithm for the concurrent mapping and localization problem that arises in mobile robotics is presented, which addresses the problem in which a team of robots builds a map on-line while simultaneously accommodating errors in the robots\u2019 odometry."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Robotics Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107703086"
                        ],
                        "name": "J. Pan",
                        "slug": "J.-Pan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Pan",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781230"
                        ],
                        "name": "D. Pack",
                        "slug": "D.-Pack",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Pack",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pack"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14037223"
                        ],
                        "name": "A. Kosaka",
                        "slug": "A.-Kosaka",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Kosaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kosaka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These are systems that use no explicit representation at all about the space in which navigation is to take place, but rather resort to recognizing objects found in the environment or to tracking those objects by generating motions based on visual observations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Mobile robotics, navigation, computer vision, indoor navigation, outdoor navigation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15929820,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6690fa35314623e77086380cc13b9f275c810054",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We report here a new but robust vision based control architecture for indoor mobile robot navigation On one hand this architecture takes advantage of the high throughput of neural networks for the processing of camera images And on the other it employs fuzzy logic to deal with the uncertainty in the inferences drawn from the vision data In particular we will present in this paper the sixteen fuzzy terms that have proved su cient for the desired navigational behavior The reader should also note that the architecture we present in this paper allows our robot to simultaneously navigate and avoid obstacles both static and dynamic Introduction As the readers probably know our laboratory has previously produced two radically di erent control architectures for vision based navigation by indoor robots The FINALE system presented in allowed our robot to navigate at roughly meters minute using model based vision in which expectation maps constructed from a geometrical model of the hallways are compared with camera images to determine the location of the robot The FINALE system is heavily geometrical in the sense that it requires that a D model of the hallways be known Our second architecture the NEURO NAV system presented in was more human friendly in the sense that it used topological models of hallways As discussed in from the topological models of space the path planner in NEURO NAV outputs a sequence of navigational commands like straight to the second T junction turn right straight to the third door on the left In executing these commands the robot invokes an ensemble of neural networks for steering control For example the input to one of the neural networks the Hallway Follower consists of the Hough space of the camera image each output node of this network corresponds to a command to the robot to turn right or left by a certain number of degrees With the same hardware as for FINALE the NEURO NAV system is able to generate a steering command every three seconds Readers should refer to for other approaches The work reported in this paper represents an advance over NEURO NAV an advance that will allow our robot to ultimately go beyond just navigation and also engage in more complex tasks such as simultaneously nding an object while it is navigating The robot might for example look for a re extinguisher whose location is only approximately known in the hallway This enhanced level of intelligence is made possible by using fuzzy logic to couple the output of the various neural networks to a supervisory controller which decides what commands to issue to the actuators More speci cally while in the NEURO NAV system each steering command produced by one of the neural network was considered as a categorical command in our present work it is now treated as a command with a certain ambiguity associated with it a much more natural thing to do Similarly the NEURO NAV system treated each distance estimate produced by one of the neural networks categorically In our new system an ambiguity interval is associated with each such estimate Some references are helpful to understand how fuzzy inference takes place in real time applications In what follows in Section we will rst describe the overall architecture of our new system that we have named FUZZY NAV Section will then delve into the Fuzzy Supervisory Controller the heart of FUZZY NAV We will display some of the rules and talk about the linguistic variables and the associated fuzzy terms and their membership functions Finally in Section we will discuss the working of the entire system and show some results This work was supported by the O ce of Naval Research under Grant ONR N Architecture of FUZZY NAV"
            },
            "slug": "FUZZY-NAV-A-Vision-Based-Robot-Navigation-using-for-Pan-Pack",
            "title": {
                "fragments": [],
                "text": "FUZZY NAV A Vision Based Robot Navigation Architecture using Fuzzy Inference for Uncertainty Reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A new but robust vision based control architecture for indoor mobile robot navigation that takes advantage of the high throughput of neural networks for the processing of camera images and employs fuzzy logic to deal with the uncertainty in the inferences drawn from the vision data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3133035"
                        ],
                        "name": "J. F. Melero",
                        "slug": "J.-F.-Melero",
                        "structuredName": {
                            "firstName": "Joaqu\u00edn",
                            "lastName": "Melero",
                            "middleNames": [
                                "Ferruz"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. F. Melero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144405166"
                        ],
                        "name": "A. Ollero",
                        "slug": "A.-Ollero",
                        "structuredName": {
                            "firstName": "An\u00edbal",
                            "lastName": "Ollero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ollero"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42106079,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5b15289fb28e19db15502cad90cc2b4a55200bc",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a method for mobile robot position estimation based on feature tracking through the sequence of images provided by a conventional video camera. The block-based correspondence algorithms selects candidates through grey level matching and local coherence analysis; a DSP-based multiprocessing image processing system is used to achieve real time performance. Point-to-point correspondences are used to estimate the motion parameters of a mobile robot. Motion-based procedures to eliminate false matching have been implemented. The method has been applied to a mobile robot in non-structured and outdoor environments."
            },
            "slug": "Autonomous-mobile-robot-motion-control-in-based-on-Melero-Ollero",
            "title": {
                "fragments": [],
                "text": "Autonomous mobile robot motion control in non-structured environments based on real-time video processing"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "This paper describes a method for mobile robot position estimation based on feature tracking through the sequence of images provided by a conventional video camera using block-based correspondence algorithms and a DSP-based multiprocessing image processing system."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716933"
                        ],
                        "name": "I. Cox",
                        "slug": "I.-Cox",
                        "structuredName": {
                            "firstName": "Ingemar",
                            "lastName": "Cox",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Cox"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42137137,
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "id": "e06d63c59ce29f713453f79480e1c59dd85e5d25",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the position estimation system for an autonomous robot vehicle called Blanche, which is designed for use in structured office or factory environments. Blanche is intended to be low cost, depending on only two sensors, an optical rangefinder and odometry. Briefly, the position estimation system consists of odometry supplemented with a fast, robust matching algorithm which determines the congruence between the range data and a 2D map of its environment. This is used to correct any errors existing in the odometry estimate. The integration of odometry with fast, robust matching allows for accurate estimates of the robot\u2019s position and accurate estimates of the robot\u2019s position allow for fast, robust matching. That is, the system is self sustaining."
            },
            "slug": "Blanche:-Position-Estimation-For-An-Autonomous-Cox",
            "title": {
                "fragments": [],
                "text": "Blanche: Position Estimation For An Autonomous Robot Vehicle"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. IEEE/RSJ International Workshop on Intelligent Robots and Systems '. (IROS '89) 'The Autonomous Mobile Robots and Its Applications"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3305788"
                        ],
                        "name": "P. Gaussier",
                        "slug": "P.-Gaussier",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Gaussier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gaussier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2787925"
                        ],
                        "name": "C. Joulain",
                        "slug": "C.-Joulain",
                        "structuredName": {
                            "firstName": "C\u00e9dric",
                            "lastName": "Joulain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Joulain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2020609"
                        ],
                        "name": "S. Zrehen",
                        "slug": "S.-Zrehen",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Zrehen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Zrehen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4534898"
                        ],
                        "name": "J. Banquet",
                        "slug": "J.-Banquet",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Banquet",
                            "middleNames": [
                                "Paul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Banquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39482544"
                        ],
                        "name": "A. Revel",
                        "slug": "A.-Revel",
                        "structuredName": {
                            "firstName": "Arnaud",
                            "lastName": "Revel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Revel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[43] and Joulian et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Reprinted with permission from [43]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6407431,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8251b4563142163d11d2734319f2f30cb070768",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe how a mobile robot controlled only by visual information can retrieve a particular goal location in an open environment. Our model does not need a precise map nor to learn all the possible positions in the environment. The system is a neural architecture inspired from neurobiological studies using the recognition of visual patterns called landmarks. The robot merges this visual information and its azimuth to build a plastic representation of its location. This representation is used to learn the best movement to reach the goal. A simple and fast online learning of a few places located near the goal allows the robot to reach the goal from anywhere in its neighborhood. The system uses only an egocentric representation of the robot environment and presents very high generalization capabilities. We describe an efficient implementation tested on our robot in two real indoor environments. We show the limitations of the model and its possible extensions to create autonomous robots only guided by visual information."
            },
            "slug": "Visual-navigation-in-an-open-environment-without-Gaussier-Joulain",
            "title": {
                "fragments": [],
                "text": "Visual navigation in an open environment without map"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The model uses only an egocentric representation of the robot environment and presents very high generalization capabilities and the limitations of the model and its possible extensions to create autonomous robots only guided by visual information are shown."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34588530"
                        ],
                        "name": "C. Thorpe",
                        "slug": "C.-Thorpe",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Thorpe",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Thorpe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 87
                            }
                        ],
                        "text": "Some of the earliest systems to use color to distinguish shadows from obstacles are by Thorpe et al. [143] and Turk et al. [151]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 31
                            }
                        ],
                        "text": "Navlab, initially developed by Thorpe et al. [142], has now gone through 10 implementations [124], [142], [143], [61], [63] based on vehicles ranging from minivans to metro buses and equipped with computing hardware ranging from supercomputers (Navlab 1) to laptops (Navlab 5 and later)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 105
                            }
                        ],
                        "text": "Next, an interest operator (now known as Moravec\u2019s interest operator, which was later improved by Thorpe [140] for FIDO [141]) was applied to extract distinctive features in the images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 106428763,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0ad4efa83cb5a8eced1a9ee27e5c1fdf174cfe25",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "FIDO is a vision and navigation system for the CMU \"Rover\" mobile robot. The interest operator picks distinctive points to be tracked from image to image, both for stereo pairs of images and for images taken from a different Rover position. The performance of a simple interest operator is analy/.cd, and a theory formed to explain its defects. A new interest operator is built, implementing the suggested improvements. Tests run on the original and the new interest operators, as well as on several other operators, show almost identical performance. The reasons for the lack of improvement arc discussed."
            },
            "slug": "An-analysis-of-interest-operators-for-FIDO-Thorpe",
            "title": {
                "fragments": [],
                "text": "An analysis of interest operators for FIDO"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2817684"
                        ],
                        "name": "U. Regensburger",
                        "slug": "U.-Regensburger",
                        "structuredName": {
                            "firstName": "Uwe",
                            "lastName": "Regensburger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Regensburger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3005649"
                        ],
                        "name": "V. Graefe",
                        "slug": "V.-Graefe",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Graefe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Graefe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Mobile robotics, navigation, computer vision, indoor navigation, outdoor navigation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7384108,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "074f81a7ff44b63b431f087b480c2d080fe694b4",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "As part of a driver support system for motor vehicles on freeways, an obstacle recognition system was developed within the EUREKA project PROMETHEUS over the last four years. The obstacle recognition system uses a single monochrome TV camera and a multi-processor robot vision system. The recognition system includes a road tracker and an obstacle detector that were described elsewhere, and an object classification and tracking module presented here. The module is based on generic 2D object models. It recognizes obstacles in real time and from distances of 200 to 300 m, sufficient for high-speed driving. The module was extensively tested in real-world scenes on the German Autobahn and on various other roads, including city streets with dense traffic.<<ETX>>"
            },
            "slug": "Visual-recognition-of-obstacles-on-roads-Regensburger-Graefe",
            "title": {
                "fragments": [],
                "text": "Visual recognition of obstacles on roads"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The obstacle recognition system uses a single monochrome TV camera and a multi-processor robot vision system to recognize obstacles in real time and from distances of 200 to 300 m, sufficient for high-speed driving."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS'94)"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16339756"
                        ],
                        "name": "M. Magee",
                        "slug": "M.-Magee",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Magee",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Magee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705627"
                        ],
                        "name": "J. Aggarwal",
                        "slug": "J.-Aggarwal",
                        "structuredName": {
                            "firstName": "Jake",
                            "lastName": "Aggarwal",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Aggarwal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 206530525,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "8d3f642aaae9f6e06b70f1399067e94db4cb8c02",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A procedure for determining the position uniquely, of a mobile robot in a three-dimensional space is presented. The method consists of viewing a single sphere with horizontal and vertical calibration great circles and computing distance and elevation and azimuth angles with respect to the sphere. The method is simple and provides good results as long as the sphere is projected onto a large portion of the image plane. Results using simulated and actual data are presented."
            },
            "slug": "Determining-the-position-of-a-robot-using-a-single-Magee-Aggarwal",
            "title": {
                "fragments": [],
                "text": "Determining the position of a robot using a single calibration object"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A procedure for determining the position uniquely, of a mobile robot in a three-dimensional space is presented and provides good results as long as the sphere is projected onto a large portion of the image plane."
            },
            "venue": {
                "fragments": [],
                "text": "ICRA"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719955"
                        ],
                        "name": "R. Simmons",
                        "slug": "R.-Simmons",
                        "structuredName": {
                            "firstName": "Reid",
                            "lastName": "Simmons",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Simmons"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737088"
                        ],
                        "name": "E. Krotkov",
                        "slug": "E.-Krotkov",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Krotkov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Krotkov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2624386"
                        ],
                        "name": "L. Chrisman",
                        "slug": "L.-Chrisman",
                        "structuredName": {
                            "firstName": "Lonnie",
                            "lastName": "Chrisman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Chrisman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7668712"
                        ],
                        "name": "Fabio Gagliardi Cozman",
                        "slug": "Fabio-Gagliardi-Cozman",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Cozman",
                            "middleNames": [
                                "Gagliardi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabio Gagliardi Cozman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145482925"
                        ],
                        "name": "R. Goodwin",
                        "slug": "R.-Goodwin",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Goodwin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Goodwin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52105557"
                        ],
                        "name": "Lalitesh K. Katragadda",
                        "slug": "Lalitesh-K.-Katragadda",
                        "structuredName": {
                            "firstName": "Lalitesh",
                            "lastName": "Katragadda",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lalitesh K. Katragadda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145314818"
                        ],
                        "name": "Sven Koenig",
                        "slug": "Sven-Koenig",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Koenig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sven Koenig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103368091"
                        ],
                        "name": "G. Krishnaswamy",
                        "slug": "G.-Krishnaswamy",
                        "structuredName": {
                            "firstName": "Gita",
                            "lastName": "Krishnaswamy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Krishnaswamy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71517672"
                        ],
                        "name": "Y. Shinoda",
                        "slug": "Y.-Shinoda",
                        "structuredName": {
                            "firstName": "Yoshikazu",
                            "lastName": "Shinoda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Shinoda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144136473"
                        ],
                        "name": "W. Whittaker",
                        "slug": "W.-Whittaker",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Whittaker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Whittaker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577453"
                        ],
                        "name": "P. Klarer",
                        "slug": "P.-Klarer",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Klarer",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Klarer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 153
                            }
                        ],
                        "text": "Sometimes in these sorts of applications, the robot is supposed to just wander around, exploring the vicinity of the robot without a clearcut goal [91], [136]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 52
                            }
                        ],
                        "text": "Also using the RATLER vehicle, the work reported in [136] implements a system that uses stereo vision to build a map that represents the terrain up to seven meters in front of the rover."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 690171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da541002ce1d337ad578ba3e692b7537a9721d8d",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Reliable navigation is critical for a lunar rover, both for autonomous traverses and safeguarded remote teleoperation. This paper describes an implemented system that has autonomously driven a prototype wheeled lunar rover over a kilometer in natural, outdoor terrain. The navigation system uses stereo terrain maps to perform local obstacle avoidance, and arbitrates steering recommendations from both the user and the rover. The paper describes the system architecture, each of the major components, and the experimental results to date."
            },
            "slug": "Experience-with-rover-navigation-for-lunar-like-Simmons-Krotkov",
            "title": {
                "fragments": [],
                "text": "Experience with rover navigation for lunar-like terrains"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This paper describes an implemented system that has autonomously driven a prototype wheeled lunar rover over a kilometer in natural, outdoor terrain and the system architecture, each of the major components, and the experimental results to date."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human Robot Interaction and Cooperative Robots"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691940"
                        ],
                        "name": "J. Miura",
                        "slug": "J.-Miura",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Miura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Miura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700379"
                        ],
                        "name": "Y. Shirai",
                        "slug": "Y.-Shirai",
                        "structuredName": {
                            "firstName": "Yoshiaki",
                            "lastName": "Shirai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Shirai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17850832,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e1e33d79c81a73f07580fc4950cb0d31c3e2648",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an uncertainty model of stereo vision and its application to a visionmotion planning for a mobile robot. In general, recognition of an environment requires much computation and the recognition result includes uncertainty. In planning, therefore, a trade-o must be considered between the cost of visual recognition and the e ect of information obtained by recognition. Such a trade-o must be formulated on the basis of a model of vision which describes the required time for visual processing and uncertainty of information to be obtained. In this paper, an uncertainty model of stereo vision is described, in which not only the quantization error but also false matchings of features are considered. A strategy for resolving ambiguous matchings is also proposed. Using the uncertainty model in the planner, an optimal plan for a real world problem is generated. An e cient solving strategy is also described which employs a pruning method based on the lower bound of the total cost calculated by the assumption of perfect sensor information."
            },
            "slug": "An-Uncertainty-Model-of-Stereo-Vision-and-its-to-of-Miura-Shirai",
            "title": {
                "fragments": [],
                "text": "An Uncertainty Model of Stereo Vision and its Application to Vision-Motion Planning of Robot"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An uncertainty model of stereo vision is described, in which not only the quantization error but also false matchings of features are considered and an optimal plan for a real world problem is generated."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750045"
                        ],
                        "name": "M. Kabuka",
                        "slug": "M.-Kabuka",
                        "structuredName": {
                            "firstName": "Mansur",
                            "lastName": "Kabuka",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kabuka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35809384"
                        ],
                        "name": "\u00c1. Arenas",
                        "slug": "\u00c1.-Arenas",
                        "structuredName": {
                            "firstName": "\u00c1lvaro",
                            "lastName": "Arenas",
                            "middleNames": [
                                "Enrique"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c1. Arenas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "When cameras are not mounted sideways, as in [66], properties of the shapes used for the artificial landmarks allow simple algorithms to be used for the localization of the robot."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "The artificial landmarks vary from circles with a unique bar-code for each landmark [66] (same figure) to reflecting tapes stretched along the robot path, as reported by Tsumura in [149] (Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18484984,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56390b4bba2f1093520aaed96a1050f0df9579fc",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "As mobile robots are taking on more and more of the tasks that were normally delegated to humans, they need to acquire higher degrees of autonomous operation, which calls for accurate and efficient position determination and/or verification. The critical geometric dimensions of a standard pattern are used here to locate the relative position of the mobile robot with respect to the pattern; by doing so, the method does not depend on values of any intrinsic camera parameters, except the focal length. In addition, this method has the advantages of simplicity and flexibility. This standard pattern is also provided with a unique identification code, using bar codes, that enables the system to find the absolute location of the pattern. These bar codes also assist in the scanning algorithms to locate the pattern in the environment. A thorough error analysis and experimental results obtained through software simulation are presented, as well as the current direction of our work."
            },
            "slug": "Position-verification-of-a-mobile-robot-using-Kabuka-Arenas",
            "title": {
                "fragments": [],
                "text": "Position verification of a mobile robot using standard pattern"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The critical geometric dimensions of a standard pattern are used here to locate the relative position of the mobile robot with respect to the pattern; by doing so, the method does not depend on values of any intrinsic camera parameters, except the focal length."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Journal on Robotics and Automation"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3005649"
                        ],
                        "name": "V. Graefe",
                        "slug": "V.-Graefe",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Graefe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Graefe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Mobile robotics, navigation, computer vision, indoor navigation, outdoor navigation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 110514178,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23589b67ace871c77bfb1b07ff68ee0fb396d01e",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "As part of an effort to demonstrate a fully autonomous road vehicle able to participate in normal freeway traffic, a system of visual object recognition modules was developed within the EUREKA project PROMETHEUS over the last four years. Together, the modules constitute a basis for an intelligent road vehicle, able to recognize traffic situations in real time and to react accordingly. They were implemented on a specially developed robot vision system and extensively tested in real-world scenes on the German Autobahn and on other roads."
            },
            "slug": "Vision-For-Intelligent-Road-Vehicles-Graefe",
            "title": {
                "fragments": [],
                "text": "Vision For Intelligent Road Vehicles"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A system of visual object recognition modules developed within the EUREKA project PROMETHEUS constitute a basis for an intelligent road vehicle, able to recognize traffic situations in real time and to react accordingly."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Intelligent Vehicles '93 Symposium"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7668712"
                        ],
                        "name": "Fabio Gagliardi Cozman",
                        "slug": "Fabio-Gagliardi-Cozman",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Cozman",
                            "middleNames": [
                                "Gagliardi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabio Gagliardi Cozman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737088"
                        ],
                        "name": "E. Krotkov",
                        "slug": "E.-Krotkov",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Krotkov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Krotkov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "For localization by global positioning, presented in [29] is an approach for measuring the position of the sun using a digital inclinometer, a camera with neutral density filters, and an adjustable platform."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 205
                            }
                        ],
                        "text": "The techniques that have been developed for localization in unstructured environments include: external camera observation [99], far-point (mountain peaks) landmark triangulation [139], global positioning [29], etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 462,
                                "start": 458
                            }
                        ],
                        "text": "When maps need to be built by a robot, they may be created in either a vehiclecentered coordinate frame vehicle [81] (and updated [82] as the vehicle moves) or with respect to some external reference, such as an external camera attached to a companion device [99] (as in Mars Pathfinder and Lander), or a virtual viewpoint with respect to which range data is projected to form a Cartesian Elevation Map [30] or a global positioning reference such as the sun [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11252538,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "d4fc0c78e30c400462fb8d2e41cdba7b892476b8",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper explores the possibility of using Sun altitude for localization of a robot in totally unknown territory. A set of Sun altitudes is obtained by processing a sequence of time-indexed images of the sky. Each altitude constrains the viewer to a circle on the surface of a celestial body, called the circle of equal altitude. A set of circles of equal altitude can be intersected to yield viewer position. We use this principle to obtain the position on Earth. Since altitude measurements are corrupted by noise, a least-square estimate is numerically calculated from the sequence of altitudes. The paper discusses the necessary theory for Sun-based localization, the technical issues of camera calibration and image processing, and presents preliminary results with real data."
            },
            "slug": "Robot-localization-using-a-computer-vision-sextant-Cozman-Krotkov",
            "title": {
                "fragments": [],
                "text": "Robot localization using a computer vision sextant"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The necessary theory for Sun-based localization, the technical issues of camera calibration and image processing, and preliminary results with real data are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1995 IEEE International Conference on Robotics and Automation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34767446"
                        ],
                        "name": "J. Borenstein",
                        "slug": "J.-Borenstein",
                        "structuredName": {
                            "firstName": "Johann",
                            "lastName": "Borenstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Borenstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687718"
                        ],
                        "name": "Y. Koren",
                        "slug": "Y.-Koren",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Koren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Koren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These are systems that use no explicit representation at all about the space in which navigation is to take place, but rather resort to recognizing objects found in the environment or to tracking those objects by generating motions based on visual observations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 757244,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "529cf7a716e6c9da99c6a468730f22398f75c1a4",
            "isKey": false,
            "numCitedBy": 2389,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "A real-time obstacle avoidance method for mobile robots which has been developed and implemented is described. This method, named the vector field histogram (VFH), permits the detection of unknown obstacles and avoids collisions while simultaneously steering the mobile robot toward the target. The VFH method uses a two-dimensional Cartesian histogram grid as a world model. This world model is updated continuously with range data sampled by onboard range sensors. The VFH method subsequently uses a two-stage data-reduction process to compute the desired control commands for the vehicle. Experimental results from a mobile robot traversing densely cluttered obstacle courses in smooth and continuous motion and at an average speed of 0.6-0.7 m/s are shown. A comparison of the VFN method to earlier methods is given. >"
            },
            "slug": "The-vector-field-histogram-fast-obstacle-avoidance-Borenstein-Koren",
            "title": {
                "fragments": [],
                "text": "The vector field histogram-fast obstacle avoidance for mobile robots"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A real-time obstacle avoidance method for mobile robots which has been developed and implemented, named the vector field histogram (VFH), permits the detection of unknown obstacles and avoids collisions while simultaneously steering the mobile robot toward the target."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Robotics Autom."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144637856"
                        ],
                        "name": "Y. Hwang",
                        "slug": "Y.-Hwang",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Hwang",
                            "middleNames": [
                                "Koo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Hwang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237406"
                        ],
                        "name": "N. Ahuja",
                        "slug": "N.-Ahuja",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Ahuja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ahuja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3500714,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "e44da0276d3703c05eba934cf0611750b0b45067",
            "isKey": false,
            "numCitedBy": 946,
            "numCiting": 213,
            "paperAbstract": {
                "fragments": [],
                "text": "Motion planning is one of the most important areas of robotics research. The complexity of the motion-planning problem has hindered the development of practical algorithms. This paper surveys the work on gross-motion planning, including motion planners for point robots, rigid robots, and manipulators in stationary, time-varying, constrained, and movable-object environments. The general issues in motion planning are explained. Recent approaches and their performances are briefly described, and possible future research directions are discussed."
            },
            "slug": "Gross-motion-planning\u2014a-survey-Hwang-Ahuja",
            "title": {
                "fragments": [],
                "text": "Gross motion planning\u2014a survey"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper surveys the work on gross-motion planning, including motion planners for point robots, rigid robots, and manipulators in stationary, time-varying, constrained, and movable-object environments."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2491801"
                        ],
                        "name": "D. J. Braunegg",
                        "slug": "D.-J.-Braunegg",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Braunegg",
                            "middleNames": [
                                "Jerome"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. J. Braunegg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32017678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0957ba29ab2471106b1e174213760d6f5f225568",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "To use a world model, a mobile robot must be able to determine its own position in the world. MARVEL, a system that builds and maintains its own models of world locations and uses these models to recognize its world position from stereo vision input, supports truly autonomous navigation. MARVEL is designed to be robust with respect to input errors and to respond to a gradually changing world by updating its world location models. In over 1000 recognition tests using real-world data, MARVEL yielded a false negative rate under 10% with zero false positives.<<ETX>>"
            },
            "slug": "MARVEL:-a-system-that-recognizes-world-locations-Braunegg",
            "title": {
                "fragments": [],
                "text": "MARVEL: a system that recognizes world locations with stereo vision"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "MARVEL, a system that builds and maintains its own models of world locations and uses these models to recognize its world position from stereo vision input, supports truly autonomous navigation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 27
                            }
                        ],
                        "text": ") In a recent contribution [144], Thrun has proposed an integrated approach that seeks to combine the best of the occupancy-grid-based and the topology-based approaches."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15619777,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6313ea766a72c3b9c69f65a3245f583178712ff2",
            "isKey": false,
            "numCitedBy": 1117,
            "numCiting": 122,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Metric-Topological-Maps-for-Indoor-Mobile-Thrun",
            "title": {
                "fragments": [],
                "text": "Learning Metric-Topological Maps for Indoor Mobile Robot Navigation"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695800"
                        ],
                        "name": "P. Zingaretti",
                        "slug": "P.-Zingaretti",
                        "structuredName": {
                            "firstName": "Primo",
                            "lastName": "Zingaretti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Zingaretti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680758"
                        ],
                        "name": "A. Carbonaro",
                        "slug": "A.-Carbonaro",
                        "structuredName": {
                            "firstName": "Antonella",
                            "lastName": "Carbonaro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Carbonaro"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10917227,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3bc5aacc1c281894adbc75cf259844bfbf08e0c2",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Route-following-based-on-adaptive-visual-landmark-Zingaretti-Carbonaro",
            "title": {
                "fragments": [],
                "text": "Route following based on adaptive visual landmark matching"
            },
            "venue": {
                "fragments": [],
                "text": "Robotics Auton. Syst."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34767446"
                        ],
                        "name": "J. Borenstein",
                        "slug": "J.-Borenstein",
                        "structuredName": {
                            "firstName": "Johann",
                            "lastName": "Borenstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Borenstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687718"
                        ],
                        "name": "Y. Koren",
                        "slug": "Y.-Koren",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Koren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Koren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These are systems that use no explicit representation at all about the space in which navigation is to take place, but rather resort to recognizing objects found in the environment or to tracking those objects by generating motions based on visual observations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9000470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e96329c3a3b23cc34b1801f46f3dd1b4e9dcb778",
            "isKey": false,
            "numCitedBy": 631,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The method described, named the vector field histogram (VFH), permits the detection of unknown obstacles and avoids collisions while simultaneously steering the mobile robot toward the target. A VFH-controlled mobile robot maneuvers quickly and without stopping among densely cluttered obstacles. The VFH method uses a two-dimensional Cartesian histogram grid as a world model. This world model is updated continuously and in real time with range data sampled by the onboard ultrasonic range sensors. Based on the accumulated environmental data, the VFH method then computes a one-dimensional polar histogram that is constructed around the robot's momentary location. Each sector in the polar histogram holds the polar obstacle density in that direction. Finally, the algorithm selects the most suitable sector from among all polar histogram sectors with low obstacle density, and the steering of the robot is aligned with that direction. Experimental results from a mobile robot traversing a densely cluttered obstacle course at an average speed of 0.7 m/s demonstrate the power of the VFH method.<<ETX>>"
            },
            "slug": "Real-time-obstacle-avoidance-for-fast-mobile-robots-Borenstein-Koren",
            "title": {
                "fragments": [],
                "text": "Real-time obstacle avoidance for fast mobile robots in cluttered environments"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The method described, named the vector field histogram (VFH), permits the detection of unknown obstacles and avoids collisions while simultaneously steering the mobile robot toward the target."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings., IEEE International Conference on Robotics and Automation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097660"
                        ],
                        "name": "M. Turk",
                        "slug": "M.-Turk",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Turk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Turk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2353298"
                        ],
                        "name": "D. Morgenthaler",
                        "slug": "D.-Morgenthaler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Morgenthaler",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Morgenthaler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3117383"
                        ],
                        "name": "K. Gremban",
                        "slug": "K.-Gremban",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Gremban",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Gremban"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144615423"
                        ],
                        "name": "M. Marra",
                        "slug": "M.-Marra",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Marra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marra"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 99
                            }
                        ],
                        "text": "However, instead of storing models of \u201csunny road\u201d and \u201cshaded road,\u201d as in Navlab and VITS [143], [151] (which are useful only when the robot is in completely sunny or completely shady 262 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 23
                            }
                        ],
                        "text": "In a manner similar to [151], Mori uses what is referred to as the color impression factor\u2014Red minus Blue\u2014to compensate for hue shifts due to variations in sunlight conditions, weather, season, view position, etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33216879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "166f43546199def0c4a743d6e426e7cafd36594e",
            "isKey": false,
            "numCitedBy": 427,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "A description is given of VITS (for vision task sequencer), the vision system for the autonomous land vehicle (ALV) Alvin, addressing in particular the task of road-following. The ALV vision system builds symbolic descriptions of road and obstacle boundaries using both video and range sensors. The authors discuss various road segmentation methods for video-based road-following, along with approaches to boundary extraction and transformation of boundaries in the image plane into a vehicle-centered three-dimensional scene model. >"
            },
            "slug": "VITS-A-Vision-System-for-Autonomous-Land-Vehicle-Turk-Morgenthaler",
            "title": {
                "fragments": [],
                "text": "VITS-A Vision System for Autonomous Land Vehicle Navigation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The authors discuss various road segmentation methods for video-based road-following, along with approaches to boundary extraction and transformation of boundaries in the image plane into a vehicle-centered three-dimensional scene model."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689716"
                        ],
                        "name": "D. Kortenkamp",
                        "slug": "D.-Kortenkamp",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kortenkamp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kortenkamp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3164501"
                        ],
                        "name": "T. Weymouth",
                        "slug": "T.-Weymouth",
                        "structuredName": {
                            "firstName": "Terry",
                            "lastName": "Weymouth",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Weymouth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12536506,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4342fbfe5a0126ea0a1944ea076e8eac1bf1edc8",
            "isKey": false,
            "numCitedBy": 335,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Topological maps represent the world as a network of nodes and arcs: the nodes are distinctive places in the environment and the arcs represent paths between places. A significant issue in building topological maps is defining distinctive places. Most previous work in topological mapping has concentrated on using sonar sensors to define distinctive places. However, sonar sensors are limited in range and angular resolution, which can make it difficult to distinguish between different distinctive places. Our approach combines a sonar-based definition of distinctive places with visual information. We use the robot's sonar sensors to determine where to capture images and use cues extracted from those images to help perform place recognition. Information from these two sensing modalities is combined using a simple Bayesian network. Results described in this paper show that our robot is able to perform place recognition without having to move through a sequence of places, as is the case with most currently implemented systems."
            },
            "slug": "Topological-Mapping-for-Mobile-Robots-Using-a-of-Kortenkamp-Weymouth",
            "title": {
                "fragments": [],
                "text": "Topological Mapping for Mobile Robots Using a Combination of Sonar and Vision Sensing"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper uses the robot's sonar sensors to determine where to capture images and use cues extracted from those images to help perform place recognition, and combines a sonar-based definition of distinctive places with visual information using a simple Bayesian network."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057113665"
                        ],
                        "name": "Rolf Schuster",
                        "slug": "Rolf-Schuster",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rolf Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145464035"
                        ],
                        "name": "N. Ansari",
                        "slug": "N.-Ansari",
                        "structuredName": {
                            "firstName": "Nirwan",
                            "lastName": "Ansari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ansari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1397619900"
                        ],
                        "name": "A. Bani-Hashemi",
                        "slug": "A.-Bani-Hashemi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Bani-Hashemi",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bani-Hashemi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10673822,
            "fieldsOfStudy": [
                "Materials Science"
            ],
            "id": "cf1105abf29d57ac777930e5a01235c105ff530d",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper analyzes the use of vanishing points for steering a robot. Parallel lines in the environment of the robot are used to compute vanishing points which serve as a reference for guiding the robot. To accomplish the steering task, three subtasks are performed: detection of straight lines, computation of vanishing points, and robot steering using vanishing points. Straight lines are detected by employing a high precision edge detector and a line-fitting algorithm. The cross product method introduced by Magee and Aggarwal (1984) is modified to make the detection of vanishing points appropriate for an indoor environment. Properties of vanishing points under camera rotation and translation are derived. Using these properties, the location of the vanishing points can serve as a reference for steering the robot. A model of the robot environment is defined, summarizing the minimum number of constraints necessary for the method to work. Finally, the limitations as well as the advantages of using vanishing points in robot navigation are discussed. >"
            },
            "slug": "Steering-a-robot-with-vanishing-points-Schuster-Ansari",
            "title": {
                "fragments": [],
                "text": "Steering a robot with vanishing points"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The paper analyzes the use of vanishing points for steering a robot by modifying the cross product method introduced by Magee and Aggarwal (1984) to make the detection of vanishing Points appropriate for an indoor environment."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Robotics Autom."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144367692"
                        ],
                        "name": "E. Huber",
                        "slug": "E.-Huber",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Huber",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Huber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689716"
                        ],
                        "name": "D. Kortenkamp",
                        "slug": "D.-Kortenkamp",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kortenkamp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kortenkamp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 128
                            }
                        ],
                        "text": "The reader is also referred to attempts at behavior-based approaches to vision-based navigation in mapless spaces [111], [112], [56]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195867782,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3dfc2d0c3563806e2c69b62091274c5f9d1cffa5",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "CAIR-2 is an intelligent mobile robot developed for guidance and delivery. One of the major features of CAIR-2 is that it provides the active vision capability based on real-time visual feedback using on-board low-cost processors. Instead of using the expensive special image processors, we exploit conventional processors such as Motolora 68040 and reduce processing time using the smart system and vision software. Vision system of CAIR-2 consists of real-time kernel, image saver, database, and two vision modules. First Stage Vision Module (FSVM) segments 256x256 gray-scale images into a set of regions using the partition-mode-test algorithm. Targets are then recognized by their shapes. Once targets are initially located by FSVM, Second Stage Vision Module (SSVM) can easily find and keep tracking targets using the focus-of-attention strategy based on Kalman filter. In case false targets are detected, the verification module can rapidly correct them. Real-time kernel, image saver, database are being exploited to manage and process the large image data efficiently and thus reduce the overall processing time. Combining the above four mechanisms effectively, while robot moves around both indoors and outdoors, vision system of CAIR-2 can recognize and track multiple moving targets simultaneously every one thirtieth of a second in average."
            },
            "slug": "Using-Stereo-Vision-to-Pursue-Moving-Agents-with-a-Huber-Kortenkamp",
            "title": {
                "fragments": [],
                "text": "Using Stereo Vision to Pursue Moving Agents with a Mobile Robot"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Real-time kernel, image saver, database, and two vision modules are being exploited to manage and process the large image data efficiently and thus reduce the overall processing time."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1995 IEEE International Conference on Robotics and Automation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48855558"
                        ],
                        "name": "D. Pomerleau",
                        "slug": "D.-Pomerleau",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Pomerleau",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pomerleau"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Mobile robotics, navigation, computer vision, indoor navigation, outdoor navigation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14721010,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb609137212908e8948c5af1a57a4de7827e1c90",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Autonomous navigation is a difficult problem for traditional vision and robotic techniques, primarily because of the noise and variability associated with real world scenes. Autonomous navigation systems based on traditional image processing and pattern recognition techniques often perform well under certain conditions, but have problems with others. Part of the difficulty stems from the fact that the processing performed by these systems remains fixed across various environments."
            },
            "slug": "Neural-Network-Vision-for-Robot-Driving-Pomerleau",
            "title": {
                "fragments": [],
                "text": "Neural Network Vision for Robot Driving"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This research presents a new generation of autonomous navigation systems based on traditional image processing and pattern recognition techniques that perform well under certain conditions, but have problems with others."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706062"
                        ],
                        "name": "R. Arkin",
                        "slug": "R.-Arkin",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Arkin",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Arkin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18442739,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6e49f4844fe1e56cca09bf9ebb7051aca78fc40",
            "isKey": false,
            "numCitedBy": 457,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Motor schemas are proposed as a basic unit of behavior specification for the navigation of a mobile robot. These are multiple concurrent processes which operate in conjunction with associated perceptual schemas and contribute independently to the overall concerted action of the vehicle. The motivation behind the use of schemas for this domain is drawn from neuroscientific, psychological and robotic sources. A variant of the potential field method is used to produce the appropriate velocity and steering commands for the robot. An implementation strategy based on available tools at UMASS is described. Simulation results show the feasibility of this approach."
            },
            "slug": "Motor-schema-based-navigation-for-a-mobile-robot:-Arkin",
            "title": {
                "fragments": [],
                "text": "Motor schema based navigation for a mobile robot: An approach to programming by behavior"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "Motor schemas are proposed as a basic unit of behavior specification for the navigation of a mobile robot and a variant of the potential field method is used to produce the appropriate velocity and steering commands for the robot."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1987 IEEE International Conference on Robotics and Automation"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054282875"
                        ],
                        "name": "M. Meng",
                        "slug": "M.-Meng",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703247"
                        ],
                        "name": "A. Kak",
                        "slug": "A.-Kak",
                        "structuredName": {
                            "firstName": "Avinash",
                            "lastName": "Kak",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kak"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This can be done along the lines of NEURO-NAV [102], [103], and FUZZY-NAV [121]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In the work reported in [103], NEURO-NAV was"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "NEURO-NAV system of Meng and Kak [102], [103] that used"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, in the NEURO-NAV system [102], [103], a graph topologically representing a layout of the hallways is used for driving the vision processes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Reprinted with permission from [103]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2979085,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56ddc843d10fecb8072ba78c6dcc2ad68f4a1876",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A reasoning and control architecture for vision-guided navigation that makes a robot more humanlike is presented. This system, called NEURO-NAV, discards the more traditional geometrical representation of the environment, and instead uses a semantically richer nonmetrical representation in which a hallway is modeled by the order of appearance of various landmarks and by adjacency relationships. With such a representation, it becomes possible for the robot to respond to commands such as, 'follow the corridor and turn right at the second T junction'. This capability is achieved by an ensemble of neural networks whose activation and deactivation are controlled by a rule-based supervisory controller. The individual neural networks in the ensemble are trained to interpret visual information and perform primitive navigational tasks such as hallway following and landmark detection.<<ETX>>"
            },
            "slug": "Mobile-robot-navigation-using-neural-networks-and-Meng-Kak",
            "title": {
                "fragments": [],
                "text": "Mobile robot navigation using neural networks and nonmetrical environmental models"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A reasoning and control architecture for vision-guided navigation that makes a robot more humanlike is presented, which uses a semantically richer nonmetrical representation in which a hallway is modeled by the order of appearance of various landmarks and by adjacency relationships."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Control Systems"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054282875"
                        ],
                        "name": "M. Meng",
                        "slug": "M.-Meng",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703247"
                        ],
                        "name": "A. Kak",
                        "slug": "A.-Kak",
                        "structuredName": {
                            "firstName": "Avinash",
                            "lastName": "Kak",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These are systems that use no explicit representation at all about the space in which navigation is to take place, but rather resort to recognizing objects found in the environment or to tracking those objects by generating motions based on visual observations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Mobile robotics, navigation, computer vision, indoor navigation, outdoor navigation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6744253,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ef74ea47610dca668c5da4664d70fe60e1a9f49",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors describe a vision-guided mobile robot navigation system, called NEURO-NAV, that is human-like in two senses. The robot can function with non-metrical models of the environment in much the same manner as humans. It does not need a geometric model of the environment. It is sufficient if the environment is modeled by the order of appearance of various landmarks and by adjacency relationships. Also, the robot can response to human-supplied commands. This capability is achieved by an ensemble of neural networks whose activation and deactivation are controlled by a supervisory controller that is rule-based. The individual neural networks in the ensemble are trained to interpret visual information and perform primitive navigational tasks such as hallway following and landmark detection.<<ETX>>"
            },
            "slug": "NEURO-NAV:-a-neural-network-based-architecture-for-Meng-Kak",
            "title": {
                "fragments": [],
                "text": "NEURO-NAV: a neural network based architecture for vision-guided mobile robot navigation using non-metrical models of the environment"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A vision-guided mobile robot navigation system that is human-like in two senses, called NEURO-NAV, that is achieved by an ensemble of neural networks whose activation and deactivation are controlled by a supervisory controller that is rule-based."
            },
            "venue": {
                "fragments": [],
                "text": "[1993] Proceedings IEEE International Conference on Robotics and Automation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703247"
                        ],
                        "name": "A. Kak",
                        "slug": "A.-Kak",
                        "structuredName": {
                            "firstName": "Avinash",
                            "lastName": "Kak",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580436"
                        ],
                        "name": "K. M. Andress",
                        "slug": "K.-M.-Andress",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Andress",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M. Andress"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398496292"
                        ],
                        "name": "C. Lopez-Abadia",
                        "slug": "C.-Lopez-Abadia",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Lopez-Abadia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lopez-Abadia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35615366"
                        ],
                        "name": "M. Carroll",
                        "slug": "M.-Carroll",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Carroll",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Carroll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "28063275"
                        ],
                        "name": "J. R. Lewis",
                        "slug": "J.-R.-Lewis",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Lewis",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Lewis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These are systems that use no explicit representation at all about the space in which navigation is to take place, but rather resort to recognizing objects found in the environment or to tracking those objects by generating motions based on visual observations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16391751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a3ae3486ea24d57f66693adbe8ece1df79293189",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Hierarchical-Evidence-Accumulation-in-the-Pseiki-in-Kak-Andress",
            "title": {
                "fragments": [],
                "text": "Hierarchical Evidence Accumulation in the Pseiki System and Experiments in Model-Driven Mobile Robot Navigation"
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742948"
                        ],
                        "name": "H. Choset",
                        "slug": "H.-Choset",
                        "structuredName": {
                            "firstName": "Howie",
                            "lastName": "Choset",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Choset"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70129502"
                        ],
                        "name": "I. Konukseven",
                        "slug": "I.-Konukseven",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Konukseven",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Konukseven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775644"
                        ],
                        "name": "A. Rizzi",
                        "slug": "A.-Rizzi",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Rizzi",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rizzi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 133
                            }
                        ],
                        "text": "The occupancy-grid approach to map learning is to be contrasted with the approaches that create topological representations of space [22], [23], [39], [74], [87], [97], [123], [161], [164]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11246487,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "8d1fbf641499b0fda56e4eeb846ecc48262f4efe",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new control law for robots that explore unknown environments and configuration spaces. Unlike numerical continuation methods, which produce a jagged path because of the predictor-corrector approach, the control law, introduced in this paper, performs sensor based planning by directing the head of a robot to follow continuously a roadmap. Recall that a roadmap is a one-dimensional representation of a robot's environment. Once the robot exhaustively traces out the roadmap using this control law, it has in essence explored an environment. Experiments on a mobile robot validate the control law."
            },
            "slug": "Sensor-based-planning:-a-control-law-for-generating-Choset-Konukseven",
            "title": {
                "fragments": [],
                "text": "Sensor based planning: a control law for generating the generalized Voronoi graph"
            },
            "venue": {
                "fragments": [],
                "text": "1997 8th International Conference on Advanced Robotics. Proceedings. ICAR'97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2558271"
                        ],
                        "name": "D. Lawton",
                        "slug": "D.-Lawton",
                        "structuredName": {
                            "firstName": "Daryl",
                            "lastName": "Lawton",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lawton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3101141"
                        ],
                        "name": "T. Levitt",
                        "slug": "T.-Levitt",
                        "structuredName": {
                            "firstName": "Tod",
                            "lastName": "Levitt",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Levitt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2232701"
                        ],
                        "name": "C. Mcconnell",
                        "slug": "C.-Mcconnell",
                        "structuredName": {
                            "firstName": "C",
                            "lastName": "Mcconnell",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mcconnell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3153184"
                        ],
                        "name": "J. Glicksman",
                        "slug": "J.-Glicksman",
                        "structuredName": {
                            "firstName": "Jay",
                            "lastName": "Glicksman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Glicksman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[88], Goto and Stentz [45], Kuan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 40235403,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f9e3de45fd0bb3c281e607d824653c1af4d03382",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an architecture for terrain recognition for an autonomous land vehicle. Basic components of this are a set of data bases for generic object models, perceptual structures, temporary memory for the instantiation of object and relational hypothesis, and a long term memory for storing stable hypothesis which are affixed to the terrain representation. Different inference processes operate over these data bases. We describe components of this architecture: the perceptual structure data base, the grouping processes that operate over this, and schemas. We conclude with a processing example for matching predictions from the long term terrain model to imagery and extracting significant perceptual structures for consideration as potential landmarks."
            },
            "slug": "Terrain-models-for-an-autonomous-land-vehicle-Lawton-Levitt",
            "title": {
                "fragments": [],
                "text": "Terrain models for an autonomous land vehicle"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "An architecture for terrain recognition for an autonomous land vehicle is presented, with a processing example for matching predictions from the long term terrain model to imagery and extracting significant perceptual structures for consideration as potential landmarks."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1986 IEEE International Conference on Robotics and Automation"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3005649"
                        ],
                        "name": "V. Graefe",
                        "slug": "V.-Graefe",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Graefe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Graefe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2878034"
                        ],
                        "name": "K. Kuhnert",
                        "slug": "K.-Kuhnert",
                        "structuredName": {
                            "firstName": "Klaus-Dieter",
                            "lastName": "Kuhnert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kuhnert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Equally impressive progress has been achieved in computer vision for outdoor robotics, as represented by the NAVLAB system [142], [143], [124], [146], [126], [128], the work on vision-guided road-following for \u201dAutobahns\u201d [36], [37], [35], [34], and the Prometheus system [46], [47], [48], [50], [49], [129]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Another success story in outdoor navigation is the research in road-following carried out for \u201cAutobahns\u201d [36], [37] and all the derived work that came from that [35], [34], including the EUREKA-project \u201cPrometheus\u201d [46], [47], [48], [50], [49], [129]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60864660,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5feb2f35134e1d07db78dcf7061f662120d8c0ea",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Autonomous road vehicles, guided by computer vision systems, are a topic of research in numerous places in the world. Experimental vehicles have already been driven automatically on various types of roads. Some of these vehicles are briefly introduced, and one is described in more detail. Its dynamic vision system has enabled it to reach speeds of about 100 km/h on highways and 50 km/h on secondary roads."
            },
            "slug": "Vision-based-autonomous-road-vehicles-Graefe-Kuhnert",
            "title": {
                "fragments": [],
                "text": "Vision-based autonomous road vehicles"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper describes a vehicle that has already been driven automatically on various types of roads, and its dynamic vision system has enabled it to reach speeds of about 100 km/h on highways and 50 km/H on secondary roads."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697964"
                        ],
                        "name": "S. Negahdaripour",
                        "slug": "S.-Negahdaripour",
                        "structuredName": {
                            "firstName": "Shahriar",
                            "lastName": "Negahdaripour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Negahdaripour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3053811"
                        ],
                        "name": "B. Hayashi",
                        "slug": "B.-Hayashi",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Hayashi",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hayashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697493"
                        ],
                        "name": "Y. Aloimonos",
                        "slug": "Y.-Aloimonos",
                        "structuredName": {
                            "firstName": "Yiannis",
                            "lastName": "Aloimonos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Aloimonos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12103059,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "f2e029607da3b6b6c57f3f5b90097fcf5434c5e6",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of motion recovery for a head-eye system from stereo image sequences is addressed. Two types of motions, the translation of the vehicle and the panning motion of the head, are considered. It is shown how these motions and the depth map can be estimated directly from the measurements of image gradients and time derivatives. There is no need to estimate image motion, track a scene feature over time, or establish point correspondences in a stereo image pair. The results of various experiments with real scenes are presented.<<ETX>>"
            },
            "slug": "Direct-motion-stereo-for-passive-navigation-Negahdaripour-Hayashi",
            "title": {
                "fragments": [],
                "text": "Direct motion stereo for passive navigation"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The problem of motion recovery for a head-eye system from stereo image sequences is addressed and it is shown how these motions and the depth map can be estimated directly from the measurements of image gradients and time derivatives."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144827643"
                        ],
                        "name": "N. Ayache",
                        "slug": "N.-Ayache",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Ayache",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ayache"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5535481,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "ddb92fb4c128ba4b3f0e3ffdf194252637c041fe",
            "isKey": false,
            "numCitedBy": 330,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "To give mobile robots real autonomy, and to permit them to act efficiently in a diverse, cluttered, and changing environment, they must be equipped with powerful tools for perception and reasoning. \"Artificial Vision for Mobile Robots \"presents new theoretical and practical tools useful for providing mobile robots with artificial vision in three dimensions, including passive binocular and trinocular stereo vision, local and global 3D map reconstructions, fusion of local 3D maps into a global 3D map, 3D navigation, control of uncertainty, and strategies of perception. Numerous examples from research carried out at INRIA with the Esprit Depth and Motion Analysis project are presented in a clear and concise manner.Contents. General Introduction. Stereo Vision. Introduction. Calibration. Image Representation. Binocular Stereo Vision Constraints. Binocular Stereo Vision Algorithms. Experiments in Binocular Stereo Vision. Trinocular Stereo Vision, Outlook. Multisensory Perception. Introduction. A Unified Formalism. Geometric Representation. Construction of Visual Maps. Combining Visual Maps. Results: Matching and Motion. Results: Matching and Fusion. Outlook."
            },
            "slug": "Artificial-vision-for-mobile-robots-stereo-vision-Ayache",
            "title": {
                "fragments": [],
                "text": "Artificial vision for mobile robots - stereo vision and multisensory perception"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Artificial Vision for Mobile Robots \"presents new theoretical and practical tools useful for providing mobile robots with artificial vision in three dimensions, including passive binocular and trinocular stereo vision, local and global 3D map reconstructions, fusion of local 3D maps into a global3D map, 3D navigation, control of uncertainty, and strategies of perception\"."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066894"
                        ],
                        "name": "Hans P. Moravec",
                        "slug": "Hans-P.-Moravec",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Moravec",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hans P. Moravec"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144756941"
                        ],
                        "name": "A. Elfes",
                        "slug": "A.-Elfes",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Elfes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Elfes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 69
                            }
                        ],
                        "text": "In particular, interior space was represented by CAD models of varying complexity."
                    },
                    "intents": []
                }
            ],
            "corpusId": 41852334,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "d1ec836351c0e89f5834957953d9a040dab56985",
            "isKey": false,
            "numCitedBy": 1904,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the use of multiple wide-angle sonar range measurements to map the surroundings of an autonomous mobile robot. A sonar range reading provides information concerning empty and occupied volumes in a cone (subtending 30 degrees in our case) in front of the sensor. The reading is modelled as probability profiles projected onto a rasterized map, where somewhere occupied and everywhere empty areas are represented. Range measurements from multiple points of view (taken from multiple sensors on the robot, and from the same sensors after robot moves) are systematically integrated in the map. Overlapping empty volumes re-inforce each other, and serve to condense the range of occupied volumes. The map definition improves as more readings are added. The final map shows regions probably occupied, probably unoccupied, and unknown areas. The method deals effectively with clutter, and can be used for motion planning and for extended landmark recognition. This system has been tested on the Neptune mobile robot at CMU."
            },
            "slug": "High-resolution-maps-from-wide-angle-sonar-Moravec-Elfes",
            "title": {
                "fragments": [],
                "text": "High resolution maps from wide angle sonar"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The use of multiple wide-angle sonar range measurements to map the surroundings of an autonomous mobile robot deals effectively with clutter, and can be used for motion planning and for extended landmark recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1985 IEEE International Conference on Robotics and Automation"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48701412"
                        ],
                        "name": "L. Boissier",
                        "slug": "L.-Boissier",
                        "structuredName": {
                            "firstName": "Laurence",
                            "lastName": "Boissier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Boissier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35221019"
                        ],
                        "name": "B. Hotz",
                        "slug": "B.-Hotz",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Hotz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hotz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145844931"
                        ],
                        "name": "C. Proy",
                        "slug": "C.-Proy",
                        "structuredName": {
                            "firstName": "Catherine",
                            "lastName": "Proy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Proy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33726225"
                        ],
                        "name": "O. Faugeras",
                        "slug": "O.-Faugeras",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Faugeras",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Faugeras"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717736"
                        ],
                        "name": "P. Fua",
                        "slug": "P.-Fua",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Fua",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 46539840,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "509cd12c1c08f74bfcd2aab5981adc33d0ccab0f",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present some of the work currently being conducted at CEA, CNRS, INRIA, ONERA, and CNES in the framework of the Autonomous Planetary Rover program (VAP). A system concept approach for the VAP application is first presented. Attention is then given to the VAP perception system: some results obtained with stereovision by a correlation algorithm on outdoor simulated Mars terrain scenes are shown.<<ETX>>"
            },
            "slug": "Autonomous-planetary-rover-(VAP):-on-board-system-Boissier-Hotz",
            "title": {
                "fragments": [],
                "text": "Autonomous planetary rover (VAP): on-board perception system concept and stereovision by correlation approach"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present some of the work currently being conducted at CEA, CNRS, INRIA, ONERA, and CNES in the framework of the Autonomous Planetary Rover program (VAP)."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1992 IEEE International Conference on Robotics and Automation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716933"
                        ],
                        "name": "I. Cox",
                        "slug": "I.-Cox",
                        "structuredName": {
                            "firstName": "Ingemar",
                            "lastName": "Cox",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Cox"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 37751749,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "72945310567aa5f620ff8651da169b66deac0dc5",
            "isKey": false,
            "numCitedBy": 653,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The principal components and capabilities of Blanche, an autonomous robot vehicle, are described. Blanche is designed for use in structured office or factory environments rather than unstructured natural environments, and it is assumed that an offline path planner provides the vehicle with a series of collision-free maneuvers, consisting of line and arc segments, to move the vehicle to a desired position. These segments are sent to a low-level trajectory generator and closed-loop motion control. The controller assumes accurate knowledge of the vehicle's position. Blanche's position estimation system consists of a priori map of its environment and a robust matching algorithm. The matching algorithm also estimates the precision of the corresponding match/correction that is then optimally (in a maximum-likelihood sense) combined with the current odometric position to provide an improved estimate of the vehicle's position. The system does not use passive or active beacons. Experimental results are reported. >"
            },
            "slug": "Blanche-an-experiment-in-guidance-and-navigation-of-Cox",
            "title": {
                "fragments": [],
                "text": "Blanche-an experiment in guidance and navigation of an autonomous robot vehicle"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Blanche's position estimation system consists of a priori map of its environment and a robust matching algorithm that estimates the precision of the corresponding match/correction that is then optimally combined with the current odometric position to provide an improved estimate of the vehicle's position."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Robotics Autom."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097660"
                        ],
                        "name": "M. Turk",
                        "slug": "M.-Turk",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Turk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Turk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144615423"
                        ],
                        "name": "M. Marra",
                        "slug": "M.-Marra",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Marra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 119
                            }
                        ],
                        "text": "VITS is a general framework for vision for outdoor road-following together with obstacle detection and avoidance [38], [150]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14434593,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a41fb16ff569491ec72350e0c617d12808815515",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The primary vision task in road-following for a mobile robot is to provide a description of the road environment, including possible obstacles on the road. Techniques are presented for road segmentation and obstacle detection based on color video data. Using constraints on road characteristics in the image space and in 3D color space, the road is extracted and represented by its edges. Assuming vehicle movement, obstacles are detected at a distance and an obstacle avoidance mode is entered."
            },
            "slug": "Color-Road-Segmentation-And-Video-Obstacle-Turk-Marra",
            "title": {
                "fragments": [],
                "text": "Color Road Segmentation And Video Obstacle Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Techniques are presented for road segmentation and obstacle detection based on color video data using constraints on road characteristics in the image space and in 3D color space, and the road is extracted and represented by its edges."
            },
            "venue": {
                "fragments": [],
                "text": "Other Conferences"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691940"
                        ],
                        "name": "J. Miura",
                        "slug": "J.-Miura",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Miura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Miura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700379"
                        ],
                        "name": "Y. Shirai",
                        "slug": "Y.-Shirai",
                        "structuredName": {
                            "firstName": "Yoshiaki",
                            "lastName": "Shirai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Shirai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10553687,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f2ee146b4b039de7e3c1747a30b2fbe8af32051",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A new framework of a hierarchical vision-motion planning for a mobile robot iiridcr uncertainty is proposed. An optimal plan is gcmer- ated by two-level planning: global roiitc. selection arid local path planning. In global roiite sc4ec- tion, a sequence of' observation points to acquire sufficient information to reach a destination is clc- ternlined. Both the cost and the uncrrtainty of vi- sion are considered in this planning. In local path planning, given two successive observation points, trajectories, moving speeds, and reference points for robot localization are deteriiiined so that a robot can reach the second point safely with a minimum cost. Both tlic error in localization and that in niotion control are considered in this plan- ning. A local path planner is repcatedly invoked in global route selection to tlrterinine an actual path between observation points. Oiir hierarchi- cal planner can generate an optimal plan for a miobile robot planning problcm."
            },
            "slug": "Hierarchical-Vision-motion-Planning-With-Local-Path-Miura-Shirai",
            "title": {
                "fragments": [],
                "text": "Hierarchical Vision-motion Planning With Uncertainty: Local Path Planning And Global Route Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A new framework of a hierarchical vision-motion planning for a mobile robot iiridcr uncertainty is proposed and an optimal plan is generated by two-level planning: global roiitc and local path planning."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145135843"
                        ],
                        "name": "A. Kelly",
                        "slug": "A.-Kelly",
                        "structuredName": {
                            "firstName": "Alonzo",
                            "lastName": "Kelly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kelly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722938"
                        ],
                        "name": "A. Stentz",
                        "slug": "A.-Stentz",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Stentz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stentz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Kelly and Stentz [70] have proposed an adaptive approach based on the active vision paradigm that can be used to increase the throughput of perception and, thus, to increase the maximum speed of a mobile robot."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 15
                            }
                        ],
                        "text": "[88], Goto and Stentz [45], Kuan et al. [83] and Kuan and Sharma [84], and others."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17518021,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1986673bdf81828b05fe86e7927e366d4363dbad",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "For autonomously navigating vehicles, the automatic generation of dense geometric models of the environment is a computationally expensive process. Yet, analysis suggests that some approaches to mapping the environment in mobility scenarios can waste significant computational resources. This paper proposes a relatively simple method of approaching the minimum required perceptual throughput in a terrain mapping system, and hence the fastest possible update of the environmental model. We accomplish this by exploiting the constraints of typical mobility scenarios. The technique proposed will be applicable to any application that models the environment with a terrain map or other 2-1/2 D representation."
            },
            "slug": "Minimum-throughput-adaptive-perception-for-high-Kelly-Stentz",
            "title": {
                "fragments": [],
                "text": "Minimum throughput adaptive perception for high speed mobility"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A relatively simple method of approaching the minimum required perceptual throughput in a terrain mapping system, and hence the fastest possible update of the environmental model, by exploiting the constraints of typical mobility scenarios is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2356323"
                        ],
                        "name": "Karen T. Sutherland",
                        "slug": "Karen-T.-Sutherland",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Sutherland",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen T. Sutherland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144816086"
                        ],
                        "name": "W. Thompson",
                        "slug": "W.-Thompson",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Thompson",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Thompson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 179
                            }
                        ],
                        "text": "The techniques that have been developed for localization in unstructured environments include: external camera observation [99], far-point (mountain peaks) landmark triangulation [139], global positioning [29], etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8146297,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "d798fa52771bc9cef4827d4bf4837ae8520e2700",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "A robot navigating in an unstructured outdoor environment must determine its own location in spite of problems due to environmental conditions, sensor limitations and map inaccuracies, exact measurements are seldom known, and the combination of approximate measures can lead to large errors in self-localization. The conventional approach to this problem has been to deal with the errors either during processing or after they occur. The authors maintain that it is possible to limit the errors before they occur. The authors analyze how measurement errors affect errors in localization and propose that a simple algorithm can be used to exploit the geometric properties of landmarks in the environment in order to decrease errors in localization. The authors' goal is to choose landmarks that will provide the best localization regardless of measurement error, determine the best areas in which to identify new landmarks to be used for further localization and choose paths that will provide the least chance of \"straying\". The authors show the result of implementing this concept in experiments run in simulation with USGS 30 m DEM data for a robot statically locating, following a path and identifying new landmarks. >"
            },
            "slug": "Localizing-in-unstructured-environments:-dealing-Sutherland-Thompson",
            "title": {
                "fragments": [],
                "text": "Localizing in unstructured environments: dealing with the errors"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The authors' goal is to choose landmarks that will provide the best localization regardless of measurement error, determine the best areas in which to identify new landmarks to be used for further localization and choose paths that will provided the least chance of \"straying\"."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Robotics Autom."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796560"
                        ],
                        "name": "I. Horswill",
                        "slug": "I.-Horswill",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Horswill",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Horswill"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1664897,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "acd3ebee328ca6181c3ac32498942a1218f82951",
            "isKey": false,
            "numCitedBy": 250,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper I will describe Polly, a low cost visionbased robot that gives primitive tours. The system is very simple, robust and efficient, and runs on a hardware platform which could be duplicated for less than $10K US. The system was built to explore how knowledge about the structure the environment can be used in a principled way to simplify both visual and motor processing. I will argue that very simple and efficient visual mechanisms can often be used to solve real problems in real (unmodified) environments in a principled manner. I will give an overview of the robot, discuss the properties of its environment, show how they can be used to simplify the design of the system, and discuss what lessons can drawn for the design of other systems."
            },
            "slug": "Polly:-A-Vision-Based-Artificial-Agent-Horswill",
            "title": {
                "fragments": [],
                "text": "Polly: A Vision-Based Artificial Agent"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "Polly is a low cost visionbased robot that gives primitive tours and it is argued that very simple and efficient visual mechanisms can often be used to solve real problems in real (unmodified) environments in a principled manner."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706062"
                        ],
                        "name": "R. Arkin",
                        "slug": "R.-Arkin",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Arkin",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Arkin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 32185930,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc86aea4a0dedaa7525aeb68464722445eceab50",
            "isKey": false,
            "numCitedBy": 1180,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Motor schemas serve as the basic unit of behavior specifica tion for the navigation of a mobile robot. They are multiple concurrent processes that operate in conjunction with asso ciated perceptual schemas and contribute independently to the overall concerted action of the vehicle. The motivation be hind the use of schemas for this domain is drawn from neuro- scientific, psychological, and robotic sources. A variant of the potential field method is used to produce the appropriate velocity and steering commands for the robot. Simulation re sults and actual mobile robot experiments demonstrate the feasibility of this approach."
            },
            "slug": "Motor-Schema-\u2014-Based-Mobile-Robot-Navigation-Arkin",
            "title": {
                "fragments": [],
                "text": "Motor Schema \u2014 Based Mobile Robot Navigation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A variant of the potential field method is used to produce the appropriate velocity and steering commands for the robot and demonstrates the feasibility of this approach."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Robotics Res."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066894"
                        ],
                        "name": "Hans P. Moravec",
                        "slug": "Hans-P.-Moravec",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Moravec",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hans P. Moravec"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 119
                            }
                        ],
                        "text": "WRITING a survey paper on computer vision for mobilerobot navigation\u2014a subject that was in high prominence in the 1980s and the first half of the 1990s\u2014is daunting, if not actually hazardous to one\u2019s career."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28775203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f292b8adbba7746703fd32e3a8f88c6fe1782d9",
            "isKey": false,
            "numCitedBy": 474,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The Stanford Cart was a remotely controlled TV-equipped mobile robot. A computer program was written which drove the Cart through cluttered spaces, gaining its knowledge of the world entirely from images broadcast by an on-board TV system. The CMU Rover is a more capable, and neatly operational, robot being built to develop and extend the Stanford work and to explore new directions. The Cart used several kinds of stereopsis to locate objects around it in three dimensions and to deduce its own motion. It planned an obstacle-avoiding path to a desired destination on the basis of a model built with this information. The plan changed as the Cart perceived new obstacles on its journey. The system was reliable for short runs, but slow. The Cart moved 1 m every 10 to 15 min, in lurches. After rolling a meter it stopped, took some pictures, and thought about them for a long time. Then it planned a new path, executed a little of it, and paused again. It successfully drove the Cart through several 20-m courses (each taking about 5 h) complex enough to necessitate three or four avoiding swerves; it failed in other trials in revealing ways. The Rover system has been designed with maximum mechanical and control system flexibility to support a wide range of research in perception and control. It features an omnidirectional steering system, a dozen on-board processors for essential real-time tasks, and a large remote computer to be helped by a high-speed digitizing/data playback unit and a high-performance array processor. Distributed high-level control software similar in organization to the Hearsay II speech-understanding system and the beginnings of a vision library are being readied. By analogy with the evolution of natural intelligence, we believe that incrementally solving the control and perception problems of an autonomous mobile mechanism is one of the best ways of arriving at general artificial intelligence."
            },
            "slug": "The-Stanford-Cart-and-the-CMU-Rover-Moravec",
            "title": {
                "fragments": [],
                "text": "The Stanford Cart and the CMU Rover"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The CMU Rover is a more capable, and neatly operational, robot being built to develop and extend the Stanford work and to explore new directions, with maximum mechanical and control system flexibility to support a wide range of research in perception and control."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116470384"
                        ],
                        "name": "Takayuki Nakamura",
                        "slug": "Takayuki-Nakamura",
                        "structuredName": {
                            "firstName": "Takayuki",
                            "lastName": "Nakamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takayuki Nakamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144657032"
                        ],
                        "name": "M. Asada",
                        "slug": "M.-Asada",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Asada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Asada"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 121
                            }
                        ],
                        "text": "The reader is also referred to attempts at behavior-based approaches to vision-based navigation in mapless spaces [111], [112], [56]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9577956,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37c9484d89b8c4d118d031ac2b1fbbd3fba389ec",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we proposed a method by which a stereo vision-based mobile robot learns to reach a target by detecting and avoiding occlusions. We call the internal representation that describes the learning behavior \"stereo sketch\". First, an input scene is segmented into homogeneous regions by the enhanced ISODATA algorithm with minimum description length principle in terms of image coordinates and disparity information obtained from the fast stereo matching unit based on the coarse-to-fine control method. Then, in terms of the segmented regions including the target area and their occlusion status identified during the stereo and motion disparity estimation process, we construct a state space for the reinforcement learning method to obtain a target reaching behavior. As a result the robot can avoid obstacles without explicitly describing them. We give the computer simulation results and real robot implementation to show the validity of our method."
            },
            "slug": "Stereo-sketch:-stereo-vision-based-target-reaching-Nakamura-Asada",
            "title": {
                "fragments": [],
                "text": "Stereo sketch: stereo vision-based target reaching behavior acquisition with occlusion detection and avoidance"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A stereo vision-based mobile robot learns to reach a target by detecting and avoiding occlusions by constructing a state space for the reinforcement learning method to obtain a target reaching behavior and can avoid obstacles without explicitly describing them."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Robotics and Automation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40088645"
                        ],
                        "name": "A. Rizzi",
                        "slug": "A.-Rizzi",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Rizzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rizzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2768993"
                        ],
                        "name": "G. Bianco",
                        "slug": "G.-Bianco",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Bianco",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Bianco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2977556"
                        ],
                        "name": "R. Cassinis",
                        "slug": "R.-Cassinis",
                        "structuredName": {
                            "firstName": "Riccardo",
                            "lastName": "Cassinis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cassinis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10183397,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "298de4fe45c6be9291dfa4a2b2cd905e3b378b99",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-bee-inspired-visual-homing-using-color-images-Rizzi-Bianco",
            "title": {
                "fragments": [],
                "text": "A bee-inspired visual homing using color images"
            },
            "venue": {
                "fragments": [],
                "text": "Robotics Auton. Syst."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2915583"
                        ],
                        "name": "E. Dickmanns",
                        "slug": "E.-Dickmanns",
                        "structuredName": {
                            "firstName": "Ernst",
                            "lastName": "Dickmanns",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Dickmanns"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 240
                            }
                        ],
                        "text": "Equally impressive progress has been achieved in computer vision for outdoor robotics, as represented by the NAVLAB system [142], [143], [124], [146], [126], [128], the work on vision-guided road-following for \u201dAutobahns\u201d [36], [37], [35], [34], and the Prometheus system [46], [47], [48], [50], [49], [129]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 168
                            }
                        ],
                        "text": "Another success story in outdoor navigation is the research in road-following carried out for \u201cAutobahns\u201d [36], [37] and all the derived work that came from that [35], [34], including the EUREKA-project \u201cPrometheus\u201d [46], [47], [48], [50], [49], [129]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 110607676,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e50cce5d8306a4660c2849f7b1022bd666fd0dec",
            "isKey": true,
            "numCitedBy": 25,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A survey is given on the development of machine vision for road vehicle guidance. Through early work in real-time simulation with real hardware in the loop at UBM, and through the EUREKA-project \u2018Prometheus\u2019 from 1987 to 1994 following the 100 th anniversary of road vehicle developments since 1886, Europe has pioneered the field. Since first fully autonomous testdrives in 1986, and first participation in public traffic in 1992, considerable progress has been achieved. With continuous growth in computing power of microprocessors at a rate of about one order of magnitude every four to five years, sufficient performance levels for dynamic machine vision will be available within one or two decades. The principles of dynamic vision as developed at UBM and performance levels achieved are discussed."
            },
            "slug": "Computer-Vision-and-Highway-Automation-Dickmanns",
            "title": {
                "fragments": [],
                "text": "Computer Vision and Highway Automation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The principles of dynamic vision as developed at UBM and performance levels achieved are discussed and it is suggested that sufficient performance levels for dynamic machine vision will be available within one or two decades."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737088"
                        ],
                        "name": "E. Krotkov",
                        "slug": "E.-Krotkov",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Krotkov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Krotkov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39124290"
                        ],
                        "name": "R. Hoffman",
                        "slug": "R.-Hoffman",
                        "structuredName": {
                            "firstName": "Regis",
                            "lastName": "Hoffman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hoffman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 130
                            }
                        ],
                        "text": "When maps need to be built by a robot, they may be created in either a vehiclecentered coordinate frame vehicle [81] (and updated [82] as the vehicle moves) or with respect to some external reference, such as an external camera attached to a companion device [99] (as in Mars Pathfinder and Lander), or a virtual viewpoint with respect to which range data is projected to form a Cartesian Elevation Map [30] or a global positioning reference such as the sun [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20332184,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "271d8b858c4b8484df8850103d14e067d1c4ba2b",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents a terrain mapping system for walking robots that constructs quantitative models of surface geometry, The accuracy of the constructed maps enables safe, power-efficient locomotion over the natural, rugged terrain found on planetary surfaces. The mapping system acquires range images with a laser rangefinder, preprocesses and stores the images, and constructs elevation maps from them at arbitrary resolutions, in arbitrary reference frames. To quantify performance in terms of accuracy, timing, and memory utilization, the authors conducted extensive tests in natural, rugged terrain, producing hundreds of millions of map points. The results indicate that the mapping system 1) is one of the few that can handle extremely rugged terrain, and 2) exhibits a high degree of real-world robustness due to its aggressive detection of image-based errors and in its compensation for time-varying errors. >"
            },
            "slug": "Terrain-mapping-for-a-walking-planetary-rover-Krotkov-Hoffman",
            "title": {
                "fragments": [],
                "text": "Terrain mapping for a walking planetary rover"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results indicate that the mapping system is one of the few that can handle extremely rugged terrain, and exhibits a high degree of real-world robustness due to its aggressive detection of image-based errors and in its compensation for time-varying errors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Robotics Autom."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737088"
                        ],
                        "name": "E. Krotkov",
                        "slug": "E.-Krotkov",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Krotkov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Krotkov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "Krotkov and Hebert [81] have presented a mapping and positioning systems for RATLER\u2014Robotic All-Terrain Lunar Explorer Rover."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "When maps need to be built by a robot, they may be created in either a vehiclecentered coordinate frame vehicle [81] (and updated [82] as the vehicle moves) or with respect to some external reference, such as an external camera attached to a companion device [99] (as in Mars Pathfinder and Lander), or a virtual viewpoint with respect to which range data is projected to form a Cartesian Elevation Map [30] or a global positioning reference such as the sun [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11365090,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd3c2dabd95883647fd1b2e7f456d5006f38b965",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe practical, effective approaches to outdoor mapping and positioning, and present results from systems implemented for a prototype lunar rover. For mapping, we have developed a binocular head and mounted it on a motion-averaging mast. This head provides images to a normalized correlation matcher, that intelligently selects what part of the image to process (saving time), and subsamples the images (again saving time) without subsampling disparities (which would reduce accuracy). The mapping system has operated successfully during long-duration field exercises, processing streams of thousands of images. The positioning system employs encoders, inclinometers, a compass, and a turn-rate sensor to maintain the position and orientation of the rover as it traverses. The system succeeds in the face of significant sensor noise by virtue of sensor modelling, plus extensive filtering and data screening."
            },
            "slug": "Mapping-and-positioning-for-a-prototype-lunar-rover-Krotkov-Hebert",
            "title": {
                "fragments": [],
                "text": "Mapping and positioning for a prototype lunar rover"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "Practical, effective approaches to outdoor mapping and positioning, and results from systems implemented for a prototype lunar rover are described, and a binocular head is developed and mounted on a motion-averaging mast."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1995 IEEE International Conference on Robotics and Automation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14037223"
                        ],
                        "name": "A. Kosaka",
                        "slug": "A.-Kosaka",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Kosaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kosaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38145612"
                        ],
                        "name": "Goichi Nakazawa",
                        "slug": "Goichi-Nakazawa",
                        "structuredName": {
                            "firstName": "Goichi",
                            "lastName": "Nakazawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Goichi Nakazawa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8647264,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7400bb27cb1b31afd92e2dcacb9030615768aea4",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A vision-based motion tracking method described in this paper estimates the 3D position and orientation of a moving object of known shape at an average speed of 2.5 seconds per image frame even in complex environments using a conventional computer power. Given a coarse estimate of the initial 3D object pose, the method first generates apt expectation view from which visible model features are automatically selected. The method then extracts potentially matched image features from image regions bounded by the propagation of object motion uncertainty. The special aspect of our vision-based trading is an optimal correspondence search for model features and image features in which we use a Kalman filter-based updating scheme to perform the precise 3D object pose estimation. Experimental results are presented to demonstrate the robustness of the method even in the presence of occlusion."
            },
            "slug": "Vision-based-motion-tracking-of-frigid-objects-of-Kosaka-Nakazawa",
            "title": {
                "fragments": [],
                "text": "Vision-based motion tracking of frigid objects using prediction of uncertainties"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A vision-based motion tracking method that estimates the 3D position and orientation of a moving object of known shape at an average speed of 2.5 seconds per image frame even in complex environments using a conventional computer power is described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1995 IEEE International Conference on Robotics and Automation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2878034"
                        ],
                        "name": "K. Kuhnert",
                        "slug": "K.-Kuhnert",
                        "structuredName": {
                            "firstName": "Klaus-Dieter",
                            "lastName": "Kuhnert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kuhnert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 166
                            }
                        ],
                        "text": "This effort resulted in driving speeds of 96 km/h on a highway and 40 km/h on unmarked roads using a method of feature extraction called controlled correlation [85], [86]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57526347,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "132703cd91946edec3aeffc865fd61c60bbd5fca",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "One crucial component of a control system for autonomous vehicle guidance is real time image analysis. This system part is burdened by the maximum flow of information. To overcome the high demands in computation power a combination of knowledge based scene analysis and special hardware has been developed. The use of knowledge based image analysis supports real time processing not by schematically evaluating all parts of the image, but only evaluating those which contain relevant information. This is due to the fact that in many practical problems the relevant information is very unevenly distributed over the image. Preknowledge of the problem or the aim of the mission and expectations or predictions about the scene sustantially reduce the amount of information to be processed. The operations during such an analysis may be divided into two classes - simple processes, e.g. filters, correlation, contour processing and simple search strategies - complex search and control strategy This classification supplied the concept for a special hardware. The complex tasks are performed by a universal processor 80286 while the remaining tasks are executed by a special coprocessor (including image memory). This combination permits the use of filter masks with a arbitrary geometry together with a powerful search strategy. A number of these basic modules may be configured into a multiprocessor system. The universal processor is programmed in a high level language. To support the coprocessor a set of software tools has been built. They permit interactive graphical manipulation of filtermasks, generation of simple search strategies and non real time simulation. Also the real data structures that control the function of the coprocessor are generated by this software package. The system is used within our autonomous vehicle project. One set of algorithms tracks the border lines of the road even if they are broken or disturbed by dirt. Also shadows of bridges crossing the road are tolerated. Another algorithm tracks prominent points on other objects (e.g. vehicles) to collect possible candidates of obstacles during the real time run. A complete image analysis for the relevant features is performed in one video cycle (16.6 ms)."
            },
            "slug": "A-Vision-System-for-Real-Time-Road-and-Object-for-Kuhnert",
            "title": {
                "fragments": [],
                "text": "A Vision System for Real Time Road and Object Recognition for Vehicle Guidance"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A combination of knowledge based scene analysis and special hardware has been developed that permits the use of filter masks with a arbitrary geometry together with a powerful search strategy for real time image analysis of autonomous vehicle guidance."
            },
            "venue": {
                "fragments": [],
                "text": "Other Conferences"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2915583"
                        ],
                        "name": "E. Dickmanns",
                        "slug": "E.-Dickmanns",
                        "structuredName": {
                            "firstName": "Ernst",
                            "lastName": "Dickmanns",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Dickmanns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "95650690"
                        ],
                        "name": "A. Zapp",
                        "slug": "A.-Zapp",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Zapp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zapp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Mobile robotics, navigation, computer vision, indoor navigation, outdoor navigation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62747567,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e521cde5d67b592af126dfdf9429fff89807050",
            "isKey": false,
            "numCitedBy": 306,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "An efficient method for guiding high speed land vehicles along roadways by computer vision has been developed and demonstrated with image sequence processing hardware in a real-time simulation loop 1. The approach is tailored to a well structured highway environment with good lanemarkings. Contour correlation and high order world models are the basic elements of the method, realised in a special multi-microprocessor (on board) computer system. Perspec-tive projection and dynamical models (Kalman filter) are used in an integrated approach for the design of the visual feedback control system. By determining road curvature explicity from the visual input, previously encountered steady state errors in curves are eliminated. The performance of the system will be demonstrated by a video film. The operation of the image sequence processing system has been tested on a typical Autobahn-scene at velocities up to 100 km/h."
            },
            "slug": "A-Curvature-based-Scheme-for-Improving-Road-Vehicle-Dickmanns-Zapp",
            "title": {
                "fragments": [],
                "text": "A Curvature-based Scheme for Improving Road Vehicle Guidance by Computer Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An efficient method for guiding high speed land vehicles along roadways by computer vision has been developed and demonstrated with image sequence processing hardware in a real-time simulation loop 1."
            },
            "venue": {
                "fragments": [],
                "text": "Other Conferences"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 146
                            }
                        ],
                        "text": "In some of the very first vision systems, the knowledge of the environment consisted of a grid representation in which each object in the environment was represented by a 2D projection of its volume onto the horizontal plane."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 922523,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "c0bd664584b68dea3bf21f80d451b4892c16c515",
            "isKey": false,
            "numCitedBy": 489,
            "numCiting": 168,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes a methodology for programming robots known as probabilistic robotics. The probabilistic paradigm pays tribute to the inherent uncertainty in robot perception, relying on explicit representations of uncertainty when determining what to do. This article surveys some of the progress in the field, using in-depth examples to illustrate some of the nuts and bolts of the basic approach. My central conjecture is that the probabilistic approach to robotics scales better to complex real-world applications than approaches that ignore a robot's uncertainty."
            },
            "slug": "Probabilistic-Algorithms-in-Robotics-Thrun",
            "title": {
                "fragments": [],
                "text": "Probabilistic Algorithms in Robotics"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is proposed that the probabilistic approach to robotics scales better to complex real-world applications than approaches that ignore a robot's uncertainty."
            },
            "venue": {
                "fragments": [],
                "text": "AI Mag."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144497046"
                        ],
                        "name": "N. Nilsson",
                        "slug": "N.-Nilsson",
                        "structuredName": {
                            "firstName": "Nils",
                            "lastName": "Nilsson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Nilsson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 146
                            }
                        ],
                        "text": "WRITING a survey paper on computer vision for mobilerobot navigation\u2014a subject that was in high prominence in the 1980s and the first half of the 1990s\u2014is daunting, if not actually hazardous to one\u2019s career."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 106973706,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "476ba2a1c5204d46e420506afacb4b0da6abb868",
            "isKey": false,
            "numCitedBy": 836,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : From 1960 through 1972, the Artificial Intelligence Center at SRI conducted research on a mobile robot system nicknamed \"Shakey.\" Endowed with a limited ability to perceive and model its environment, Shakey could perform tasks that required planning, route finding, and the rearranging of simple objects. Although the Shakey project led to numerous advances in AI techniques, many of which were reported in the literature, much specific in formation that might be useful in current robotics research appears only in a series of relatively inaccessible SRI technical reports. Our purpose here, consequently, is to make this material more readily available by extracting and reprinting those sections of the reports that seem particularly interesting, relevant and important."
            },
            "slug": "Shakey-the-Robot-Nilsson",
            "title": {
                "fragments": [],
                "text": "Shakey the Robot"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3005649"
                        ],
                        "name": "V. Graefe",
                        "slug": "V.-Graefe",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Graefe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Graefe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27359588,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5aa5866461fc560ffe88ab866eac2ac86a9ba665",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ". the architecture of multi-processor computer systems The logical structure of robot vision systems is analyzed as a basis for designing such systems. Both sensor data fusion and knowledge representation in a vision system may be broken down into four hierarchical levels where at each level knowledge represen- ."
            },
            "slug": "Dynamic-Vision-Systems-for-Autonomous-Mobile-Robots-Graefe",
            "title": {
                "fragments": [],
                "text": "Dynamic Vision Systems for Autonomous Mobile Robots"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "The logical structure of robot vision systems is analyzed as a basis for designing such systems and both sensor data fusion and knowledge representation in a vision system may be broken down into four hierarchical levels."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. IEEE/RSJ International Workshop on Intelligent Robots and Systems '. (IROS '89) 'The Autonomous Mobile Robots and Its Applications"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145228535"
                        ],
                        "name": "B. Wilcox",
                        "slug": "B.-Wilcox",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Wilcox",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Wilcox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2192229"
                        ],
                        "name": "D. Gennery",
                        "slug": "D.-Gennery",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Gennery",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gennery"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 110746797,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "f090dff7df1ad4d6690199edd951661b79029055",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Some technical issues concerning a Mars rover launched in the 1990\u2019s are discussed. Two particular modes of controlling the travelling of the vehicle are described. In one mode, most of the control is from Earth, by human operators viewing stereo pictures sent from the rover and designating short routes to follow. In the other mode, computer vision is used in order to make the rover more autonomous, but reliability is aided by the use of orbital imagery and approximate long routes sent from Earth. In the latter case, it is concluded that average travel rates of around 10 km/day are feasible."
            },
            "slug": "A-Mars-Rover-for-the-1990's-Wilcox-Gennery",
            "title": {
                "fragments": [],
                "text": "A Mars Rover for the 1990's"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "Some technical issues concerning a Mars rover launched in the 1990\u2019s are discussed and it is concluded that average travel rates of around 10 km/day are feasible."
            },
            "venue": {
                "fragments": [],
                "text": "Autonomous Robot Vehicles"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782162"
                        ],
                        "name": "L. Matthies",
                        "slug": "L.-Matthies",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Matthies",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Matthies"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698190"
                        ],
                        "name": "E. Gat",
                        "slug": "E.-Gat",
                        "structuredName": {
                            "firstName": "Erann",
                            "lastName": "Gat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145253927"
                        ],
                        "name": "R. Harrison",
                        "slug": "R.-Harrison",
                        "structuredName": {
                            "firstName": "Reid",
                            "lastName": "Harrison",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Harrison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145228535"
                        ],
                        "name": "B. Wilcox",
                        "slug": "B.-Wilcox",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Wilcox",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Wilcox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143681261"
                        ],
                        "name": "R. Volpe",
                        "slug": "R.-Volpe",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Volpe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Volpe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34796196"
                        ],
                        "name": "T. Litwin",
                        "slug": "T.-Litwin",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Litwin",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Litwin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 123
                            }
                        ],
                        "text": "The techniques that have been developed for localization in unstructured environments include: external camera observation [99], far-point (mountain peaks) landmark triangulation [139], global positioning [29], etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 259
                            }
                        ],
                        "text": "When maps need to be built by a robot, they may be created in either a vehiclecentered coordinate frame vehicle [81] (and updated [82] as the vehicle moves) or with respect to some external reference, such as an external camera attached to a companion device [99] (as in Mars Pathfinder and Lander), or a virtual viewpoint with respect to which range data is projected to form a Cartesian Elevation Map [30] or a global positioning reference such as the sun [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 193
                            }
                        ],
                        "text": "Because the deadreckoning-based positional errors tend to accumulate, it was determined by simulation of typical navigation scenarios that the rover would be allowed to travel at most 10 m/day [99]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6954855,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1027b70f028c624774de3d9bbc43067f0f2e261",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "In 1996, NASA will launch the Mars Pathfinder spacecraft, which will carry an 11 kg rover to explore the immediate vicinity of the lander. To assess the capabilities of the rover, as well as to set priorities for future rover research, it is essential to evaluate the performance of its autonomous navigation system as a function of terrain characteristics. Unfortunately, very little of this kind of evaluation has been done, for either planetary rovers or terrestrial applications. To fill this gap, we have constructed a new microrover testbed consisting of the Rocky 3.2 vehicle and an indoor test arena with overhead cameras for automatic, real-time tracking of the true rover position and heading. We create Mars analog terrains in this arena by randomly distributing rocks according to an exponential model of Mars rock size frequency created from Viking lander imagery. To date, we have recorded detailed logs from over 85 navigation trials in this testbed. In this paper, we outline current plans for Mars exploration over the next decade, summarize the design of the lander and rover for the 1996 Pathfinder mission, and introduce a decomposition of rover navigation into four major functions: goal designation, rover localization, hazard detection, and path selection. We then describe the Pathfinder approach to each function, present results to date of evaluating the performance of each function, and outline our approach to enhancing performance for future missions. The results show key limitations in the quality of rover localization, the speed of hazard detection, and the ability of behavior control algorithms for path selection to negotiate the rock frequencies likely to be encountered on Mars. We believe that the facilities, methodologies, and to some extent the specific performance results presented here will provide valuable examples for efforts to evaluate robotic vehicle performance in other applications."
            },
            "slug": "Mars-microrover-navigation:-Performance-evaluation-Matthies-Gat",
            "title": {
                "fragments": [],
                "text": "Mars microrover navigation: performance evaluation and enhancement"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The results show key limitations in the quality of rover localization, the speed of hazard detection, and the ability of behavior control algorithms for path selection to negotiate the rock frequencies likely to be encountered on Mars."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human Robot Interaction and Cooperative Robots"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145585296"
                        ],
                        "name": "B. Kuipers",
                        "slug": "B.-Kuipers",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Kuipers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kuipers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082660"
                        ],
                        "name": "Y. Byun",
                        "slug": "Y.-Byun",
                        "structuredName": {
                            "firstName": "Yung-Tai",
                            "lastName": "Byun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Byun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 146
                            }
                        ],
                        "text": "For topological representations of space\u2014in particular, large-scale space\u2014 the reader is also referred to the pioneering work of Kuipers and Byun [87]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "The occupancy-grid approach to map learning is to be contrasted with the approaches that create topological representations of space [22], [23], [39], [74], [87], [97], [123], [161], [164]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2013334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f83795b293c075642a6aab1628eead66d2b4ffd2",
            "isKey": false,
            "numCitedBy": 1019,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-robot-exploration-and-mapping-strategy-based-on-a-Kuipers-Byun",
            "title": {
                "fragments": [],
                "text": "A robot exploration and mapping strategy based on a semantic hierarchy of spatial representations"
            },
            "venue": {
                "fragments": [],
                "text": "Robotics Auton. Syst."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145356886"
                        ],
                        "name": "R. Wallace",
                        "slug": "R.-Wallace",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Wallace"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[152]andWallace[153],Lawtonetal."
                    },
                    "intents": []
                }
            ],
            "corpusId": 26853069,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "ff828fb5f3e6d17aabbcc762389a626cc70dfdce",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper present a color vision based robot road following program. The program uses an adaptive color model to classify pixels as road or shoulder features. To update the color statistics, the program tracks the shape of the road through the image sequence. As long as the program has either a reasonable color model from the previous image or a reasonable estimation of the expected road position it can steer a robot vehicle along a road reliably even in the presence of surface color variation, fluctuation of illumination conditions and deviation of sensor response. We present results from several test runs of a robot vehicle in a natural outdoor environment. The paper includes a discussion of failure modes of the program that have been catalogued and analyzed in order to guide future developments."
            },
            "slug": "Robot-road-following-by-adaptive-color-and-shape-Wallace",
            "title": {
                "fragments": [],
                "text": "Robot road following by adaptive color classification and shape tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A color vision based robot road following program that can steer a robot vehicle along a road reliably even in the presence of surface color variation, fluctuation of illumination conditions and deviation of sensor response."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1987 IEEE International Conference on Robotics and Automation"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698040"
                        ],
                        "name": "C. Olson",
                        "slug": "C.-Olson",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Olson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Olson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782162"
                        ],
                        "name": "L. Matthies",
                        "slug": "L.-Matthies",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Matthies",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Matthies"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 144
                            }
                        ],
                        "text": "are expected to be alleviated in future missions by using new approaches, such as using the maximum-likelihood approach for matching range maps [118]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13961946,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "450ca03f434e5f7d15061b9b58ca6a5fba59d102",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes maximum likelihood estimation techniques for performing rover localization in natural terrain by matching range maps. An occupancy map of the local terrain is first generated using stereo vision. The position of the rover with respect to a previously generated occupancy map is then computed by comparing the maps using a probabilistic formulation of image matching techniques. Our motivation for this work is the desire for greater autonomy in Mars rovers. These techniques have been applied to data obtained from the Sojourner Mars rover and run on-board the Rocky 7 Mars rover prototype."
            },
            "slug": "Maximum-likelihood-rover-localization-by-matching-Olson-Matthies",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood rover localization by matching range maps"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "Maximum likelihood estimation techniques for performing rover localization in natural terrain by matching range maps are described, applied to data obtained from the Sojourner Mars rover and run on-board the Rocky 7 Mars rover prototype."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE International Conference on Robotics and Automation (Cat. No.98CH36146)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2792922"
                        ],
                        "name": "Robert Pagnot",
                        "slug": "Robert-Pagnot",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Pagnot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Pagnot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "28934538"
                        ],
                        "name": "P. Grandjean",
                        "slug": "P.-Grandjean",
                        "structuredName": {
                            "firstName": "Pierrick",
                            "lastName": "Grandjean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Grandjean"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 136
                            }
                        ],
                        "text": "In such cases, the vision system can make use of at most a generic characterization of the possible obstacles in the environment, as in [120] (Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9583653,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "e7686acfa8925959e609d1cc339475a14cdf5b99",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The cross-country navigation approach presented in this paper relies on two simplification ideas: obstacles are defected by scanning columns of a range image and motion steps are single straight-line trajectories. The resulting system is fast and only prohibits steep slopes or cluttered terrains. It also features a sophisticated planning criterion that yields good global performances. Its reliability has been demonstrated by successful outdoor traverses 70 meters long on a representative test site."
            },
            "slug": "Fast-cross-country-navigation-on-fair-terrains-Pagnot-Grandjean",
            "title": {
                "fragments": [],
                "text": "Fast cross-country navigation on fair terrains"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "The cross-country navigation approach relies on two simplification ideas: obstacles are defected by scanning columns of a range image and motion steps are single straight-line trajectories, which is fast and only prohibits steep slopes or cluttered terrains."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1995 IEEE International Conference on Robotics and Automation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72419159"
                        ],
                        "name": "R. Brooks",
                        "slug": "R.-Brooks",
                        "structuredName": {
                            "firstName": "Rodney",
                            "lastName": "Brooks",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Brooks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10542804,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4af77aafa93f810e403461e5ee911287aa16d76e",
            "isKey": false,
            "numCitedBy": 9081,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "A new architecture for controlling mobile robots is described. Layers of control system are built to let the robot operate at increasing levels of competence. Layers are made up of asynchronous modules that communicate over low-bandwidth channels. Each module is an instance of a fairly simple computational machine. Higher-level layers can subsume the roles of lower levels by suppressing their outputs. However, lower levels continue to function as higher levels are added. The result is a robust and flexible robot control system. The system has been used to control a mobile robot wandering around unconstrained laboratory areas and computer machine rooms. Eventually it is intended to control a robot that wanders the office areas of our laboratory, building maps of its surroundings using an onboard arm to perform simple tasks."
            },
            "slug": "A-robust-layered-control-system-for-a-mobile-robot-Brooks",
            "title": {
                "fragments": [],
                "text": "A robust layered control system for a mobile robot"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A new architecture for controlling mobile robots is described, building a robust and flexible robot control system that has been used to control a mobile robot wandering around unconstrained laboratory areas and computer machine rooms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE J. Robotics Autom."
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388700951"
                        ],
                        "name": "Tomas Lozano-Perez",
                        "slug": "Tomas-Lozano-Perez",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Lozano-Perez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Lozano-Perez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1977841"
                        ],
                        "name": "M. Wesley",
                        "slug": "M.-Wesley",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Wesley",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wesley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17397594,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d9e3db556c5611f45182af845a5199701adb8cb6",
            "isKey": false,
            "numCitedBy": 2366,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a collision avoidance algorithm for planning a safe path for a polyhedral object moving among known polyhedral objects. The algorithm transforms the obstacles so that they represent the locus of forbidden positions for an arbitrary reference point on the moving object. A trajectory of this reference point which avoids all forbidden regions is free of collisions. Trajectories are found by searching a network which indicates, for each vertex in the transformed obstacles, which other vertices can be reached safely."
            },
            "slug": "An-algorithm-for-planning-collision-free-paths-Lozano-Perez-Wesley",
            "title": {
                "fragments": [],
                "text": "An algorithm for planning collision-free paths among polyhedral obstacles"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A collision avoidance algorithm for planning a safe path for a polyhedral object moving among known polyhedral objects that transforms the obstacles so that they represent the locus of forbidden positions for an arbitrary reference point on the moving object."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144637856"
                        ],
                        "name": "Y. Hwang",
                        "slug": "Y.-Hwang",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Hwang",
                            "middleNames": [
                                "Koo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Hwang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237406"
                        ],
                        "name": "N. Ahuja",
                        "slug": "N.-Ahuja",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Ahuja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ahuja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9132784,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "1cf496e7db9712bfd6f9373b161cbc359031f568",
            "isKey": false,
            "numCitedBy": 596,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "A path-planning algorithm for the classical mover's problem in three dimensions using a potential field representation of obstacles is presented. A potential function similar to the electrostatic potential is assigned to each obstacle, and the topological structure of the free space is derived in the form of minimum potential valleys. Path planning is done at two levels. First, a global planner selects a robot's path from the minimum potential valleys and its orientations along the path that minimize a heuristic estimate of the path length and the chance of collision. Then, a local planner modifies the path and orientations to derive the final collision-free path and orientations. If the local planner fails, a new path and orientations are selected by the global planner and subsequently examined by the local planner. This process is continued until a solution is found or there are no paths left to be examined. The algorithm solves a much wider class of problems than other heuristic algorithms and at the same time runs much faster than exact algorithms (typically 5 to 30 min on a Sun 3/260). >"
            },
            "slug": "A-potential-field-approach-to-path-planning-Hwang-Ahuja",
            "title": {
                "fragments": [],
                "text": "A potential field approach to path planning"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A path-planning algorithm for the classical mover's problem in three dimensions using a potential field representation of obstacles is presented and solves a much wider class of problems than other heuristic algorithms and at the same time runs much faster than exact algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Robotics Autom."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48939336"
                        ],
                        "name": "H. Mori",
                        "slug": "H.-Mori",
                        "structuredName": {
                            "firstName": "Hideo",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107806776"
                        ],
                        "name": "K. Kobayashi",
                        "slug": "K.-Kobayashi",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Kobayashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kobayashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47984535"
                        ],
                        "name": "Naotaka Ohtuki",
                        "slug": "Naotaka-Ohtuki",
                        "structuredName": {
                            "firstName": "Naotaka",
                            "lastName": "Ohtuki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naotaka Ohtuki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2777864"
                        ],
                        "name": "S. Kotani",
                        "slug": "S.-Kotani",
                        "structuredName": {
                            "firstName": "Shinji",
                            "lastName": "Kotani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kotani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[110]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 39658912,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "154b5d612470fd90b15a391d0fbf8c20e56cea2c",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The color vision in the outdoor environment has two problems. The first is the hue shift of the object under the changes of the season, weather, the hour and the view position. The second is the hue shift of the object between in the sunny place and in the shaded place. We propose the color impression factor to memory the hue of the object. The color impression factor is defined as R-B. It is the function of the sunlight condition (sunny/shaded) and the hour. We show this factor is useful to discriminate objects from the shadows in the asphalt paved road-yellow tactile blocks from the asphalt paved road."
            },
            "slug": "Color-impression-factor:-an-image-understanding-for-Mori-Kobayashi",
            "title": {
                "fragments": [],
                "text": "Color impression factor: an image understanding method for outdoor mobile robots"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The color impression factor is proposed and it is shown this factor is useful to discriminate objects from the shadows in the asphalt paved road-yellow tactile blocks from the asphalt paving road."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48855558"
                        ],
                        "name": "D. Pomerleau",
                        "slug": "D.-Pomerleau",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Pomerleau",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pomerleau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Mobile robotics, navigation, computer vision, indoor navigation, outdoor navigation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18420840,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3",
            "isKey": false,
            "numCitedBy": 1456,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer back-propagation network designed for the task of road following. Currently ALVINN takes images from a camera and a laser range finder as input and produces as output the direction the vehicle should travel in order to follow the road. Training has been conducted using simulated road images. Successful tests on the Carnegie Mellon autonomous navigation test vehicle indicate that the network can effectively follow real roads under certain field conditions. The representation developed to perform the task differs dramatically when the network is trained under various conditions, suggesting the possibility of a novel adaptive autonomous navigation system capable of tailoring its processing to the conditions at hand."
            },
            "slug": "ALVINN:-An-Autonomous-Land-Vehicle-in-a-Neural-Pomerleau",
            "title": {
                "fragments": [],
                "text": "ALVINN: An Autonomous Land Vehicle in a Neural Network"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer back-propagation network designed for the task of road following that can effectively follow real roads under certain field conditions."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2915583"
                        ],
                        "name": "E. Dickmanns",
                        "slug": "E.-Dickmanns",
                        "structuredName": {
                            "firstName": "Ernst",
                            "lastName": "Dickmanns",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Dickmanns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3098536"
                        ],
                        "name": "Birger D. Mysliwetz",
                        "slug": "Birger-D.-Mysliwetz",
                        "structuredName": {
                            "firstName": "Birger",
                            "lastName": "Mysliwetz",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Birger D. Mysliwetz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 20358630,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "06bf92b7298c6a6608504b9c27452b0272729a4b",
            "isKey": false,
            "numCitedBy": 618,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "The general problem of recognizing both horizontal and vertical road curvature parameters while driving along the road has been solved recursively. A differential geometry representation decoupled for the two curvature components has been selected. Based on the planar solution of E.D. Dickmanns and A. Zapp (1986) and its refinements, a simple spatio-temporal model of the driving process makes it possible to take both spatial and temporal constraints into account effectively. The estimation process determines nine road and vehicle state parameters recursively at 25 Hz (40 ms) using four Intel 80286 and one 386 microprocessors. Results with the test vehicle (VaMoRs), which is a 5-ton van, are given for a hilly country road. >"
            },
            "slug": "Recursive-3-D-Road-and-Relative-Ego-State-Dickmanns-Mysliwetz",
            "title": {
                "fragments": [],
                "text": "Recursive 3-D Road and Relative Ego-State Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The general problem of recognizing both horizontal and vertical road curvature parameters while driving along the road has been solved recursively and a differential geometry representation decoupled for the two curvature components has been selected."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2787925"
                        ],
                        "name": "C. Joulain",
                        "slug": "C.-Joulain",
                        "structuredName": {
                            "firstName": "C\u00e9dric",
                            "lastName": "Joulain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Joulain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3305788"
                        ],
                        "name": "P. Gaussier",
                        "slug": "P.-Gaussier",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Gaussier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gaussier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39482544"
                        ],
                        "name": "A. Revel",
                        "slug": "A.-Revel",
                        "structuredName": {
                            "firstName": "Arnaud",
                            "lastName": "Revel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Revel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34165854"
                        ],
                        "name": "B. Gas",
                        "slug": "B.-Gas",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Gas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Gas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[65] developed an"
                    },
                    "intents": []
                }
            ],
            "corpusId": 39961514,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27f02a7051528ce5d104bcb5efa707f8c04674cb",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe how a mobile robot can autonomously learn and \"recognize\" simple objects present somewhere in an indoor visual scene. The experiment involves transposing a classical conditioning experiment on a mobile robot. We propose the use of a selective attention mechanism to reduce the amount of computation involved by the complete image analysis. Objects are categorized according to their associated actions that are learned in accordance with a reward/punishment procedure. Our approach emphasizes the importance of a movement reflex mechanism based on the use of the same egocentric representation from the visual information to the motor output. Finally, we highlight the impact of information coding in self organised topological maps on the robot performances."
            },
            "slug": "Learning-to-build-visual-categories-from-Joulain-Gaussier",
            "title": {
                "fragments": [],
                "text": "Learning to build visual categories from perception-action associations"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This paper describes how a mobile robot can autonomously learn and \"recognize\" simple objects present somewhere in an indoor visual scene and proposes the use of a selective attention mechanism to reduce the amount of computation involved by the complete image analysis."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145228535"
                        ],
                        "name": "B. Wilcox",
                        "slug": "B.-Wilcox",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Wilcox",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Wilcox"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "project [158], [99], [101]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "39) [157], [158], [12], [81], [99]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 33327748,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "2db3659885b2b269a5388102bd0e0829f5a58328",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Future missions to the moon, Mars, or other planetary surfaces will use planetary rovers for exploration or other tasks. Operation of these rovers as unmanned robotic vehicles with some form of remote or semi-autonomous control is desirable to reduce the cost and increase the capability and safety of many types of missions. However, the long time delays and relatively low bandwidths associated with radio communications between planets precludes a total \u201ctelepresence\u201d approach to controlling the vehicle. A program to develop planetary rover technology has been initiated at the Jet Propulsion Laboratory (JPL) under sponsorship of the National Aeronautics and Space Administration (NASA). Developmental systems with the necessary sensing, computing, power, and mobility resources to demonstrate realistic forms of control for various missions have been developed and initial testing has been completed. These testbed systems, the associated navigation techniques currently used and planned for implementation, and long-term mission strategies employing them are described."
            },
            "slug": "Robotic-vehicles-for-planetary-exploration-Wilcox",
            "title": {
                "fragments": [],
                "text": "Robotic vehicles for planetary exploration"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Developmental systems with the necessary sensing, computing, power, and mobility resources to demonstrate realistic forms of control for various missions have been developed and initial testing has been completed."
            },
            "venue": {
                "fragments": [],
                "text": "Applied Intelligence"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716933"
                        ],
                        "name": "I. Cox",
                        "slug": "I.-Cox",
                        "structuredName": {
                            "firstName": "Ingemar",
                            "lastName": "Cox",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Cox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7136913"
                        ],
                        "name": "J. Leonard",
                        "slug": "J.-Leonard",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Leonard",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Leonard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "Such a representation is usually referred to as \u201doccupancy map\u201d and it was formally introduced in [109]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207508276,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67169ede34cb5cb977a8f5c790c95ed844a5060e",
            "isKey": false,
            "numCitedBy": 208,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modeling-a-Dynamic-Environment-Using-a-Bayesian-Cox-Leonard",
            "title": {
                "fragments": [],
                "text": "Modeling a Dynamic Environment Using a Bayesian Multiple Hypothesis Approach"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783759"
                        ],
                        "name": "T. Jochem",
                        "slug": "T.-Jochem",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Jochem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jochem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48855558"
                        ],
                        "name": "D. Pomerleau",
                        "slug": "D.-Pomerleau",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Pomerleau",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pomerleau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34588530"
                        ],
                        "name": "C. Thorpe",
                        "slug": "C.-Thorpe",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Thorpe",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Thorpe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Mobile robotics, navigation, computer vision, indoor navigation, outdoor navigation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13052355,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "580f3c543b8e2c91704de2dffb9b7f09bfcc3c61",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of artificial neural networks in the domain of autonomous driving has produced promising results. ALVINN has shown that a neural system can drive a vehicle reliably and safely on many different types of roads, ranging from paved paths to interstate highways. The next step in the evolution of autonomous driving systems is to intelligently handle road junctions. In this paper the authors present an addition to the basic ALVINN driving system which makes autonomous detection of roads and traversal of simple intersections possible. The addition is based on geometrically modelling the world, accurately imaging interesting parts of the scene using this model, and monitoring ALVINN's response to the created image."
            },
            "slug": "Vision-based-neural-network-road-and-intersection-Jochem-Pomerleau",
            "title": {
                "fragments": [],
                "text": "Vision-based neural network road and intersection detection and traversal"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An addition is presented to the basic ALVINN driving system which makes autonomous detection of roads and traversal of simple intersections possible, based on geometrically modelling the world, accurately imaging interesting parts of the scene using this model, and monitoring ALVinN's response to the created image."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human Robot Interaction and Cooperative Robots"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742234"
                        ],
                        "name": "D. Kuan",
                        "slug": "D.-Kuan",
                        "structuredName": {
                            "firstName": "Darwin",
                            "lastName": "Kuan",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kuan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153818507"
                        ],
                        "name": "U. Sharma",
                        "slug": "U.-Sharma",
                        "structuredName": {
                            "firstName": "Uma",
                            "lastName": "Sharma",
                            "middleNames": [
                                "Kant"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Sharma"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "[83] and Kuan and Sharma [84], and others."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 49
                            }
                        ],
                        "text": "[88], Goto and Stentz [45], Kuan et al. [83] and Kuan and Sharma [84], and others."
                    },
                    "intents": []
                }
            ],
            "corpusId": 20896078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbedc136d61a6c8e874626e4a9d01e5ff97d3730",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a model-based geometric reasoning module for autonomous road following. Vision-guided road following requires extracting road boundaries from images in real-time to guide the navigation of autonomous vehicles on roadway. The detected road region boundary is error-prone due to imperfect image segmentation. To achieve robust system performance, a geometric reasoning module that uses spatial and temporal constraints to perform model-based reasoning is required. Local geometric supports for each road edge segment are collected and recorded and global consistency checking is performed to obtain a consistent interpretation of the raw data. Cases involving incomplete sensor data, on curved roads where only one side of the road is visible, and incorrect segmentation due to shadows, road patches, or unusual road conditions, can usually be detected and corrected. This reasoning module has been integrated into a road following system which is capable of supporting autonomous road following at 19 km/hr."
            },
            "slug": "Model-based-geometric-reasoning-for-autonomous-road-Kuan-Sharma",
            "title": {
                "fragments": [],
                "text": "Model based geometric reasoning for autonomous road following"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper describes a model-based geometric reasoning module for autonomous road following that has been integrated into a road following system which is capable of supporting autonomous roadFollowing at 19 km/hr."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1987 IEEE International Conference on Robotics and Automation"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783759"
                        ],
                        "name": "T. Jochem",
                        "slug": "T.-Jochem",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Jochem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jochem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48855558"
                        ],
                        "name": "D. Pomerleau",
                        "slug": "D.-Pomerleau",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Pomerleau",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pomerleau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34588530"
                        ],
                        "name": "C. Thorpe",
                        "slug": "C.-Thorpe",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Thorpe",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Thorpe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 110187164,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4f1acef4367506dff7c7eb10f305d4a2347cf53",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Many systems have been created which can keep an autonomous vehicle within a driving lane, but little experimental work has been reported that describes methods to transition a vehicle between lanes. Three techniques to accomplish lane transition using the ALVINN lane keeping system are reported here. The most basic involves intelligently switching between two trained ALVINN networks. The other techniques use active control of virtual camera views to move the vehicle into the destination lane."
            },
            "slug": "Vision-guided-lane-transition-Jochem-Pomerleau",
            "title": {
                "fragments": [],
                "text": "Vision guided lane transition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Three techniques to accomplish lane transition using the ALVINN lane keeping system are reported here, the most basic involves intelligently switching between two trained ALVinN networks."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Intelligent Vehicles '95. Symposium"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144479211"
                        ],
                        "name": "P. Rives",
                        "slug": "P.-Rives",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Rives",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Rives"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40547861"
                        ],
                        "name": "J. Borrelly",
                        "slug": "J.-Borrelly",
                        "structuredName": {
                            "firstName": "Jean-Jacques",
                            "lastName": "Borrelly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Borrelly"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "In [130], for example, Rives and Borrelly took a visual servoing approach and devised a controller that takes as inputs the lines extracted from the image of a pipe and uses this information to generate steering commands for the ROV Vortex vehicle."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9398132,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "id": "d2558ea5c4623500c608731ff6c658fb58ca7748",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a visual servoing approach applied to the control of an autonomous underwater vehicle executing a pipe inspection task. After having briefly recalled the theoretical framework used, we design a closed loop control scheme using visual data as feedback. This scheme is analyzed using the facilities of our homemade Simparc simulation package which can handle both control and sensory aspects. The last part of the paper presents results obtained on, the Vortex underwater vehicle which is an experimental ROV built by the Ifremer company."
            },
            "slug": "Underwater-pipe-inspection-task-using-visual-Rives-Borrelly",
            "title": {
                "fragments": [],
                "text": "Underwater pipe inspection task using visual servoing techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A closed loop control scheme using visual data as feedback is designed and analyzed using the facilities of the homemade Simparc simulation package which can handle both control and sensory aspects."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732218"
                        ],
                        "name": "C. Balkenius",
                        "slug": "C.-Balkenius",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Balkenius",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Balkenius"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 220
                            }
                        ],
                        "text": "For another appearance-based approach that uses elastic template matching\u2014a template is a set of features, each extracted from a 7x7 window in the image and their relative positions\nwithin the entire image\u2014see the work of Balkenius [9]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 50
                            }
                        ],
                        "text": "within the entire image\u2014see the work of Balkenius [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16823431,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bdc54129a2070b09681ebdd3d96bd33366751095",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Spatial-learning-with-perceptually-grounded-Balkenius",
            "title": {
                "fragments": [],
                "text": "Spatial learning with perceptually grounded representations"
            },
            "venue": {
                "fragments": [],
                "text": "Robotics Auton. Syst."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036494"
                        ],
                        "name": "A. Bernardino",
                        "slug": "A.-Bernardino",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Bernardino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bernardino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398909021"
                        ],
                        "name": "J. Santos-Victor",
                        "slug": "J.-Santos-Victor",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Santos-Victor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Santos-Victor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "In a more recent work, [10], Bernardino and Santos-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8744371,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7317f8749d6c9ab2ec49bbe406e07270b301ba8",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Visual-behaviours-for-binocular-tracking-Bernardino-Santos-Victor",
            "title": {
                "fragments": [],
                "text": "Visual behaviours for binocular tracking"
            },
            "venue": {
                "fragments": [],
                "text": "Robotics Auton. Syst."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48855558"
                        ],
                        "name": "D. Pomerleau",
                        "slug": "D.-Pomerleau",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Pomerleau",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pomerleau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7813587,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d652a1980e743c7c85ff6066409ea1e3be4d685",
            "isKey": false,
            "numCitedBy": 642,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The ALVINN (Autonomous Land Vehicle In a Neural Network) project addresses the problem of training artificial neural networks in real time to perform difficult perception tasks. ALVINN is a backpropagation network designed to drive the CMU Navlab, a modified Chevy van. This paper describes the training techniques that allow ALVINN to learn in under 5 minutes to autonomously control the Navlab by watching the reactions of a human driver. Using these techniques, ALVINN has been trained to drive in a variety of circumstances including single-lane paved and unpaved roads, and multilane lined and unlined roads, at speeds of up to 20 miles per hour."
            },
            "slug": "Efficient-Training-of-Artificial-Neural-Networks-Pomerleau",
            "title": {
                "fragments": [],
                "text": "Efficient Training of Artificial Neural Networks for Autonomous Navigation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The training techniques that allow ALVINN to learn in under 5 minutes to autonomously control the Navlab by watching the reactions of a human driver are described."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48855558"
                        ],
                        "name": "D. Pomerleau",
                        "slug": "D.-Pomerleau",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Pomerleau",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pomerleau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783759"
                        ],
                        "name": "T. Jochem",
                        "slug": "T.-Jochem",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Jochem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jochem"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Mobile robotics, navigation, computer vision, indoor navigation, outdoor navigation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206451606,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "90a51a695300a6582490e1a599e5b87606fcc47f",
            "isKey": false,
            "numCitedBy": 322,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The Ralph vision system helps automobile drivers steer, by sampling an image, assessing the road curvature, and determining the lateral offset of the vehicle relative to the lane center. Ralph has performed well under extensive tests, including a coast-to-coast, 2,850-mile drive."
            },
            "slug": "Rapidly-Adapting-Machine-Vision-for-Automated-Pomerleau-Jochem",
            "title": {
                "fragments": [],
                "text": "Rapidly Adapting Machine Vision for Automated Vehicle Steering"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The Ralph vision system helps automobile drivers steer, by sampling an image, assessing the road curvature, and determining the lateral offset of the vehicle relative to the lane center."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Expert"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2915583"
                        ],
                        "name": "E. Dickmanns",
                        "slug": "E.-Dickmanns",
                        "structuredName": {
                            "firstName": "Ernst",
                            "lastName": "Dickmanns",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Dickmanns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "95650690"
                        ],
                        "name": "A. Zapp",
                        "slug": "A.-Zapp",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Zapp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zapp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Equally impressive progress has been achieved in computer vision for outdoor robotics, as represented by the NAVLAB system [142], [143], [124], [146], [126], [128], the work on vision-guided road-following for \u201dAutobahns\u201d [36], [37], [35], [34], and the Prometheus system [46], [47], [48], [50], [49], [129]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Another success story in outdoor navigation is the research in road-following carried out for \u201cAutobahns\u201d [36], [37] and all the derived work that came from that [35], [34], including the EUREKA-project \u201cPrometheus\u201d [46], [47], [48], [50], [49], [129]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 109580419,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "7899cd7e9bedf39c611c46780904eba524c429f8",
            "isKey": false,
            "numCitedBy": 238,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "AUTONOMOUS-HIGH-SPEED-ROAD-VEHICLE-GUIDANCE-BY-Dickmanns-Zapp",
            "title": {
                "fragments": [],
                "text": "AUTONOMOUS HIGH SPEED ROAD VEHICLE GUIDANCE BY COMPUTER VISION"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580436"
                        ],
                        "name": "K. M. Andress",
                        "slug": "K.-M.-Andress",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Andress",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M. Andress"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703247"
                        ],
                        "name": "A. Kak",
                        "slug": "A.-Kak",
                        "structuredName": {
                            "firstName": "Avinash",
                            "lastName": "Kak",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kak"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60449998,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "63fe42c3d788a4539d9437ec9b857e9fc702b522",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A fundamental goal of computer vision is the development of systems capable of carrying out scene interpretation while taking into account all the available knowledge. In this article, we focus on how the interpretation task can be aided by the expected scene information (such as map knowledge), which, in most cases, would not be in registration with the perceived scene. The proposed approach is applicable to the interpretation of scenes with three-dimensional structures as long as it is possible to generate the equivalent two-dimensional orthogonal or perspective projections of the structures in the expected scene. The system is implemented as a two-panel, six-level blackboard and uses the Dempster-Shafer formalism to accomplish inexact reasoning in a hierarchical space. Inexact reasoning involves exploiting, at different levels of abstraction, any internal geometric consistencies in the data and between the data and the expected scene. As they are discovered, these consistencies are used to update the system's belief in associating a data element with a particular entity from the expected scene."
            },
            "slug": "Evidence-accumulation-&-flow-of-control-Andress-Kak",
            "title": {
                "fragments": [],
                "text": "Evidence accumulation & flow of control"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This article focuses on how the interpretation task can be aided by the expected scene information (such as map knowledge), which, in most cases, would not be in registration with the perceived scene."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144628595"
                        ],
                        "name": "S. Argamon",
                        "slug": "S.-Argamon",
                        "structuredName": {
                            "firstName": "Shlomo",
                            "lastName": "Argamon",
                            "middleNames": [
                                "Engelson"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Argamon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145313556"
                        ],
                        "name": "D. McDermott",
                        "slug": "D.-McDermott",
                        "structuredName": {
                            "firstName": "Drew",
                            "lastName": "McDermott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. McDermott"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "The occupancy-grid approach to map learning is to be contrasted with the approaches that create topological representations of space [22], [23], [39], [74], [87], [97], [123], [161], [164]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 38811217,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d76560825a5cdc8afe49fc30f1aa3c2e16b564b8",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "An issue that must be addressed in map-learning systems is that of error accumulation. The primary emphasis in the literature has been on reducing errors entering the map. The authors suggest that this methodology must reach a point of diminishing returns, and hence focus on explicit error detection and correction. By identifying the possible types of mapping errors, structural constraints can be exploited to detect and diagnose mapping errors. Such robust mapping requires little overhead beyond that needed for nonrobust mapping. A mapping system was implemented based on those ideas. Extensive testing in simulation demonstrated the effectiveness of the proposed error-correction strategies.<<ETX>>"
            },
            "slug": "Error-correction-in-mobile-robot-map-learning-Argamon-McDermott",
            "title": {
                "fragments": [],
                "text": "Error correction in mobile robot map learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The authors suggest that this methodology must reach a point of diminishing returns, and hence focus on explicit error detection and correction, and suggest that robust mapping requires little overhead beyond that needed for nonrobust mapping."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1992 IEEE International Conference on Robotics and Automation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715427"
                        ],
                        "name": "G. Welch",
                        "slug": "G.-Welch",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Welch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Welch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144052443"
                        ],
                        "name": "G. Bishop",
                        "slug": "G.-Bishop",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Bishop",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Bishop"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 215767582,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "54c7c5fdb9b58be7a0b959d8e625b3489229ad3b",
            "isKey": false,
            "numCitedBy": 2499,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "In 1960, R.E. Kalman published his famous paper describing a recursive solution to the discrete-data linear filtering problem. Since that time, due in large part to advances in digital computing, the Kalman filter has been the subject of extensive research and application, particularly in the area of autonomous or assisted navigation. The Kalman filter is a set of mathematical equations that provides an efficient computational (recursive) means to estimate the state of a process, in a way that minimizes the mean of the squared error. The filter is very powerful in several aspects: it supports estimations of past, present, and even future states, and it can do so even when the precise nature of the modeled system is unknown. The purpose of this paper is to provide a practical introduction to the discrete Kalman filter. This introduction includes a description and some discussion of the basic discrete Kalman filter, a derivation, description and some discussion of the extended Kalman filter, and a relatively simple (tangible) example with real numbers & results."
            },
            "slug": "An-Introduction-to-Kalman-Filter-Welch-Bishop",
            "title": {
                "fragments": [],
                "text": "An Introduction to Kalman Filter"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A practical introduction to the discrete Kalman filter, a set of mathematical equations that provides an efficient computational means to estimate the state of a process, in a way that minimizes the mean of the squared error."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH 2001"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783759"
                        ],
                        "name": "T. Jochem",
                        "slug": "T.-Jochem",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Jochem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jochem"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Mobile robotics, navigation, computer vision, indoor navigation, outdoor navigation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26763706,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44a9ab39fd4ce6387c0a3f3aeb576c3bca17ed76",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : ALVINN is a simulated neural network for road following. In its most basic form, it is trained to take a subsampled, preprocessed video image as input, and produce a steering wheel position as output. ALVINN has demonstrated robust performance in a wide variety of situations, but is limited due to its lack of geometric models. Grafting geometric reasoning onto a non-geometric base would be difficult and would create a system with diluted capabilities. A much better approach is to leave the basic neural network intact, preserving its real-time performance and generalization capabilities, and to apply geometric transformations to the input image and the output steering vector. These transformations form a new set of tools and techniques called Virtual Active Vision. The thesis for this work is: Virtual Active Vision tools will improve the capabilities of neural network based autonomous driving systems."
            },
            "slug": "USING-VIRTUAL-ACTIVE-VISION-TOOLS-TO-IMPROVE-TASKS-Jochem",
            "title": {
                "fragments": [],
                "text": "USING VIRTUAL ACTIVE VISION TOOLS TO IMPROVE AUTONOMOUS DRIVING TASKS"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Virtual Active Vision tools will improve the capabilities of neural network based autonomous driving systems by applying geometric transformations to the input image and the output steering vector."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30385977"
                        ],
                        "name": "T. Tsumura",
                        "slug": "T.-Tsumura",
                        "structuredName": {
                            "firstName": "Toshihiro",
                            "lastName": "Tsumura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tsumura"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 26492901,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "83293326a8b504345c00ef883a8ed30bce358774",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent trends and advances of automated guidance and positioning techniques and systems are surveyed, especially good for application to automated factory of many kinds in Japan."
            },
            "slug": "Survey-of-automated-guided-vehicle-in-a-Japanese-Tsumura",
            "title": {
                "fragments": [],
                "text": "Survey of automated guided vehicle in a Japanese factory"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Recent trends and advances of automated guidance and positioning techniques and systems are surveyed, especially good for application to automated factory of many kinds in Japan."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1986 IEEE International Conference on Robotics and Automation"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059306616"
                        ],
                        "name": "David Pierce",
                        "slug": "David-Pierce",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pierce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Pierce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145585296"
                        ],
                        "name": "B. Kuipers",
                        "slug": "B.-Kuipers",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Kuipers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kuipers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 169
                            }
                        ],
                        "text": "The occupancy-grid approach to map learning is to be contrasted with the approaches that create topological representations of space [22], [23], [39], [74], [87], [97], [123], [161], [164]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5991764,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd301db4541d81999c184fe9b6167fd5df94ff86",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Using the methods demonstrated in this paper, a robot with an unknown sensorimotor system can learn sets of features and behaviors adequate to explore a continuous environment and abstract it to a finite-state automaton. The structure of this automaton can then be learned from experience, and constitutes a cognitive map of the environment. A generate-and-test method is used to define a hierarchy of features defined on the raw sense vector culminating in a set of continuously differentiable local state variables. Control laws based on these local state variables are defined for robustly following paths that implement repeatable state transitions. These state transitions are the basis for a finite-state automaton, a discrete abstraction of the robot's continuous world. A variety of existing methods can learn the structure of the automaton defined by the resulting states and transitions. A simple example of the performance of our implemented system is presented."
            },
            "slug": "Learning-to-Explore-and-Build-Maps-Pierce-Kuipers",
            "title": {
                "fragments": [],
                "text": "Learning to Explore and Build Maps"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "Using the methods demonstrated in this paper, a robot with an unknown sensorimotor system can learn sets of features and behaviors adequate to explore a continuous environment and abstract it to a finite-state automaton, a discrete abstraction of the robot's continuous world."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47913574"
                        ],
                        "name": "S. Tsugawa",
                        "slug": "S.-Tsugawa",
                        "structuredName": {
                            "firstName": "Sadayuki",
                            "lastName": "Tsugawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Tsugawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1993172"
                        ],
                        "name": "T. Yatabe",
                        "slug": "T.-Yatabe",
                        "structuredName": {
                            "firstName": "Teruo",
                            "lastName": "Yatabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Yatabe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49439068"
                        ],
                        "name": "Takeshi Hirose",
                        "slug": "Takeshi-Hirose",
                        "structuredName": {
                            "firstName": "Takeshi",
                            "lastName": "Hirose",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takeshi Hirose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041036"
                        ],
                        "name": "S. Matsumoto",
                        "slug": "S.-Matsumoto",
                        "structuredName": {
                            "firstName": "Shuntetsu",
                            "lastName": "Matsumoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Matsumoto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[148] for a car that could drive autonomously, albeit under a highly constrained set of conditions, at 30 km/hr."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12694776,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81613dd7e7e06a9d15ae4eb97e2e7f141f2a8c76",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an automobile with artificial intelligence, which consists of a road pattern recognition unit and a problem solving unit. The vehicle is completely autonomous and can be driven without a human driver. The road pattern recognition unit involving a pair of TV cameras and a processing unit identifies obstacles in front of the vehicle and outputs data regarding to the locations of the obstacles. The problem solving unit is a microcomputer system and determines control optimal to the environment around the vehicle based on the data. The algorithm employed in it is a table-look-up method, in which the location of the optimal control is addressed in the table by key words generated from the data. The table was heuristically made by means of digital simulation. The vehicle was successfully driven under various road environments at the speed within 30 Km/h."
            },
            "slug": "An-Automobile-with-Artificial-Intelligence-Tsugawa-Yatabe",
            "title": {
                "fragments": [],
                "text": "An Automobile with Artificial Intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "An automobile with artificial intelligence, which consists of a road pattern recognition unit and a problem solving unit, which is completely autonomous and can be driven without a human driver is described."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782162"
                        ],
                        "name": "L. Matthies",
                        "slug": "L.-Matthies",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Matthies",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Matthies"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1826964"
                        ],
                        "name": "T. Balch",
                        "slug": "T.-Balch",
                        "structuredName": {
                            "firstName": "Tucker",
                            "lastName": "Balch",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Balch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145228535"
                        ],
                        "name": "B. Wilcox",
                        "slug": "B.-Wilcox",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Wilcox",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Wilcox"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "project [158], [99], [101]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1892807,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "917af2828e3ea2692d29873a257300a783b1fc74",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "A new laser-based optical sensor system that provides hazard detection for planetary rovers is presented. The sensor can support safe travel at speeds up to 12 cm/second for large (1 m) rovers in full sunlight on Earth or Mars. This is at least a 5 times improvement over the sensor aboard NASA's Mars Pathfinder rover. The system overcomes limitations in the older design that require image differencing to detect a laser stripe in full sun. The new system ensures the projected laser light is detectable in a single image, eliminating the requirement for additional difference images. The improvement is significant since any reduction in image gathering or processing time provides for faster rover motion. The savings are even more important in the case of a Mars rover since power and radiation-hardening requirements lead to severely constrained computational resources. The paper includes a thorough discussion of design details and tradeoffs for optical hazard sensing that will benefit future efforts in this area."
            },
            "slug": "Fast-optical-hazard-detection-for-planetary-rovers-Matthies-Balch",
            "title": {
                "fragments": [],
                "text": "Fast optical hazard detection for planetary rovers using multiple spot laser triangulation"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A new laser-based optical sensor system that provides hazard detection for planetary rovers is presented that can support safe travel at speeds up to 12 cm/second for large (1 m) rovers in full sunlight on Earth or Mars."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of International Conference on Robotics and Automation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090818"
                        ],
                        "name": "M. Isard",
                        "slug": "M.-Isard",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Isard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Isard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8369379,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c5a0951cea300222834497c8e12ac2be99cd11e",
            "isKey": false,
            "numCitedBy": 1435,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of tracking curves in dense visual clutter is a challenging one. Trackers based on Kalman filters are of limited use; because they are based on Gaussian densities which are unimodal, they cannot represent simultaneous alternative hypotheses. Extensions to the Kalman filter to handle multiple data associations work satisfactorily in the simple case of point targets, but do not extend naturally to continuous curves. A new, stochastic algorithm is proposed here, the Condensation algorithm \u2014 Conditional Density Propagation over time. It uses \u2018factored sampling\u2019, a method previously applied to interpretation of static images, in which the distribution of possible interpretations is represented by a randomly generated set of representatives. The Condensation algorithm combines factored sampling with learned dynamical models to propagate an entire probability distribution for object position and shape, over time. The result is highly robust tracking of agile motion in clutter, markedly superior to what has previously been attainable from Kalman filtering. Notwithstanding the use of stochastic methods, the algorithm runs in near real-time."
            },
            "slug": "Contour-Tracking-by-Stochastic-Propagation-of-Isard-Blake",
            "title": {
                "fragments": [],
                "text": "Contour Tracking by Stochastic Propagation of Conditional Density"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The Condensation algorithm combines factored sampling with learned dynamical models to propagate an entire probability distribution for object position and shape, over time, and is markedly superior to what has previously been attainable from Kalman filtering."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49325667"
                        ],
                        "name": "H. Kamada",
                        "slug": "H.-Kamada",
                        "structuredName": {
                            "firstName": "Hiroshi",
                            "lastName": "Kamada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kamada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1962601"
                        ],
                        "name": "M. Yoshida",
                        "slug": "M.-Yoshida",
                        "structuredName": {
                            "firstName": "Masumi",
                            "lastName": "Yoshida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yoshida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58598330,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c71b35ae3fa388fd531efa886153d658f8326e13",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "We developed a visual control system for an unmanned vehicle. The system consists of a dynamic image processor and a fuzzy logic control mechanism. It quickly recognizes markers lined along a road and thereby navigates a driverless vehicle. The markers are detected in real time by pipeline processing in the color identification processor and logical filter; the marker sequence is recognized by an improved Hough transform, then the fuzzy logic control mechanism decides the steering angle. To use the information on the movement of the vehicle, we constructed fuzzy inference rules on how position changes with time. We developed an LSI (large-scale integrated circuit) chip for the logical filter to realize a very compact and practical system (23 \u00d7 30 \u00d7 9.5 cm). We mounted this system on a vehicle, and it successfully drove around a test track."
            },
            "slug": "A-visual-control-system-using-image-processing-and-Kamada-Yoshida",
            "title": {
                "fragments": [],
                "text": "A visual control system using image processing and fuzzy theory"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "An LSI (large-scale integrated circuit) chip for the logical filter to realize a very compact and practical system (23 \u00d7 30 \u00d7 9.5 cm) and constructed fuzzy inference rules on how position changes with time."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144292444"
                        ],
                        "name": "Mei Chen",
                        "slug": "Mei-Chen",
                        "structuredName": {
                            "firstName": "Mei",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mei Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783759"
                        ],
                        "name": "T. Jochem",
                        "slug": "T.-Jochem",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Jochem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jochem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48855558"
                        ],
                        "name": "D. Pomerleau",
                        "slug": "D.-Pomerleau",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Pomerleau",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pomerleau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Mobile robotics, navigation, computer vision, indoor navigation, outdoor navigation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 351707,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b7ff414fd62b5dfb0854c177901d9668a606865",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "AURORA is a vision-based system designed to warn a vehicle driver of possible impending roadway departure accidents. It employs a downward looking color video camera with a wide angle lens, a digitizer, and a portable Sun Sparc workstation. Using a binormalized adjustable template correlation algorithm, it reliably detects lane markers on structured roads at 60 Hz. A time-to-lane-crossing (TLC) measurement is calculated for each image based on the estimation of vehicle's lateral position and velocity. This measurement is used to trigger an alarm when the TLC falls below a preset threshold. Promising results have been achieved under a variety of weather and lighting conditions, on many road types."
            },
            "slug": "AURORA:-a-vision-based-roadway-departure-warning-Chen-Jochem",
            "title": {
                "fragments": [],
                "text": "AURORA: a vision-based roadway departure warning system"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "AURORA is a vision-based system designed to warn a vehicle driver of possible impending roadway departure accidents, using a downward looking color video camera with a wide angle lens, a digitizer, and a portable Sun Sparc workstation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human Robot Interaction and Cooperative Robots"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090818"
                        ],
                        "name": "M. Isard",
                        "slug": "M.-Isard",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Isard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Isard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6821810,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "963dddc907f56bd1d6c98dd40f560eb8786e49ea",
            "isKey": false,
            "numCitedBy": 5523,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of tracking curves in dense visual clutter is challenging. Kalman filtering is inadequate because it is based on Gaussian densities which, being unimo dal, cannot represent simultaneous alternative hypotheses. The Condensation algorithm uses \u201cfactored sampling\u201d, previously applied to the interpretation of static images, in which the probability distribution of possible interpretations is represented by a randomly generated set. Condensation uses learned dynamical models, together with visual observations, to propagate the random set over time. The result is highly robust tracking of agile motion. Notwithstanding the use of stochastic methods, the algorithm runs in near real-time."
            },
            "slug": "CONDENSATION\u2014Conditional-Density-Propagation-for-Isard-Blake",
            "title": {
                "fragments": [],
                "text": "CONDENSATION\u2014Conditional Density Propagation for Visual Tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The Condensation algorithm uses \u201cfactored sampling\u201d, previously applied to the interpretation of static images, in which the probability distribution of possible interpretations is represented by a randomly generated set."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2756158"
                        ],
                        "name": "E. Chown",
                        "slug": "E.-Chown",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Chown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Chown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26525112"
                        ],
                        "name": "S. Kaplan",
                        "slug": "S.-Kaplan",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Kaplan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kaplan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689716"
                        ],
                        "name": "D. Kortenkamp",
                        "slug": "D.-Kortenkamp",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kortenkamp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kortenkamp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 139
                            }
                        ],
                        "text": "The occupancy-grid approach to map learning is to be contrasted with the approaches that create topological representations of space [22], [23], [39], [74], [87], [97], [123], [161], [164]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7904839,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "db9a5e34cf6451f55ffeb88d16890c67e6ac73a9",
            "isKey": false,
            "numCitedBy": 223,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "An integrated representation of large-scale space, or cognitive map, called PLAN, is presented that attempts to address a broader spectrum of issues than has been previously attempted in a single model. Rather than examining woyfinding OS ct process separate from the rest of cognition, one of the fundamental goals of this work is to examine how the wayfinding process is integrated into general cognition. One result of this approach is that the model is \u201cheads-up,\u201d or scene-based, because it takes advantage of the properties of the human visual system and, particularly, the visual system\u2019s split into two pathways. The emphasis on the human location or \u201cwhere\u201d system is new to cognitive mapping and is part of an attempt to synthesize prototype theory, associative networks and location together in a connectionist system. Not all of PLAN is new, however. Many of its parts have anologues in one or another preexisting theory. What makes PLAN unique is integrating the various components into a coherent whole, and the capacity of this resulting system to speak to o wide range of constraints. Our approach emphasizes adaptiveness: thus, our focus on such issues as ease of use and efficiency of learning. The result is a model that has a stronger relationship both to the environment, and to the ways thot humans interact with it, compared with previous models. The resulting model is examined in some detail and compared to other systems."
            },
            "slug": "Prototypes,-Location,-and-Associative-Networks-a-of-Chown-Kaplan",
            "title": {
                "fragments": [],
                "text": "Prototypes, Location, and Associative Networks (PLAN): Towards a Unified Theory of Cognitive Mapping"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "Tour de France has a global overview that can quickly yield the relationships between distant landmarks, and therefore can be used as a guide to explore the relationship between these landmarks."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35140681"
                        ],
                        "name": "P. Backes",
                        "slug": "P.-Backes",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Backes",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Backes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4286954"
                        ],
                        "name": "K. Tso",
                        "slug": "K.-Tso",
                        "structuredName": {
                            "firstName": "Kam",
                            "lastName": "Tso",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15494973"
                        ],
                        "name": "G. Tharp",
                        "slug": "G.-Tharp",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Tharp",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tharp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "In goal designation, human operators at the mission control used a Web-based tool [8] to specify waypoints in 3D views of the landing site that were generated from images obtained using the stereo cameras on the lander."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7410906,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "105e337a7445d43e3b8015281fc15f5ab5f68d45",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The Web Interface for Telescience (WITS) is an Internet-based tool that the Mars Pathfinder mission used for both mission operations at JPL and public outreach. WITS enables the viewing of downlinked images and results in various ways, terrain feature measurement and annotation, and planning of daily mission activities. WITS is written in the Java language and is accessible by mission scientists and the general public via a web browser. The public can use WITS to plan and simulate their own rover missions. WITS will also be used in the 1998 lander and 2001, 2003, and 2005 rover missions to Mars."
            },
            "slug": "Mars-pathfinder-mission-Internet-based-operations-Backes-Tso",
            "title": {
                "fragments": [],
                "text": "Mars pathfinder mission Internet-based operations using WITS"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "The Web Interface for Telescience (WITS) is an Internet-based tool that the Mars Pathfinder mission used for both mission operations at JPL and public outreach and the public can use WITS to plan and simulate their own rover missions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE International Conference on Robotics and Automation (Cat. No.98CH36146)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709512"
                        ],
                        "name": "L. Kaelbling",
                        "slug": "L.-Kaelbling",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Kaelbling",
                            "middleNames": [
                                "Pack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaelbling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2453007"
                        ],
                        "name": "A. Cassandra",
                        "slug": "A.-Cassandra",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Cassandra",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cassandra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "us a powerful formalism for dealing with this issue [67]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5613003,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "116d7798c1123cf7fad4176e98f58fd49de4f8f1",
            "isKey": false,
            "numCitedBy": 3954,
            "numCiting": 137,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Planning-and-Acting-in-Partially-Observable-Domains-Kaelbling-Littman",
            "title": {
                "fragments": [],
                "text": "Planning and Acting in Partially Observable Stochastic Domains"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1963058"
                        ],
                        "name": "N. Sawasaki",
                        "slug": "N.-Sawasaki",
                        "structuredName": {
                            "firstName": "Naoyuki",
                            "lastName": "Sawasaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Sawasaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40588774"
                        ],
                        "name": "T. Morita",
                        "slug": "T.-Morita",
                        "structuredName": {
                            "firstName": "Toshihiko",
                            "lastName": "Morita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Morita"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46322703"
                        ],
                        "name": "T. Uchiyama",
                        "slug": "T.-Uchiyama",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Uchiyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Uchiyama"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 104
                            }
                        ],
                        "text": "For both landmark tracking and obstacle detection, the system uses a dedicated piece of hardware called TRV ([132]) that is capable of calculating local correlations of 250 pairs of image regions at 30fps. Localization is achieved by a multiple landmark detection algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 109
                            }
                        ],
                        "text": "For both landmark tracking and obstacle detection, the system uses a dedicated piece of hardware called TRV ([132]) that is capable of calculating local correlations of 250 pairs of image regions at 30fps."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 47
                            }
                        ],
                        "text": "Obstacle detection is also performed using the TRV."
                    },
                    "intents": []
                }
            ],
            "corpusId": 31354197,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "611f7a2ad08bc38e8fcc0770840cfce5b5a0a838",
            "isKey": true,
            "numCitedBy": 25,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the design and the implementation of a high-speed visual tracking system based on local correlation matching. The system consists of only two PC board modules, which conform to VME bus specifications. The video module is used to input and output video signals, while the tracking module is used to perform visual tracking with the local correlation method. The tracking module contains three frame memories, an image flow controller, a DMA controller for image data transfer, and a dedicated LSI to perform the correlation matching of 8-bit gray-scale images. The processing time is 110 microseconds for an 8/spl times/8 pixel template and 290 microseconds for a 16/spl times/16 pixel template. This paper also describes some of our experimental implementations, including human tracking, recognition of hand movements and signs, and traffic monitoring. Compared to conventional general-purpose image processing systems, our hardware system is compact and inexpensive. This high-performance system can be used for a wide variety of motion analysis applications."
            },
            "slug": "Design-and-implementation-of-high-speed-visual-for-Sawasaki-Morita",
            "title": {
                "fragments": [],
                "text": "Design and implementation of high-speed visual tracking system for real-time motion analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This paper describes the design and the implementation of a high-speed visual tracking system based on local correlation matching, which consists of only two PC board modules, which conform to VME bus specifications."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 13th International Conference on Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580436"
                        ],
                        "name": "K. M. Andress",
                        "slug": "K.-M.-Andress",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Andress",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M. Andress"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703247"
                        ],
                        "name": "A. Kak",
                        "slug": "A.-Kak",
                        "structuredName": {
                            "firstName": "Avinash",
                            "lastName": "Kak",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kak"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "described in [1], [69] which used evidential reasoning for image interpretation; the system presented by Tsubouchi and"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1898533,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "554e050d612cc88896939f7c33d44bea642d3b32",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "........ ................................... ............................................................... CHAPTER"
            },
            "slug": "Evidence-Accumulation-and-Flow-of-Control-in-a-Andress-Kak",
            "title": {
                "fragments": [],
                "text": "Evidence Accumulation and Flow of Control in a Hierarchical Spatial Reasoning System"
            },
            "venue": {
                "fragments": [],
                "text": "AI Mag."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31211229"
                        ],
                        "name": "R. K\u00e1lm\u00e1n",
                        "slug": "R.-K\u00e1lm\u00e1n",
                        "structuredName": {
                            "firstName": "Rudolf",
                            "lastName": "K\u00e1lm\u00e1n",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. K\u00e1lm\u00e1n"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1242324,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "255a77422b1da74da05d1714b7875356187385bd",
            "isKey": false,
            "numCitedBy": 12015,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "A unitary, lightweight outer garment constructed of a thin polyethylene film includes front and rear panels which are joined together forming a medial body member, paired arms which extend outwardly and downwardly from the upper portion of the body member, and a head opening which is located in the upper margin of the body member. The arms and body member are arranged to have unjoined coplanar lower margins and the inner side margin of each arm and the adjacent side margin of the body member lie congruously along a common line. The garment is formed by placing two rectangular sheets of the film, having a width equal to the finished length of the garment, in overlying engagement with one another on a cutting surface, thermally die cutting the sheets into the appropriate shape, and thermally sealing the resulting cut margins to complete the garment."
            },
            "slug": "A-new-approach-to-linear-filtering-and-prediction-K\u00e1lm\u00e1n",
            "title": {
                "fragments": [],
                "text": "A new approach to linear filtering and prediction problems\" transaction of the asme~journal of basic"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A unitary, lightweight outer garment constructed of a thin polyethylene film includes front and rear panels which are joined together forming a medial body member, paired arms which extend outwardly and downwardly from the upper portion of the body member and a head opening which is located in the upper margin of theBody member."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1960
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145634767"
                        ],
                        "name": "Din-Chang Tseng",
                        "slug": "Din-Chang-Tseng",
                        "structuredName": {
                            "firstName": "Din-Chang",
                            "lastName": "Tseng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Din-Chang Tseng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70448116"
                        ],
                        "name": "Chip-Hong Chang",
                        "slug": "Chip-Hong-Chang",
                        "structuredName": {
                            "firstName": "Chip-Hong",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chip-Hong Chang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Mobile robotics, navigation, computer vision, indoor navigation, outdoor navigation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62725912,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "7306a6680cc6252e77f4b9794f8c770a90cf1497",
            "isKey": false,
            "numCitedBy": 125,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A general approach for achieving color image segmentation using uniform-chromaticity-scale perceptual color attributes is proposed. At first chromatic and achromatic areas in a perceptual IHS color space are defined. Then the image is separated into chromatic and achromatic regions according to the region locations in the color space. 1-D histogram thresholding for each color attribute is performed to split the chromatic and achromatic regions, respectively. Finally the region growing is used to solve the oversegmentation problem. In an experiment the power of the proposed approach is demonstrated.<<ETX>>"
            },
            "slug": "Color-segmentation-using-perceptual-attributes-Tseng-Chang",
            "title": {
                "fragments": [],
                "text": "Color segmentation using perceptual attributes"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A general approach for achieving color image segmentation using uniform-chromaticity-scale perceptual color attributes is proposed and the region growing is used to solve the oversegmentation problem."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings., 11th IAPR International Conference on Pattern Recognition. Vol. III. Conference C: Image, Speech and Signal Analysis,"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116470384"
                        ],
                        "name": "Takayuki Nakamura",
                        "slug": "Takayuki-Nakamura",
                        "structuredName": {
                            "firstName": "Takayuki",
                            "lastName": "Nakamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takayuki Nakamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144657032"
                        ],
                        "name": "M. Asada",
                        "slug": "M.-Asada",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Asada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Asada"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 114
                            }
                        ],
                        "text": "The reader is also referred to attempts at behavior-based approaches to vision-based navigation in mapless spaces [111], [112], [56]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14599770,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "ec1e01237d1067ba893ee0e9d8606ed954e3aa06",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "\u8996\u899a\u3092\u6301\u3064\u79fb\u52d5\u30ed\u30dc\u30c3\u30c8\u304c,\u8907\u96d1\u306a\u74b0\u5883\u5185\u3092\u81ea\u5f8b\u7684\u306b\u8d70 \u884c\u3059\u308b\u305f\u3081\u306b\u306f,\u74b0\u5883\u306b\u9069\u5fdc\u3057\u3066\u884c\u52d5\u3092\u6c7a\u5b9a\u3059\u308b\u3053\u3068\u304c\u91cd\u8981 \u3067\u3042\u308b.\u305d\u306e\u305f\u3081\u306b\u306f,\u30ed\u30dc\u30c3\u30c8\u81ea\u8eab\u304c\u74b0\u5883\u306b\u95a2\u3059\u308b\u4f55\u3089\u304b \u306e\u30e2\u30c7\u30eb\u3092\u6301\u3063\u3066\u3044\u308b\u5fc5\u8981\u304c\u3042\u308b. \u5f93\u6765\u304b\u3089\u306e\u7814\u7a76\u306b\u304a\u3044\u3066\u306f,\u30ed\u30dc\u30c3\u30c8\u4e0a\u306e\u8996\u899a\u30bb\u30f3\u30b5\u304b \u3089\u306e2\u6b21\u5143\u753b\u50cf\u60c5\u5831\u3092\u51e6\u7406\u3057\u3066\u8a73\u7d30\u306a3\u6b21\u5143\u5e7e\u4f55\u60c5\u5831\u3092\u518d\u69cb \u6210\u3057,\u3053\u306e3\u6b21\u5143\u60c5\u5831\u3092\u7528\u3044\u3066\u30ed\u30dc\u30c3\u30c8\u306e\u53d6\u308a\u5dfb\u304f\u74b0\u5883\u3092\u8868 \u73fe\u3057\u3066\u3044\u308b\u7814\u7a76\u4f8b\u304c\u591a\u3044.\u3053\u306e\u69d8\u306a\u624b\u6cd5\u306f,\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf \u30d3\u30b8\u30e7\u30f3\u306e\u5206\u91ce\u306b\u304a\u3044\u3066,Ullman1)\u306b\u3088\u3063\u3066\u201dshape from motion\u201d\u306e\u554f\u984c\u304c\u5b9a\u5f0f\u5316\u3055\u308c\u3066\u4ee5\u6765,\u591a\u304f\u306e\u7814\u7a76\u304c\u306a\u3055\u308c\u3066 \u3044\u308b.\u6700\u8fd1\u306e\u7814\u7a76\u3067\u306f,2\u6b21\u5143\u753b\u50cf\u7cfb\u5217\u304b\u30892, 3),\u30a2\u30d5\u30a3\u30f3 \u307e\u305f\u306f\u6295\u5f71\u4e0d\u5909\u91cf\u304b\u30894, 5, 6, 7),\u30ab\u30e1\u30e9\u30ad\u30e3\u30ea\u30d6\u30ec\u30fc\u30b7\u30e7 \u30f3\u304b\u30898),\u305d\u308c\u305e\u308c3\u6b21\u5143\u60c5\u5831\u3092\u6c42\u3081\u308b\u7814\u7a76\u304c\u3042\u308b.\u3053\u308c\u3089 \u306e\u7814\u7a76\u3067\u306f,3\u6b21\u5143\u60c5\u5831\u3084\u305d\u306e\u4e0d\u5909\u91cf\u3092\u3067\u304d\u308b\u3060\u3051\u6b63\u78ba\u306b\u6c42 \u3081\u308b\u3053\u3068\u3092\u76ee\u7684\u3068\u3057\u3066\u3044\u308b.\u3057\u304b\u3057\u306a\u304c\u3089,\u3069\u308c\u304f\u3089\u3044\u306e\u518d \u69cb\u6210\u306e\u7cbe\u5ea6\u304c\u5fc5\u8981\u3067\u3042\u308b\u304b\u3092\u6c7a\u3081\u308b\u3053\u3068\u306f\u96e3\u3057\u3044.3\u6b21\u5143\u60c5 \u5831\u306f\u4e00\u822c\u7684\u306a\u5f62\u5f0f\u3092\u3057\u3066\u304a\u308a\u305d\u306e\u60c5\u5831\u3092\u4f7f\u7528\u3059\u308b\u969b\u306b\u7c21\u5358\u306b \u5909\u63db\u304c\u53ef\u80fd\u3067\u3042\u308b\u3068\u3057\u3066,3\u6b21\u5143\u60c5\u5831\u3092\u3082\u3068\u306b\u3057\u305f\u74b0\u5883\u306e\u8868 \u73fe\u304c,\u30ed\u30dc\u30c3\u30c8\u306e\u30ca\u30d3\u30b2\u30fc\u30b7\u30e7\u30f3\u7b49\u306b\u6709\u52b9\u3067\u3042\u308b\u3068\u4e00\u822c\u7684\u306b \u8a00\u308f\u308c\u3066\u3044\u308b.\u3057\u304b\u3057\u306a\u304c\u3089,\u30ed\u30dc\u30c3\u30c8\u3092\u4f7f\u7528\u3059\u308b\u969b\u306b\u306f, \u30bb\u30f3\u30b5\u60c5\u5831\u3084\u30e2\u30fc\u30bf\u30fc\u5236\u5fa1\u306b\u5bfe\u3057\u3066\u5b9f\u6642\u9593\u51e6\u7406\u3057\u306a\u3051\u308c\u3070 \u306a\u3089\u306a\u3044.\u3055\u3089\u306b,\u76ee\u7684\u3068\u3059\u308b\u30bf\u30b9\u30af\u306b\u4f9d\u5b58\u3057\u3066\u30ed\u30dc\u30c3\u30c8\u3092 \u53d6\u308a\u5dfb\u304f\u74b0\u5883\u5168\u4f53\u3092\u8868\u73fe\u3059\u308b\u5fc5\u8981\u306f\u306a\u304f,\u307e\u305f\u898b\u3048\u65b9\u306e\u7570\u306a \u308b\u74b0\u5883\u3092\u5fc5\u305a\u3057\u3082\u533a\u5225\u3059\u308b\u5fc5\u8981\u3082\u306a\u3044.\u4f8b\u3048\u3070, \u305f\u3068\u3048\u3070, \u969c\u5bb3\u7269\u767a\u898b\u3084\u56de\u907f\u3092\u30bf\u30b9\u30af\u3068\u3057\u305f\u5834\u5408,\u673a\u3084\u6905\u5b50\u304c\u4e71\u96d1\u306b\u914d \u7f6e\u3055\u308c\u305f\u5c4b\u5185\u74b0\u5883\u3082,\u5ca9\u306a\u3069\u3092\u542b\u3080\u5c4b\u5916\u74b0\u5883\u3082\u8b58\u5225\u3059\u308b\u5fc5\u8981 \u304c\u306a\u3044.\u3053\u308c\u307e\u3067\u306e3\u6b21\u5143\u5e7e\u4f55\u60c5\u5831\u306e\u518d\u69cb\u6210\u3092\u4e3b\u773c\u3068\u3057\u3066\u6765 \u305f\u30a2\u30d7\u30ed\u30fc\u30c1\u3067\u306f,\u3053\u308c\u3089\u3092\u540c\u4e00\u8996\u3059\u308b\u3053\u3068\u306f\u56f0\u96e3\u3067\u3042\u308b\u3068 \u8003\u3048\u3089\u308c\u308b. \u305d\u3053\u3067,\u8996\u899a\u60c5\u5831\u3092\u7528\u3044\u3066\u4e0e\u3048\u3089\u308c\u305f\u30bf\u30b9\u30af\u3092\u9054\u6210\u3059\u308b \u81ea\u5f8b\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u306b\u3068\u3063\u3066,\u3069\u306e\u3088\u3046\u306a\u74b0\u5883\u8868\u73fe\u304c\u9069\u5f53\u3067\u3042 \u308b\u304b\u306b\u3064\u3044\u3066\u8003\u3048\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044.\u3053\u306e\u554f\u984c\u306b\u5bfe\u3057\u3066,\u30ed \u30dc\u30c3\u30c8\u5b66\u7fd2\u306e\u7814\u7a76\u8005\u306f,\u30ed\u30dc\u30c3\u30c8\u306b\u5bfe\u3057\u3066,\u5916\u754c\u304b\u3089\u77e5\u899a\u3055 \u308c\u305f\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u884c\u52d5\u3059\u308b\u3053\u3068,\u3059\u306a\u308f\u3061,\u74b0\u5883\u72b6\u614b\u3068\u30ed \u30dc\u30c3\u30c8\u81ea\u8eab\u306e\u884c\u52d5\u306e\u6700\u9069\u306a\u95a2\u4fc2\u3092\u5b66\u7fd2\u3055\u305b\u3088\u3046\u3068\u3057\u3066\u304d\u305f 9).\u3053\u306e\u72b6\u614b\u3068\u884c\u52d5\u306e\u6700\u9069\u306a\u95a2\u4fc2\u304c,\u30bf\u30b9\u30af\u3082\u3057\u304f\u306f\u30ed\u30dc\u30c3 \u30c8\u306e\u884c\u52d5\u306b\u57fa\u3065\u304f\u74b0\u5883\u8868\u73fe\u3068\u8003\u3048\u3089\u308c\u308b.\u3053\u308c\u306b\u3088\u3063\u3066,\u30bf \u30b9\u30af\u3092\u9054\u6210\u3059\u308b\u305f\u3081\u306b\u8a73\u7d30\u306a3\u6b21\u5143\u60c5\u5831\u3092\u518d\u69cb\u6210\u3059\u308b\u5fc5\u8981\u304c \u306a\u304f\u306a\u308b.\u3057\u304b\u3057\u306a\u304c\u3089,\u3053\u308c\u307e\u3067\u306e\u30ed\u30dc\u30c3\u30c8\u5b66\u7fd2\u306e\u7814\u7a76\u306b \u304a\u3044\u3066\u306f,\u7279\u5b9a\u306e\u30bf\u30b9\u30af\u306e\u884c\u52d5\u5b66\u7fd2\u3092\u884c\u306a\u3046\u305f\u3081\u306b,\u74b0\u5883\u3092 \u8a18\u8ff0\u3059\u308b\u8a18\u8ff0\u5b50\u306b\u5bfe\u3059\u308b\u5019\u88dc\u306f,\u77e5\u899a\u3055\u308c\u305f\u83ab\u5927\u306a\u30c7\u30fc\u30bf\u306e \u4e2d\u304b\u3089\u524d\u3082\u3063\u3066\u9078\u629e\u3055\u308c\u3066\u3044\u308b.\u307e\u305f\u3053\u308c\u3089\u306e\u8a18\u8ff0\u5b50\u306e\u5019\u88dc \u306f,\u74b0\u5883\u30b7\u30fc\u30f3\u306e\u69cb\u6210\u8981\u7d20\u3084\u7279\u5b9a\u306e\u72b6\u6cc1\u3084\u30bf\u30b9\u30af\u306b\u4f9d\u5b58\u3057\u3066 \u6c7a\u5b9a\u3055\u308c\u3066\u3044\u308b. \u305d\u3053\u3067,\u3053\u306e\u5831\u544a\u3067\u306f,\u74b0\u5883\u30b7\u30fc\u30f3\u306e\u69cb\u6210\u8981\u7d20\u306b\u72ec\u7acb\u3067, \u30e2\u30fc\u30bf\u30b3\u30de\u30f3\u30c9\u3068\u5bc6\u63a5\u306b\u95a2\u9023\u3057\u305f\u30ed\u30d0\u30b9\u30c8\u306a\u8a18\u8ff0\u5b50\u3067\u3042\u308b\u753b \u50cf\u904b\u52d5\u60c5\u5831\u3092\u5229\u7528\u3057\u3066,\u5b9f\u30ed\u30dc\u30c3\u30c8\u306b,\u76ee\u7684\u306e\u30bf\u30b9\u30af\u3092\u9054\u6210 \u3055\u305b\u308b\u305f\u3081\u306e\u884c\u52d5\u306b\u57fa\u3065\u304f\u74b0\u5883\u8868\u73fe\u3092\u7372\u5f97\u3055\u305b\u308b\u624b\u6cd5\u306b\u3064\u3044 \u3066\u8ff0\u3079\u308b.\u3053\u3053\u3067\u306f,\u76ee\u7684\u306e\u30bf\u30b9\u30af\u3092\u9054\u6210\u3059\u308b\u305f\u3081\u306e\u884c\u52d5\u306b \u57fa\u3065\u304f\u74b0\u5883\u8868\u73fe\u3092\u904b\u52d5\u30b9\u30b1\u30c3\u30c1\u3068\u547c\u3093\u3067\u3044\u308b.\u30ed\u30dc\u30c3\u30c8\u306e\u30bf \u30b9\u30af\u3068\u3057\u3066\u306f,\u52d5\u7684\u74b0\u5883\u5185\u3067\u969c\u5bb3\u7269\u3092\u56de\u907f\u3057\u306a\u304c\u3089\u5bfe\u8c61\u7269\u4f53 \u3092\u8ffd\u8de1\u3059\u308b\u3068\u3044\u3046\u30bf\u30b9\u30af\u3092\u60f3\u5b9a\u3057\u3066\u3044\u308b."
            },
            "slug": "Motion-Sketch:-Acquisition-of-Visual-Motion-Guided-Nakamura-Asada",
            "title": {
                "fragments": [],
                "text": "Motion Sketch: Acquisition of Visual Motion Guided Behaviors"
            },
            "tldr": {
                "abstractSimilarityScore": 0,
                "text": "It is confirmed that 3.3% of the population believes in reincarnation, but not all believe in it in the same way."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48855558"
                        ],
                        "name": "D. Pomerleau",
                        "slug": "D.-Pomerleau",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Pomerleau",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pomerleau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Mobile robotics, navigation, computer vision, indoor navigation, outdoor navigation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3667444,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "id": "56679790b9af53da957b30a2acad6f5cbdda425a",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Reliability-estimation-for-neural-network-based-Pomerleau",
            "title": {
                "fragments": [],
                "text": "Reliability estimation for neural network based autonomous driving"
            },
            "venue": {
                "fragments": [],
                "text": "Robotics Auton. Syst."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35949179"
                        ],
                        "name": "Juiyao Pan",
                        "slug": "Juiyao-Pan",
                        "structuredName": {
                            "firstName": "Juiyao",
                            "lastName": "Pan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juiyao Pan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742124"
                        ],
                        "name": "G. DeSouza",
                        "slug": "G.-DeSouza",
                        "structuredName": {
                            "firstName": "Guilherme",
                            "lastName": "DeSouza",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. DeSouza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703247"
                        ],
                        "name": "A. Kak",
                        "slug": "A.-Kak",
                        "structuredName": {
                            "firstName": "Avinash",
                            "lastName": "Kak",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11035614,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed63831d2e69abdba3673983fdb3f0427726dd63",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "There exist in the literature today many contributions dealing with the incorporation of fuzzy logic in expert systems. However, unfortunately, much of what has been proposed can only be applied to small-scale expert systems; that is, when the number of rules is in the dozens as opposed to in the hundreds. The more traditional (nonfuzzy) expert systems are able to cope with large numbers of rules by using Rete networks for maintaining matches of all the rules and all the facts. (A Rete network obviates the need to match the rules with the facts on every cycle of the inference engine.) In this paper, we present a more general Rete network that is particularly suitable for reasoning with fuzzy logic. The generalized Rete network consists of a cascade of three networks: the pattern network, the join network, and the evidence aggregation network. The first two layers are modified versions of similar layers for the traditional Rete networks and the last, the aggregation layer, is a new concept that allows fuzzy evidence to be aggregated when fuzzy inferences are made about the same fuzzy variable by different rules."
            },
            "slug": "FuzzyShell:-a-large-scale-expert-system-shell-using-Pan-DeSouza",
            "title": {
                "fragments": [],
                "text": "FuzzyShell: a large-scale expert system shell using fuzzy logic for uncertainty reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper presents a more general Rete network that is particularly suitable for reasoning with fuzzy logic and consists of a cascade of three networks: the pattern network, the join network, and the evidence aggregation network."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Fuzzy Syst."
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 288,
                                "start": 284
                            }
                        ],
                        "text": "Equally impressive progress has been achieved in computer vision for outdoor robotics, as represented by the NAVLAB system [142], [143], [124], [146], [126], [128], the work on vision-guided road-following for \u201dAutobahns\u201d [36], [37], [35], [34], and the Prometheus system [46], [47], [48], [50], [49], [129]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "imminent danger or even to drive automatically during monotonous periods [47], [48], [49]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 228
                            }
                        ],
                        "text": "Another success story in outdoor navigation is the research in road-following carried out for \u201cAutobahns\u201d [36], [37] and all the derived work that came from that [35], [34], including the EUREKA-project \u201cPrometheus\u201d [46], [47], [48], [50], [49], [129]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Driverless Highway Vehicles"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Int'l Hi-Tech Forum"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 278
                            }
                        ],
                        "text": "Equally impressive progress has been achieved in computer vision for outdoor robotics, as represented by the NAVLAB system [142], [143], [124], [146], [126], [128], the work on vision-guided road-following for \u201dAutobahns\u201d [36], [37], [35], [34], and the Prometheus system [46], [47], [48], [50], [49], [129]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "imminent danger or even to drive automatically during monotonous periods [47], [48], [49]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 222
                            }
                        ],
                        "text": "Another success story in outdoor navigation is the research in road-following carried out for \u201cAutobahns\u201d [36], [37] and all the derived work that came from that [35], [34], including the EUREKA-project \u201cPrometheus\u201d [46], [47], [48], [50], [49], [129]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vision for Autonomous Mobile Robots"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE Workshop Advanced Motion Control"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34767446"
                        ],
                        "name": "J. Borenstein",
                        "slug": "J.-Borenstein",
                        "structuredName": {
                            "firstName": "Johann",
                            "lastName": "Borenstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Borenstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1410861749"
                        ],
                        "name": "Y.",
                        "slug": "Y.",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Y.",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y."
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121277263"
                        ],
                        "name": "Koren",
                        "slug": "Koren",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Koren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Koren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "In some of the reported work [20], the CAD models were replaced by simpler models, such as occupancy maps, topological maps or even sequences of images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 115137279,
            "fieldsOfStudy": [],
            "id": "d20d5455871bedbc1e9dfb1d8272e4669e5e7d4f",
            "isKey": false,
            "numCitedBy": 489,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Real-Time-Obstacle-Avoidance-for-Fast-Mobile-Robots-Borenstein-Y.",
            "title": {
                "fragments": [],
                "text": "Real-Time Obstacle Avoidance for Fast Mobile Robots"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 195702951,
            "fieldsOfStudy": [],
            "id": "e11878b232a604d9e5d049721d2b4cd6075574f2",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blanche: Position Estimation for an Autonomous Robot Vehicle"
            },
            "venue": {
                "fragments": [],
                "text": "Autonomous Robot Vehicles"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34588530"
                        ],
                        "name": "C. Thorpe",
                        "slug": "C.-Thorpe",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Thorpe",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Thorpe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61131489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78a64945af71ef747aa9ec4bd8b8a6eeac8d3847",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fido:-vision-and-navigation-for-a-robot-rover-Thorpe",
            "title": {
                "fragments": [],
                "text": "Fido: vision and navigation for a robot rover"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2263388"
                        ],
                        "name": "P. K. Ghosh",
                        "slug": "P.-K.-Ghosh",
                        "structuredName": {
                            "firstName": "Pijush",
                            "lastName": "Ghosh",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. K. Ghosh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3250452"
                        ],
                        "name": "S. Mudur",
                        "slug": "S.-Mudur",
                        "structuredName": {
                            "firstName": "Sudhir",
                            "lastName": "Mudur",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mudur"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61523068,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b5cfa32dc382c468fd7410a28783b4b85712844",
            "isKey": false,
            "numCitedBy": 1464,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Three-Dimensional-Computer-Vision:-A-Geometric-Ghosh-Mudur",
            "title": {
                "fragments": [],
                "text": "Three-Dimensional Computer Vision: A Geometric Viewpoint"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145926447"
                        ],
                        "name": "J. Weng",
                        "slug": "J.-Weng",
                        "structuredName": {
                            "firstName": "Juyang",
                            "lastName": "Weng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5439252"
                        ],
                        "name": "Shaoyun Chen",
                        "slug": "Shaoyun-Chen",
                        "structuredName": {
                            "firstName": "Shaoyun",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoyun Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8116503,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29a3e19d1b745d3af2fb19ff036c3f4244d9c999",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Vision-guided-navigation-using-SHOSLIF-Weng-Chen",
            "title": {
                "fragments": [],
                "text": "Vision-guided navigation using SHOSLIF"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 120
                            }
                        ],
                        "text": "Next, an interest operator (now known as Moravec\u2019s interest operator, which was later improved by Thorpe [140] for FIDO [141]) was applied to extract distinctive features in the images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "FIDO: Vision and Navigation for a Mobile Robot"
            },
            "venue": {
                "fragments": [],
                "text": "FIDO: Vision and Navigation for a Mobile Robot"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proc. Sixth Int'l Joint Conf. Artificial Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Sixth Int'l Joint Conf. Artificial Intelligence"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[83] and Kuan and Sharma [84], and others."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Real-Time Road Following Vision System"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. SPIE Int'l Soc. Optical Eng.-Mobile Robots"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 160
                            }
                        ],
                        "text": "This effort resulted in driving speeds of 96 km/h on a highway and 40 km/h on unmarked roads using a method of feature extraction called controlled correlation [85], [86]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Towards the Objective Evaluation of Low-Level Vision Operators"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Sixth European Conf. Artificial Intelligence"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Mobile robotics, navigation, computer vision, indoor navigation, outdoor navigation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic Vision System for Autonomous Mobile Robots"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE Int'l Conf. Intelligent Robots and Systems"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vision-based Vehicle Guidance, I. Masaki"
            },
            "venue": {
                "fragments": [],
                "text": "Vision-based Vehicle Guidance, I. Masaki"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These are systems that use no explicit representation at all about the space in which navigation is to take place, but rather resort to recognizing objects found in the environment or to tracking those objects by generating motions based on visual observations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Error Modeling in Stereo Vision"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE J. Robotics and Automation"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "World Scientific"
            },
            "venue": {
                "fragments": [],
                "text": "World Scientific"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "C ONDENSATION Conditional Density Propagation forVisual Tracking"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Mobile robotics, navigation, computer vision, indoor navigation, outdoor navigation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vision-based Autonomous Road Vehicles Vision-Based Vehicle Guidance, I. Masaki"
            },
            "venue": {
                "fragments": [],
                "text": "Vision-based Autonomous Road Vehicles Vision-Based Vehicle Guidance, I. Masaki"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vision-Based Motion Tracking Using Prediction of Uncertainties"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE Int'l Conf. Robotics and Automation"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "For more information on this or any other computing topic, please visit our Digital Library at http://computer.org/publications/dlib"
            },
            "venue": {
                "fragments": [],
                "text": "For more information on this or any other computing topic, please visit our Digital Library at http://computer.org/publications/dlib"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Mobile robotics, navigation, computer vision, indoor navigation, outdoor navigation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recursive 3D Road and Relative Egostate Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "year": 1992
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 72,
            "methodology": 40
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 178,
        "totalPages": 18
    },
    "page_url": "https://www.semanticscholar.org/paper/Vision-for-Mobile-Robot-Navigation:-A-Survey-DeSouza-Kak/15a0b5ad64ef55d566213e9af57452651b6e5c6f?sort=total-citations"
}