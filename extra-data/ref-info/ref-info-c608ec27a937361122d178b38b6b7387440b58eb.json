{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18086786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5db7dc2239f820eae498b07a955f31b3d113179f",
            "isKey": false,
            "numCitedBy": 634,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Real-world learning tasks may involve high-dimensional data sets with arbitrary patterns of missing data. In this paper we present a framework based on maximum likelihood density estimation for learning from such data set.s. We use mixture models for the density estimates and make two distinct appeals to the Expectation-Maximization (EM) principle (Dempster et al., 1977) in deriving a learning algorithm--EM is used both for the estimation of mixture components and for coping with missing data. The resulting algorithm is applicable to a wide range of supervised as well as unsupervised learning problems. Results from a classification benchmark--the iris data set--are presented."
            },
            "slug": "Supervised-learning-from-incomplete-data-via-an-EM-Ghahramani-Jordan",
            "title": {
                "fragments": [],
                "text": "Supervised learning from incomplete data via an EM approach"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A framework based on maximum likelihood density estimation for learning from high-dimensional data sets with arbitrary patterns of missing data is presented and results from a classification benchmark--the iris data set--are presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2879453"
                        ],
                        "name": "V. Castelli",
                        "slug": "V.-Castelli",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Castelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Castelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35473938,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f67e9a6bd7c688f1c9c653584a4fa1f9c7fda2a6",
            "isKey": false,
            "numCitedBy": 235,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-exponential-value-of-labeled-samples-Castelli-Cover",
            "title": {
                "fragments": [],
                "text": "On the exponential value of labeled samples"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145780619"
                        ],
                        "name": "David J. Miller",
                        "slug": "David-J.-Miller",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Miller",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143672627"
                        ],
                        "name": "A. Rao",
                        "slug": "A.-Rao",
                        "structuredName": {
                            "firstName": "Ajit",
                            "lastName": "Rao",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143629177"
                        ],
                        "name": "K. Rose",
                        "slug": "K.-Rose",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Rose",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2608771"
                        ],
                        "name": "A. Gersho",
                        "slug": "A.-Gersho",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Gersho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gersho"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 44872810,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "85cf3d1e397faab094238efb0094aef2d49d49c2",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "A global optimization method is introduced that minimize the rate of misclassification. We first derive the theoretical basis for the method, on which we base the development of a novel design algorithm and demonstrate its effectiveness and superior performance in the design of practical classifiers for some of the most popular structures currently in use. The method, grounded in ideas from statistical physics and information theory, extends the deterministic annealing approach for optimization, both to incorporate structural constraints on data assignments to classes and to minimize the probability of error as the cost objective. During the design, data are assigned to classes in probability so as to minimize the expected classification error given a specified level of randomness, as measured by Shannon's entropy. The constrained optimization is equivalent to a free-energy minimization, motivating a deterministic annealing approach in which the entropy and expected misclassification cost are reduced with the temperature while enforcing the classifier's structure. In the limit, a hard classifier is obtained. This approach is applicable to a variety of classifier structures, including the widely used prototype-based, radial basis function, and multilayer perceptron classifiers. The method is compared with learning vector quantization, back propagation (BP), several radial basis function design techniques, as well as with paradigms for more directly optimizing all these structures to minimize probability of error. The annealing method achieves significant performance gains over other design methods on a number of benchmark examples from the literature, while often retaining design complexity comparable with or only moderately greater than that of strict descent methods. Substantial gains, both inside and outside the training set, are achieved for complicated examples involving high-dimensional data and large class overlap."
            },
            "slug": "A-global-optimization-technique-for-statistical-Miller-Rao",
            "title": {
                "fragments": [],
                "text": "A global optimization technique for statistical classifier design"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "The theoretical basis for the annealing method is derived, on which it is based the development of a novel design algorithm and its effectiveness and superior performance in the design of practical classifiers for some of the most popular structures currently in use are demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 77
                            }
                        ],
                        "text": "The practical significance of this mixed training problem was recognized in (Lippmann 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15355298,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "333756f18144c75c61e6e4cda8821861353ead19",
            "isKey": false,
            "numCitedBy": 627,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "The author extends a previous review and focuses on feed-forward neural-net classifiers for static patterns with continuous-valued inputs. He provides a taxonomy of neural-net classifiers, examining probabilistic, hyperplane, kernel, and exemplar classifiers. He then discusses back-propagation and decision-tree classifiers; matching classifier complexity to training data; GMDH (generalized method of data handling) networks and high-order nets; K nearest-neighbor classifiers; the feature-map classifier; the learning vector quantizer; hypersphere classifiers; and radial-basis function classifiers.<<ETX>>"
            },
            "slug": "Pattern-classification-using-neural-networks-Lippmann",
            "title": {
                "fragments": [],
                "text": "Pattern classification using neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "The author extends a previous review and focuses on feed-forward neural-net classifiers for static patterns with continuous-valued inputs, examining probabilistic, hyperplane, kernel, and exemplar classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Communications Magazine"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145652961"
                        ],
                        "name": "L. Xu",
                        "slug": "L.-Xu",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 78
                            }
                        ],
                        "text": "The joint data likelihood was previously considered in a learning context in (Xu et al. 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 481261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8eb7f54bd9cec4e2d8c866d98c76a875374b594",
            "isKey": false,
            "numCitedBy": 259,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an alternative model for mixtures of experts which uses a different parametric form for the gating network. The modified model is trained by the EM algorithm. In comparison with earlier models--trained by either EM or gradient ascent--there is no need to select a learning stepsize. We report simulation experiments which show that the new architecture yields faster convergence. We also apply the new model to two problem domains: piecewise nonlinear function approximation and the combination of multiple previously trained classifiers."
            },
            "slug": "An-Alternative-Model-for-Mixtures-of-Experts-Xu-Jordan",
            "title": {
                "fragments": [],
                "text": "An Alternative Model for Mixtures of Experts"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "An alternative model for mixtures of experts which uses a different parametric form for the gating network, trained by the EM algorithm, and which yields faster convergence."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2209630"
                        ],
                        "name": "Behzad M. Shahshahani",
                        "slug": "Behzad-M.-Shahshahani",
                        "structuredName": {
                            "firstName": "Behzad",
                            "lastName": "Shahshahani",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Behzad M. Shahshahani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773449"
                        ],
                        "name": "D. Landgrebe",
                        "slug": "D.-Landgrebe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Landgrebe",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Landgrebe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 149
                            }
                        ],
                        "text": "We maximize the joint data likelihood and suggest two di\"tinct EM algorithms for this purpose, whereas the conditional likelihood was considered in (Shashahani and Landgrebe 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 117
                            }
                        ],
                        "text": "While we build on their work, as well as on our own previous work (Miller and Uyar 1996), our approach differs from (Shashahani and Landgrebe 1994) in several important respects."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 10
                            }
                        ],
                        "text": "However, (Shashahani and Landgrebe 1994) found that unlabelled data could be used effectively in label-deficient situations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 136
                            }
                        ],
                        "text": "The \"partitioned\" mixture (PM) model: This is the previously mentioned case where mixture components are \"hard-partitioned\" to classes (Shashahani and Landgrebe 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 52
                            }
                        ],
                        "text": "Here we compared EM-I with the method suggested in (Shashahani and Landgrebe 1994) (SL), based on the PM model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11081015,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "334867ed99a0af07d8a53dae4f7fdeffffdecc09",
            "isKey": true,
            "numCitedBy": 581,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we study the use of unlabeled sam- ples in reducing the problem of small training sample size that can severely affect the recognition rate of classifiers when the dimensionality of the multispectral data is high. We show that by using additional unlabeled samples that are available at no extra cost, the performance may be improved, and therefore the Hughes phenomenon can be mitigated. Furthermore, by ex- periments, we show that by using additional unlabeled samples more representative estimates can be obtained. We also pro- pose a semiparametric method for incorporating the training (Le., labeled) and unlabeled samples simultaneously into the parameter estimation process."
            },
            "slug": "The-effect-of-unlabeled-samples-in-reducing-the-and-Shahshahani-Landgrebe",
            "title": {
                "fragments": [],
                "text": "The effect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "By using additional unlabeled samples that are available at no extra cost, the performance may be improved, and therefore the Hughes phenomenon can be mitigated and therefore more representative estimates can be obtained."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Geosci. Remote. Sens."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700754"
                        ],
                        "name": "Volker Tresp",
                        "slug": "Volker-Tresp",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Tresp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volker Tresp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759183"
                        ],
                        "name": "R. Neuneier",
                        "slug": "R.-Neuneier",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Neuneier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Neuneier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109973472"
                        ],
                        "name": "Subutai Ahmad",
                        "slug": "Subutai-Ahmad",
                        "structuredName": {
                            "firstName": "Subutai",
                            "lastName": "Ahmad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subutai Ahmad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 162
                            }
                        ],
                        "text": "As such, it is related to , albeit distinct from supervised learning involving missing and/or noisy jeaturecomponents, addressed in (Ghahramani and Jordan 1995),(Tresp et al. 1995 ). in the following way."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6947586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1fac3079848709f9ce97eded9dd14e9a800c74d",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We present efficient algorithms for dealing with the problem of missing inputs (incomplete feature vectors) during training and recall. Our approach is based on the approximation of the input data distribution using Parzen windows. For recall, we obtain closed form solutions for arbitrary feedforward networks. For training, we show how the backpropagation step for an incomplete pattern can be approximated by a weighted averaged backpropagation step. The complexity of the solutions for training and recall is independent of the number of missing features. We verify our theoretical results using one classification and one regression problem."
            },
            "slug": "Efficient-Methods-for-Dealing-with-Missing-Data-in-Tresp-Neuneier",
            "title": {
                "fragments": [],
                "text": "Efficient Methods for Dealing with Missing Data in Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This work presents efficient algorithms for dealing with the problem of missing inputs (incomplete feature vectors) during training and recall based on the approximation of the input data distribution using Parzen windows."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17585310"
                        ],
                        "name": "M. I. Jordan",
                        "slug": "M.-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 67000854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6d8a7fc2e2d53923832f9404376512068ca2a57",
            "isKey": false,
            "numCitedBy": 2136,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain."
            },
            "slug": "Hierarchical-Mixtures-of-Experts-and-the-EM-Jordan-Jacobs",
            "title": {
                "fragments": [],
                "text": "Hierarchical mixtures of experts and the EM algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "An Expectation-Maximization (EM) algorithm for adjusting the parameters of the tree-structured architecture for supervised learning and an on-line learning algorithm in which the parameters are updated incrementally."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145016534"
                        ],
                        "name": "J. Moody",
                        "slug": "J.-Moody",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Moody",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moody"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319258"
                        ],
                        "name": "C. Darken",
                        "slug": "C.-Darken",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Darken",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Darken"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 270
                            }
                        ],
                        "text": "\u2026are the probabilities P[1'7l.i = jl3!il (in parentheses), and with the \"expert\" for component j just the probability {3c,Ii' Elsewhere (Miller 1996), it has been shown that the associated classifier decision function is in fact equivalent to that of an RBF classifier (Moody and Darken 1989) ."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 31251383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e7c4f513f24c3b82a1138b9f22ed87ed00cbe76",
            "isKey": false,
            "numCitedBy": 4527,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a network architecture which uses a single internal layer of locally-tuned processing units to learn both classification tasks and real-valued function approximations (Moody and Darken 1988). We consider training such networks in a completely supervised manner, but abandon this approach in favor of a more computationally efficient hybrid learning method which combines self-organized and supervised learning. Our networks learn faster than backpropagation for two reasons: the local representations ensure that only a few units respond to any given input, thus reducing computational overhead, and the hybrid learning rules are linear rather than nonlinear, thus leading to faster convergence. Unlike many existing methods for data analysis, our network architecture and learning rules are truly adaptive and are thus appropriate for real-time use."
            },
            "slug": "Fast-Learning-in-Networks-of-Locally-Tuned-Units-Moody-Darken",
            "title": {
                "fragments": [],
                "text": "Fast Learning in Networks of Locally-Tuned Processing Units"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work proposes a network architecture which uses a single internal layer of locally-tuned processing units to learn both classification tasks and real-valued function approximations (Moody and Darken 1988)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 92
                            }
                        ],
                        "text": "Interestingly, for the mixed training problem, there are two Expectation-Maximization (EM) (Dempster et al. 1977) formulations that can be applied to maximize the likelihood associated with a given probability model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 67
                            }
                        ],
                        "text": "While we build on their work, as well as on our own previous work (Miller and Uyar 1996), our approach differs from (Shashahani and Landgrebe 1994) in several important respects."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 166
                            }
                        ],
                        "text": "First, we suggest a more powerful mixture-based probability model with an associated classifier structure that has been shown to be equivalent to the RBF classifier (Miller 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 8
                            }
                        ],
                        "text": "DA-RBF (Miller et at."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 83
                            }
                        ],
                        "text": "We have generally found this model to be more powerful than the \"partitioned\" one (Miller Uyar 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A global optimization method for statistical classifier design"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Signal Processing"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 270
                            }
                        ],
                        "text": "\u2026are the probabilities P[1'7l.i = jl3!il (in parentheses), and with the \"expert\" for component j just the probability {3c,Ii' Elsewhere (Miller 1996), it has been shown that the associated classifier decision function is in fact equivalent to that of an RBF classifier (Moody and Darken 1989) ."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast learning in locally-tuned processing units"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 217
                            }
                        ],
                        "text": "Note that the probabilities have a \"mixture of experts\" structure, where the \"gating units\" are the probabilities P[1'7l.i = jl3!il (in parentheses), and with the \"expert\" for component j just the probability {3c,Ii' Elsewhere (Miller 1996), it has been shown that the associated classifier decision function is in fact equivalent to that of an RBF classifier (Moody and Darken 1989) ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 166
                            }
                        ],
                        "text": "First, we suggest a more powerful mixture-based probability model with an associated classifier structure that has been shown to be equivalent to the RBF classifier (Miller 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 144
                            }
                        ],
                        "text": "\u2026units\" are the probabilities P[1'7l.i = jl3!il (in parentheses), and with the \"expert\" for component j just the probability {3c,Ii' Elsewhere (Miller 1996), it has been shown that the associated classifier decision function is in fact equivalent to that of an RBF classifier (Moody and Darken\u2026"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A mixture model equivalent to the radial basis function classifier"
            },
            "venue": {
                "fragments": [],
                "text": "A mixture model equivalent to the radial basis function classifier"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 67
                            }
                        ],
                        "text": "While we build on their work, as well as on our own previous work (Miller and Uyar 1996), our approach differs from (Shashahani and Landgrebe 1994) in several important respects."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 83
                            }
                        ],
                        "text": "The overall posterior probability takes the form [ 1 This model was introduced in (Miller and Uyar 1996) and was shown there to lead to performance improvement over the PM model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A generalized Gaussian mixture classifier with learning based on both labelled and unlabelled data"
            },
            "venue": {
                "fragments": [],
                "text": "Conf. on Info. Sci. and Sys"
            },
            "year": 1996
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 6,
            "methodology": 3,
            "result": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 14,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/A-Mixture-of-Experts-Classifier-with-Learning-Based-Miller-Uyar/c608ec27a937361122d178b38b6b7387440b58eb?sort=total-citations"
}