{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2255318"
                        ],
                        "name": "R. Liere",
                        "slug": "R.-Liere",
                        "structuredName": {
                            "firstName": "Ray",
                            "lastName": "Liere",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Liere"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729906"
                        ],
                        "name": "Prasad Tadepalli",
                        "slug": "Prasad-Tadepalli",
                        "structuredName": {
                            "firstName": "Prasad",
                            "lastName": "Tadepalli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prasad Tadepalli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7530337,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80ef14d2a1b8c7efbf45bedae9d001fe5446c7de",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In many real-world domains, supervised learning requires a large number of training examples. In this paper, we describe an active learning method that uses a committee of learners to reduce the number of training examples required for learning. Our approach is similar to the Query by Committee framework, where disagreement among the committee members on the predicted label for the input part of the example is used to signal the need for knowing the actual value of the label. Our experiments are conducted in the text categorization domain, which is characterized by a large number of features, many of which are irrelevant. We report here on experiments using a committee of Winnowbased learners and demonstrate that this approach can reduce the number of labeled training examples required over that used by a single Winnow learner by l-2 orders of magnitude. 1. Hntroduction"
            },
            "slug": "Active-Learning-with-Committees-for-Text-Liere-Tadepalli",
            "title": {
                "fragments": [],
                "text": "Active Learning with Committees for Text Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper reports on experiments using a committee of Winnowbased learners and demonstrates that this approach can reduce the number of labeled training examples required over that used by a single Winnow learner by l-2 orders of magnitude."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1460876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "63287d3220fe96d5cbf73067545abbb88cc180a6",
            "isKey": false,
            "numCitedBy": 426,
            "numCiting": 84,
            "paperAbstract": {
                "fragments": [],
                "text": "In many important text classification problems, acquiring class labels for training documents is costly, while gathering large quantities of unlabeled data is cheap. This paper shows that the accuracy of text classifiers trained with a small number of labeled documents can be improved by augmenting this small training set with a large pool of unlabeled documents. We present a theoretical argument showing that, under common assumptions, unlabeled data contain information about the target function. We then introduce an algorithm for learning from labeled and unlabeled text based on the combination of Expectation-Maximization with a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents; it then trains a new classifier using the labels for all the documents, and iterates to convergence. Experimental results, obtained using text from three different realworld tasks, show that the use of unlabeled data reduces classification error by up to 33%."
            },
            "slug": "Learning-to-Classify-Text-from-Labeled-and-Nigam-McCallum",
            "title": {
                "fragments": [],
                "text": "Learning to Classify Text from Labeled and Unlabeled Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that the accuracy of text classifiers trained with a small number of labeled documents can be improved by augmenting this small training set with a large pool of unlabeled documents, and an algorithm is introduced based on the combination of Expectation-Maximization with a naive Bayes classifier."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 15704538,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33c1f29dbc73cc4d0f787d6b048b465649b2d92b",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "At ACM SIGIR '94, I compared the effectiveness of uncertainty sampling with that of random sampling and relevance sampling in choosing training data for a text categorization data set [1]. (Relevance sampling is the application of relevance feedback [3] to producing a training sample.) I have discovered a bug in my experimental software which caused the relevance sampling results reported in the SIGIR '94 paper to be incorrect. (The uncertainty sampling and random sampling results in that paper were correct.) I have since fixed the bug and rerun the experiments. This note presents the corrected results, along with additional data supporting the original claim that uncertainty sampling has an advantage over relevance sampling in most training situations."
            },
            "slug": "A-sequential-algorithm-for-training-text-and-data-Lewis",
            "title": {
                "fragments": [],
                "text": "A sequential algorithm for training text classifiers: corrigendum and additional data"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A bug in my experimental software caused the relevance sampling results reported in the SIGIR '94 paper to be incorrect, and this note presents the corrected results, along with additional data supporting the original claim that uncertainty sampling has an advantage over relevance sampling in most training situations."
            },
            "venue": {
                "fragments": [],
                "text": "SIGF"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144628595"
                        ],
                        "name": "S. Argamon",
                        "slug": "S.-Argamon",
                        "structuredName": {
                            "firstName": "Shlomo",
                            "lastName": "Argamon",
                            "middleNames": [
                                "Engelson"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Argamon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17288818,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f78d6f79b3ef103cb2d8d170632eb74d9496412",
            "isKey": false,
            "numCitedBy": 501,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Committee-Based-Sampling-For-Training-Probabilistic-Dagan-Argamon",
            "title": {
                "fragments": [],
                "text": "Committee-Based Sampling For Training Probabilistic Classifiers"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34938639"
                        ],
                        "name": "W. Gale",
                        "slug": "W.-Gale",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gale",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gale"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 915058,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5194b668c67aa83c037e71599a087f63c98eb713",
            "isKey": false,
            "numCitedBy": 2405,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability to cheaply train text classifiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual. An algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task. This method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness."
            },
            "slug": "A-sequential-algorithm-for-training-text-Lewis-Gale",
            "title": {
                "fragments": [],
                "text": "A sequential algorithm for training text classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task and reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145780619"
                        ],
                        "name": "David J. Miller",
                        "slug": "David-J.-Miller",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Miller",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2309473"
                        ],
                        "name": "H. S. Uyar",
                        "slug": "H.-S.-Uyar",
                        "structuredName": {
                            "firstName": "Hasan",
                            "lastName": "Uyar",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. S. Uyar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17425751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c608ec27a937361122d178b38b6b7387440b58eb",
            "isKey": false,
            "numCitedBy": 346,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We address statistical classifier design given a mixed training set consisting of a small labelled feature set and a (generally larger) set of unlabelled features. This situation arises, e.g., for medical images, where although training features may be plentiful, expensive expertise is required to extract their class labels. We propose a classifier structure and learning algorithm that make effective use of unlabelled data to improve performance. The learning is based on maximization of the total data likelihood, i.e. over both the labelled and unlabelled data subsets. Two distinct EM learning algorithms are proposed, differing in the EM formalism applied for unlabelled data. The classifier, based on a joint probability model for features and labels, is a \"mixture of experts\" structure that is equivalent to the radial basis function (RBF) classifier, but unlike RBFs, is amenable to likelihood-based training. The scope of application for the new method is greatly extended by the observation that test data, or any new data to classify, is in fact additional, unlabelled data - thus, a combined learning/classification operation - much akin to what is done in image segmentation - can be invoked whenever there is new data to classify. Experiments with data sets from the UC Irvine database demonstrate that the new learning algorithms and structure achieve substantial performance gains over alternative approaches."
            },
            "slug": "A-Mixture-of-Experts-Classifier-with-Learning-Based-Miller-Uyar",
            "title": {
                "fragments": [],
                "text": "A Mixture of Experts Classifier with Learning Based on Both Labelled and Unlabelled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A classifier structure and learning algorithm that make effective use of unlabelled data to improve performance and is a \"mixture of experts\" structure that is equivalent to the radial basis function (RBF) classifier, but unlike RBFs, is amenable to likelihood-based training."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 33
                            }
                        ],
                        "text": "Following several other studies [Joachims, 1998; Liere and Tadepalli,1997] we build binary classi ers for each of the 10 most populous classes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 1
                            }
                        ],
                        "text": "[Joachims, 1998] Thorsten Joachims."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2427083,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40212e9474c3ddf3d8c6ffd13dd3211ec9406c49",
            "isKey": false,
            "numCitedBy": 8601,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore they are fully automatic, eliminating the need for manual parameter tuning."
            },
            "slug": "Text-Categorization-with-Support-Vector-Machines:-Joachims",
            "title": {
                "fragments": [],
                "text": "Text Categorization with Support Vector Machines: Learning with Many Relevant Features"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper explores the use of Support Vector Machines for learning text classifiers from examples and analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task."
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18086786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5db7dc2239f820eae498b07a955f31b3d113179f",
            "isKey": false,
            "numCitedBy": 634,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Real-world learning tasks may involve high-dimensional data sets with arbitrary patterns of missing data. In this paper we present a framework based on maximum likelihood density estimation for learning from such data set.s. We use mixture models for the density estimates and make two distinct appeals to the Expectation-Maximization (EM) principle (Dempster et al., 1977) in deriving a learning algorithm--EM is used both for the estimation of mixture components and for coping with missing data. The resulting algorithm is applicable to a wide range of supervised as well as unsupervised learning problems. Results from a classification benchmark--the iris data set--are presented."
            },
            "slug": "Supervised-learning-from-incomplete-data-via-an-EM-Ghahramani-Jordan",
            "title": {
                "fragments": [],
                "text": "Supervised learning from incomplete data via an EM approach"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A framework based on maximum likelihood density estimation for learning from high-dimensional data sets with arbitrary patterns of missing data is presented and results from a classification benchmark--the iris data set--are presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740213"
                        ],
                        "name": "Pedro M. Domingos",
                        "slug": "Pedro-M.-Domingos",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Domingos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro M. Domingos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694780"
                        ],
                        "name": "M. Pazzani",
                        "slug": "M.-Pazzani",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Pazzani",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pazzani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7413266,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "217a0ecf9795721f9f3661f5562a5b1afd4a3b59",
            "isKey": false,
            "numCitedBy": 873,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The simple Bayesian classi er (SBC) is commonly thought to assume that attributes are independent given the class, but this is apparently contradicted by the surprisingly good performance it exhibits in many domains that contain clear attribute dependences. No explanation for this has been proposed so far. In this paper we show that the SBC does not in fact assume attribute independence, and can be optimal even when this assumption is violated by a wide margin. The key to this nding lies in the distinction between classi cation and probability estimation: correct classi cation can be achieved even when the probability estimates used contain large errors. We show that the previously-assumed region of optimality of the SBC is a second-order in nitesimal fraction of the actual one. This is followed by the derivation of several necessary and several su cient conditions for the optimality of the SBC. For example, the SBC is optimal for learning arbitrary conjunctions and disjunctions, even though they violate the independence assumption. The paper also reports empirical evidence of the SBC's competitive performance in domains containing substantial degrees of attribute dependence. 1 THE SIMPLE BAYESIAN"
            },
            "slug": "Beyond-Independence:-Conditions-for-the-Optimality-Domingos-Pazzani",
            "title": {
                "fragments": [],
                "text": "Beyond Independence: Conditions for the Optimality of the Simple Bayesian Classifier"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "It is shown that the simple Bayesian classi er (SBC) does not in fact assume attribute independence, and can be optimal even when this assumption is violated by a wide margin, and the previously-assumed region of optimality is a second-order in nitesimal fraction of the actual one."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6922975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cb09327e68400bf05e6f373e046a3a08e82510e",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "In many applications of natural language processing it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations \"eat a peach\" and \"eat a beach\" is more likely. Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in a given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on \"most similar\" words.We describe a probabilistic word association model based on distributional word similarity, and apply it to improving probability estimates for unseen word bigrams in a variant of Katz's back-off model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error."
            },
            "slug": "Similarity-Based-Estimation-of-Word-Cooccurrence-Dagan-Pereira",
            "title": {
                "fragments": [],
                "text": "Similarity-Based Estimation of Word Cooccurrence Probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A probabilistic word association model based on distributional word similarity is described, and it is applied to improving probability estimates for unseen word bigrams in a variant of Katz's back-off model."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 1
                            }
                        ],
                        "text": "[Joachims, 1997] Thorsten Joachims."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 64
                            }
                        ],
                        "text": "With four commonly used assumptions[Domingos and Pazzani, 1997; Joachims, 1997] about the nature of the gen-erative parametric model, we use training data to calculate Bayes optimalestimates of the model parameters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 145
                            }
                        ],
                        "text": "\u2026Kamal Nigamyknigam@cs.cmu.eduzJust Research4616 Henry StreetPittsburgh, PA 15213 ySchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213AbstractThis paper shows how a text classi er's need for labeled training datacan be reduced by a combination of active learning and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 191
                            }
                        ],
                        "text": "Employing EM in Pool-Based Active Learningfor Text Classi cationAndrew McCallumzymccallum@jprc.com Kamal Nigamyknigam@cs.cmu.eduzJust Research4616 Henry StreetPittsburgh, PA 15213 ySchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213AbstractThis paper shows how a text classi er's need for labeled training datacan be reduced by a combination of active learning and ExpectationMaximization (EM) on a pool of unlabeled data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 191
                            }
                        ],
                        "text": "The results are based on datasets from UseNet and newswires.2The Newsgroups data set, collected by Ken Lang, contains about 20,000 ar-ticles evenly divided among 20 UseNet discussion groups [Joachims, 1997]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5842708,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "094fc15bc058b0d62a661a1460885a9490bdb1bd",
            "isKey": true,
            "numCitedBy": 1533,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : A probabilistic analysis of the Rocchio relevance feedback algorithm, one of the most popular learning methods from information retrieval, is presented in a text categorization framework. The analysis results in a probabilistic version of the Rocchio classifier and offers an explanation for the TFIDF word weighting heuristic. The Rocchio classifier, its probabilistic variant and a standard naive Bayes classifier are compared on three text categorization tasks. The results suggest that the probabilistic algorithms are preferable to the heuristic Rocchio classifier."
            },
            "slug": "A-Probabilistic-Analysis-of-the-Rocchio-Algorithm-Joachims",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A Probabilistic analysis of the Rocchio relevance feedback algorithm, one of the most popular learning methods from information retrieval, is presented in a text categorization framework and suggests that the probabilistic algorithms are preferable to the heuristic Rocchio classifier."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6713452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5eb328cf7e94995199e4c82a1f4d0696430a80b5",
            "isKey": false,
            "numCitedBy": 1193,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data."
            },
            "slug": "Distributional-Clustering-of-English-Words-Pereira-Tishby",
            "title": {
                "fragments": [],
                "text": "Distributional Clustering of English Words"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Deterministic annealing is used to find lowest distortion sets of clusters: as the annealed parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2209630"
                        ],
                        "name": "Behzad M. Shahshahani",
                        "slug": "Behzad-M.-Shahshahani",
                        "structuredName": {
                            "firstName": "Behzad",
                            "lastName": "Shahshahani",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Behzad M. Shahshahani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773449"
                        ],
                        "name": "D. Landgrebe",
                        "slug": "D.-Landgrebe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Landgrebe",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Landgrebe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11081015,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "334867ed99a0af07d8a53dae4f7fdeffffdecc09",
            "isKey": false,
            "numCitedBy": 581,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we study the use of unlabeled sam- ples in reducing the problem of small training sample size that can severely affect the recognition rate of classifiers when the dimensionality of the multispectral data is high. We show that by using additional unlabeled samples that are available at no extra cost, the performance may be improved, and therefore the Hughes phenomenon can be mitigated. Furthermore, by ex- periments, we show that by using additional unlabeled samples more representative estimates can be obtained. We also pro- pose a semiparametric method for incorporating the training (Le., labeled) and unlabeled samples simultaneously into the parameter estimation process."
            },
            "slug": "The-effect-of-unlabeled-samples-in-reducing-the-and-Shahshahani-Landgrebe",
            "title": {
                "fragments": [],
                "text": "The effect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "By using additional unlabeled samples that are available at no extra cost, the performance may be improved, and therefore the Hughes phenomenon can be mitigated and therefore more representative estimates can be obtained."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Geosci. Remote. Sens."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2976268"
                        ],
                        "name": "D. Cohn",
                        "slug": "D.-Cohn",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cohn",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cohn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6170752,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08627b368e6b7d142cfcece74d52d21d48cc64a6",
            "isKey": false,
            "numCitedBy": 411,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-Network-Exploration-Using-Optimal-Experiment-Cohn",
            "title": {
                "fragments": [],
                "text": "Neural Network Exploration Using Optimal Experiment Design"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2976268"
                        ],
                        "name": "D. Cohn",
                        "slug": "D.-Cohn",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cohn",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cohn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9242771,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1150f9289c6151506e3f7cf0e6ebbcfd49f1dace",
            "isKey": false,
            "numCitedBy": 2057,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "For many types of learners one can compute the statistically \"optimal\" way to select data. We review how these techniques have been used with feedforward neural networks [MacKay, 1992; Cohn, 1994]. We then show how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural networks are expensive and approximate, the techniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate."
            },
            "slug": "Active-Learning-with-Statistical-Models-Cohn-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Active Learning with Statistical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work shows how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A sequential algorithm for trainingtext classi ers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 135
                            }
                        ],
                        "text": "Two other stud-ies have used EM to combine labeled and unlabeled data without activelearning for classi cation, but on non-text tasks [Miller and Uyar, 1997;Shahshahani and Landgrebe, 1994]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 1
                            }
                        ],
                        "text": "[Miller and Uyar, 1997] David J. Miller and Hasan S. Uyar."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Active learning withcommittees for text categorization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximumlikelihood from incomplete data via the EM . algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the RoyalStatistical Society , Series B"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Active learning withstatistical models"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Arti cial Intelligence Research"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 57
                            }
                        ],
                        "text": "Active learning approaches this same problem in a di erent way."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimations of dependences based on statistical data"
            },
            "venue": {
                "fragments": [],
                "text": "Estimations of dependences based on statistical data"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A sequential algorithm for training"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Engelson . Committeebasedsampling for training probabilistic classi ers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 3,
            "methodology": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 23,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Employing-Em-in-Pool-based-Active-Learning-for-Text-Nigamyknigam/6f17768a9fe231a2fd38708be90f98db3890c986?sort=total-citations"
}