{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789756"
                        ],
                        "name": "B. Leibe",
                        "slug": "B.-Leibe",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Leibe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Leibe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "They often provide very coarse segmentations by overlapping small patches [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2900658,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "398bff5abb9c35b2de63bf6b5ecae244c082ca6e",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 203,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis is concerned with the problem of visual object categorization, that is of reeognizing unseen-before objects, localizing them in cluttered real-worid images, and assigning the correct category label. This capability is one of the core compe\u00ac tencies of the human visual system. Yet, Computer vision Systems are still far from reaching a comparable level of Performance.Moreover,Computer visionresearch has in the past mainly focused on the simpler and more specific problem of identifying known objects under novel viewing conditions. The visual categorization problem is closely linked to the task of figure-ground segmentation, that is of dividing the image into an object and a non-objeet part. Historically, figure-ground segmentation has often been seen as an important and even necessary preprocessing step for object recognition. However, purely bottomup approacheshave so far been unable to yield segmentationsof sufficient quality, so that most current recognition approacheshave been designed to work independently from segmentation. In contrast,this thesis considers object categorization and figure-ground segmen\u00ac tation as two interleaved processes that closely collaborate towards a common goal. The core part of our work is a probabilisticformulation which integrates both capabilities into a common framework. As shown in our experiments, the tight coupling between those two processes allows them to profit from each other and improve their individual Performances. The resulting approach can detect categorical objects in novel images and automatically compute a segmentationfor them. This segmenta\u00ac tion is then used to again improve recognition by allowing the System to focus its effort on object pixels and discard misleading influencesfrom the background. In addition to improving the recognition Performance for individual hypotheses, the top-down segmentation also allows to determine exactly from where a hypoth\u00ac esis draws its support. We use this information to design a hypothesis verification stage based on the MDL principle that resolves ambiguities between overlapping hypotheseson a per-pixel level and factorsout the effects of partialocclusion. Altogether, this procedureconstitutes a novel mechanismin object detection that allows to analyze scenes containing multiple objects in a principled manner. Our results show that it presents an improvement over conventional criteria based on bounding box overlap and permitsmore aecurate aeeeptancedecisions. Our approach is based on a highly flexible implicit representation for object shape that can combine the information of local parts observed on different training exam\u00ac ples and interpolate between the correspondingobjects. As a result, the proposed method can learn object modeis already from few training examples and achieve competitive object detection Performance with training sets that are between one and two orders of magnitude smaller than those used in comparable Systems. An extensive evaluation on several large data sets shows that the system is applicable to many different object categories, including both rigid and articulated objects."
            },
            "slug": "Interleaved-Object-Categorization-and-Segmentation-Leibe-Schiele",
            "title": {
                "fragments": [],
                "text": "Interleaved Object Categorization and Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This thesis considers object categorization and figure-ground segmentation as two interleaved processes that closely collaborate towards a common goal and develops a probabilistic formulation which integrates both capabilities into a common framework that allows to analyze scenes containing multiple objects in a principled manner."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25633106"
                        ],
                        "name": "Eran Borenstein",
                        "slug": "Eran-Borenstein",
                        "structuredName": {
                            "firstName": "Eran",
                            "lastName": "Borenstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eran Borenstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 48
                            }
                        ],
                        "text": "In an effort to address these concerns, we join [4,5,6,7,8,9] in suggesting the use of multiple segmentations per image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16680003,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1c873e500f03a495bdf51931179459699ad6d94",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We construct a Bayesian model that integrates topdown with bottom-up criteria, capitalizing on their relative merits to obtain figure-ground segmentation that is shape-specific and texture invariant. A hierarchy of bottom-up segments in multiple scales is used to construct a prior on all possible figure-ground segmentations of the image. This prior is used by our top-down part to query and detect object parts in the image using stored shape templates. The detected parts are integrated to produce a global approximation for the object\u2019s shape, which is then used by an inference algorithm to produce the final segmentation. Experiments with a large sample of horse and runner images demonstrate strong figure-ground segmentation despite high object and background variability. The segmentations are robust to changes in appearance since the matching component depends on shape criteria alone. The model may be useful for additional visual tasks requiring labeling, such as the segmentation of multiple scene objects."
            },
            "slug": "Shape-Guided-Object-Segmentation-Borenstein-Malik",
            "title": {
                "fragments": [],
                "text": "Shape Guided Object Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A Bayesian model is constructed that integrates topdown with bottom-up criteria, capitalizing on their relative merits to obtain figure-ground segmentation that is shape-specific and texture invariant and robust to changes in appearance since the matching component depends on shape criteria alone."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698689"
                        ],
                        "name": "N. Jojic",
                        "slug": "N.-Jojic",
                        "structuredName": {
                            "firstName": "Nebojsa",
                            "lastName": "Jojic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Jojic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 18
                            }
                        ],
                        "text": "The approaches in [14,15], as well as others, enforce spatial constraints on object parts that are too rigid for highly deformable objects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5489503,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79bd5e61e20594c13a90e66293e1abb4179935a2",
            "isKey": false,
            "numCitedBy": 412,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of learning object class models and object segmentations from unannotated images. We introduce LOCUS (learning object classes with unsupervised segmentation) which uses a generative probabilistic model to combine bottom-up cues of color and edge with top-down cues of shape and pose. A key aspect of this model is that the object appearance is allowed to vary from image to image, allowing for significant within-class variation. By iteratively updating the belief in the object's position, size, segmentation and pose, LOCUS avoids making hard decisions about any of these quantities and so allows for each to be refined at any stage. We show that LOCUS successfully learns an object class model from unlabeled images, whilst also giving segmentation accuracies that rival existing supervised methods. Finally, we demonstrate simultaneous recognition and segmentation in novel images using the learned models for a number of object classes, as well as unsupervised object discovery and tracking in video."
            },
            "slug": "LOCUS:-learning-object-classes-with-unsupervised-Winn-Jojic",
            "title": {
                "fragments": [],
                "text": "LOCUS: learning object classes with unsupervised segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "LOCUS (learning object classes with unsupervised segmentation) is introduced which uses a generative probabilistic model to combine bottom-up cues of color and edge with top-down cues of shape and pose, allowing for significant within-class variation."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3045340"
                        ],
                        "name": "Tomasz Malisiewicz",
                        "slug": "Tomasz-Malisiewicz",
                        "structuredName": {
                            "firstName": "Tomasz",
                            "lastName": "Malisiewicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomasz Malisiewicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "In fact, [1] argues that segmentation is rarely \u2018correct\u2019, and [2] shows that often an object encompasses multiple regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 92
                            }
                        ],
                        "text": "For each image size, segmentations with 9, 21 and 33 regions are generated (as suggested in [2])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": "Unfortunately, recent extensive experiments in [1] and [2] have shown that a single region generated by an image segmentation can rarely be equated to a physical object or object part."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15599169,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "562b04ee16f27c47625b4989ab011510518d0b0a",
            "isKey": false,
            "numCitedBy": 331,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Sliding window scanning is the dominant paradigm in object recognition research today. But while much success has been reported in detecting several rectangular-shaped object classes (i.e. faces, cars, pedestrians), results have been much less impressive for more general types of objects. Several researchers have advocated the use of image segmentation as a way to get a better spatial support for objects. In this paper, our aim is to address this issue by studying the following two questions: 1) how important is good spatial support for recognition? 2) can segmentation provide better spatial support for objects? To answer the first, we compare recognition performance using ground-truth segmentation vs. bounding boxes. To answer the second, we use the multiple segmentation approach to evaluate how close can real segments approach the ground-truth for real objects, and at what cost. Our results demonstrate the importance of finding the right spatial support for objects, and the feasibility of doing so without excessive computational burden."
            },
            "slug": "Improving-Spatial-Support-for-Objects-via-Multiple-Malisiewicz-Efros",
            "title": {
                "fragments": [],
                "text": "Improving Spatial Support for Objects via Multiple Segmentations"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "The multiple segmentation approach is used to evaluate how close can real segments approach the ground-truth for real objects, and at what cost."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[4] assume that the entire object falls within one image segmentation region, which is unlikely given object complexity and the simplicity of bottom-up features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 48
                            }
                        ],
                        "text": "In an effort to address these concerns, we join [4,5,6,7,8,9] in suggesting the use of multiple segmentations per image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2066830,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56766bab76cdcd541bf791730944a5e453006239",
            "isKey": false,
            "numCitedBy": 740,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a large dataset of images, we seek to automatically determine the visually similar object and scene classes together with their image segmentation. To achieve this we combine two ideas: (i) that a set of segmented objects can be partitioned into visual object classes using topic discovery models from statistical text analysis; and (ii) that visual object classes can be used to assess the accuracy of a segmentation. To tie these ideas together we compute multiple segmentations of each image and then: (i) learn the object classes; and (ii) choose the correct segmentations. We demonstrate that such an algorithm succeeds in automatically discovering many familiar objects in a variety of image datasets, including those from Caltech, MSRC and LabelMe."
            },
            "slug": "Using-Multiple-Segmentations-to-Discover-Objects-in-Russell-Freeman",
            "title": {
                "fragments": [],
                "text": "Using Multiple Segmentations to Discover Objects and their Extent in Image Collections"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work compute multiple segmentations of each image and then learns the object classes and chooses the correct segmentations, demonstrating that such an algorithm succeeds in automatically discovering many familiar objects in a variety of image datasets, including those from Caltech, MSRC and LabelMe."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800213"
                        ],
                        "name": "Charless C. Fowlkes",
                        "slug": "Charless-C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charless",
                            "lastName": "Fowlkes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charless C. Fowlkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144891282"
                        ],
                        "name": "David R. Martin",
                        "slug": "David-R.-Martin",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Martin",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David R. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 136
                            }
                        ],
                        "text": "The second set of nine segmentations is generated using the normalized cuts algorithm with the \u2018probability of boundary\u2019 features as in [11,12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 130
                            }
                        ],
                        "text": "An example of intersections of regions (IofRs) and the 18 segmentations that generated them: 3 from Mean Shift [10], 9 from Ncuts [11,12], and 6 from Felzenszwalb and Huttenlocher\u2019s method [13]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8443636,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e2dac76a5876104f273b0a30617dac4c1c3033f",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper studies the problem of combining region and boundary cues for natural image segmentation. We employ a large database of manually segmented images in order to learn an optimal affinity function between pairs of pixels. These pairwise affinities can then be used to cluster the pixels into visually coherent groups. Region cues are computed as the similarity in brightness, color, and texture between image patches. Boundary cues are incorporated by looking for the presence of an \"intervening contour\", a large gradient along a straight line connecting two pixels. We first use the dataset of human segmentations to individually optimize parameters of the patch and gradient features for brightness, color, and texture cues. We then quantitatively measure the power of different feature combinations by computing the precision and recall of classifiers trained using those features. The mutual information between the output of the classifiers and the same-segment indicator function provides an alternative evaluation technique that yields identical conclusions. As expected, the best classifier makes use of brightness, color, and texture features, in both patch and gradient forms. We find that for brightness, the gradient cue outperforms the patch similarity. In contrast, using color patch similarity yields better results than using color gradients. Texture is the most powerful of the three channels, with both patches and gradients carrying significant independent information. Interestingly, the proximity of the two pixels does not add any information beyond that provided by the similarity cues. We also find that the convexity assumptions made by the intervening contour approach are supported by the ecological statistics of the dataset."
            },
            "slug": "Learning-affinity-functions-for-image-segmentation:-Fowlkes-Martin",
            "title": {
                "fragments": [],
                "text": "Learning affinity functions for image segmentation: combining patch-based and gradient-based approaches"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A large database of manually segmented images is employed in order to learn an optimal affinity function between pairs of pixels, and it is found that for brightness, the gradient cue outperforms the patch similarity; in contrast, using color patch similarity yields better results than using color gradients."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143774737"
                        ],
                        "name": "J. Shotton",
                        "slug": "J.-Shotton",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Shotton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shotton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716777"
                        ],
                        "name": "A. Criminisi",
                        "slug": "A.-Criminisi",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Criminisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Criminisi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 139
                            }
                        ],
                        "text": "Many of the existing approaches to using bottom-up segmentation for recognition have higher complexity and are less intuitive than our own [8,14,16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 18
                            }
                        ],
                        "text": "The approaches in [14,15], as well as others, enforce spatial constraints on object parts that are too rigid for highly deformable objects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[14], and is comparable to that of Verbeek and Triggs [17] (however [17] uses a different split of the data)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "For the MSRC data set, our class-averaged and overall pixel accuracy results out-perform those of [14], and are comparable to those of [17]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 6075144,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb444dc25bab36a8e273ed654d49e3841905e5af",
            "isKey": true,
            "numCitedBy": 1349,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new approach to learning a discriminative model of object classes, incorporating appearance, shape and context information efficiently. The learned model is used for automatic visual recognition and semantic segmentation of photographs. Our discriminative model exploits novel features, based on textons, which jointly model shape and texture. Unary classification and feature selection is achieved using shared boosting to give an efficient classifier which can be applied to a large number of classes. Accurate image segmentation is achieved by incorporating these classifiers in a conditional random field. Efficient training of the model on very large datasets is achieved by exploiting both random feature selection and piecewise training methods. \n \nHigh classification and segmentation accuracy are demonstrated on three different databases: i) our own 21-object class database of photographs of real objects viewed under general lighting conditions, poses and viewpoints, ii) the 7-class Corel subset and iii) the 7-class Sowerby database used in [1]. The proposed algorithm gives competitive results both for highly textured (e.g. grass, trees), highly structured (e.g. cars, faces, bikes, aeroplanes) and articulated objects (e.g. body, cow)."
            },
            "slug": "TextonBoost:-Joint-Appearance,-Shape-and-Context-Shotton-Winn",
            "title": {
                "fragments": [],
                "text": "TextonBoost: Joint Appearance, Shape and Context Modeling for Multi-class Object Recognition and Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A new approach to learning a discriminative model of object classes, incorporating appearance, shape and context information efficiently, is proposed, which is used for automatic visual recognition and semantic segmentation of photographs."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256849"
                        ],
                        "name": "R. Unnikrishnan",
                        "slug": "R.-Unnikrishnan",
                        "structuredName": {
                            "firstName": "Ranjith",
                            "lastName": "Unnikrishnan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Unnikrishnan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2997956"
                        ],
                        "name": "C. Pantofaru",
                        "slug": "C.-Pantofaru",
                        "structuredName": {
                            "firstName": "Caroline",
                            "lastName": "Pantofaru",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Pantofaru"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "In fact, [1] argues that segmentation is rarely \u2018correct\u2019, and [2] shows that often an object encompasses multiple regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 123
                            }
                        ],
                        "text": "Is there one segmentation/parameter/feature combination which would give the \u2018best\u2019 partition for every image? As shown in [1], the answer is \u2018no\u2019."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "Most importantly, [1] has argued that a particular algorithm and parameter choice will create segmentations of different quality on different images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 47
                            }
                        ],
                        "text": "Unfortunately, recent extensive experiments in [1] and [2] have shown that a single region generated by an image segmentation can rarely be equated to a physical object or object part."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17173984,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16365ad9bf7fd7dd8b1867ef0d6b791488770b52",
            "isKey": false,
            "numCitedBy": 820,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Unsupervised image segmentation is an important component in many image understanding algorithms and practical vision systems. However, evaluation of segmentation algorithms thus far has been largely subjective, leaving a system designer to judge the effectiveness of a technique based only on intuition and results in the form of a few example segmented images. This is largely due to image segmentation being an ill-defined problem-there is no unique ground-truth segmentation of an image against which the output of an algorithm may be compared. This paper demonstrates how a recently proposed measure of similarity, the normalized probabilistic rand (NPR) index, can be used to perform a quantitative comparison between image segmentation algorithms using a hand-labeled set of ground-truth segmentations. We show that the measure allows principled comparisons between segmentations created by different algorithms, as well as segmentations on different images. We outline a procedure for algorithm evaluation through an example evaluation of some familiar algorithms - the mean-shift-based algorithm, an efficient graph-based segmentation algorithm, a hybrid algorithm that combines the strengths of both methods, and expectation maximization. Results are presented on the 300 images in the publicly available Berkeley segmentation data set"
            },
            "slug": "Toward-Objective-Evaluation-of-Image-Segmentation-Unnikrishnan-Pantofaru",
            "title": {
                "fragments": [],
                "text": "Toward Objective Evaluation of Image Segmentation Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is demonstrated how a recently proposed measure of similarity, the normalized probabilistic rand (NPR) index, can be used to perform a quantitative comparison between image segmentation algorithms using a hand-labeled set of ground-truth segmentations."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2997956"
                        ],
                        "name": "C. Pantofaru",
                        "slug": "C.-Pantofaru",
                        "structuredName": {
                            "firstName": "Caroline",
                            "lastName": "Pantofaru",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Pantofaru"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1891864"
                        ],
                        "name": "Gyuri Dork\u00f3",
                        "slug": "Gyuri-Dork\u00f3",
                        "structuredName": {
                            "firstName": "Gyuri",
                            "lastName": "Dork\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gyuri Dork\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "The region features used are normalized average position (2D), RCF (300D), color histogram (100D), region size divided by image size (1D), and the number of IofRs encompassed (1D)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 129
                            }
                        ],
                        "text": "Our approach thus far has classified regions independently, incorporating contextual and spatial information implicitly by using RCFs, and by using large regions from some segmentations to smooth the labeling of smaller regions in others."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "Since overall image context is often informative, we also aggregate the color and RCF histograms over the entire image for a final set of 802 features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 48
                            }
                        ],
                        "text": "In an effort to address these concerns, we join [4,5,6,7,8,9] in suggesting the use of multiple segmentations per image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 73
                            }
                        ],
                        "text": "We hypothesize that use of multiple segmentations of various scales, and RCFs which model the image surrounding a region, cause most of the label smoothing to occur without the random field."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 113
                            }
                        ],
                        "text": "The image structure within and near a region is captured by a 300-dimensional region-based context feature (RCF) [5], which is based on a distance-weighted histogram of quantized SIFTs [26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 77
                            }
                        ],
                        "text": "The union of the two regions is described using normalized average position, RCF, and color histogram."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 202
                            }
                        ],
                        "text": "The first three segmentations are generated by the mean shift-based segmentation algorithm [10] using pixel position, color (in the L*u*v* color space), and a histogram of quantized textons as features [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 145
                            }
                        ],
                        "text": "To compare two regions we use the smaller region size divided by larger region size, the symmetrical KLdivergence between the individual regions\u2019 RCFs (scaled between 0 and 1), the KL-divergence between the two color histograms, and the normalized difference in region positions."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5987348,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0304e82a3aeb318da27d6d3725fb61fe80ebc60",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a method for object class detection and localization which combines regions generated by image segmentation with local patches. Region-based descriptors can model and match regular textures reliably, but fail on parts of the object which are textureless. They also cannot repeatably identify interest points on their boundaries. By incorporating information from patch-based descriptors near the regions into a new feature, the Region-based Context Feature (RCF), we can address these issues. We apply Region-based Context Features in a semi-supervised learning framework for object detection and localization. This framework produces object-background segmentation masks of deformable objects. Numerical results are presented for pixel-level performance."
            },
            "slug": "Combining-Regions-and-Patches-for-Object-Class-Pantofaru-Dork\u00f3",
            "title": {
                "fragments": [],
                "text": "Combining Regions and Patches for Object Class Localization"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A method for object class detection and localization which combines regions generated by image segmentation with local patches, and applies Region-based Context Features in a semi-supervised learning framework for object Detection and localization."
            },
            "venue": {
                "fragments": [],
                "text": "2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3717791"
                        ],
                        "name": "M. P. Kumar",
                        "slug": "M.-P.-Kumar",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Kumar",
                            "middleNames": [
                                "Pawan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. P. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635539"
                        ],
                        "name": "P. Torr",
                        "slug": "P.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Torr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 136
                            }
                        ],
                        "text": "The object segmentation problem can also be approached by pixel or patchbased methods that do not use image segmentation regions, as in [17,18,19] and others."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7854172,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72a2c172cf49edb4a33708e05f53938f4d475432",
            "isKey": false,
            "numCitedBy": 403,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a principled Bayesian method for detecting and segmenting instances of a particular object category within an image, providing a coherent methodology for combining top down and bottom up cues. The work draws together two powerful formulations: pictorial structures (PS) and Markov random fields (MRFs) both of which have efficient algorithms for their solution. The resulting combination, which we call the object category specific MRF, suggests a solution to the problem that has long dogged MRFs namely that they provide a poor prior for specific shapes. In contrast, our model provides a prior that is global across the image plane using the PS. We develop an efficient method, OBJ CUT, to obtain segmentations using this model. Novel aspects of this method include an efficient algorithm for sampling the PS model, and the observation that the expected log likelihood of the model can be increased by a single graph cut. Results are presented on two object categories, cows and horses. We compare our methods to the state of the art in object category specific image segmentation and demonstrate significant improvements."
            },
            "slug": "OBJ-CUT-Kumar-Torr",
            "title": {
                "fragments": [],
                "text": "OBJ CUT"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A principled Bayesian method for detecting and segmenting instances of a particular object category within an image, providing a coherent methodology for combining top down and bottom up cues and developing an efficient method, OBJ CUT, to obtain segmentations using this model."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114833718"
                        ],
                        "name": "Xiaofeng Ren",
                        "slug": "Xiaofeng-Ren",
                        "structuredName": {
                            "firstName": "Xiaofeng",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaofeng Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Region intersections differ from superpixels [29] as they are constructed by intersecting larger regions, not by image segmentation with small kernel bandwidths or enforcing many regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13571735,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a9049a50dfe94fa4473880a9b60c99333ade685",
            "isKey": false,
            "numCitedBy": 1644,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a two-class classification model for grouping. Human segmented natural images are used as positive examples. Negative examples of grouping are constructed by randomly matching human segmentations and images. In a preprocessing stage an image is over-segmented into super-pixels. We define a variety of features derived from the classical Gestalt cues, including contour, texture, brightness and good continuation. Information-theoretic analysis is applied to evaluate the power of these grouping cues. We train a linear classifier to combine these features. To demonstrate the power of the classification model, a simple algorithm is used to randomly search for good segmentations. Results are shown on a wide range of images."
            },
            "slug": "Learning-a-classification-model-for-segmentation-Ren-Malik",
            "title": {
                "fragments": [],
                "text": "Learning a classification model for segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A two-class classification model for grouping is proposed that defines a variety of features derived from the classical Gestalt cues, including contour, texture, brightness and good continuation, and trains a linear classifier to combine these features."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3188179"
                        ],
                        "name": "A. Opelt",
                        "slug": "A.-Opelt",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Opelt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Opelt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40446025"
                        ],
                        "name": "M. Fussenegger",
                        "slug": "M.-Fussenegger",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Fussenegger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fussenegger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718587"
                        ],
                        "name": "A. Pinz",
                        "slug": "A.-Pinz",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Pinz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pinz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144543541"
                        ],
                        "name": "P. Auer",
                        "slug": "P.-Auer",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Auer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Auer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16562909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c91808994a250d7be332400a534a9291ca3b60e",
            "isKey": false,
            "numCitedBy": 333,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe the first stage of a new learning system for object detection and recognition. For our system we propose Boosting (5) as the underlying learning technique. This allows the use of very diverse sets of visual features in the learning process within a com- mon framework: Boosting \u2014 together with a weak hypotheses finder \u2014 may choose very inhomogeneous features as most relevant for combina- tion into a final hypothesis. As another advantage the weak hypotheses finder may search the weak hypotheses space without explicit calculation of all available hypotheses, reducing computation time. This contrasts the related work of Agarwal and Roth (1) where Winnow was used as learning algorithm and all weak hypotheses were calculated explicitly. In our first empirical evaluation we use four types of local descriptors: two basic ones consisting of a set of grayvalues and intensity moments and two high level descriptors: moment invariants (8) and SIFTs (12). The descriptors are calculated from local patches detected by an inter- est point operator. The weak hypotheses finder selects one of the local patches and one type of local descriptor and efficiently searches for the most discriminative similarity threshold. This differs from other work on Boosting for object recognition where simple rectangular hypotheses (22) or complex classifiers (20) have been used. In relatively simple images, where the objects are prominent, our approach yields results comparable to the state-of-the-art (3). But we also obtain very good results on more complex images, where the objects are located in arbitrary positions, poses, and scales in the images. These results indicate that our flexible approach, which also allows the inclusion of features from segmented re- gions and even spatial relationships, leads us a significant step towards generic object recognition."
            },
            "slug": "Weak-Hypotheses-and-Boosting-for-Generic-Object-and-Opelt-Fussenegger",
            "title": {
                "fragments": [],
                "text": "Weak Hypotheses and Boosting for Generic Object Detection and Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The first stage of a new learning system for object detection and recognition using Boosting as the underlying learning technique and the inclusion of features from segmented re- gions and even spatial relationships leads us a significant step towards generic object recognition."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14848918,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b94c7ff9532ab26c3aedbee3988ec4c7a237c173",
            "isKey": false,
            "numCitedBy": 12812,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We have applied this approach to segmenting static images and found results very encouraging."
            },
            "slug": "Normalized-cuts-and-image-segmentation-Shi-Malik",
            "title": {
                "fragments": [],
                "text": "Normalized cuts and image segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work treats image segmentation as a graph partitioning problem and proposes a novel global criterion, the normalized cut, for segmenting the graph, which measures both the total dissimilarity between the different groups as well as the total similarity within the groups."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143774737"
                        ],
                        "name": "J. Shotton",
                        "slug": "J.-Shotton",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Shotton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shotton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 136
                            }
                        ],
                        "text": "The object segmentation problem can also be approached by pixel or patchbased methods that do not use image segmentation regions, as in [17,18,19] and others."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11200969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9e6771a0eeaf95a9400e4fc94911d258983ffe9",
            "isKey": false,
            "numCitedBy": 312,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of detecting and segmenting partially occluded objects of a known category. We first define a part labelling which densely covers the object. Our Layout Consistent Random Field (LayoutCRF) model then imposes asymmetric local spatial constraints on these labels to ensure the consistent layout of parts whilst allowing for object deformation. Arbitrary occlusions of the object are handled by avoiding the assumption that the whole object is visible. The resulting system is both efficient to train and to apply to novel images, due to a novel annealed layout-consistent expansion move algorithm paired with a randomised decision tree classifier. We apply our technique to images of cars and faces and demonstrate state-of-the-art detection and segmentation performance even in the presence of partial occlusion."
            },
            "slug": "The-Layout-Consistent-Random-Field-for-Recognizing-Winn-Shotton",
            "title": {
                "fragments": [],
                "text": "The Layout Consistent Random Field for Recognizing and Segmenting Partially Occluded Objects"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This paper addresses the problem of detecting and segmenting partially occluded objects of a known category by defining a part labelling which densely covers the object and imposing asymmetric local spatial constraints on these labels to ensure the consistent layout of parts whilst allowing for object deformation."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144891282"
                        ],
                        "name": "David R. Martin",
                        "slug": "David-R.-Martin",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Martin",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David R. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800213"
                        ],
                        "name": "Charless C. Fowlkes",
                        "slug": "Charless-C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charless",
                            "lastName": "Fowlkes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charless C. Fowlkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 136
                            }
                        ],
                        "text": "The second set of nine segmentations is generated using the normalized cuts algorithm with the \u2018probability of boundary\u2019 features as in [11,12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 130
                            }
                        ],
                        "text": "An example of intersections of regions (IofRs) and the 18 segmentations that generated them: 3 from Mean Shift [10], 9 from Ncuts [11,12], and 6 from Felzenszwalb and Huttenlocher\u2019s method [13]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8165754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33a7a59f785ef46091c30c4c85ef88c6bdabab51",
            "isKey": false,
            "numCitedBy": 2381,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness, color, and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, we train a classifier using human labeled images as ground truth. The output of this classifier provides the posterior probability of a boundary at each image location and orientation. We present precision-recall curves showing that the resulting detector significantly outperforms existing approaches. Our two main results are 1) that cue combination can be performed adequately with a simple linear model and 2) that a proper, explicit treatment of texture is required to detect boundaries in natural images."
            },
            "slug": "Learning-to-detect-natural-image-boundaries-using-Martin-Fowlkes",
            "title": {
                "fragments": [],
                "text": "Learning to detect natural image boundaries using local brightness, color, and texture cues"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The two main results are that cue combination can be performed adequately with a simple linear model and that a proper, explicit treatment of texture is required to detect boundaries in natural images."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721683"
                        ],
                        "name": "J. Verbeek",
                        "slug": "J.-Verbeek",
                        "structuredName": {
                            "firstName": "Jakob",
                            "lastName": "Verbeek",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Verbeek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "[14], and is comparable to that of Verbeek and Triggs [17] (however [17] uses a different split of the data)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "We also compare to the more recent work by Verbeek and Triggs [17], although it uses a different data split."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 136
                            }
                        ],
                        "text": "The object segmentation problem can also be approached by pixel or patchbased methods that do not use image segmentation regions, as in [17,18,19] and others."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": "For the MSRC data set, our class-averaged and overall pixel accuracy results out-perform those of [14], and are comparable to those of [17]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 15631740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2060431ccc8dfa586e91adb1eaa4018ac505740f",
            "isKey": true,
            "numCitedBy": 252,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Considerable advances have been made in learning to recognize and localize visual object classes. Simple bag-of-feature approaches label each pixel or patch independently. More advanced models attempt to improve the coherence of the labellings by introducing some form of inter-patch coupling: traditional spatial models such as MRF's provide crisper local labellings by exploiting neighbourhood-level couplings, while aspect models such as PLSA and LDA use global relevance estimates (global mixing proportions for the classes appearing in the image) to shape the local choices. We point out that the two approaches are complementary, combining them to produce aspect-based spatial field models that outperform both approaches. We study two spatial models: one based on averaging over forests of minimal spanning trees linking neighboring image regions, the other on an efficient chain-based Expectation Propagation method for regular 8-neighbor Markov random fields. The models can be trained using either patch-level labels or image-level keywords. As input features they use factored observation models combining texture, color and position cues. Experimental results on the MSR Cambridge data sets show that combining spatial and aspect models significantly improves the region-level classification accuracy. In fact our models trained with image-level labels outperform PLSA trained with pixel-level ones."
            },
            "slug": "Region-Classification-with-Markov-Field-Aspect-Verbeek-Triggs",
            "title": {
                "fragments": [],
                "text": "Region Classification with Markov Field Aspect Models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Combining spatial and aspect models significantly improves the region-level classification accuracy, and models trained with image-level labels outperform PLSA trained with pixel-level ones."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "Classifiers for P ( ci = c s j |rs i , r j , I ) are learned by logistic AdaBoost [6] with the following features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 282
                            }
                        ],
                        "text": "The pij reflect the likelihood that the parent regions of adjacent IofRs belong to the same object in each segmentation:\npij \u221d \u2211\ns\npsij , p s ij =\n{ 1 if rsi = r s j\nP ( csi = c s j |rsi , rsj , I ) otherwise\n(5)\nClassifiers for P ( csi = c s j |rsi , rsj , I ) are learned by logistic AdaBoost [6] with the following features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 314,
                                "start": 306
                            }
                        ],
                        "text": "If we consider the homogeneity as a measure of the likelihood of a particular region, P (rsi |I), then we can write segmentation integration method 2 as:\nP (ci = k|I) \u221d \u2211\ns\nP (rsi |I)P (csi = k|rsi , I) (2)\nThe classifier used to determine P (rsi |I) is a set of boosted decision trees, trained using logistic AdaBoost [30,31]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 48
                            }
                        ],
                        "text": "In an effort to address these concerns, we join [4,5,6,7,8,9] in suggesting the use of multiple segmentations per image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[6] suggest learning a classifier to predict the \u2018homogeneity\u2019 of a region with respect to the class labels."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "The binary potentials penalize discontinuity between adjacent labels as suggested by [6]:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37206,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5276705d71e3dac961ab5d06b86a7b806cc9af64",
            "isKey": false,
            "numCitedBy": 718,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans have an amazing ability to instantly grasp the overall 3D structure of a scene\u2014ground orientation, relative positions of major landmarks, etc.\u2014even from a single image. This ability is completely missing in most popular recognition algorithms, which pretend that the world is flat and/or view it through a patch-sized peephole. Yet it seems very likely that having a grasp of this \u201csurface layout\u201d of a scene should be of great assistance for many tasks, including recognition, navigation, and novel view synthesis.In this paper, we take the first step towards constructing the surface layout, a labeling of the image intogeometric classes. Our main insight is to learn appearance-based models of these geometric classes, which coarsely describe the 3D scene orientation of each image region. Our multiple segmentation framework provides robust spatial support, allowing a wide variety of cues (e.g., color, texture, and perspective) to contribute to the confidence in each geometric label. In experiments on a large set of outdoor images, we evaluate the impact of the individual cues and design choices in our algorithm. We further demonstrate the applicability of our method to indoor images, describe potential applications, and discuss extensions to a more complete notion of surface layout."
            },
            "slug": "Recovering-Surface-Layout-from-an-Image-Hoiem-Efros",
            "title": {
                "fragments": [],
                "text": "Recovering Surface Layout from an Image"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper takes the first step towards constructing the surface layout, a labeling of the image intogeometric classes, to learn appearance-based models of these geometric classes, which coarsely describe the 3D scene orientation of each image region."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 189
                            }
                        ],
                        "text": "An example of intersections of regions (IofRs) and the 18 segmentations that generated them: 3 from Mean Shift [10], 9 from Ncuts [11,12], and 6 from Felzenszwalb and Huttenlocher\u2019s method [13]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 110
                            }
                        ],
                        "text": "The final six segmentations are generated using the graph-based method by Felzenszwalb and Huttenlocher (F-H) [13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The F-H method captures corners and thin, wiry objects more easily, but also produces imprecise boundaries."
                    },
                    "intents": []
                }
            ],
            "corpusId": 207663697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aeeffe327e6c93e9010c7b1e401caa9113723851",
            "isKey": false,
            "numCitedBy": 3751,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions."
            },
            "slug": "Efficient-Graph-Based-Image-Segmentation-Felzenszwalb-Huttenlocher",
            "title": {
                "fragments": [],
                "text": "Efficient Graph-Based Image Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "An efficient segmentation algorithm is developed based on a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image and it is shown that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "the small number of web-based efforts to label data, such as LabelMe [33] and Peekaboom [34], produce inaccurate labels like the \u2018void\u2019 labels throughout this paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 66
                            }
                        ],
                        "text": "Even\nthe small number of web-based efforts to label data, such as LabelMe [33] and Peekaboom [34], produce inaccurate labels like the \u2018void\u2019 labels throughout this paper."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1900911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "092c275005ae49dc1303214f6d02d134457c7053",
            "isKey": false,
            "numCitedBy": 3076,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nWe seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.\n"
            },
            "slug": "LabelMe:-A-Database-and-Web-Based-Tool-for-Image-Russell-Torralba",
            "title": {
                "fragments": [],
                "text": "LabelMe: A Database and Web-Based Tool for Image Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A web-based tool that allows easy image annotation and instant sharing of such annotations is developed and a large dataset that spans many object categories, often containing multiple instances over a wide variety of images is collected."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144035504"
                        ],
                        "name": "Z. Tu",
                        "slug": "Z.-Tu",
                        "structuredName": {
                            "firstName": "Zhuowen",
                            "lastName": "Tu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772671"
                        ],
                        "name": "Xiangrong Chen",
                        "slug": "Xiangrong-Chen",
                        "structuredName": {
                            "firstName": "Xiangrong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangrong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 139
                            }
                        ],
                        "text": "Many of the existing approaches to using bottom-up segmentation for recognition have higher complexity and are less intuitive than our own [8,14,16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 48
                            }
                        ],
                        "text": "In an effort to address these concerns, we join [4,5,6,7,8,9] in suggesting the use of multiple segmentations per image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1752880,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cca9200d9da958b7f90eab901b2f30c04f1e0e9c",
            "isKey": false,
            "numCitedBy": 656,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a Bayesian framework for parsing images into their constituent visual patterns. The parsing algorithm optimizes the posterior probability and outputs a scene representation as a \u201cparsing graph\u201d, in a spirit similar to parsing sentences in speech and natural language. The algorithm constructs the parsing graph and re-configures it dynamically using a set of moves, which are mostly reversible Markov chain jumps. This computational framework integrates two popular inference approaches\u2014generative (top-down) methods and discriminative (bottom-up) methods. The former formulates the posterior probability in terms of generative models for images defined by likelihood functions and priors. The latter computes discriminative probabilities based on a sequence (cascade) of bottom-up tests/filters. In our Markov chain algorithm design, the posterior probability, defined by the generative models, is the invariant (target) probability for the Markov chain, and the discriminative probabilities are used to construct proposal probabilities to drive the Markov chain. Intuitively, the bottom-up discriminative probabilities activate top-down generative models. In this paper, we focus on two types of visual patterns\u2014generic visual patterns, such as texture and shading, and object patterns including human faces and text. These types of patterns compete and cooperate to explain the image and so image parsing unifies image segmentation, object detection, and recognition (if we use generic visual patterns only then image parsing will correspond to image segmentation (Tu and Zhu, 2002. IEEE Trans. PAMI, 24(5):657\u2013673). We illustrate our algorithm on natural images of complex city scenes and show examples where image segmentation can be improved by allowing object specific knowledge to disambiguate low-level segmentation cues, and conversely where object detection can be improved by using generic visual patterns to explain away shadows and occlusions."
            },
            "slug": "Image-Parsing:-Unifying-Segmentation,-Detection,-Tu-Chen",
            "title": {
                "fragments": [],
                "text": "Image Parsing: Unifying Segmentation, Detection, and Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A Bayesian framework for parsing images into their constituent visual patterns that optimizes the posterior probability and outputs a scene representation as a \u201cparsing graph\u201d, in a spirit similar to parsing sentences in speech and natural language is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144035504"
                        ],
                        "name": "Z. Tu",
                        "slug": "Z.-Tu",
                        "structuredName": {
                            "firstName": "Zhuowen",
                            "lastName": "Tu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 139
                            }
                        ],
                        "text": "Many of the existing approaches to using bottom-up segmentation for recognition have higher complexity and are less intuitive than our own [8,14,16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20918902,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c045011905caf139326bcbd944398053dbb33e21",
            "isKey": false,
            "numCitedBy": 371,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a computational paradigm called Data-Driven Markov Chain Monte Carlo (DDMCMC) for image segmentation in the Bayesian statistical framework. The paper contributes to image segmentation in four aspects. First, it designs efficient and well-balanced Markov Chain dynamics to explore the complex solution space and, thus, achieves a nearly global optimal solution independent of initial segmentations. Second, it presents a mathematical principle and a K-adventurers algorithm for computing multiple distinct solutions from the Markov chain sequence and, thus, it incorporates intrinsic ambiguities in image segmentation. Third, it utilizes data-driven (bottom-up) techniques, such as clustering and edge detection, to compute importance proposal probabilities, which drive the Markov chain dynamics and achieve tremendous speedup in comparison to the traditional jump-diffusion methods. Fourth, the DDMCMC paradigm provides a unifying framework in which the role of many existing segmentation algorithms, such as, edge detection, clustering, region growing, split-merge, snake/balloon, and region competition, are revealed as either realizing Markov chain dynamics or computing importance proposal probabilities. Thus, the DDMCMC paradigm combines and generalizes these segmentation methods in a principled way. The DDMCMC paradigm adopts seven parametric and nonparametric image models for intensity and color at various regions. We test the DDMCMC paradigm extensively on both color and gray-level images and some results are reported in this paper."
            },
            "slug": "Image-Segmentation-by-Data-Driven-Markov-Chain-Tu-Zhu",
            "title": {
                "fragments": [],
                "text": "Image Segmentation by Data-Driven Markov Chain Monte Carlo"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The DDMCMC paradigm provides a unifying framework in which the role of many existing segmentation algorithms are revealed as either realizing Markov chain dynamics or computing importance proposal probabilities and generalizes these segmentation methods in a principled way."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144891282"
                        ],
                        "name": "David R. Martin",
                        "slug": "David-R.-Martin",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Martin",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David R. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800213"
                        ],
                        "name": "Charless C. Fowlkes",
                        "slug": "Charless-C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charless",
                            "lastName": "Fowlkes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charless C. Fowlkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082299938"
                        ],
                        "name": "D. Tal",
                        "slug": "D.-Tal",
                        "structuredName": {
                            "firstName": "Doron",
                            "lastName": "Tal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 56
                            }
                        ],
                        "text": "Even humans do not agree on a \u2018correct\u2019 image partition [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 64193,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a1ed876196ec9733acb1daa6d65e35ff0414291",
            "isKey": false,
            "numCitedBy": 6038,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes. We define an error measure which quantifies the consistency between segmentations of differing granularities and find that different human segmentations of the same image are highly consistent. Use of this dataset is demonstrated in two applications: (1) evaluating the performance of segmentation algorithms and (2) measuring probability distributions associated with Gestalt grouping factors as well as statistics of image region properties."
            },
            "slug": "A-database-of-human-segmented-natural-images-and-to-Martin-Fowlkes",
            "title": {
                "fragments": [],
                "text": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes is presented and an error measure is defined which quantifies the consistency between segmentations of differing granularities."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685020"
                        ],
                        "name": "D. Comaniciu",
                        "slug": "D.-Comaniciu",
                        "structuredName": {
                            "firstName": "Dorin",
                            "lastName": "Comaniciu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Comaniciu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145776090"
                        ],
                        "name": "P. Meer",
                        "slug": "P.-Meer",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Meer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Meer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "An example of intersections of regions (IofRs) and the 18 segmentations that generated them: 3 from Mean Shift [10], 9 from Ncuts [11,12], and 6 from Felzenszwalb and Huttenlocher\u2019s method [13]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "The first three segmentations are generated by the mean shift-based segmentation algorithm [10] using pixel position, color (in the L*u*v* color space), and a histogram of quantized textons as features [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 691081,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74f4ecc3e4e5b91fbb54330b285ed5214afe2001",
            "isKey": false,
            "numCitedBy": 11480,
            "numCiting": 122,
            "paperAbstract": {
                "fragments": [],
                "text": "A general non-parametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. The basic computational module of the technique is an old pattern recognition procedure: the mean shift. For discrete data, we prove the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density. The relation of the mean shift procedure to the Nadaraya-Watson estimator from kernel regression and the robust M-estimators; of location is also established. Algorithms for two low-level vision tasks discontinuity-preserving smoothing and image segmentation - are described as applications. In these algorithms, the only user-set parameter is the resolution of the analysis, and either gray-level or color images are accepted as input. Extensive experimental results illustrate their excellent performance."
            },
            "slug": "Mean-Shift:-A-Robust-Approach-Toward-Feature-Space-Comaniciu-Meer",
            "title": {
                "fragments": [],
                "text": "Mean Shift: A Robust Approach Toward Feature Space Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is proved the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2820687"
                        ],
                        "name": "Joost van de Weijer",
                        "slug": "Joost-van-de-Weijer",
                        "structuredName": {
                            "firstName": "Joost",
                            "lastName": "Weijer",
                            "middleNames": [
                                "van",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joost van de Weijer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "Color is described by a 100-dimensional histogram of quantized hue features [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206596226,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4fb18a9f0d3a849d314daef1053d369998c5144",
            "isKey": false,
            "numCitedBy": 544,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Although color is commonly experienced as an indispensable quality in describing the world around us, state-of-the art local feature-based representations arc mostly based on shape description, and ignore color information. The description of color is hampered by the large amount of variations which causes the measured color values to vary significantly. In this paper we aim to extend the description of local features with color information. To accomplish a wide applicability of the color descriptor, it should be robust to: 1. photometric changes commonly encountered in the real world, 2. varying image quality; from high quality images to snap-shot photo quality and compressed internet images. Based on these requirements we derive a set of color descriptors. The set of proposed descriptors are compared by extensive testing on multiple applications areas, namely, matching, retrieval and classification, and on a wide variety of image qualities. The results show that color descriptors remain reliable under photometric and geometrical changes, and with decreasing image quality. For all experiments a combination of color and shape outperforms a pure shape-based approach."
            },
            "slug": "Coloring-Local-Feature-Extraction-Weijer-Schmid",
            "title": {
                "fragments": [],
                "text": "Coloring Local Feature Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results show that color descriptors remain reliable under photometric and geometrical changes, and with decreasing image quality, and for all experiments a combination of color and shape outperforms a pure shape-based approach."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692688"
                        ],
                        "name": "Yuri Boykov",
                        "slug": "Yuri-Boykov",
                        "structuredName": {
                            "firstName": "Yuri",
                            "lastName": "Boykov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuri Boykov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1922280"
                        ],
                        "name": "O. Veksler",
                        "slug": "O.-Veksler",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Veksler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Veksler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2984143"
                        ],
                        "name": "R. Zabih",
                        "slug": "R.-Zabih",
                        "structuredName": {
                            "firstName": "Ramin",
                            "lastName": "Zabih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zabih"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "We enforce that E(ci, cj)\u2265 0 and use graph cuts with alpha-expansion to minimize the energy [32]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2430892,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3120324069ec20eed853d3f9bbbceb32e4173b93",
            "isKey": false,
            "numCitedBy": 3913,
            "numCiting": 116,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we address the problem of minimizing a large class of energy functions that occur in early vision. The major restriction is that the energy function's smoothness term must only involve pairs of pixels. We propose two algorithms that use graph cuts to compute a local minimum even when very large moves are allowed. The first move we consider is an /spl alpha/-/spl beta/-swap: for a pair of labels /spl alpha/,/spl beta/, this move exchanges the labels between an arbitrary set of pixels labeled a and another arbitrary set labeled /spl beta/. Our first algorithm generates a labeling such that there is no swap move that decreases the energy. The second move we consider is an /spl alpha/-expansion: for a label a, this move assigns an arbitrary set of pixels the label /spl alpha/. Our second algorithm, which requires the smoothness term to be a metric, generates a labeling such that there is no expansion move that decreases the energy. Moreover, this solution is within a known factor of the global minimum. We experimentally demonstrate the effectiveness of our approach on image restoration, stereo and motion."
            },
            "slug": "Fast-approximate-energy-minimization-via-graph-cuts-Boykov-Veksler",
            "title": {
                "fragments": [],
                "text": "Fast approximate energy minimization via graph cuts"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes two algorithms that use graph cuts to compute a local minimum even when very large moves are allowed, and generates a labeling such that there is no expansion move that decreases the energy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3328108"
                        ],
                        "name": "Luis von Ahn",
                        "slug": "Luis-von-Ahn",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Ahn",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis von Ahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3319095"
                        ],
                        "name": "Ruoran Liu",
                        "slug": "Ruoran-Liu",
                        "structuredName": {
                            "firstName": "Ruoran",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruoran Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143624243"
                        ],
                        "name": "M. Blum",
                        "slug": "M.-Blum",
                        "structuredName": {
                            "firstName": "Manuel",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Blum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "the small number of web-based efforts to label data, such as LabelMe [33] and Peekaboom [34], produce inaccurate labels like the \u2018void\u2019 labels throughout this paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 83
                            }
                        ],
                        "text": "Even\nthe small number of web-based efforts to label data, such as LabelMe [33] and Peekaboom [34], produce inaccurate labels like the \u2018void\u2019 labels throughout this paper."
                    },
                    "intents": []
                }
            ],
            "corpusId": 207158556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fff3876663581b51273cde3f6151960b75a0e12f",
            "isKey": false,
            "numCitedBy": 631,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Peekaboom, an entertaining web-based game that can help computers locate objects in images. People play the game because of its entertainment value, and as a side effect of them playing, we collect valuable image metadata, such as which pixels belong to which object in the image. The collected data could be applied towards constructing more accurate computer vision algorithms, which require massive amounts of training and testing data not currently available. Peekaboom has been played by thousands of people, some of whom have spent over 12 hours a day playing, and thus far has generated millions of data points. In addition to its purely utilitarian aspect, Peekaboom is an example of a new, emerging class of games, which not only bring people together for leisure purposes, but also exist to improve artificial intelligence. Such games appeal to a general audience, while providing answers to problems that computers cannot yet solve."
            },
            "slug": "Peekaboom:-a-game-for-locating-objects-in-images-Ahn-Liu",
            "title": {
                "fragments": [],
                "text": "Peekaboom: a game for locating objects in images"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "Peekaboom is an entertaining web-based game that can help computers locate objects in images and is an example of a new, emerging class of games, which not only bring people together for leisure purposes, but also exist to improve artificial intelligence."
            },
            "venue": {
                "fragments": [],
                "text": "CHI"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 282
                            }
                        ],
                        "text": "The pij reflect the likelihood that the parent regions of adjacent IofRs belong to the same object in each segmentation:\npij \u221d \u2211\ns\npsij , p s ij =\n{ 1 if rsi = r s j\nP ( csi = c s j |rsi , rsj , I ) otherwise\n(5)\nClassifiers for P ( csi = c s j |rsi , rsj , I ) are learned by logistic AdaBoost [6] with the following features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 314,
                                "start": 306
                            }
                        ],
                        "text": "If we consider the homogeneity as a measure of the likelihood of a particular region, P (rsi |I), then we can write segmentation integration method 2 as:\nP (ci = k|I) \u221d \u2211\ns\nP (rsi |I)P (csi = k|rsi , I) (2)\nThe classifier used to determine P (rsi |I) is a set of boosted decision trees, trained using logistic AdaBoost [30,31]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 112
                            }
                        ],
                        "text": "The classifier used to determine P (r i |I) is a set of boosted decision trees, trained using logistic AdaBoost [30,31]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207651918,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b54c9359e8858842d1b1b744ac5ca573b8031dcc",
            "isKey": false,
            "numCitedBy": 662,
            "numCiting": 103,
            "paperAbstract": {
                "fragments": [],
                "text": "We give a unified account of boosting and logistic regression in which each learning problem is cast in terms of optimization of Bregman distances. The striking similarity of the two problems in this framework allows us to design and analyze algorithms for both simultaneously, and to easily adapt algorithms designed for one problem to the other. For both problems, we give new algorithms and explain their potential advantages over existing methods. These algorithms are iterative and can be divided into two types based on whether the parameters are updated sequentially (one at a time) or in parallel (all at once). We also describe a parameterized family of algorithms that includes both a sequential- and a parallel-update algorithm as special cases, thus showing how the sequential and parallel approaches can themselves be unified. For all of the algorithms, we give convergence proofs using a general formalization of the auxiliary-function proof technique. As one of our sequential-update algorithms is equivalent to AdaBoost, this provides the first general proof of convergence for AdaBoost. We show that all of our algorithms generalize easily to the multiclass case, and we contrast the new algorithms with the iterative scaling algorithm. We conclude with a few experimental results with synthetic data that highlight the behavior of the old and newly proposed algorithms in different settings."
            },
            "slug": "Logistic-Regression,-AdaBoost-and-Bregman-Distances-Collins-Schapire",
            "title": {
                "fragments": [],
                "text": "Logistic Regression, AdaBoost and Bregman Distances"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A unified account of boosting and logistic regression in which each learning problem is cast in terms of optimization of Bregman distances, and a parameterized family of algorithms that includes both a sequential- and a parallel-update algorithm as special cases are described, thus showing how the sequential and parallel approaches can themselves be unified."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 282
                            }
                        ],
                        "text": "The pij reflect the likelihood that the parent regions of adjacent IofRs belong to the same object in each segmentation:\npij \u221d \u2211\ns\npsij , p s ij =\n{ 1 if rsi = r s j\nP ( csi = c s j |rsi , rsj , I ) otherwise\n(5)\nClassifiers for P ( csi = c s j |rsi , rsj , I ) are learned by logistic AdaBoost [6] with the following features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 314,
                                "start": 306
                            }
                        ],
                        "text": "If we consider the homogeneity as a measure of the likelihood of a particular region, P (rsi |I), then we can write segmentation integration method 2 as:\nP (ci = k|I) \u221d \u2211\ns\nP (rsi |I)P (csi = k|rsi , I) (2)\nThe classifier used to determine P (rsi |I) is a set of boosted decision trees, trained using logistic AdaBoost [30,31]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 112
                            }
                        ],
                        "text": "The classifier used to determine P (r i |I) is a set of boosted decision trees, trained using logistic AdaBoost [30,31]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2354909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "277c2139eb4e11455a0b16759b7249c3b95b479e",
            "isKey": false,
            "numCitedBy": 1349,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The main and important contribution of this paper is in establishing a connection between boosting, a newcomer to the statistics scene, and additive models. One of the main properties of boosting that has made it interesting to statisticians and others is its relative (but not complete) immunity to overrtting. As pointed out by the authors, the current paper does not address this issue. Leo Breiman 1] tried to explain this behaviour in terms of bias and variance. In our paper with Bartlett and Lee 4], we gave an explanation in terms of the \\margins\" of the training examples and the VC-dimension of the base class. Breiman, as well as the current paper, point out that our bounds are very rough and yield bounds that are not useful in practice. While this is clearly true at this time, it is also true that the analysis given by Breiman and by this paper yield no provable bounds whatsoever. It is completely unclear whether this analysis can be used to predict the performance of classiication rules outside of the training sample. At the root of this argument about boosting is a much more fundamental argument about the type of prior assumptions that one should make when embarking on the task of inducing a classiication rule from data. The assumption that seems to underlie the use of maximum likelihood in the 1"
            },
            "slug": "Discussion-of-the-Paper-\\additive-Logistic-a-View-Friedman-Hastie",
            "title": {
                "fragments": [],
                "text": "Discussion of the Paper \\additive Logistic Regression: a Statistical View of Boosting\" By"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This paper establishes a connection between boosting, a newcomer to the statistics scene, and additive models and investigates the assumption that seems to underlie the use of maximum likelihood in the additive models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3283682"
                        ],
                        "name": "Arik Azran",
                        "slug": "Arik-Azran",
                        "structuredName": {
                            "firstName": "Arik",
                            "lastName": "Azran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arik Azran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 48
                            }
                        ],
                        "text": "In an effort to address these concerns, we join [4,5,6,7,8,9] in suggesting the use of multiple segmentations per image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9637481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b597b43e1c28868a9b08b2450950b22241fde541",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Spectral clustering is a simple yet powerful method for finding structure in data using spectral properties of an associated pairwise similarity matrix. This paper provides new insights into how the method works and uses these to derive new algorithms which given the data alone automatically learn different plausible data partitionings. The main theoretical contribution is a generalization of a key result in the field, the multicut lemma [7]. We use this generalization to derive two algorithms. The first uses the eigenvalues of a given affinity matrix to infer the number of clusters in data, and the second combines learning the affinity matrix with inferring the number of clusters. A hierarchical implementation of the algorithms is also derived. The algorithms are theoretically motivated and demonstrated on nontrivial data sets."
            },
            "slug": "Spectral-Methods-for-Automatic-Multiscale-Data-Azran-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Spectral Methods for Automatic Multiscale Data Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper provides new insights into how the method works and uses these to derive new algorithms which given the data alone automatically learn different plausible data partitionings."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644050191"
                        ],
                        "name": "G. LoweDavid",
                        "slug": "G.-LoweDavid",
                        "structuredName": {
                            "firstName": "G",
                            "lastName": "LoweDavid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. LoweDavid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 185
                            }
                        ],
                        "text": "The image structure within and near a region is captured by a 300-dimensional region-based context feature (RCF) [5], which is based on a distance-weighted histogram of quantized SIFTs [26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 174065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4cab9c4b571761203ed4c3a4c5a07dd615f57a91",
            "isKey": false,
            "numCitedBy": 25501,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ..."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-LoweDavid",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3182415"
                        ],
                        "name": "Tingyao Wu",
                        "slug": "Tingyao-Wu",
                        "structuredName": {
                            "firstName": "Tingyao",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tingyao Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2567246"
                        ],
                        "name": "R. C. Weng",
                        "slug": "R.-C.-Weng",
                        "structuredName": {
                            "firstName": "Ruby",
                            "lastName": "Weng",
                            "middleNames": [
                                "Chiu-Hsing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Weng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "LIBSVM provides P (cr = k|r), the probability that the label of region cr is k, as in [24], and our classification of the region is argmaxkP (cr = k|r)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7664224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d02083f5e9206ff88ed1e284da8665d4bee81ba",
            "isKey": false,
            "numCitedBy": 1891,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Pairwise coupling is a popular multi-class classification method that combines all comparisons for each pair of classes. This paper presents two approaches for obtaining class probabilities. Both methods can be reduced to linear systems and are easy to implement. We show conceptually and experimentally that the proposed approaches are more stable than the two existing popular methods: voting and the method by Hastie and Tibshirani (1998)"
            },
            "slug": "Probability-Estimates-for-Multi-class-by-Pairwise-Wu-Lin",
            "title": {
                "fragments": [],
                "text": "Probability Estimates for Multi-class Classification by Pairwise Coupling"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Two approaches for obtaining class probabilities can be reduced to linear systems and are easy to implement and shown conceptually and experimentally that the proposed approaches are more stable than the two existing popular methods."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 216
                            }
                        ],
                        "text": "Although the official metric for the PASCAL challenge was class-averaged accuracy, examining the overall pixel accuracy provides a very different picture with Brookes achieving 58.4%, our method achieving 50.1%, and TKK achieving 24.4% accuracy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "The TKK entry [28] does out-perform ours, however it is not directly comparable as it was an entry in the detection challenge and so trained on thousands of additional images not in the 422-image segmentation training set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "The TKK [28] entry had higher performance, but as an entry into the detection challenge it was trained using a much larger data set of"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Helsinki University of Technology, PASCAL VOC2007 Challenge"
            },
            "venue": {
                "fragments": [],
                "text": "http://www.pascal-network.org/challenges/VOC/voc2007"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 216
                            }
                        ],
                        "text": "Although the official metric for the PASCAL challenge was class-averaged accuracy, examining the overall pixel accuracy provides a very different picture with Brookes achieving 58.4%, our method achieving 50.1%, and TKK achieving 24.4% accuracy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "The TKK [28] entry had higher performance, but as an entry into the detection challenge it was trained using a much larger data set of thousands of images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "The TKK entry [28] does out-perform ours, however it is not directly comparable as it was an entry in the detection challenge and so trained on thousands of additional images not in the 422-image segmentation training set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Helsinki University of Technology, PASCAL"
            },
            "venue": {
                "fragments": [],
                "text": "VOC 2007 Challenge"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "We enforce that E(ci, cj)\u2265 0 and use graph cuts with alpha-expansion to minimize the energy [32]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 45232003,
            "fieldsOfStudy": [],
            "id": "a57520b68e73b5e1fc3668b443daf74ebe957cc7",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast Approximate Energy Minimization via Graph Cuts"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112683288"
                        ],
                        "name": "Srinath Sridhar",
                        "slug": "Srinath-Sridhar",
                        "structuredName": {
                            "firstName": "Srinath",
                            "lastName": "Sridhar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Srinath Sridhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49876729"
                        ],
                        "name": "Franziska Mueller",
                        "slug": "Franziska-Mueller",
                        "structuredName": {
                            "firstName": "Franziska",
                            "lastName": "Mueller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Franziska Mueller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699058"
                        ],
                        "name": "M. Zollh\u00f6fer",
                        "slug": "M.-Zollh\u00f6fer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Zollh\u00f6fer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Zollh\u00f6fer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1863006"
                        ],
                        "name": "D. Casas",
                        "slug": "D.-Casas",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Casas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Casas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2663734"
                        ],
                        "name": "Antti Oulasvirta",
                        "slug": "Antti-Oulasvirta",
                        "structuredName": {
                            "firstName": "Antti",
                            "lastName": "Oulasvirta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antti Oulasvirta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680185"
                        ],
                        "name": "C. Theobalt",
                        "slug": "C.-Theobalt",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Theobalt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Theobalt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5238630,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fdbcf7d898f25bb745519e705ed31b2327ff24e7",
            "isKey": false,
            "numCitedBy": 878,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computer-Vision-\u2013-ECCV-2006-Sridhar-Mueller",
            "title": {
                "fragments": [],
                "text": "Computer Vision \u2013 ECCV 2006"
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Computer Science"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "The comparisons in this paper are performed on two difficult data sets, the MSRC 21-class data set [21] and the PASCAL Visual Object Challenge 2007 segmentation competition [22] data set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The MSRC 21-class object recognition database"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 173
                            }
                        ],
                        "text": "The comparisons in this paper are performed on two difficult data sets, the MSRC 21-class data set [21] and the PASCAL Visual Object Challenge 2007 segmentation competition [22] data set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The PASCAL VOC2007"
            },
            "venue": {
                "fragments": [],
                "text": "http://www.pascal-network.org/challenges/VOC/voc2007"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "We compare our approach with that of the Oxford Brookes [27] entry into the segmentation competition [27]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "For the PASCAL 2007 data set, we out-perform the Oxford Brookes entry [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Oxford Brookes entry, PASCAL VOC2007 Segmentation Challenge"
            },
            "venue": {
                "fragments": [],
                "text": "http://www.pascal-network.org/challenges/VOC/voc2007"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 173
                            }
                        ],
                        "text": "The comparisons in this paper are performed on two difficult data sets, the MSRC 21-class data set [21] and the PASCAL Visual Object Challenge 2007 segmentation competition [22] data set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The PASCAL VOC"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 20,
            "methodology": 21,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 39,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Object-Recognition-by-Integrating-Multiple-Image-Pantofaru-Schmid/0f78804622e975deff527e9401701f738c6f51a8?sort=total-citations"
}