{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1762283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e68c54f39e87daf3a8bdc0ee005aece3c652d11",
            "isKey": false,
            "numCitedBy": 3960,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Although Bayesian analysis has been in use since Laplace, the Bayesian method of model-comparison has only recently been developed in depth. In this paper, the Bayesian approach to regularization and model-comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other data modeling problems. Regularizing constants are set by examining their posterior probability distribution. Alternative regularizers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. Occam's razor is automatically embodied by this process. The way in which Bayes infers the values of regularizing constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling."
            },
            "slug": "Bayesian-Interpolation-Mackay",
            "title": {
                "fragments": [],
                "text": "Bayesian Interpolation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The Bayesian approach to regularization and model-comparison is demonstrated by studying the inference problem of interpolating noisy data by examining the posterior probability distribution of regularizing constants and noise levels."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 118
                            }
                        ],
                        "text": "Problems for which models can be written in the form (2) include linear interpolation with a xed basisset (Gull 1988; MacKay 1992a), nonlinear regression with a neural network (MacKay 1992c), nonlinearclassi cation (MacKay 1992b), and image deconvolution (Gull 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This article is concerned with an alternative Bayesian viewpoint of neural network learning ( MacKay, 1991, 1992c;  Buntine & Weigend, 1991; Neal, 1993a, 1996; Ripley, 1996), in which the data error is interpreted as defining a likelihood function, and the regularizer corresponds to a prior probability distribution over the weights.,Methods for controlling these multiple hyperparameters were developed by  MacKay (1991)  using gaussian approximations and by Neal (1993a) using Markov chain Monte Carlo methods.,In this article I compare the approximate strategies of  MacKay (1991)  and Buntine and Weigend (1991) for handling hyperparameters, assuming a Bayesian approach to neural networks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 244
                            }
                        ],
                        "text": "Much interest has centred on the question, for models like the one de ned in equations (3{4), of howthe constants and | or the ratio = | should be set, and Gull (1989) has derived an appealingBayesian prescription for these constants (see also MacKay (1992a) for a review)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 256
                            }
                        ],
                        "text": "\u2026applied to nonlinear models by makingappropriate local linearizations (so that the integral over the parameters is made approximately ratherthan exactly) and has been used successfully in image reconstruction (Gull 1989; Weir 1991) and in neuralnetworks (MacKay 1992c; Thodberg 1993; MacKay 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 66
                            }
                        ],
                        "text": "Under general conditions, the error bars on log are log jD ' p2= (MacKay 1992a) (seesection 8)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 32
                            }
                        ],
                        "text": "This procedure is suggested in (MacKay 1992c) as a `quick and dirty' approximation to the evidence frame-work."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 153
                            }
                        ],
                        "text": "There is then an analogous choice between either (a) optimizing separately ateach local optimum in w, and using a Gaussian approximation conditioned on (MacKay 1992c); or (b) tting multiple Gaussians to local maxima of the true posterior with the hyperparameter integrated out."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 129
                            }
                        ],
                        "text": "This paper is concerned with an alternative Bayesian viewpoint of neural network learning (MacKay1991; Buntine and Weigend 1991; MacKay 1992c; Neal 1993a; Ripley 1995; Neal 1996) in which the dataerror is interpreted as de ning a likelihood function, and the regularizer corresponds to a prior\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 15
                            }
                        ],
                        "text": "As reviewed in MacKay (1992a), the most probable value of satis es a simple implicit equation,1 MP = Pk1 w2i (16)where wi are the components of the vector wMPj MP and is the number of well{determined parameters,which can be expressed in terms of the eigenvalues a of the matrix rrED(w): = k Trace =\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 17
                            }
                        ],
                        "text": "In (MacKay 1991; MacKay 1992c) it was shown that it made theoretical sense, and couldbe practically bene cial, to use multiple hyperparameters f cg, each one controlling a di erent aspect ofthe prior probability distribution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123141880,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c528d4eb732bf4de66566ff7b502b3311560cb08",
            "isKey": false,
            "numCitedBy": 421,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian framework for model comparison and regularisation is demonstrated by studying interpolation and classification problems modelled with both linear and non-linear models. This framework quantitatively embodies 'Occam's razor'. Over-complex and under-regularised models are automatically inferred to be less probable, even though their flexibility allows them to fit the data better. \nWhen applied to 'neural networks', the Bayesian framework makes possible (1) objective comparison of solutions using alternative network architectures; (2) objective stopping rules for network pruning or growing procedures; (3) objective choice of type of weight decay terms (or regularisers); (4) on-line techniques for optimising weight decay (or regularisation constant) magnitude; (5) a measure of the effective number of well-determined parameters in a model; (6) quantified estimates of the error bars on network parameters and on network output. In the case of classification models, it is shown that the careful incorporation of error bar information into a classifier's predictions yields improved performance. \nComparisons of the inferences of the Bayesian framework with more traditional cross-validation methods help detect poor underlying assumptions in learning models. \nThe relationship of the Bayesian learning framework to 'active learning' is examined. Objective functions are discussed which measure the expected informativeness of candidate data measurements, in the context of both interpolation and classification problems. \nThe concepts and methods described in this thesis are quite general and will be applicable to other data modelling problems whether they involve regression, classification or density estimation."
            },
            "slug": "Bayesian-methods-for-adaptive-models-Mackay",
            "title": {
                "fragments": [],
                "text": "Bayesian methods for adaptive models"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "The Bayesian framework for model comparison and regularisation is demonstrated by studying interpolation and classification problems modelled with both linear and non-linear models, and it is shown that the careful incorporation of error bar information into a classifier's predictions yields improved performance."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7748868,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d565fb42892f20c52b9fc615cc537835f30d094",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian \"evidence\" approximation has recently been employed to determine the noise and weight-penalty terms used in back-propagation. This paper shows that for neural nets it is far easier to use the exact result than it is to use the evidence approximation. Moreover, unlike the evidence approximation, the exact result neither has to be re-calculated for every new data set, nor requires the running of computer code (the exact result is closed form). In addition, it turns out that the evidence procedure's MAP estimate for neural nets is, in toto, approximation error. Another advantage of the exact analysis is that it does not lead one to incorrect intuition, like the claim that using evidence one can \"evaluate different priors in light of the data\". This paper also discusses sufficiency conditions for the evidence approximation to hold, why it can sometimes give \"reasonable\" results, etc."
            },
            "slug": "On-the-Use-of-Evidence-in-Neural-Networks-Wolpert",
            "title": {
                "fragments": [],
                "text": "On the Use of Evidence in Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It turns out that the evidence procedure's MAP estimate for neural nets is, in toto, approximation error, and the exact result neither has to be re-calculated for every new data set, nor requires the running of computer code."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 118
                            }
                        ],
                        "text": "Problems for which models can be written in the form (2) include linear interpolation with a xed basisset (Gull 1988; MacKay 1992a), nonlinear regression with a neural network (MacKay 1992c), nonlinearclassi cation (MacKay 1992b), and image deconvolution (Gull 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 244
                            }
                        ],
                        "text": "Much interest has centred on the question, for models like the one de ned in equations (3{4), of howthe constants and | or the ratio = | should be set, and Gull (1989) has derived an appealingBayesian prescription for these constants (see also MacKay (1992a) for a review)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 256
                            }
                        ],
                        "text": "\u2026applied to nonlinear models by makingappropriate local linearizations (so that the integral over the parameters is made approximately ratherthan exactly) and has been used successfully in image reconstruction (Gull 1989; Weir 1991) and in neuralnetworks (MacKay 1992c; Thodberg 1993; MacKay 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 66
                            }
                        ],
                        "text": "Under general conditions, the error bars on log are log jD ' p2= (MacKay 1992a) (seesection 8)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 32
                            }
                        ],
                        "text": "This procedure is suggested in (MacKay 1992c) as a `quick and dirty' approximation to the evidence frame-work."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 153
                            }
                        ],
                        "text": "There is then an analogous choice between either (a) optimizing separately ateach local optimum in w, and using a Gaussian approximation conditioned on (MacKay 1992c); or (b) tting multiple Gaussians to local maxima of the true posterior with the hyperparameter integrated out."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 129
                            }
                        ],
                        "text": "This paper is concerned with an alternative Bayesian viewpoint of neural network learning (MacKay1991; Buntine and Weigend 1991; MacKay 1992c; Neal 1993a; Ripley 1995; Neal 1996) in which the dataerror is interpreted as de ning a likelihood function, and the regularizer corresponds to a prior\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 15
                            }
                        ],
                        "text": "As reviewed in MacKay (1992a), the most probable value of satis es a simple implicit equation,1 MP = Pk1 w2i (16)where wi are the components of the vector wMPj MP and is the number of well{determined parameters,which can be expressed in terms of the eigenvalues a of the matrix rrED(w): = k Trace =\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 17
                            }
                        ],
                        "text": "In (MacKay 1991; MacKay 1992c) it was shown that it made theoretical sense, and couldbe practically bene cial, to use multiple hyperparameters f cg, each one controlling a di erent aspect ofthe prior probability distribution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16543854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b959164d1efca4b73986ba5d21e664aadbbc0457",
            "isKey": false,
            "numCitedBy": 2590,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian \"evidence\" automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained."
            },
            "slug": "A-Practical-Bayesian-Framework-for-Backpropagation-Mackay",
            "title": {
                "fragments": [],
                "text": "A Practical Bayesian Framework for Backpropagation Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks that automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052393"
                        ],
                        "name": "H. H. Thodberg",
                        "slug": "H.-H.-Thodberg",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Thodberg",
                            "middleNames": [
                                "Henrik"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. H. Thodberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15593225,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3fb617767f9e500e84ed03fb48acdcf088f33dc",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "MacKay's Bayesian framework for backpropagation is a practical and powerful means of improving the generalisation ability of neural networks. The framework is reviewed and extended in a pedagogical way. The notation is simpliied using the ordinary weight decay parameter, and the noise parameter is shown to be nothing more than an overall scale. A detailed and explicit procedure for adjusting several weight decay parameters is given. Pruning is incorporated into the Bayesian framework. Appropriate symmetry factors on sparse architectures are deduced. Bayesian weight decay is demonstrated using artiicial data generated by a sparsely connected network. Pruning yields computational advantages: by removing unimportant weights the posterior weight distribution becomes Gaussian, and pruning removes zero-modes of the Hessian and redundant hidden units. In addition, pruning improves generalisation. The Bayesian evidence is used as a stop criterion for pruning. Bayesian backprop is applied in the prediction of fat content in minced meat from near infrared spectra. It outperforms \\early stopping\" as well as quadratic regression. The evidence of a committee of diierently trained networks is computed and the corresponding improved generalisation is veriied. The error bars on the predictions of the fat content are computed. There are three contributors: The random noise, the uncertainty in the weights, and the deviation among the committee members. Finally the Bayesian framework is compared to Moody's GPE."
            },
            "slug": "Ace-of-Bayes-:-Application-of-Neural-Thodberg",
            "title": {
                "fragments": [],
                "text": "Ace of Bayes : Application of Neural"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Bayesian backprop is applied in the prediction of fat content in minced meat from near infrared spectra and outperforms \\early stopping\" as well as quadratic regression."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17947141,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "9f87a11a523e4680e61966e36ea2eac516096f23",
            "isKey": false,
            "numCitedBy": 2597,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible."
            },
            "slug": "A-View-of-the-Em-Algorithm-that-Justifies-Sparse,-Neal-Hinton",
            "title": {
                "fragments": [],
                "text": "A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step is shown empirically to give faster convergence in a mixture estimation problem."
            },
            "venue": {
                "fragments": [],
                "text": "Learning in Graphical Models"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 145
                            }
                        ],
                        "text": "\u2026with an alternative Bayesian viewpoint of neural network learning (MacKay1991; Buntine and Weigend 1991; MacKay 1992c; Neal 1993a; Ripley 1995; Neal 1996) in which the dataerror is interpreted as de ning a likelihood function, and the regularizer corresponds to a prior probabilitydistribution\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 183
                            }
                        ],
                        "text": "A partialsolution can still be obtained by using Monte Carlo methods to simulate the full probability distribution (seeNeal (1993b) for an excellent review of Monte Carlo methods and Neal (1996) for the application of thesemethods to hierarchical models)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 194
                            }
                        ],
                        "text": "\u2026the deterministic method of Bryan (1990), who nds it most convenient numerically toretain as an explicit variable, and integrate it out last , and the Markov chain Monte Carlo implementationof Neal (1996) which samples the hyperparameters and parameters from the joint distribution P (w; jD;H)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 90
                            }
                        ],
                        "text": "This paper is concerned with an alternative Bayesian viewpoint of neural network learning (MacKay 1991; Buntine and Weigend 1991; MacKay 1992c; Neal 1993a; Ripley 1995; Neal 1996) in which the data error is interpreted as de ning a likelihood function, and the regularizer corresponds to a prior probability distribution over the weights."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60809283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db869fa192a3222ae4f2d766674a378e47013b1b",
            "isKey": true,
            "numCitedBy": 3642,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial \"neural networks\" are widely used as flexible models for classification and regression applications, but questions remain about how the power of these models can be safely exploited when training data is limited. This book demonstrates how Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional training methods. Insight into the nature of these complex Bayesian models is provided by a theoretical investigation of the priors over functions that underlie them. A practical implementation of Bayesian neural network learning using Markov chain Monte Carlo methods is also described, and software for it is freely available over the Internet. Presupposing only basic knowledge of probability and statistics, this book should be of interest to researchers in statistics, engineering, and artificial intelligence."
            },
            "slug": "Bayesian-Learning-for-Neural-Networks-Neal",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Bayesian Learning for Neural Networks shows that Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional neural network learning methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052393"
                        ],
                        "name": "H. H. Thodberg",
                        "slug": "H.-H.-Thodberg",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Thodberg",
                            "middleNames": [
                                "Henrik"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. H. Thodberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16086099,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cdaf986fa6b79a0d5f3533350d4ce91145fb546",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "MacKay's (1992) Bayesian framework for backpropagation is a practical and powerful means to improve the generalization ability of neural networks. It is based on a Gaussian approximation to the posterior weight distribution. The framework is extended, reviewed, and demonstrated in a pedagogical way. The notation is simplified using the ordinary weight decay parameter, and a detailed and explicit procedure for adjusting several weight decay parameters is given. Bayesian backprop is applied in the prediction of fat content in minced meat from near infrared spectra. It outperforms \"early stopping\" as well as quadratic regression. The evidence of a committee of differently trained networks is computed, and the corresponding improved generalization is verified. The error bars on the predictions of the fat content are computed. There are three contributors: The random noise, the uncertainty in the weights, and the deviation among the committee members. The Bayesian framework is compared to Moody's GPE (1992). Finally, MacKay and Neal's automatic relevance determination, in which the weight decay parameters depend on the input number, is applied to the data with improved results."
            },
            "slug": "A-review-of-Bayesian-neural-networks-with-an-to-Thodberg",
            "title": {
                "fragments": [],
                "text": "A review of Bayesian neural networks with an application to near infrared spectroscopy"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "MacKay's Bayesian framework for backpropagation is a practical and powerful means to improve the generalization ability of neural networks and is applied in the prediction of fat content in minced meat from near infrared spectra."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 118
                            }
                        ],
                        "text": "Problems for which models can be written in the form (2) include linear interpolation with a xed basisset (Gull 1988; MacKay 1992a), nonlinear regression with a neural network (MacKay 1992c), nonlinearclassi cation (MacKay 1992b), and image deconvolution (Gull 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 244
                            }
                        ],
                        "text": "Much interest has centred on the question, for models like the one de ned in equations (3{4), of howthe constants and | or the ratio = | should be set, and Gull (1989) has derived an appealingBayesian prescription for these constants (see also MacKay (1992a) for a review)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 256
                            }
                        ],
                        "text": "\u2026applied to nonlinear models by makingappropriate local linearizations (so that the integral over the parameters is made approximately ratherthan exactly) and has been used successfully in image reconstruction (Gull 1989; Weir 1991) and in neuralnetworks (MacKay 1992c; Thodberg 1993; MacKay 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 66
                            }
                        ],
                        "text": "Under general conditions, the error bars on log are log jD ' p2= (MacKay 1992a) (seesection 8)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 32
                            }
                        ],
                        "text": "This procedure is suggested in (MacKay 1992c) as a `quick and dirty' approximation to the evidence frame-work."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 153
                            }
                        ],
                        "text": "There is then an analogous choice between either (a) optimizing separately ateach local optimum in w, and using a Gaussian approximation conditioned on (MacKay 1992c); or (b) tting multiple Gaussians to local maxima of the true posterior with the hyperparameter integrated out."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 129
                            }
                        ],
                        "text": "This paper is concerned with an alternative Bayesian viewpoint of neural network learning (MacKay1991; Buntine and Weigend 1991; MacKay 1992c; Neal 1993a; Ripley 1995; Neal 1996) in which the dataerror is interpreted as de ning a likelihood function, and the regularizer corresponds to a prior\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 15
                            }
                        ],
                        "text": "As reviewed in MacKay (1992a), the most probable value of satis es a simple implicit equation,1 MP = Pk1 w2i (16)where wi are the components of the vector wMPj MP and is the number of well{determined parameters,which can be expressed in terms of the eigenvalues a of the matrix rrED(w): = k Trace =\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 17
                            }
                        ],
                        "text": "In (MacKay 1991; MacKay 1992c) it was shown that it made theoretical sense, and couldbe practically bene cial, to use multiple hyperparameters f cg, each one controlling a di erent aspect ofthe prior probability distribution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6530745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7abda1941534d3bb558dd959025d67f1df526303",
            "isKey": false,
            "numCitedBy": 792,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Three Bayesian ideas are presented for supervised adaptive classifiers. First, it is argued that the output of a classifier should be obtained by marginalizing over the posterior distribution of the parameters; a simple approximation to this integral is proposed and demonstrated. This involves a \"moderation\" of the most probable classifier's outputs, and yields improved performance. Second, it is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems. This framework successfully chooses the magnitude of weight decay terms, and ranks solutions found using different numbers of hidden units. Third, an information-based data selection criterion is derived and demonstrated within this framework."
            },
            "slug": "The-Evidence-Framework-Applied-to-Classification-Mackay",
            "title": {
                "fragments": [],
                "text": "The Evidence Framework Applied to Classification Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems and an information-based data selection criterion is derived and demonstrated within this framework."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143791812"
                        ],
                        "name": "S. Gull",
                        "slug": "S.-Gull",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gull",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gull"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 107
                            }
                        ],
                        "text": "Problems for which models can be written in the form (2) include linear interpolation with a xed basisset (Gull 1988; MacKay 1992a), nonlinear regression with a neural network (MacKay 1992c), nonlinearclassi cation (MacKay 1992b), and image deconvolution (Gull 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 230
                            }
                        ],
                        "text": "This method can be applied to nonlinear models by making appropriate local linearizations (so that the integral over the parameters is made approximately rather than exactly) and has been used successfully in image reconstruction (Gull 1989; Weir 1991) and in neural networks (MacKay 1992c; Thodberg 1993; MacKay 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 180
                            }
                        ],
                        "text": "(2) If practical Bayesian methods involve approximations such as tting a Gaussian to a posterior dis-tribution, then one should think twice before integrating out hyperparameters (Gull 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 257
                            }
                        ],
                        "text": "Problems for which models can be written in the form (2) include linear interpolation with a xed basis set (Gull 1988; MacKay 1992a), nonlinear regression with a neural network (MacKay 1992c), nonlinear classi cation (MacKay 1992b), and image deconvolution (Gull 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 99
                            }
                        ],
                        "text": "This comparison is also relevant to other ill{posed problems such as image reconstruction problems (Gull 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 154
                            }
                        ],
                        "text": "For each data analysis problem, one may evaluate the critical max above which the posterior would be measurably a ected by the large tail of the evidence (Gull 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118754484,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "82fa37d5be8e747131a5857992cc33bb95469ce3",
            "isKey": true,
            "numCitedBy": 316,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian derivation of \u201cClassic\u201d MaxEnt image processing (Skilling 1989a) shows that exp(\u03b1S(f,m)), where S(f,m) is the entropy of image f relative to model m, is the only consistent prior probability distribution for positive, additive images. In this paper the derivation of \u201cClassic\u201d MaxEnt is completed, showing that it leads to a natural choice for the regularising parameter \u03b1, that supersedes the traditional practice of setting x2=N. The new condition is that the dimensionless measure of structure -2\u03b1S should be equal to the number of good singular values contained in the data. The performance of this new condition is discussed with reference to image deconvolution, but leads to a reconstruction that is visually disappointing. A deeper hypothesis space is proposed that overcomes these difficulties, by allowing for spatial correlations across the image."
            },
            "slug": "Developments-in-Maximum-Entropy-Data-Analysis-Gull",
            "title": {
                "fragments": [],
                "text": "Developments in Maximum Entropy Data Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143791812"
                        ],
                        "name": "S. Gull",
                        "slug": "S.-Gull",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gull",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gull"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117915279,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6272baf82e2e442edab4fb613ef2b7186bf5f1fb",
            "isKey": false,
            "numCitedBy": 288,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The principles of Bayesian reasoning are reviewed and applied to problems of inference from data sampled from Poisson, Gaussian and Cauchy distributions. Probability distributions (priors and likelihoods) are assigned in appropriate hypothesis spaces using the Maximum Entropy Principle, and then manipulated via Bayes\u2019 Theorem. Bayesian hypothesis testing requires careful consideration of the prior ranges of any parameters involved, and this leads to a quantitive statement of Occam\u2019s Razor. As an example of this general principle we offer a solution to an important problem in regression analysis; determining the optimal number of parameters to use when fitting graphical data with a set of basis functions."
            },
            "slug": "Bayesian-Inductive-Inference-and-Maximum-Entropy-Gull",
            "title": {
                "fragments": [],
                "text": "Bayesian Inductive Inference and Maximum Entropy"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143949487"
                        ],
                        "name": "N. Galatsanos",
                        "slug": "N.-Galatsanos",
                        "structuredName": {
                            "firstName": "Nikolas",
                            "lastName": "Galatsanos",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Galatsanos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112242"
                        ],
                        "name": "V. Mesarovic",
                        "slug": "V.-Mesarovic",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Mesarovic",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Mesarovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143719659"
                        ],
                        "name": "R. Molina",
                        "slug": "R.-Molina",
                        "structuredName": {
                            "firstName": "Rafael",
                            "lastName": "Molina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Molina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144842935"
                        ],
                        "name": "A. Katsaggelos",
                        "slug": "A.-Katsaggelos",
                        "structuredName": {
                            "firstName": "Aggelos",
                            "lastName": "Katsaggelos",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Katsaggelos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10219716,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "63e7ff91b65949a1924c71c051697372657e3540",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we examine the restoration problem when the point-spread function (PSF) of the degradation system is partially known. For this problem, the PSF is assumed to be the sum of a known deterministic and an unknown random component. This problem has been examined before; however, in most previous works the problem of estimating the parameters that define the restoration filters was not addressed. In this paper, two iterative algorithms that simultaneously restore the image and estimate the parameters of the restoration filter are proposed using evidence analysis (EA) within the hierarchical Bayesian framework. We show that the restoration step of the first of these algorithms is in effect almost identical to the regularized constrained total least-squares (RCTLS) filter, while the restoration step of the second is identical to the linear minimum mean square-error (LMMSE) filter for this problem. Therefore, in this paper we provide a solution to the parameter estimation problem of the RCTLS filter. We further provide an alternative approach to the expectation-maximization (EM) framework to derive a parameter estimation algorithm for the LMMSE filter. These iterative algorithms are derived in the discrete Fourier transform (DFT) domain; therefore, they are computationally efficient even for large images. Numerical experiments are presented that test and compare the proposed algorithms."
            },
            "slug": "Hierarchical-Bayesian-image-restoration-from-known-Galatsanos-Mesarovic",
            "title": {
                "fragments": [],
                "text": "Hierarchical Bayesian image restoration from partially known blurs"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Two iterative algorithms that simultaneously restore the image and estimate the parameters of the restoration filter are proposed using evidence analysis (EA) within the hierarchical Bayesian framework and are derived in the discrete Fourier transform (DFT) domain."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32536207"
                        ],
                        "name": "D. Camp",
                        "slug": "D.-Camp",
                        "structuredName": {
                            "firstName": "Drew",
                            "lastName": "Camp",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Camp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A novel approach to the approximation of Bayesian inference has been introduced by  Hinton and van Camp (1993) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9346534,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25c9f33aceac6dcff357727cbe2faf145b01d13c",
            "isKey": false,
            "numCitedBy": 934,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Supervised neural networks generalize well if there is much less information in the weights than there is in the output vectors of the training cases. So during learning, it is important to keep the weights simple by penalizing the amount of information they contain. The amount of information in a weight can be controlled by adding Gaussian noise and the noise level can be adapted during learning to optimize the trade-o between the expected squared error of the network and the amount of information in the weights. We describe a method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non-linear hidden units. Provided the output units are linear, the exact derivatives can be computed e ciently without time-consuming Monte Carlo simulations. The idea of minimizing the amount of information that is required to communicate the weights of a neural network leads to a number of interesting schemes for encoding the weights."
            },
            "slug": "Keeping-the-neural-networks-simple-by-minimizing-of-Hinton-Camp",
            "title": {
                "fragments": [],
                "text": "Keeping the neural networks simple by minimizing the description length of the weights"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non-linear hidden units without time-consuming Monte Carlo simulations is described."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '93"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16302605,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d275cf94e620bf5b3776bba8a88acccdcfcd9a19",
            "isKey": false,
            "numCitedBy": 213,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The attempt to find a single \"optimal\" weight vector in conventional network training can lead to overfitting and poor generalization. Bayesian methods avoid this, without the need for a validation set, by averaging the outputs of many networks with weights sampled from the posterior distribution given the training data. This sample can be obtained by simulating a stochastic dynamical system that has the posterior as its stationary distribution."
            },
            "slug": "Bayesian-Learning-via-Stochastic-Dynamics-Neal",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning via Stochastic Dynamics"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Bayesian methods avoid overfitting and poor generalization by averaging the outputs of many networks with weights sampled from the posterior distribution given the training data, by simulating a stochastic dynamical system that has the posterior as its stationary distribution."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14814125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c83684f6207697c12850db423fd9747572cf1784",
            "isKey": false,
            "numCitedBy": 376,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Connectionist feed-forward networks, t rained with backpropagat ion, can be used both for nonlinear regression and for (discrete one-of-C ) classification. This paper presents approximate Bayesian meth ods to statistical components of back-propagat ion: choosing a cost funct ion and penalty term (interpreted as a form of prior probability), pruning insignifican t weights, est imat ing the uncertainty of weights, predict ing for new pat terns (\"out -of-sample\") , est imating the uncertainty in the choice of this predict ion (\"erro r bars\" ), estimating the generalizat ion erro r, comparing different network st ructures, and handling missing values in the t raining patterns. These methods extend some heurist ic techniques suggested in the literature, and in most cases require a small addit ional facto r in comput at ion during back-propagat ion, or computation once back-pro pagat ion has finished."
            },
            "slug": "Bayesian-Back-Propagation-Buntine-Weigend",
            "title": {
                "fragments": [],
                "text": "Bayesian Back-Propagation"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92579735"
                        ],
                        "name": "Peter Craven",
                        "slug": "Peter-Craven",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Craven",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Craven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 92
                            }
                        ],
                        "text": "This relates closely to the `generalized maximum likelihood'or `MLII' method in statistics (Wahba 1975)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14094416,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b477dd12dd49e44a62c1a303501df5fb6706c7e9",
            "isKey": false,
            "numCitedBy": 3541,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "SummarySmoothing splines are well known to provide nice curves which smooth discrete, noisy data. We obtain a practical, effective method for estimating the optimum amount of smoothing from the data. Derivatives can be estimated from the data by differentiating the resulting (nearly) optimally smoothed spline.We consider the modelyi(ti)+\u03b5i,i=1, 2, ...,n,ti\u2208[0, 1], whereg\u2208W2(m)={f:f,f\u2032, ...,f(m\u22121) abs. cont.,f(m)\u2208\u21122[0,1]}, and the {\u03b5i} are random errors withE\u03b5i=0,E\u03b5i\u03b5j=\u03c32\u03b4ij. The error variance \u03c32 may be unknown. As an estimate ofg we take the solutiongn, \u03bb to the problem: Findf\u2208W2(m) to minimize\n$$\\frac{1}{n}\\sum\\limits_{j = 1}^n {(f(t_j ) - y_j )^2 + \\lambda \\int\\limits_0^1 {(f^{(m)} (u))^2 du} }$$\n. The functiongn, \u03bb is a smoothing polynomial spline of degree 2m\u22121. The parameter \u03bb controls the tradeoff between the \u201croughness\u201d of the solution, as measured by\n$$\\int\\limits_0^1 {[f^{(m)} (u)]^2 du}$$\n, and the infidelity to the data as measured by\n$$\\frac{1}{n}\\sum\\limits_{j = 1}^n {(f(t_j ) - y_j )^2 }$$\n, and so governs the average square errorR(\u03bb; g)=R(\u03bb) defined by\n$$R(\\lambda ) = \\frac{1}{n}\\sum\\limits_{j = 1}^n {(g_{n,\\lambda } (t_j ) - g(t_j ))^2 }$$\n. We provide an estimate\n$$\\hat \\lambda$$\n, called the generalized cross-validation estimate, for the minimizer ofR(\u03bb). The estimate\n$$\\hat \\lambda$$\n is the minimizer ofV(\u03bb) defined by\n$$V(\\lambda ) = \\frac{1}{n}\\parallel (I - A(\\lambda ))y\\parallel ^2 /\\left[ {\\frac{1}{n}{\\text{Trace(}}I - A(\\lambda ))} \\right]^2$$\n, wherey=(y1, ...,yn)t andA(\u03bb) is then\u00d7n matrix satisfying(gn, \u03bb (t1), ...,gn, \u03bb (tn))t=A (\u03bb) y. We prove that there exist a sequence of minimizers\n$$\\tilde \\lambda = \\tilde \\lambda (n)$$\n ofEV(\u03bb), such that as the (regular) mesh{ti}i=1n becomes finer,\n$$\\mathop {\\lim }\\limits_{n \\to \\infty } ER(\\tilde \\lambda )/\\mathop {\\min }\\limits_\\lambda ER(\\lambda ) \\downarrow 1$$\n. A Monte Carlo experiment with several smoothg's was tried withm=2,n=50 and several values of \u03c32, and typical values of\n$$R(\\hat \\lambda )/\\mathop {\\min }\\limits_\\lambda R(\\lambda )$$\n were found to be in the range 1.01\u20131.4. The derivativeg\u2032 ofg can be estimated by\n$$g'_{n,\\hat \\lambda } (t)$$\n. In the Monte Carlo examples tried, the minimizer of\n$$R_D (\\lambda ) = \\frac{1}{n}\\sum\\limits_{j = 1}^n {(g'_{n,\\lambda } (t_j ) - } g'(t_j ))$$\n tended to be close to the minimizer ofR(\u03bb), so that\n$$\\hat \\lambda$$\n was also a good value of the smoothing parameter for estimating the derivative."
            },
            "slug": "Smoothing-noisy-data-with-spline-functions-Craven-Wahba",
            "title": {
                "fragments": [],
                "text": "Smoothing noisy data with spline functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "91591909"
                        ],
                        "name": "R. Bryan",
                        "slug": "R.-Bryan",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Bryan",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bryan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 184
                            }
                        ],
                        "text": "@3 logP (Dj ;H) @(log )3 MP ' w2 MPj MP = 2 : (55) The rst derivative is exact, assuming that the eigenvalues a are independent of , which is true in the case of a Gaussian prior on w (Bryan 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118931437,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bebf4b5709ce271196e00d05561569b93475d34e",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A numerical algorithm for the solution of the Classic Maximum Entropy problem is presented, for use when the data are considerably oversampled, so that the amount of independent information they contain is very much less than the actual number of data points. Examples of problems for which this algorithm is particularly appropriate are dynamic light scattering, solution scattering and fibre diffraction. The application of a general purpose entropy maximisation program is then comparatively inefficient. In the new algorithm the independent variables are in the singular space of the transform between map (or image or spectrum) and data, and much fewer in number than either the data or the reconstruction. This reduction in the dimension allows a direct evaluation of the posterior probability of the solution, and thus enables the \u2018Classic Maxent\u2019 problem to be solved completely."
            },
            "slug": "SOLVING-OVERSAMPLED-DATA-PROBLEMS-BY-MAXIMUM-Bryan",
            "title": {
                "fragments": [],
                "text": "SOLVING OVERSAMPLED DATA PROBLEMS BY MAXIMUM ENTROPY."
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A numerical algorithm for the solution of the Classic Maximum Entropy problem is presented, for use when the data are considerably oversampled, so that the amount of independent information they contain is very much less than the actual number of data points."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152315379"
                        ],
                        "name": "Marvin H. J. Guber",
                        "slug": "Marvin-H.-J.-Guber",
                        "structuredName": {
                            "firstName": "Marvin",
                            "lastName": "Guber",
                            "middleNames": [
                                "H.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marvin H. J. Guber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121303978,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5a10b9657a5d0d11dcc89f668f21f8dac73b4694",
            "isKey": false,
            "numCitedBy": 631,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1 Introduction.- 2 Single Stationary Sinusoid Plus Noise.- 3 The General Model Equation Plus Noise.- 4 Estimating the Parameters.- 5 Model Selection.- 6 Spectral Estimation.- 7 Applications.- 8 Summary and Conclusions.- A Choosing a Prior Probability.- B Improper Priors as Limits.- C Removing Nuisance Parameters.- D Uninformative Prior Probabilities.- E Computing the \"Student t-Distribution\"."
            },
            "slug": "Bayesian-Spectrum-Analysis-and-Parameter-Estimation-Guber",
            "title": {
                "fragments": [],
                "text": "Bayesian Spectrum Analysis and Parameter Estimation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7469934"
                        ],
                        "name": "E. M.",
                        "slug": "E.-M.",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "M.",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. M."
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 103
                            }
                        ],
                        "text": "The objective function chosen to measure the quality of the approximation is a variationalfree energy (Feynman 1972), F ( ) = Z dkwQ(w; ) log P (Djw;H)P (wjH)Q(w; ) (32)The free energy F ( ) is bounded below by logP (DjH) and only attains this value forQ(w; ) = P (wjD;H)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 103
                            }
                        ],
                        "text": "The objective function chosen to measure the quality of the approximation is a variational free energy (Feynman, 1972),"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4070518,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "5c96465c7c5a291b16d134db6bd1dbaa79ad8d1b",
            "isKey": false,
            "numCitedBy": 5309,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "AbstractPROF. R. H. FOWLER'S monumental work on statistical mechanics has, in this the second edition, in his own modest words, been rearranged and brought more up to date. But the new volume is much more than a revision, in that it is explicitly based on quantum mechanics from the outset ; the first dynamical equation found written in the formal presentation is a wave-equation. Prof. Fowler states in justification that although classical mechanics is used to derive the quantum mechanics by a process of generalization, \"once the laws of quantum mechanics have been thus guessed, as they must be before we can discuss the theorems of statistical mechanics, quantized systems naturally come first. In 1935 this attitude hardly needs apology\". In consequence of this, the concluding chapter of the first edition, dealing with quantum statistics, has been incorporated in the new exposition from the start, and what is effectively the opening chapter is now doubled in length. Otherwise there is no change of general structure. But a comparison of the two editions shows that the chapter of the first edition entitled \"Thermionics\" is now more than four times as long, and contains a treatment of the electron theory of metals and of semi-conductors ; that the chapter on dielectrics and magnetic constants has been similarly extended, and now includes an account of ferro-magnetism ; lastly, there is a new concluding chapter on \"co-operative\" and other phenomena. The number of references is almost doubled. It is difficult to over-estimate the amount of work involved in thus re-writing and extending what was already an encyclop\u00e6dic work.Statistical Mechanics: \n the Theory of the Properties of Matter in Equilibrium. By Prof. R. H. Fowler. Second edition, revised and enlarged. Pp. x + 864. (Cambridge: At the University Press, 1936.) 50s. net."
            },
            "slug": "Statistical-Mechanics:-E.",
            "title": {
                "fragments": [],
                "text": "Statistical Mechanics:"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1937
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145948621"
                        ],
                        "name": "G. Box",
                        "slug": "G.-Box",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Box",
                            "middleNames": [
                                "E.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Box"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36184409"
                        ],
                        "name": "G. C. Tiao",
                        "slug": "G.-C.-Tiao",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Tiao",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. C. Tiao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The pioneering work of Box and Tiao (1973) used this approach to develop Bayesian robust statistics."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122028907,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a205103d4f25ae39f417bac7bd5142302d7f448c",
            "isKey": false,
            "numCitedBy": 4326,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Nature of Bayesian Inference Standard Normal Theory Inference Problems Bayesian Assessment of Assumptions: Effect of Non-Normality on Inferences About a Population Mean with Generalizations Bayesian Assessment of Assumptions: Comparison of Variances Random Effect Models Analysis of Cross Classification Designs Inference About Means with Information from More than One Source: One-Way Classification and Block Designs Some Aspects of Multivariate Analysis Estimation of Common Regression Coefficients Transformation of Data Tables References Indexes."
            },
            "slug": "Bayesian-inference-in-statistical-analysis-Box-Tiao",
            "title": {
                "fragments": [],
                "text": "Bayesian inference in statistical analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This chapter discusses Bayesian Assessment of Assumptions, which investigates the effect of non-Normality on Inferences about a Population Mean with Generalizations in the context of a Bayesian inference model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 134
                            }
                        ],
                        "text": "United Kingdom.mackay@mrao.cam.ac.ukSubmitted to Neural ComputationAbstractI examine two approximate methods for computational implementation of Bayesian hierarchical mod-els, that is, models which include unknown hyperparameters such as regularization constants and noiselevels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 274
                            }
                        ],
                        "text": "\u2026in many dimensions.1 The over tting problem and hyperparameters in neural networksFeedforward neural networks are often trained to solve regression and classi cation problems using algorithmsthat minimize an error function, a measure of goodness of t to the training data (Rumelhart et al. 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 205001834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "isKey": false,
            "numCitedBy": 20335,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."
            },
            "slug": "Learning-representations-by-back-propagating-errors-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating errors"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794321"
                        ],
                        "name": "B. Huberman",
                        "slug": "B.-Huberman",
                        "structuredName": {
                            "firstName": "Bernardo",
                            "lastName": "Huberman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Huberman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 217236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f707a81a278d1598cd0a4493ba73f22dcdf90639",
            "isKey": false,
            "numCitedBy": 655,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Inspired by the information theoretic idea of minimum description length, we add a term to the back propagation cost function that penalizes network complexity. We give the details of the procedure, called weight-elimination, describe its dynamics, and clarify the meaning of the parameters involved. From a Bayesian perspective, the complexity term can be usefully interpreted as an assumption about prior distribution of the weights. We use this procedure to predict the sunspot time series and the notoriously noisy series of currency exchange rates."
            },
            "slug": "Generalization-by-Weight-Elimination-with-to-Weigend-Rumelhart",
            "title": {
                "fragments": [],
                "text": "Generalization by Weight-Elimination with Application to Forecasting"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This work adds a term to the back propagation cost function that penalizes network complexity, called weight-elimination, and uses this procedure to predict the sunspot time series and the notoriously noisy series of currency exchange rates."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145833095"
                        ],
                        "name": "S. Kothari",
                        "slug": "S.-Kothari",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Kothari",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kothari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681982"
                        ],
                        "name": "H. Oh",
                        "slug": "H.-Oh",
                        "structuredName": {
                            "firstName": "Heekuck",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Oh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 196
                            }
                        ],
                        "text": "One way of describing the over tting problem is to view the neural network as an approximation orestimation tool and describe the control of complexity as a trade{o between bias and variance (see Bishop(1995) for a review)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "One way of describing the overfitting problem is to view the neural network as an approximation or estimation tool and describe the control of complexity as a trade-off between bias and variance (see  Bishop, 1995,  for a review)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 177751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbc0a468ab103ae29717703d4aa9f682f6a2b664",
            "isKey": false,
            "numCitedBy": 15339,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-Networks-for-Pattern-Recognition-Kothari-Oh",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "(This is the main reason for choosing the objective function F.\u00b5/ rather than some other measure of distance between Q.wI\u00b5/and P.wjD;H/.) A longer review of ensemble learning, including references to applications, may be found in  MacKay (1995) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 90
                            }
                        ],
                        "text": "Alonger review of Ensemble Learning including references to applications may be found in (MacKay 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1837515,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df7087d5495ce6d404a8d67f722b97fe66f138b9",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Ensemble learning by variational free energy minimization is a framework for statistical inference in which an ensemble of parameter vectors is optimized rather than a single parameter vector. The ensemble approximates the posterior probability distribution of the parameters."
            },
            "slug": "Developments-in-Probabilistic-Modelling-with-Neural-Mackay",
            "title": {
                "fragments": [],
                "text": "Developments in Probabilistic Modelling with Neural Networks - Ensemble Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This paper presents a framework for statistical inference in which an ensemble of parameter vectors is optimized rather than a single parameter vector and approximates the posterior probability distribution of the parameters."
            },
            "venue": {
                "fragments": [],
                "text": "SNN Symposium on Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144465303"
                        ],
                        "name": "C. Strauss",
                        "slug": "C.-Strauss",
                        "structuredName": {
                            "firstName": "Charlie",
                            "lastName": "Strauss",
                            "middleNames": [
                                "E.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Strauss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36878862"
                        ],
                        "name": "D. R. Wolf",
                        "slug": "D.-R.-Wolf",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolf",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. R. Wolf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "An alternative procedure for computing inferences under the same Bayesian model has been suggested by Buntine and Weigend (1991),  Strauss, Wolpert, and Wolf (1993) , and Wolpert (1993).,The gaussian fitted by the method suggested by Buntine and Weigend (1991),  Strauss et al. (1993) , and Wolpert (1993) appears to be a poor representation of the true posterior."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117713287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d46f510f2419567204053ec5a4fb308e73c57fb",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "First, the correct entropic prior is computed by marginalization of alpha. This is followed by a discussion of improvements to the \u201cevidence\u201d approximation. Surprisingly, it appears that the approximations used to restore the famous \u201cSusie\u201d image may have questionable aspects."
            },
            "slug": "Alpha,-Evidence,-and-the-Entropic-Prior-Strauss-Wolpert",
            "title": {
                "fragments": [],
                "text": "Alpha, Evidence, and the Entropic Prior"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The correct entropic prior is computed by marginalization of alpha, and the approximations used to restore the famous \u201cSusie\u201d image may have questionable aspects."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 92
                            }
                        ],
                        "text": "This relates closely to the `generalized maximum likelihood' or `MLII' method in statistics (Wahba 1975)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 92
                            }
                        ],
                        "text": "This relates closely to the `generalized maximum likelihood'or `MLII' method in statistics (Wahba 1975)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121176122,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "88ce531f22108f687cbb576bcb0cd660b2a694bc",
            "isKey": false,
            "numCitedBy": 539,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "On considere des procedures de lissage spline etudiees en 1978 et 1983 et leur extension a la resolution d'equations d'operateurs lineaires avec donnees bruitees"
            },
            "slug": "A-Comparison-of-GCV-and-GML-for-Choosing-the-in-the-Wahba",
            "title": {
                "fragments": [],
                "text": "A Comparison of GCV and GML for Choosing the Smoothing Parameter in the Generalized Spline Smoothing Problem"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 85
                            }
                        ],
                        "text": "There are various1\nregularizers, the simplest and most popular being `weight decay' (Hinton and Sejnowski 1986) (also knownas `ridge regression')."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 71
                            }
                        ],
                        "text": "If nothing is done to control the complexity of the resulting neural network, an inevitable consequence oferror{minimization will be over tting | the neural network will learn a function which ts spurious detailsand noise in the data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58779360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8592e46a5435d18bba70557846f47290b34c1aa5",
            "isKey": false,
            "numCitedBy": 1336,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References"
            },
            "slug": "Learning-and-relearning-in-Boltzmann-machines-Hinton-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning and relearning in Boltzmann machines"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, and an Example of the Effects of Damage."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "18112763"
                        ],
                        "name": "F. Reif",
                        "slug": "F.-Reif",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Reif",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Reif"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715631"
                        ],
                        "name": "S. Rice",
                        "slug": "S.-Rice",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Rice",
                            "middleNames": [
                                "A"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rice"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 77
                            }
                        ],
                        "text": "Butif the system has a large number of degrees of freedom, it is well known (Reif 1965) that for most macroscopicpurposes, the two distributions are indistinguishable, because most of the probability mass of the canonicalensemble is concentrated in the states in a small interval around E.The same\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 77
                            }
                        ],
                        "text": "But if the system has a large number of degrees of freedom, it is well known (Reif 1965) that for most macroscopic purposes, the two distributions are indistinguishable, because most of the probability mass of the canonical ensemble is concentrated in the states in a small interval around E."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118835488,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "e3e2901a80d7f663fc4ff7cf89456cf79d040e6e",
            "isKey": false,
            "numCitedBy": 3398,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This book is designed for the junior-senior thermodynamics course given in all departments as a standard part of the curriculum. The book is devoted to a discussion of some of the basic physical concepts and methods useful in the description of situations involving systems which consist of very many particulars. It attempts, in particular, to introduce the reader to the disciplines of thermodynamics, statistical mechanics, and kinetic theory from a unified and modern point of view. The presentation emphasizes the essential unity of the subject matter and develops physical insight by stressing the microscopic content of the theory."
            },
            "slug": "Fundamentals-of-Statistical-and-Thermal-Physics-Reif-Rice",
            "title": {
                "fragments": [],
                "text": "Fundamentals of Statistical and Thermal Physics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 62
                            }
                        ],
                        "text": "The free energy minimization approach is like an EM algorithm (Dempster et al. 1977), in which we wish to nd the most probable and do this by introducing an E{step in which a distribution over w is obtained (Neal and Hinton 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 63
                            }
                        ],
                        "text": "The free energy minimization approach is like an EM algorithm (Dempster et al. 1977), in which wewish to nd the most probable and do this by introducing an E{step in which a distribution over w isobtained (Neal and Hinton 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 145
                            }
                        ],
                        "text": "\u2026is concerned with an alternative Bayesian viewpoint of neural network learning (MacKay1991; Buntine and Weigend 1991; MacKay 1992c; Neal 1993a; Ripley 1995; Neal 1996) in which the dataerror is interpreted as de ning a likelihood function, and the regularizer corresponds to a prior\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 146
                            }
                        ],
                        "text": "\u2026is EW where EW is half the sum of the squares of theweights fwig in the neural network, EW = 12Xi w2i : (1)The motivation for this regularizer is that functions with a complex dependence on the inputs of a networkrequire larger weights than simple functions, so this regularizer penalizes the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9584248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "877a887e7af7daebcb685e4d7b5e80f764035581",
            "isKey": false,
            "numCitedBy": 4043,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Title Type pattern recognition with neural networks in c++ PDF pattern recognition and neural networks PDF neural networks for pattern recognition advanced texts in econometrics PDF neural networks for applied sciences and engineering from fundamentals to complex pattern recognition PDF an introduction to biological and artificial neural networks for pattern recognition spie tutorial text vol tt04 tutorial texts in optical engineering PDF"
            },
            "slug": "Pattern-Recognition-and-Neural-Networks-LeCun-Bengio",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 147
                            }
                        ],
                        "text": "\u2026EW is half the sum of the squares of theweights fwig in the neural network, EW = 12Xi w2i : (1)The motivation for this regularizer is that functions with a complex dependence on the inputs of a networkrequire larger weights than simple functions, so this regularizer penalizes the more complex\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 145
                            }
                        ],
                        "text": "\u2026with an alternative Bayesian viewpoint of neural network learning (MacKay1991; Buntine and Weigend 1991; MacKay 1992c; Neal 1993a; Ripley 1995; Neal 1996) in which the dataerror is interpreted as de ning a likelihood function, and the regularizer corresponds to a prior probabilitydistribution\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 183
                            }
                        ],
                        "text": "A partialsolution can still be obtained by using Monte Carlo methods to simulate the full probability distribution (seeNeal (1993b) for an excellent review of Monte Carlo methods and Neal (1996) for the application of thesemethods to hierarchical models)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 194
                            }
                        ],
                        "text": "\u2026the deterministic method of Bryan (1990), who nds it most convenient numerically toretain as an explicit variable, and integrate it out last , and the Markov chain Monte Carlo implementationof Neal (1996) which samples the hyperparameters and parameters from the joint distribution P (w; jD;H)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks. Number 118 in Lecture Notes in Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks. Number 118 in Lecture Notes in Statistics"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 72
                            }
                        ],
                        "text": "Methods for controlling these multiple hyperparameters were developedby MacKay (1991) using Gaussian approximations and by Neal (1993a) using Markov chain Monte Carlomethods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 54
                            }
                        ],
                        "text": "In this paper I compare the approximate strategies of MacKay (1991) and Buntine and Weigend (1991)for handling hyperparameters, assuming a Bayesian approach to neural networks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 91
                            }
                        ],
                        "text": "This paper is concerned with an alternative Bayesian viewpoint of neural network learning (MacKay1991; Buntine and Weigend 1991; MacKay 1992c; Neal 1993a; Ripley 1995; Neal 1996) in which the dataerror is interpreted as de ning a likelihood function, and the regularizer corresponds to a prior\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 109
                            }
                        ],
                        "text": "The regularizer in this case is EW where EW is half the sum of the squares of theweights fwig in the neural network, EW = 12Xi w2i : (1)The motivation for this regularizer is that functions with a complex dependence on the inputs of a networkrequire larger weights than simple functions, so this\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 98
                            }
                        ],
                        "text": "One way of describing the over tting problem is to view the neural network as an approximation orestimation tool and describe the control of complexity as a trade{o between bias and variance (see Bishop(1995) for a review)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 4
                            }
                        ],
                        "text": "In (MacKay 1991; MacKay 1992c) it was shown that it made theoretical sense, and couldbe practically bene cial, to use multiple hyperparameters f cg, each one controlling a di erent aspect ofthe prior probability distribution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Methods for Adaptive Models. California Institute of Technology dissertation"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian Methods for Adaptive Models. California Institute of Technology dissertation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2796350"
                        ],
                        "name": "P. Diaconis",
                        "slug": "P.-Diaconis",
                        "structuredName": {
                            "firstName": "Persi",
                            "lastName": "Diaconis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Diaconis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123740858,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bed94ff3851ea67c44da91968cf04acb6fe50f2a",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bayesian-Numerical-Analysis-Diaconis",
            "title": {
                "fragments": [],
                "text": "Bayesian Numerical Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145369815"
                        ],
                        "name": "K. Ritter",
                        "slug": "K.-Ritter",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Ritter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ritter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118940380,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee67b18db12698aea68331c5f2c61cb8fd0f6510",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bayesian-numerical-analysis-Ritter",
            "title": {
                "fragments": [],
                "text": "Bayesian numerical analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400920289"
                        ],
                        "name": "N. Weir",
                        "slug": "N.-Weir",
                        "structuredName": {
                            "firstName": "Nic",
                            "lastName": "Weir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Weir"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117181385,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "56917e0825718186fe42569d6820fafb6a075c58",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Applications-of-Maximum-Entropy-Techniques-to-HST-Weir",
            "title": {
                "fragments": [],
                "text": "Applications of Maximum Entropy Techniques to HST Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 128
                            }
                        ],
                        "text": "1977), in which we wish to nd the most probable and do this by introducing an E{step in which a distribution over w is obtained (Neal and Hinton 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 206
                            }
                        ],
                        "text": "The free energy minimization approach is like an EM algorithm (Dempster et al. 1977), in which wewish to nd the most probable and do this by introducing an E{step in which a distribution over w isobtained (Neal and Hinton 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62562212,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "418cc44768ff9d0ed8cf4cef79869f90ab672f7b",
            "isKey": false,
            "numCitedBy": 215,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-new-view-of-the-EM-algorithm-that-justifies-and-Neal",
            "title": {
                "fragments": [],
                "text": "A new view of the EM algorithm that justifies incremental and other variants"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 318,
                                "start": 276
                            }
                        ],
                        "text": "This method can be applied to nonlinear models by making appropriate local linearizations (so that the integral over the parameters is made approximately rather than exactly) and has been used successfully in image reconstruction (Gull 1989; Weir 1991) and in neural networks (MacKay 1992c; Thodberg 1993; MacKay 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian non-linear modelling for the 1993 energy prediction competition"
            },
            "venue": {
                "fragments": [],
                "text": "Maximum Entropy and Bayesian Methods"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Review of Bayesian neural networks with an application"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 145
                            }
                        ],
                        "text": "\u2026is concerned with an alternative Bayesian viewpoint of neural network learning (MacKay1991; Buntine and Weigend 1991; MacKay 1992c; Neal 1993a; Ripley 1995; Neal 1996) in which the dataerror is interpreted as de ning a likelihood function, and the regularizer corresponds to a prior\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 90
                            }
                        ],
                        "text": "This paper is concerned with an alternative Bayesian viewpoint of neural network learning (MacKay 1991; Buntine and Weigend 1991; MacKay 1992c; Neal 1993a; Ripley 1995; Neal 1996) in which the data error is interpreted as de ning a likelihood function, and the regularizer corresponds to a prior probability distribution over the weights."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Neural Networks. Cambridge"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data viathe EM algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Royal Statistical Society B"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 266
                            }
                        ],
                        "text": "Justi cation for the evidence approximation The central approximation in this scheme can be stated as follows: when we integrate out a parameter , the e ect for most purposes is to estimate the parameter from the data, and then constrain the parameter to that value (Box and Tiao 1973; Bretthorst 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 281,
                                "start": 264
                            }
                        ],
                        "text": "Justi cation for the evidence approximationThe central approximation in this scheme can be stated as follows: when we integrate out a parameter ,the e ect for most purposes is to estimate the parameter from the data, and then constrain the parameterto that value (Box and Tiao 1973; Bretthorst 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 23
                            }
                        ],
                        "text": "The pioneering work of Box and Tiao (1973) used this approach to develop Bayesian robust statistics."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian inference in statistical analysis. Addison{Wesley"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Keeping neural networks simple by minimizing the description length"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 285
                            }
                        ],
                        "text": "\u2026applied to nonlinear models by makingappropriate local linearizations (so that the integral over the parameters is made approximately ratherthan exactly) and has been used successfully in image reconstruction (Gull 1989; Weir 1991) and in neuralnetworks (MacKay 1992c; Thodberg 1993; MacKay 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian non-linear modelling for the 1993 energy"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 49
                            }
                        ],
                        "text": "Other regularization schemes have been suggested (Weigend et al. 1991), and the same problem of controlling the hyperparameters applies to those models too."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 50
                            }
                        ],
                        "text": "Other regularization schemes have been suggested (Weigend et al. 1991), and thesame problem of controlling the hyperparameters applies to those models too."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalization by weight{elimination"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 16,
            "methodology": 20
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 44,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Comparison-of-Approximate-Methods-for-Handling-Mackay/c4c47ebf6454e3c5a8417c580c8ecf694e34ad49?sort=total-citations"
}