{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801089"
                        ],
                        "name": "T. Evgeniou",
                        "slug": "T.-Evgeniou",
                        "structuredName": {
                            "firstName": "Theodoros",
                            "lastName": "Evgeniou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Evgeniou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708279"
                        ],
                        "name": "C. Micchelli",
                        "slug": "C.-Micchelli",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Micchelli",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Micchelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, task relatedness has been modeled through assuming that all functions learned are close to each other in some norm [3, 8, 15, 19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 76
                            }
                        ],
                        "text": "Learning multiple related tasks simultaneously has been empirically [2, 3, 8, 9, 12, 18, 19, 20] as well as theoretically [2, 4, 5] shown to often significantly improve performance relative to learning each task independently."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "Since this is not central in the present paper we postpone its discussion to a future occasion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16193644,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0af75728bec67f698a8c619645165de13780c2fa",
            "isKey": false,
            "numCitedBy": 905,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of learning many related tasks simultaneously using kernel methods and regularization. The standard single-task kernel methods, such as support vector machines and regularization networks, are extended to the case of multi-task learning. Our analysis shows that the problem of estimating many task functions with regularization can be cast as a single task learning problem if a family of multi-task kernel functions we define is used. These kernels model relations among the tasks and are derived from a novel form of regularizers. Specific kernels that can be used for multi-task learning are provided and experimentally tested on two real data sets. In agreement with past empirical work on multi-task learning, the experiments show that learning multiple related tasks simultaneously using the proposed approach can significantly outperform standard single-task learning particularly when there are many related tasks but few data per task."
            },
            "slug": "Learning-Multiple-Tasks-with-Kernel-Methods-Evgeniou-Micchelli",
            "title": {
                "fragments": [],
                "text": "Learning Multiple Tasks with Kernel Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "The experiments show that learning multiple related tasks simultaneously using the proposed approach can significantly outperform standard single-task learning particularly when there are many related tasks but few data per task."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143626983"
                        ],
                        "name": "B. Bakker",
                        "slug": "B.-Bakker",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Bakker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Bakker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790356"
                        ],
                        "name": "T. Heskes",
                        "slug": "T.-Heskes",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Heskes",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Heskes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10436583,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a43d7b8e5e1bcb7c3fbf82164cfc9d12737176e8",
            "isKey": false,
            "numCitedBy": 594,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling a collection of similar regression or classification tasks can be improved by making the tasks 'learn from each other'. In machine learning, this subject is approached through 'multitask learning', where parallel tasks are modeled as multiple outputs of the same network. In multilevel analysis this is generally implemented through the mixed-effects linear model where a distinction is made between 'fixed effects', which are the same for all tasks, and 'random effects', which may vary between tasks. In the present article we will adopt a Bayesian approach in which some of the model parameters are shared (the same for all tasks) and others more loosely connected through a joint prior distribution that can be learned from the data. We seek in this way to combine the best parts of both the statistical multilevel approach and the neural network machinery. The standard assumption expressed in both approaches is that each task can learn equally well from any other task. In this article we extend the model by allowing more differentiation in the similarities between tasks. One such extension is to make the prior mean depend on higher-level task characteristics. More unsupervised clustering of tasks is obtained if we go from a single Gaussian prior to a mixture of Gaussians. This can be further generalized to a mixture of experts architecture with the gates depending on task characteristics. All three extensions are demonstrated through application both on an artificial data set and on two real-world problems, one a school problem and the other involving single-copy newspaper sales."
            },
            "slug": "Task-Clustering-and-Gating-for-Bayesian-Multitask-Bakker-Heskes",
            "title": {
                "fragments": [],
                "text": "Task Clustering and Gating for Bayesian Multitask Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A Bayesian approach is adopted in which some of the model parameters are shared and others more loosely connected through a joint prior distribution that can be learned from the data to combine the best parts of both the statistical multilevel approach and the neural network machinery."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38070424"
                        ],
                        "name": "R. Ando",
                        "slug": "R.-Ando",
                        "structuredName": {
                            "firstName": "Rie",
                            "lastName": "Ando",
                            "middleNames": [
                                "Kubota"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ando"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "Learning multiple related tasks simultaneously has been empirically [2, 3, 8, 9, 12, 18, 19, 20] as well as theoretically [2, 4, 5] shown to often significantly improve performance relative to learning each task independently."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13650160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "944e1a7b2c5c62e952418d7684e3cade89c76f87",
            "isKey": false,
            "numCitedBy": 1414,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting."
            },
            "slug": "A-Framework-for-Learning-Predictive-Structures-from-Ando-Zhang",
            "title": {
                "fragments": [],
                "text": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data, and algorithms for structural learning will be proposed, and computational issues will be investigated."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114075909"
                        ],
                        "name": "Kai Yu",
                        "slug": "Kai-Yu",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700754"
                        ],
                        "name": "Volker Tresp",
                        "slug": "Volker-Tresp",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Tresp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volker Tresp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071649"
                        ],
                        "name": "Anton Schwaighofer",
                        "slug": "Anton-Schwaighofer",
                        "structuredName": {
                            "firstName": "Anton",
                            "lastName": "Schwaighofer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anton Schwaighofer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "This may be the case for functions capturing preferences in users\u2019 modeling problems [9, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "Learning multiple related tasks simultaneously has been empirically [2, 3, 8, 9, 12, 18, 19, 20] as well as theoretically [2, 4, 5] shown to often significantly improve performance relative to learning each task independently."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13895067,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "135687fa2e5573037748dcf44d1c3d6766809f57",
            "isKey": false,
            "numCitedBy": 406,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of multi-task learning, that is, learning multiple related functions. Our approach is based on a hierarchical Bayesian framework, that exploits the equivalence between parametric linear models and nonparametric Gaussian processes (GPs). The resulting models can be learned easily via an EM-algorithm. Empirical studies on multi-label text categorization suggest that the presented models allow accurate solutions of these multi-task problems."
            },
            "slug": "Learning-Gaussian-processes-from-multiple-tasks-Yu-Tresp",
            "title": {
                "fragments": [],
                "text": "Learning Gaussian processes from multiple tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This work considers the problem of multi-task learning, that is, learning multiple related functions, and presents a hierarchical Bayesian framework, that exploits the equivalence between parametric linear models and nonparametric Gaussian processes."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2738621"
                        ],
                        "name": "J. Abernethy",
                        "slug": "J.-Abernethy",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Abernethy",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Abernethy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144570279"
                        ],
                        "name": "F. Bach",
                        "slug": "F.-Bach",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bach",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801089"
                        ],
                        "name": "T. Evgeniou",
                        "slug": "T.-Evgeniou",
                        "structuredName": {
                            "firstName": "Theodoros",
                            "lastName": "Evgeniou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Evgeniou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152303545"
                        ],
                        "name": "Jean-Philippe Vert",
                        "slug": "Jean-Philippe-Vert",
                        "structuredName": {
                            "firstName": "Jean-Philippe",
                            "lastName": "Vert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Philippe Vert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 152
                            }
                        ],
                        "text": "We note that the problem of learning a low-rank matrix factorization which approximates a given partially observed target matrix has been considered in [1], [17] and references therein."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 947596,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10bf197aa7dd57a2b174d5ad36389a21f32707b8",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a new collaborative filtering (CF) method that combines both previously known users' preferences, i.e. standard CF, as well as product/user attributes, i.e. classical function approximation, to predict a given user's interest in a particular product. Our method is a generalized low rank matrix completion problem, where we learn a function whose inputs are pairs of vectors -- the standard low rank matrix completion problem being a special case where the inputs to the function are the row and column indices of the matrix. We solve this generalized matrix completion problem using tensor product kernels for which we also formally generalize standard kernel properties. Benchmark experiments on movie ratings show the advantages of our generalized matrix completion method over the standard matrix completion one with no information about movies or people, as well as over standard multi-task or single task learning methods."
            },
            "slug": "Low-rank-matrix-factorization-with-attributes-Abernethy-Bach",
            "title": {
                "fragments": [],
                "text": "Low-rank matrix factorization with attributes"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This work develops a new collaborative filtering method that combines both previously known users' preferences, as well as product/user attributes, i.e. standard CF, to predict a given user's interest in a particular product."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401829700"
                        ],
                        "name": "Shai Ben-David",
                        "slug": "Shai-Ben-David",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Ben-David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shai Ben-David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952147"
                        ],
                        "name": "Reba Schuller Borbely",
                        "slug": "Reba-Schuller-Borbely",
                        "structuredName": {
                            "firstName": "Reba",
                            "lastName": "Borbely",
                            "middleNames": [
                                "Schuller"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reba Schuller Borbely"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 8
                            }
                        ],
                        "text": "For example, in object recognition, it is well known that the human visual system is organized in a way such that all objects1 are represented \u2013 at the earlier stages of the visual system \u2013 using a common set of features learned, e.g. local filters similar to wavelets [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 137
                            }
                        ],
                        "text": "Learning multiple related tasks simultaneously has been empirically [2, 3, 8, 9, 12, 18, 19, 20] as well as theoretically [2, 4, 5] shown to often significantly improve performance relative to learning each task independently."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13967968,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "3a4551508f84a3f5447d3490b2db95b4d87a7969",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The approach of learning of multiple \u201crelated\u201d tasks simultaneously has proven quite successful in practice; however, theoretical justification for this success has remained elusive. The starting point for previous work on multiple task learning has been that the tasks to be learned jointly are somehow \u201calgorithmically related\u201d, in the sense that the results of applying a specific learning algorithm to these tasks are assumed to be similar. We offer an alternative approach, defining relatedness of tasks on the basis of similarity between the example generating distributions that underline these task."
            },
            "slug": "Exploiting-Task-Relatedness-for-Mulitple-Task-Ben-David-Borbely",
            "title": {
                "fragments": [],
                "text": "Exploiting Task Relatedness for Mulitple Task Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work offers an alternative approach to multiple task learning, defining relatedness of tasks on the basis of similarity between the example generating distributions that underline these task."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768120"
                        ],
                        "name": "T. Jebara",
                        "slug": "T.-Jebara",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Jebara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jebara"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "Learning multiple related tasks simultaneously has been empirically [2, 3, 8, 9, 12, 18, 19, 20] as well as theoretically [2, 4, 5] shown to often significantly improve performance relative to learning each task independently."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14128785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8559ff5718b5a5ff1ffaaf5f91efaf25ee518457",
            "isKey": false,
            "numCitedBy": 241,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We compute a common feature selection or kernel selection configuration for multiple support vector machines (SVMs) trained on different yet inter-related datasets. The method is advantageous when multiple classification tasks and differently labeled datasets exist over a common input space. Different datasets can mutually reinforce a common choice of representation or relevant features for their various classifiers. We derive a multi-task representation learning approach using the maximum entropy discrimination formalism. The resulting convex algorithms maintain the global solution properties of support vector machines. However, in addition to multiple SVM classification/regression parameters they also jointly estimate an optimal subset of features or optimal combination of kernels. Experiments are shown on standardized datasets."
            },
            "slug": "Multi-task-feature-and-kernel-selection-for-SVMs-Jebara",
            "title": {
                "fragments": [],
                "text": "Multi-task feature and kernel selection for SVMs"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A multi-task representation learning approach using the maximum entropy discrimination formalism is derived and the resulting convex algorithms maintain the global solution properties of support vector machines."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Learning multiple related tasks simultaneously has been empirically [2, 3, 8, 9, 12, 18, 19, 20] as well as theoretically [2,  4 , 5] shown to often significantly improve performance relative to learning each task independently."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Although convex optimization methods have been derived for the simpler problem of feature selection [12], prior work on multi-task feature learning has been based on more complex optimization problems which are not convex [2,  4 , 6] and, so, are at best only guaranteed to converge to a local minimum."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[ 4 ]), learning multiple tasks together significantly improves on learning the tasks independently."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Tasks may also be related in that they all share a common underlying representation [ 4 , 5, 6]. For example, in object recognition, it is well known that the human visual system is organized in a way that all objects1 are represented \u2010 at the earlier stages of the visual system \u2010 using a common set of features learned, e.g."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9803204,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "727e1e16ede6eaad241bad11c525da07b154c688",
            "isKey": true,
            "numCitedBy": 972,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "A major problem in machine learning is that of inductive bias: how to choose a learner's hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is supplied by hand through the skill and insights of experts. In this paper a model for automatically learning bias is investigated. The central assumption of the model is that the learner is embedded within an environment of related learning tasks. Within such an environment the learner can sample from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment. Under certain restrictions on the set of all hypothesis spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an environment of related tasks can potentially give much better generalization than learning a single task."
            },
            "slug": "A-Model-of-Inductive-Bias-Learning-Baxter",
            "title": {
                "fragments": [],
                "text": "A Model of Inductive Bias Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Under certain restrictions on the set of all hypothesis spaces available to the learner, it is shown that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151812271"
                        ],
                        "name": "Jian Zhang",
                        "slug": "Jian-Zhang",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "Learning multiple related tasks simultaneously has been empirically [2, 3, 8, 9, 12, 18, 19, 20] as well as theoretically [2, 4, 5] shown to often significantly improve performance relative to learning each task independently."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 156436,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12ccc43692cf774c66100e95ee2992afd7382ab5",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a probabilistic model based on Independent Component Analysis for learning multiple related tasks. In our model the task parameters are assumed to be generated from independent sources which account for the relatedness of the tasks. We use Laplace distributions to model hidden sources which makes it possible to identify the hidden, independent components instead of just modeling correlations. Furthermore, our model enjoys a sparsity property which makes it both parsimonious and robust. We also propose efficient algorithms for both empirical Bayes method and point estimation. Our experimental results on two multi-label text classification data sets show that the proposed approach is promising."
            },
            "slug": "Learning-Multiple-Related-Tasks-using-Latent-Zhang-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Learning Multiple Related Tasks using Latent Independent Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A probabilistic model based on Independent Component Analysis for learning multiple related tasks which uses Laplace distributions to model hidden sources which makes it possible to identify the hidden, independent components instead of just modeling correlations."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "Learning multiple related tasks simultaneously has been empirically [2, 3, 8, 9, 12, 18, 19, 20] as well as theoretically [2, 4, 5] shown to often significantly improve performance relative to learning each task independently."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11194336,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42de22c119f25d303032396b8f7d962f62d6498b",
            "isKey": false,
            "numCitedBy": 441,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of detecting a large number of different object classes in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, which can be slow and require much training data. We present a multi-class boosting procedure (joint boosting) that reduces both the computational and sample complexity, by finding common features that can be shared across the classes. The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required is observed to scale approximately logarithmically with the number of classes. In addition, we find that the features selected by independently trained classifiers are often specific to the class, whereas the features selected by the jointly trained classifiers are more generic features, such as lines and edges."
            },
            "slug": "Sharing-features:-efficient-boosting-procedures-for-Torralba-Murphy",
            "title": {
                "fragments": [],
                "text": "Sharing features: efficient boosting procedures for multiclass object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A multi-class boosting procedure (joint boosting) is presented that reduces both the computational and sample complexity, by finding common features that can be shared across the classes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708279"
                        ],
                        "name": "C. Micchelli",
                        "slug": "C.-Micchelli",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Micchelli",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Micchelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "This may be the case for functions capturing preferences in users\u2019 modeling problems [9, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "Since this is not central in the present paper we postpone its discussion to a future occasion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12725766,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "8b836f2797a8008da6477426490d4afef0e9335b",
            "isKey": false,
            "numCitedBy": 477,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "In this letter, we provide a study of learning in a Hilbert space of vector-valued functions. We motivate the need for extending learning theory of scalar-valued functions by practical considerations and establish some basic results for learning vector-valued functions that should prove useful in applications. Specifically, we allow an output space Y to be a Hilbert space, and we consider a reproducing kernel Hilbert space of functions whose values lie in Y. In this setting, we derive the form of the minimal norm interpolant to a finite set of data and apply it to study some regularization functionals that are important in learning theory. We consider specific examples of such functionals corresponding to multiple-output regularization networks and support vector machines, for both regression and classification. Finally, we provide classes of operator-valued kernels of the dot product and translation-invariant type."
            },
            "slug": "On-Learning-Vector-Valued-Functions-Micchelli-Pontil",
            "title": {
                "fragments": [],
                "text": "On Learning Vector-Valued Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This letter provides a study of learning in a Hilbert space of vector-valued functions and derives the form of the minimal norm interpolant to a finite set of data and applies it to study some regularization functionals that are important in learning theory."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706280"
                        ],
                        "name": "Nathan Srebro",
                        "slug": "Nathan-Srebro",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Srebro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Srebro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211659"
                        ],
                        "name": "Jason D. M. Rennie",
                        "slug": "Jason-D.-M.-Rennie",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Rennie",
                            "middleNames": [
                                "D.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason D. M. Rennie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "We briefly discuss its connection to our current work in Section 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5048382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cedf154c28178370d95510112413dc8cb48120a8",
            "isKey": false,
            "numCitedBy": 1105,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel approach to collaborative prediction, using low-norm instead of low-rank factorizations. The approach is inspired by, and has strong connections to, large-margin linear discrimination. We show how to learn low-norm factorizations by solving a semi-definite program, and discuss generalization error bounds for them."
            },
            "slug": "Maximum-Margin-Matrix-Factorization-Srebro-Rennie",
            "title": {
                "fragments": [],
                "text": "Maximum-Margin Matrix Factorization"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A novel approach to collaborative prediction is presented, using low-norm instead of low-rank factorizations, inspired by, and has strong connections to, large-margin linear discrimination."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144651919"
                        ],
                        "name": "M. Fazel",
                        "slug": "M.-Fazel",
                        "structuredName": {
                            "firstName": "Maryam",
                            "lastName": "Fazel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fazel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7846291"
                        ],
                        "name": "H. Hindi",
                        "slug": "H.-Hindi",
                        "structuredName": {
                            "firstName": "Haitham",
                            "lastName": "Hindi",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hindi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1843103"
                        ],
                        "name": "Stephen P. Boyd",
                        "slug": "Stephen-P.-Boyd",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Boyd",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen P. Boyd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6000077,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "6503fcffc6692b99f467815d2a3471116115f5bd",
            "isKey": false,
            "numCitedBy": 970,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a generalization of the trace heuristic that applies to general nonsymmetric, even non-square, matrices, and reduces to the trace heuristic when the matrix is positive semidefinite. The heuristic is to replace the (nonconvex) rank objective with the sum of the singular values of the matrix, which is the dual of the spectral norm. We show that this problem can be reduced to a semidefinite program, hence efficiently solved. To motivate the heuristic, we, show that the dual spectral norm is the convex envelope of the rank on the set of matrices with norm less than one. We demonstrate the method on the problem of minimum-order system approximation."
            },
            "slug": "A-rank-minimization-heuristic-with-application-to-Fazel-Hindi",
            "title": {
                "fragments": [],
                "text": "A rank minimization heuristic with application to minimum order system approximation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is shown that the heuristic to replace the (nonconvex) rank objective with the sum of the singular values of the matrix, which is the dual of the spectral norm, can be reduced to a semidefinite program, hence efficiently solved."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 American Control Conference. (Cat. No.01CH37148)"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "80483590"
                        ],
                        "name": "C. Pinkus",
                        "slug": "C.-Pinkus",
                        "structuredName": {
                            "firstName": "Clare",
                            "lastName": "Pinkus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Pinkus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "We now generalize problem (2.2) to the multi-task case."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2416277,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e1bc040f69e108f224190dc1f859e21eff000d21",
            "isKey": true,
            "numCitedBy": 11,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "The purpose of this paper is to describe various results which pertain to a class of variational problems in which two or more error criteria are to be controlled simultaneously in some optimal fashion. This type of problem has wide application in optimal control, statistical estimation and approximation theory. We make no pretense that our treatment is exhaustive either in the applications we cover or in the analysis we use. Rather, we wish to point out some general properties of these problems that are useful, survey some interesting examples which arise in applications, and give an analysis of certain classes of these variational problems where total positivity plays a central role."
            },
            "slug": "Variational-problems-arising-from-balancing-several-Pinkus",
            "title": {
                "fragments": [],
                "text": "Variational problems arising from balancing several error criteria"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144108246"
                        ],
                        "name": "D. Ruppert",
                        "slug": "D.-Ruppert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ruppert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ruppert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "If w, u \u2208 IRd, we define \u3008w, u\u3009 := \u2211d i=1 wiui, the standard inner product in IR d."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118901444,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5176a2f31dace77db9135dde7020d2c37f78cca0",
            "isKey": false,
            "numCitedBy": 11815,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "In the words of the authors, the goal of this book was to \u201cbring together many of the important new ideas in learning, and explain them in a statistical framework.\u201d The authors have been quite successful in achieving this objective, and their work is a welcome addition to the statistics and learning literatures. Statistics has always been interdisciplinary, borrowing ideas from diverse \u008e elds and repaying the debt with contributions, both theoretical and practical, to the other intellectual disciplines. For statistical learning, this cross-fertilization is especially noticeable. This book is a valuable resource, both for the statistician needing an introduction to machine learning and related \u008e elds and for the computer scientist wishing to learn more about statistics. Statisticians will especially appreciate that it is written in their own language. The level of the book is roughly that of a second-year doctoral student in statistics, and it will be useful as a textbook for such students. In a stimulating article, Breiman (2001) argued that statistics has been focused too much on a \u201cdata modeling culture,\u201d where the model is paramount. Breiman argued instead for an \u201calgorithmic modeling culture,\u201d with emphasis on black-box types of prediction. Breiman\u2019s article is controversial, and in his discussion, Efron objects that \u201cprediction is certainly an interesting subject, but Leo\u2019s paper overstates both its role and our profession\u2019s lack of interest in it.\u201d Although I mostly agree with Efron, I worry that the courses offered by most statistics departments include little, if any, treatment of statistical learning and prediction. (Stanford, where Efron and the authors of this book teach, is an exception.) Graduate students in statistics certainly need to know more than they do now about prediction, machine learning, statistical learning, and data mining (not disjoint subjects). I hope that graduate courses covering the topics of this book will become more common in statistics curricula. Most of the book is focused on supervised learning, where one has inputs and outputs from some system and wishes to predict unknown outputs corresponding to known inputs. The methods discussed for supervised learning include linear and logistic regression; basis expansion, such as splines and wavelets; kernel techniques, such as local regression, local likelihood, and radial basis functions; neural networks; additive models; decision trees based on recursive partitioning, such as CART; and support vector machines. There is a \u008e nal chapter on unsupervised learning, including association rules, cluster analysis, self-organizing maps, principal components and curves, and independent component analysis. Many statisticians will be unfamiliar with at least some of these algorithms. Association rules are popular for mining commercial data in what is called \u201cmarket basket analysis.\u201d The aim is to discover types of products often purchased together. Such knowledge can be used to develop marketing strategies, such as store or catalog layouts. Self-organizing maps (SOMs) involve essentially constrained k-means clustering, where prototypes are mapped to a two-dimensional curved coordinate system. Independent components analysis is similar to principal components analysis and factor analysis, but it uses higher-order moments to achieve independence, not merely zero correlation between components. A strength of the book is the attempt to organize a plethora of methods into a coherent whole. The relationships among the methods are emphasized. I know of no other book that covers so much ground. Of course, with such broad coverage, it is not possible to cover any single topic in great depth, so this book will encourage further reading. Fortunately, each chapter includes bibliographic notes surveying the recent literature. These notes and the extensive references provide a good introduction to the learning literature, including much outside of statistics. The book might be more suitable as a textbook if less material were covered in greater depth; however, such a change would compromise the book\u2019s usefulness as a reference, and so I am happier with the book as it was written."
            },
            "slug": "The-Elements-of-Statistical-Learning:-Data-Mining,-Ruppert",
            "title": {
                "fragments": [],
                "text": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This book is a valuable resource, both for the statistician needing an introduction to machine learning and related \u008e elds and for the computer scientist wishing to learn more about statistics, and statisticians will especially appreciate that it is written in their own language."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 130
                            }
                        ],
                        "text": "It is well known that using the 1-norm leads to sparse solutions, that is, many components of the learned vector at are zero, see [7] and references therein."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12929381,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "36ab8bd64210ac5c4f7d326ed2c0a5745e91320f",
            "isKey": false,
            "numCitedBy": 1067,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider inexact linear equations y \u2248 \u03a6x where y is a given vector in \u211dn, \u03a6 is a given n \u00d7 m matrix, and we wish to find x0,\u03f5 as sparse as possible while obeying \u2016y \u2212 \u03a6x0,\u03f5\u20162 \u2264 \u03f5. In general, this requires combinatorial optimization and so is considered intractable. On the other hand, the \ud835\udcc11\u2010minimization problem $${\\rm min} \\; \\|x\\|_{1}\\;\\;\\; {\\rm subject \\; to}\\;\\;\\; \\|y - \\Phi{x}\\|_{2} \\leq \\epsilon$$ is convex and is considered tractable. We show that for most \u03a6, if the optimally sparse approximation x0,\u03f5 is sufficiently sparse, then the solution x1,\u03f5 of the \ud835\udcc11\u2010minimization problem is a good approximation to x0,\u03f5."
            },
            "slug": "For-most-large-underdetermined-systems-of-the-the-Donoho",
            "title": {
                "fragments": [],
                "text": "For most large underdetermined systems of equations, the minimal \ud835\udcc11\u2010norm near\u2010solution approximates the sparsest near\u2010solution"
            },
            "tldr": {
                "abstractSimilarityScore": 34,
                "text": "It is shown that for most \u03a6, if the optimally sparse approximation x0,\u03f5 is sufficiently sparse, then the solution x1, \u03f5 of the \ud835\udcc11\u2010minimization problem is a good approximation to x0 ,\u03f5."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801089"
                        ],
                        "name": "T. Evgeniou",
                        "slug": "T.-Evgeniou",
                        "structuredName": {
                            "firstName": "Theodoros",
                            "lastName": "Evgeniou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Evgeniou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089062"
                        ],
                        "name": "Olivier Toubia",
                        "slug": "Olivier-Toubia",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Toubia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Toubia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 5
                            }
                        ],
                        "text": "Tasks may also be related in that they all share a common underlying representation [4, 5, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "Since the extended problem is nonconvex, we develop an equivalent convex optimization problem in Section 3 and present an algorithm for solving it in Section 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 80
                            }
                        ],
                        "text": "Learning multiple related tasks simultaneously has been empirically [2, 3, 8, 9, 12, 18, 19, 20] as well as theoretically [2, 4, 5] shown to often significantly improve performance relative to learning each task independently."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42526,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3aae960155251d70567cbabf227ab68903a90012",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose and test a new approach for modeling consumer heterogeneity in conjoint estimation based on convex optimization and statistical machine learning. We develop methods both for metric and choice data. Like hierarchical Bayes (HB), our methods shrink individual-level partworth estimates towards a population mean. However, while HB samples from a posterior distribution that is influenced by exogenous parameters (the parameters of the second-stage priors), we minimize a convex loss function that depends only on endogenous parameters. As a result, the amounts of shrinkage differ between the two approaches, leading to different estimation accuracies. In our comparisons, based on simulations as well as empirical data sets, the new approach overall outperforms standard HB (i.e., with relatively diffuse second-stage priors) both with metric and choice data."
            },
            "slug": "A-Convex-Optimization-Approach-to-Modeling-Consumer-Evgeniou-Pontil",
            "title": {
                "fragments": [],
                "text": "A Convex Optimization Approach to Modeling Consumer Heterogeneity in Conjoint Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A new approach for modeling consumer heterogeneity in conjoint estimation based on convex optimization and statistical machine learning outperforms standard HB both with metric and choice data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981539"
                        ],
                        "name": "Thomas Serre",
                        "slug": "Thomas-Serre",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Serre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Serre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1876834"
                        ],
                        "name": "Minjoon Kouh",
                        "slug": "Minjoon-Kouh",
                        "structuredName": {
                            "firstName": "Minjoon",
                            "lastName": "Kouh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minjoon Kouh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3202046"
                        ],
                        "name": "C. Cadieu",
                        "slug": "C.-Cadieu",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Cadieu",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Cadieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3067508"
                        ],
                        "name": "U. Knoblich",
                        "slug": "U.-Knoblich",
                        "structuredName": {
                            "firstName": "Ulf",
                            "lastName": "Knoblich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Knoblich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1852992"
                        ],
                        "name": "G. Kreiman",
                        "slug": "G.-Kreiman",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Kreiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kreiman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2935416,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "e5baffdda5c5175ad9d71e2d21703298b84cffe8",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 204,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : We describe a quantitative theory to account for the computations performed by the feedforward path of the ventral stream of visual cortex and the local circuits implementing them. We show that a model instantiating the theory is capable of performing recognition on datasets of complex images at the level of human observers in rapid categorization tasks. We also show that the theory is consistent with (and in some case has predicted) several properties of neurons in V1, V4, IT and PFC. The theory seems sufficiently comprehensive, detailed and satisfactory to represent an interesting challenge for physiologists and modelers: either disprove its basic features or propose alternative theories of equivalent scope. The theory suggests a number of open questions for visual physiology and psychophysics."
            },
            "slug": "A-Theory-of-Object-Recognition:-Computations-and-in-Serre-Kouh",
            "title": {
                "fragments": [],
                "text": "A Theory of Object Recognition: Computations and Circuits in the Feedforward Path of the Ventral Stream in Primate Visual Cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A quantitative theory to account for the computations performed by the feedforward path of the ventral stream of visual cortex and the local circuits implementing them and it is shown that a model instantiating the theory is capable of performing recognition on datasets of complex images at the level of human observers in rapid categorization tasks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21779757"
                        ],
                        "name": "P. Lenk",
                        "slug": "P.-Lenk",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Lenk",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Lenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070598"
                        ],
                        "name": "W. DeSarbo",
                        "slug": "W.-DeSarbo",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "DeSarbo",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. DeSarbo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25909206"
                        ],
                        "name": "P. Green",
                        "slug": "P.-Green",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Green",
                            "middleNames": [
                                "E."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Green"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47533598"
                        ],
                        "name": "M. R. Young",
                        "slug": "M.-R.-Young",
                        "structuredName": {
                            "firstName": "M",
                            "lastName": "Young",
                            "middleNames": [
                                "R"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. R. Young"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 85
                            }
                        ],
                        "text": "This may be the case for functions capturing preferences in users\u2019 modeling problems [9, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 154757383,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "b2c7373ff1ab984a76df8eb6a4aff811cf3e31af",
            "isKey": false,
            "numCitedBy": 504,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "The drive to satisfy customers in narrowly defined market segments has led firms to offer wider arrays of products and services. Delivering products and services with the appropriate mix of features for these highly fragmented market segments requires understanding the value that customers place on these features. Conjoint analysis endeavors to unravel the value or partworths, that customers place on the product or service's attributes from experimental subjects' evaluation of profiles based on hypothetical products or services. When the goal is to estimate the heterogeneity in the customers' partworths, traditional estimation methods, such as least squares, require each subject to respond to more profiles than product attributes, resulting in lengthy questionnaires for complex, multiattributed products or services. Long questionnaires pose practical and theoretical problems. Response rates tend to decrease with increasing questionnaire length, and more importantly, academic evidence indicates that long questionnaires may induce response biases. \n \nThe problems associated with long questionnaires call for experimental designs and estimation methods that recover the heterogeneity in the partworths with shorter questionnaires. Unlike more popular estimation methods, Hierarchical Bayes HB random effects models do not require that individual-level design matrices be of full rank, which leads to the possibility of using fewer profiles per subject than currently used. Can this theoretical possibility be practically implemented? \n \nThis paper tests this conjecture with empirical studies and mathematical analysis. The random effects model in the paper describes the heterogeneity in subject-level partworths or regression coefficients with a linear model that can include subject-level covariates. In addition, the error variances are specific to the subjects, thus allowing for the differential use of the measurement scale by different subjects. \n \nIn the empirical study, subjects' responses to a full profile design are randomly deleted to test the performance of HB methods with declining sample sizes. These simple experiments indicate that HB methods can recover heterogeneity and estimate individual-level partworths, even when individual-level least squares estimators do not exist due to insufficient degrees of freedom. \n \nMotivated by these empirical studies, the paper analytically investigates the trade-off between the number of profiles per subject and the number of subjects on the statistical accuracy of the estimators that describe the partworth heterogeneity. The paper considers two experimental designs: each subject receives the same set of profiles, and subjects receive different blocks of a fractional factorial design. In the first case, the optimal design, subject to a budget constraint, uses more subjects and fewer profiles per subject when the ratio of unexplained, partworth heterogeneity to unexplained response variance is large. In the second case, one can maintain a given level of estimation accuracy as the number of profiles per subject decreases by increasing the number of subjects assigned to each block. \n \nThese results provide marketing researchers the option of using shorter questionnaires for complex products or services. The analysis assumes that response quality is independent of questionnaire length and does not address the impact of design factors on response quality. If response quality and questionnaire length were, in fact, unrelated, then marketing researchers would still find the paper's results useful in improving the efficiency of their conjoint designs. However, if response quality were to decline with questionnaire length, as the preponderance of academic research indicates, then the option to use shorter questionnaires would become even more valuable."
            },
            "slug": "Hierarchical-Bayes-Conjoint-Analysis:-Recovery-of-Lenk-DeSarbo",
            "title": {
                "fragments": [],
                "text": "Hierarchical Bayes Conjoint Analysis: Recovery of Partworth Heterogeneity from Reduced Experimental Designs"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "@BULLET 13 binary input attributes"
            },
            "venue": {
                "fragments": [],
                "text": "@BULLET 13 binary input attributes"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "@BULLET Handle > 1 clusters of tasks. Hierarchical models of tasks/features"
            },
            "venue": {
                "fragments": [],
                "text": "@BULLET Handle > 1 clusters of tasks. Hierarchical models of tasks/features"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "@BULLET 8 PC models (training examples); 4 PC models (test examples"
            },
            "venue": {
                "fragments": [],
                "text": "@BULLET 8 PC models (training examples); 4 PC models (test examples"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "@BULLET Connection to Bayesian methods"
            },
            "venue": {
                "fragments": [],
                "text": "@BULLET Connection to Bayesian methods"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "10} (likelihood of purchase). Future Work @BULLET More general, nonlinear features"
            },
            "venue": {
                "fragments": [],
                "text": "10} (likelihood of purchase). Future Work @BULLET More general, nonlinear features"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2 (real data) @BULLET Consumers' ratings of products"
            },
            "venue": {
                "fragments": [],
                "text": "2 (real data) @BULLET Consumers' ratings of products"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "@BULLET 180 persons (tasks)"
            },
            "venue": {
                "fragments": [],
                "text": "@BULLET 180 persons (tasks)"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 16,
            "methodology": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 26,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Multi-Task-Feature-Learning-Argyriou-Evgeniou/dbb3342599c9b431a3152a0d5c813d3e56967a27?sort=total-citations"
}