{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144177248"
                        ],
                        "name": "James M. Rehg",
                        "slug": "James-M.-Rehg",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Rehg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James M. Rehg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "Speci c rules for the on-line generation of visibility orders for the hand are described in [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "7, and more details can be found in [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10034460,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40b3d2352033ff412106c3cbc3a1c8c620f12566",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Computer sensing of hand and limb motion is an important problem for applications in human-computer interaction, virtual reality, and athletic performance measurement. We describe a framework for local tracking of self-occluding motion, in which parts of the mechanism obstruct each others visibility to the camera. Our approach uses a kinematic model to predict occlusion and windowed templates to track partially occluded objects. We analyze our model of self-occlusion, discuss the implementation of our algorithm, and give experimental results for 3D hand tracking under significant amounts of self-occlusion. These results extend the DigitEyes system for articulated tracking, which we have previously developed, to handle self-occluding motions."
            },
            "slug": "Visual-Tracking-of-Self-Occluding-Articulated-Rehg-Kanade",
            "title": {
                "fragments": [],
                "text": "Visual Tracking of Self-Occluding Articulated Objects"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A framework for local tracking of self-occluding motion, in which parts of the mechanism obstruct each others visibility to the camera, is described, which uses a kinematic model to predict occlusion and windowed templates to track partially occluded objects."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144177248"
                        ],
                        "name": "James M. Rehg",
                        "slug": "James-M.-Rehg",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Rehg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James M. Rehg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10376369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c51f0d3f863fdde988cfada25f1a07a224889129",
            "isKey": false,
            "numCitedBy": 530,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Passive sensing of human hand and limb motion is important for a wide range of applications from human-computer interaction to athletic performance measurement. High degree of freedom articulated mechanisms like the human hand are difficult to track because of their large state space and complex image appearance. This article describes a model-based hand tracking system, called DigitEyes, that can recover the state of a 27 DOF hand model from ordinary gray scale images at speeds of up to 10 Hz."
            },
            "slug": "Visual-Tracking-of-High-DOF-Articulated-Structures:-Rehg-Kanade",
            "title": {
                "fragments": [],
                "text": "Visual Tracking of High DOF Articulated Structures: an Application to Human Hand Tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A model-based hand tracking system, called DigitEyes, that can recover the state of a 27 DOF hand model from ordinary gray scale images at speeds of up to 10 Hz is described."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144177248"
                        ],
                        "name": "James M. Rehg",
                        "slug": "James-M.-Rehg",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Rehg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James M. Rehg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809905"
                        ],
                        "name": "A. Witkin",
                        "slug": "A.-Witkin",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Witkin",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Witkin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "The combination of kinematic and camera transforms make up a deformation function [12], f(q; s), which maps template coordinates, s = [u v], to image coordinates, w = [x y], as a function of q."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53780196,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "bbcc7f511f66f6b5c7fbd0ccba456839bb40508d",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel solution to the 2-D tracking problem is presented. This solution has two major components: a deformation model that constrains the interpretation of motion, and a set of energy-based match criteria that specify image features to be used in tracking. The separation of the motion model from the match features is an advantage of this approach over previous tracking systems. An implementation of these ideas has been shown to exhibit fast and flexible operation over a wide class of image motions. Experimental results are given for two real-world image sequences.<<ETX>>"
            },
            "slug": "Visual-tracking-with-deformation-models-Rehg-Witkin",
            "title": {
                "fragments": [],
                "text": "Visual tracking with deformation models"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A novel solution to the 2-D tracking problem has two major components: a deformation model that constrains the interpretation of motion, and a set of energy-based match criteria that specify image features to be used in tracking."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1991 IEEE International Conference on Robotics and Automation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144177248"
                        ],
                        "name": "James M. Rehg",
                        "slug": "James-M.-Rehg",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Rehg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James M. Rehg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 104
                            }
                        ],
                        "text": "This paper extends our earlier work on the DigitEyes system for model-based articulated object tracking [10, 9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16357559,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db4e821c2b09ff8774ee6f616e4dec202c9a419e",
            "isKey": false,
            "numCitedBy": 223,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Computer sensing of hand and limb motion is an important problem for applications in human-computer interaction (HCI), virtual reality, and athletic performance measurement. Commercially available sensors are invasive, and require the user to wear gloves or targets. We have developed a noninvasive vision-based hand tracking system, called DigitEyes. Employing a kinematic hand model, the DigitEyes system has demonstrated tracking performance at speeds of up to 10 Hz, using line and point features extracted from gray scale images of unadorned, unmarked hands. We describe an application of our sensor to a 3D mouse user-interface problem.<<ETX>>"
            },
            "slug": "DigitEyes:-vision-based-hand-tracking-for-Rehg-Kanade",
            "title": {
                "fragments": [],
                "text": "DigitEyes: vision-based hand tracking for human-computer interaction"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The DigitEyes system has demonstrated tracking performance at speeds of up to 10 Hz, using line and point features extracted from gray scale images of unadorned, unmarked hands, and an application of the sensor to a 3D mouse user-interface problem is described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 IEEE Workshop on Motion of Non-rigid and Articulated Objects"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711560"
                        ],
                        "name": "Dimitris N. Metaxas",
                        "slug": "Dimitris-N.-Metaxas",
                        "structuredName": {
                            "firstName": "Dimitris",
                            "lastName": "Metaxas",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimitris N. Metaxas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750924"
                        ],
                        "name": "Demetri Terzopoulos",
                        "slug": "Demetri-Terzopoulos",
                        "structuredName": {
                            "firstName": "Demetri",
                            "lastName": "Terzopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Demetri Terzopoulos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 69
                            }
                        ],
                        "text": "Other previous work on tracking general articulated objects includes [5, 15, 8, 4, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 31199185,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "6628820be220d48c567095f3aa8ac06516b730be",
            "isKey": false,
            "numCitedBy": 459,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "A physics-based framework for 3-D shape and nonrigid motion estimation for real-time computer vision systems is presented. The framework features dynamic models that incorporate the mechanical principles of rigid and nonrigid bodies into conventional geometric primitives. Through the efficient numerical simulation of Lagrange equations of motion, the models can synthesize physically correct behaviors in response to applied forces and imposed constraints. Applying continuous Kalman filtering theory, a recursive shape and motion estimator that employs the Lagrange equations as a system model is developed. The system model continually synthesizes nonrigid motion in response to generalized forces that arise from the inconsistency between the incoming observations and the estimated model state. The observation forces also account formally for instantaneous uncertainties and incomplete information. A Riccati procedure updates a covariance matrix that transforms the forces in accordance with the system dynamics and prior observation history. Experiments involving model fitting and tracking of articulated and flexible objects from noisy 3-D data are described. >"
            },
            "slug": "Shape-and-Nonrigid-Motion-Estimation-Through-Metaxas-Terzopoulos",
            "title": {
                "fragments": [],
                "text": "Shape and Nonrigid Motion Estimation Through Physics-Based Synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A physics-based framework for 3-D shape and nonrigid motion estimation for real-time computer vision systems is presented and a recursive shape and motion estimator that employs the Lagrange equations as a system model is developed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 96443795,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d22d58f6dc9bdf7a82ba17e0fe3f3ed33d06077",
            "isKey": false,
            "numCitedBy": 177,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "In order to recover an accurate representation of a scene containing multiple moving objects, one must use estimation methods that can recover both model parameters and segmentation at the same time. Traditional approaches to this problem rely on an edge-based discontinuity model, and have problems with transparent phenomena. The authors introduce a layered model of scene segmentation based on explicitly representing the support of a homogeneous region. The model employs parallel robust estimation techniques, and uses a minimal-covering optimization to estimate the number of objects in the scene. Using a simple direct motion model of translating objects, they successfully segment real image sequences containing multiple motions.<<ETX>>"
            },
            "slug": "Robust-estimation-of-a-multi-layered-motion-Darrell-Pentland",
            "title": {
                "fragments": [],
                "text": "Robust estimation of a multi-layered motion representation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A layered model of scene segmentation based on explicitly representing the support of a homogeneous region is introduced, which employs parallel robust estimation techniques, and uses a minimal-covering optimization to estimate the number of objects in the scene."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE Workshop on Visual Motion"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052391642"
                        ],
                        "name": "J. O'Rourke",
                        "slug": "J.-O'Rourke",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "O'Rourke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. O'Rourke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699200"
                        ],
                        "name": "N. Badler",
                        "slug": "N.-Badler",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Badler",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Badler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15680007,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9df0428c30b8aab4f7e6f367e70126efdfb8fc45",
            "isKey": false,
            "numCitedBy": 516,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "A system capable of analyzing image sequences of human motion is described. The system is structured as a feedback loop between high and low levels: predictions are made at the semantic level and verifications are sought at the image level. The domain of human motion lends itself to a model-driven analysis, and the system includes a detailed model of the human body. All information extracted from the image is interpreted through a constraint network based on the structure of the human model. A constraint propagation operator is defined and its theoretical properties outlined. An implementation of this operator is described, and results of the analysis system for short image sequences are presented."
            },
            "slug": "Model-based-image-analysis-of-human-motion-using-O'Rourke-Badler",
            "title": {
                "fragments": [],
                "text": "Model-based image analysis of human motion using constraint propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A system capable of analyzing image sequences of human motion is described, structured as a feedback loop between high and low levels: predictions are made at the semantic level and verifications are sought at the image level."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1967104"
                        ],
                        "name": "David C. Hogg",
                        "slug": "David-C.-Hogg",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hogg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David C. Hogg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 51
                            }
                        ],
                        "text": "Two of the earliest systems were developed by Hogg [4] and O'Rourke and Badler [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 69
                            }
                        ],
                        "text": "Other previous work on tracking general articulated objects includes [5, 15, 8, 4, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 34873540,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "92f98b189cec1220d479e3079b942e71b244aa65",
            "isKey": false,
            "numCitedBy": 597,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Model-based-vision:-a-program-to-see-a-walking-Hogg",
            "title": {
                "fragments": [],
                "text": "Model-based vision: a program to see a walking person"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47579815"
                        ],
                        "name": "Masanobu Yamamoto",
                        "slug": "Masanobu-Yamamoto",
                        "structuredName": {
                            "firstName": "Masanobu",
                            "lastName": "Yamamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masanobu Yamamoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2638101"
                        ],
                        "name": "K. Koshikawa",
                        "slug": "K.-Koshikawa",
                        "structuredName": {
                            "firstName": "Kazutada",
                            "lastName": "Koshikawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Koshikawa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35103895,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "4ff071ca1e5207975ca8cd146f5c33eddfdb2d98",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A model-based method for analyzing a human body motion is presented. The method is based on a robot arm model which represents a human body motion. Combining the model and the gradient scheme, the movement of the configuration of the model (Human) can be directly estimated from an image sequence.<<ETX>>"
            },
            "slug": "Human-motion-analysis-based-on-a-robot-arm-model-Yamamoto-Koshikawa",
            "title": {
                "fragments": [],
                "text": "Human motion analysis based on a robot arm model"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A model-based method based on a robot arm model which represents a human body motion and the movement of the configuration of the model can be directly estimated from an image sequence."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110239969"
                        ],
                        "name": "John Y. A. Wang",
                        "slug": "John-Y.-A.-Wang",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Wang",
                            "middleNames": [
                                "Y.",
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Y. A. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5556692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e33474350c94badff77eec1c48e24ba59c8cd05",
            "isKey": false,
            "numCitedBy": 341,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard approaches to motion analysis assume that the optic flow is smooth; such techniques have trouble dealing with occlusion boundaries. The image sequence can be decomposed into a set of overlapping layers, where each layer's motion is described by a smooth flow field. The discontinuities in the description are then attributed to object opacities rather than to the flow itself, mirroring the structure of the scene. A set of techniques is devised for segmenting images into coherently moving regions using affine motion analysis and clustering techniques. It is possible to decompose an image into a set of layers along with information about occlusion and depth ordering. The techniques are applied to a flower garden sequence. The scene can be analyzed into four layers, and, the entire 30-frame sequence can be represented with a single image of each layer, along with associated motion parameters.<<ETX>>"
            },
            "slug": "Layered-representation-for-motion-analysis-Wang-Adelson",
            "title": {
                "fragments": [],
                "text": "Layered representation for motion analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A set of techniques is devised for segmenting images into coherently moving regions using affine motion analysis and clustering techniques and it is possible to decompose an image into a set of layers along with information about occlusion and depth ordering."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144152544"
                        ],
                        "name": "B. Horowitz",
                        "slug": "B.-Horowitz",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Horowitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Horowitz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 69
                            }
                        ],
                        "text": "Other previous work on tracking general articulated objects includes [5, 15, 8, 4, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28815139,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "f706f1babe84c0728be3a06a4d3023cdc44f61c2",
            "isKey": false,
            "numCitedBy": 417,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors introduce a physically correct model of elastic nonrigid motion. This model is based on the finite element method, but decouples the degrees of freedom by breaking down object motion into rigid and nonrigid vibration or deformation modes. The result is an accurate representation for both rigid and nonrigid motion that has greatly reduced dimensionality, capturing the intuition that nonrigid motion is normally coherent and not chaotic. Because of the small number of parameters involved, this representation is used to obtain accurate overstrained estimates of both rigid and nonrigid global motion. It is also shown that these estimates can be integrated over time by use of an extended Kalman filter, resulting in stable and accurate estimates of both three-dimensional shape and three-dimensional velocity. The formulation is then extended to include constrained nonrigid motion. Examples of tracking single nonrigid objects and multiple constrained objects are presented. >"
            },
            "slug": "Recovery-of-Nonrigid-Motion-and-Structure-Pentland-Horowitz",
            "title": {
                "fragments": [],
                "text": "Recovery of Nonrigid Motion and Structure"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This model is based on the finite element method, but decouples the degrees of freedom by breaking down object motion into rigid and nonrigid vibration or deformation modes, resulting in an accurate representation for both rigid andnonrigid motion that has greatly reduced dimensionality."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144042261"
                        ],
                        "name": "Mark Nitzberg",
                        "slug": "Mark-Nitzberg",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Nitzberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Nitzberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082087031"
                        ],
                        "name": "David Mumford",
                        "slug": "David-Mumford",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mumford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Mumford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 91
                            }
                        ],
                        "text": "A layered representation based on the occluding contours of a single image is described in [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 185
                            }
                        ],
                        "text": "Looking beyond model-based tracking, there is increasing interest in layered representations for computer vision, because of their potential to simplify the 3D description of the scene [1, 2, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 113
                            }
                        ],
                        "text": "In bottom-up approaches to occlusion analysis, template order is estimated from image motion [2, 14] or contours [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33373361,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "321a32929fb7d83fa84b043ba475c8342f0cc62f",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A model is described for image segmentation that tries to capture the low-level depth reconstruction exhibited in early human vision, giving an important role to edge terminations. The problem is to find a decomposition of the domain D of an image that has a minimum of disrupted edges-junctions of edges, crack tips, corners, and cusps-by creating suitable continuations for the disrupted edges behind occluding regions. The result is a decomposition of D into overlapping regions R/sub 1/ union . . . union R/sub n/ ordered by occlusion, which is called the 2.1-D sketch. Expressed as a minimization problem, the model gives rise to a family of optimal contours, called nonlinear splines, that minimize length and the square of curvature. These are essential in the construction of the 2.1-D sketch of an image, as the continuations of disrupted edges. An algorithm is described that constructs the 2.1-D sketch of an image, and gives results for several example images. The algorithm yields the same interpretations of optical illusions as the human visual system.<<ETX>>"
            },
            "slug": "The-2.1-D-sketch-Nitzberg-Mumford",
            "title": {
                "fragments": [],
                "text": "The 2.1-D sketch"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A model is described for image segmentation that tries to capture the low-level depth reconstruction exhibited in early human vision, giving an important role to edge terminations, which gives rise to a family of optimal contours, called nonlinear splines, that minimize length and the square of curvature."
            },
            "venue": {
                "fragments": [],
                "text": "[1990] Proceedings Third International Conference on Computer Vision"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70306555"
                        ],
                        "name": "L. Robert",
                        "slug": "L.-Robert",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Robert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Robert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14015746,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "1d199e40eb34d8c8fbf730e2d6118224dba91c5c",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an original approach to the problem of camera calibration. Contrary to classical techniques, which first extract the image features and then compute the camera parameters, we directly search for the camera parameters that best map three-dimensional points onto the image edges, characterized as maxims of the intensity gradient or zero-crossings of the Laplacian. Expressed as a one-stage optimization problem over the parameters of the camera, the whole calibration process is solved by classical iterative optimization. We describe experiments on synthetic and real data."
            },
            "slug": "Camera-calibration-without-feature-extraction-Robert",
            "title": {
                "fragments": [],
                "text": "Camera calibration without feature extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work directly searches for the camera parameters that best map three-dimensional points onto the image edges, characterized as maxims of the intensity gradient or zero-crossings of the Laplacian."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 12th International Conference on Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "More details are given in [11], along with the estimated state trajectories."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "Speci c rules for the on-line generation of visibility orders for the hand are described in [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "The Jacobian evaluation and the computational requirements of minimization are discussed in [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "7, and more details can be found in [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Visual tracking of selfoccluding articulated objects"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report CMU-CS-TR-94-224, Carnegie Mellon Univ. School of Comp. Sci.,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 185
                            }
                        ],
                        "text": "Looking beyond model-based tracking, there is increasing interest in layered representations for computer vision, because of their potential to simplify the 3D description of the scene [1, 2, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 81
                            }
                        ],
                        "text": "A set of ordered templates make up a layered representation for occluding motion [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Layered representation for image coding"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report 181, MIT Media Lab,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 185
                            }
                        ],
                        "text": "Looking beyond model-based tracking, there is increasing interest in layered representations for computer vision, because of their potential to simplify the 3D description of the scene [1, 2, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 81
                            }
                        ],
                        "text": "A set of ordered templates make up a layered representation for occluding motion [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Layered representation for image cod-  ing"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report 181, MIT Media Lab,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "The combination of kinematic and camera transforms make up a deformation function [12], f(q; s), which maps template coordinates, s = [u v], to image coordinates, w = [x y], as a function of q."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Visual tracking with de-  formation models"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. of the IEEE Intl. Conf.  on Robotics and Automation,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Camera calibration without feature extraction . Computer Vision Graphics and Image Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Camera calibration without feature extraction . Computer Vision Graphics and Image Processing"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hand shape identiication and tracking for sign language interpretation"
            },
            "venue": {
                "fragments": [],
                "text": "Looking at People Workshop, IJCAI"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [3], Dorner describes a system for interpreting American Sign Language from image sequences of a single hand."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hand shape identi cation and tracking for sign language interpretation"
            },
            "venue": {
                "fragments": [],
                "text": "In Looking at People Workshop,"
            },
            "year": 1993
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 11,
            "methodology": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 20,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Model-based-tracking-of-self-occluding-articulated-Rehg-Kanade/f3740a2ab2936c2d87f6a3d8b742841a383ba502?sort=total-citations"
}