{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 201
                            }
                        ],
                        "text": "\u2026Schmidhuber Department of Computer Science, University of Colorado, Campus Box 430, Boulder, CO 80309 U S A\nThe real-time recurrent learning (RTRL) algorithm (Robinson and Fallside 1987; Williams and Zipser 1989) requires O(n4) computations per time step, where n is the number of noninput units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 90
                            }
                        ],
                        "text": "Like the RTRL-algorithm [but unlike the methods described in Williams and Peng (1990) and Zipser (1989)l the algorithm computes the exact gradient."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 28
                            }
                        ],
                        "text": "Fallside 1987; Williams and Zipser 1989)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 27529351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bda89a6b28f234e9159b4fc884980bdd6163819a",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm, called RTRL, for training fully recurrent neural networks has recently been studied by Williams and Zipser (1989a, b). Whereas RTRL has been shown to have great power and generality, it has the disadvantage of requiring a great deal of computation time. A technique is described here for reducing the amount of computation required by RTRL without changing the connectivity of the networks. This is accomplished by dividing the original network into subnets for the purpose of error propagation while leaving them undivided for activity propagation. An example is given of a 12-unit network that learns to be the finite-state part of a Turing machine and runs 10 times faster using the subgrouping strategy than the original algorithm."
            },
            "slug": "A-Subgrouping-Strategy-that-Reduces-Complexity-and-Zipser",
            "title": {
                "fragments": [],
                "text": "A Subgrouping Strategy that Reduces Complexity and Speeds Up Learning in Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A technique is described here for reducing the amount of computation required by RTRL without changing the connectivity of the networks by dividing the original network into subnets for the purpose of error propagation while leaving them undivided for activity propagation."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 188
                            }
                        ],
                        "text": "\u2026Schmidhuber Department of Computer Science, University of Colorado, Campus Box 430, Boulder, CO 80309 U S A\nThe real-time recurrent learning (RTRL) algorithm (Robinson and Fallside 1987; Williams and Zipser 1989) requires O(n4) computations per time step, where n is the number of noninput units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 50
                            }
                        ],
                        "text": "The real-time recurrent learning (RTRL) algorithm (Robinson and Fallside 1987; Williams and Zipser 1989) requires O(n4) computations per time step, where n is the number of noninput units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 40
                            }
                        ],
                        "text": "Such an algorithm is the RTRL algorithm (Robinson and .Fallside 1987; Williams and Zipser 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 15
                            }
                        ],
                        "text": "Fallside 1987; Williams and Zipser 1989)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60666828,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "424710825d726e10b016204ed2bc979e2a342d10",
            "isKey": true,
            "numCitedBy": 336,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The real-time recurrent learning algorithm is a gradient-following learning algorithm for completely recurrent networks running in continually sampled time. Here we use a series of simulation experiments to investigate the power and properties of this algorithm. In the recurrent networks studied here, any unit can be connected to any other, and any unit can receive external input. These networks run continually in the sense that they sample their inputs on every update cycle, and any unit can have a training target on any cycle. The storage required and computation time on each step are independent of time and are completely determined by the size of the network, so no prior knowledge of the temporal structure of the task being learned is required. The algorithm is nonlocal in the sense that each unit must have knowledge of the complete recurrent weight matrix and error vector. The algorithm is computationally intensive in sequential computers, requiring a storage capacity of the order of the thi..."
            },
            "slug": "Experimental-Analysis-of-the-Real-time-Recurrent-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "Experimental Analysis of the Real-time Recurrent Learning Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A series of simulation experiments are used to investigate the power and properties of the real-time recurrent learning algorithm, a gradient-following learning algorithm for completely recurrent networks running in continually sampled time."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18271205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50c770b425a5bb25c77387f687a9910a9d130722",
            "isKey": false,
            "numCitedBy": 428,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous neural network learning algorithms for sequence processing are computationally expensive and perform poorly when it comes to long time lags. This paper first introduces a simple principle for reducing the descriptions of event sequences without loss of information. A consequence of this principle is that only unexpected inputs can be relevant. This insight leads to the construction of neural architectures that learn to divide and conquer by recursively decomposing sequences. I describe two architectures. The first functions as a self-organizing multilevel hierarchy of recurrent networks. The second, involving only two recurrent networks, tries to collapse a multilevel predictor hierarchy into a single recurrent net. Experiments show that the system can require less computation per time step and many fewer training sequences than conventional training algorithms for recurrent nets."
            },
            "slug": "Learning-Complex,-Extended-Sequences-Using-the-of-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning Complex, Extended Sequences Using the Principle of History Compression"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A simple principle for reducing the descriptions of event sequences without loss of information is introduced and this insight leads to the construction of neural architectures that learn to divide and conquer by recursively decomposing sequences."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47918185"
                        ],
                        "name": "Jing Peng",
                        "slug": "Jing-Peng",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Peng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12979634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26bc0449360d7016f684eafae5b5d2feded32041",
            "isKey": false,
            "numCitedBy": 634,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel variant of the familiar backpropagation-through-time approach to training recurrent networks is described. This algorithm is intended to be used on arbitrary recurrent networks that run continually without ever being reset to an initial state, and it is specifically designed for computationally efficient computer implementation. This algorithm can be viewed as a cross between epochwise backpropagation through time, which is not appropriate for continually running networks, and the widely used on-line gradient approximation technique of truncated backpropagation through time."
            },
            "slug": "An-Efficient-Gradient-Based-Algorithm-for-On-Line-Williams-Peng",
            "title": {
                "fragments": [],
                "text": "An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A novel variant of the familiar backpropagation-through-time approach to training recurrent networks is described, intended to be used on arbitrary recurrent networks that run continually without ever being reset to an initial state."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2003303"
                        ],
                        "name": "M. Gherrity",
                        "slug": "M.-Gherrity",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Gherrity",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gherrity"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "Like the RTRL-algorithm [but unlike the methods described in Williams and Peng (1990) and Zipser (1989)l the algorithm computes the exact gradient."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 576,
                                "start": 572
                            }
                        ],
                        "text": "Consider the third term:\nwhere\ndEtota\u2019(to, to + k) S j ( 7 ) = -\nanet, ( r )\nFor a given to, Si(.) can be computed for all i E LI, to 5 r 5 t o + k with a single h step BPTT-pass of the order O(kn2) operations:\nSi(r) = f:[neti(r)]e,(r) if r = t o + k r 1\nWhat remains is the computation of the second term of equation 2.2 for all wij, which requires O(n3) operations:\n( t o , t o + h ) aEtota\u2019(to, to + h ) anetk(to) T=1 5 aEtot;wij(7) T=l k e u dnetk(t0) awij(7)\nStep 4: To compute 9f(to + h ) for all possible I , i , j , perform n combinations of a BPTT-like phase with an RTRL-like calculation (one such combination for each I ) for computing as follows:\nanetl(to + h ) - 5 dnetl(to + h ) f o + h anetl(to + h ) =c dWij T = l dwij(7) q$o + h ) =\n-"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 155
                            }
                        ],
                        "text": "X k ( t ) if k E T ( t ) and 0 otherwise t 1\nE ( t ) = 5 C[ek(t)]\u2019, Etotal(t\u2019, t ) = C E ( 7 ) kEU T=f \u2019 f l\nThe algorithm is a cross between the BPTT and the RTRL algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "Step 3: Perform a combination of a BPTT-like phase with an RTRLlike calculation for computing error derivatives as described next."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Perform n + 1 RTRL-like calculations for integrating the results of these BPTT-like passes into the results obtained from previous blocks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 118
                            }
                        ],
                        "text": "Following the argumentation in Williams and Peng (19901, continuous time versions of BPTT and RTRL (Pearlmutter 1989; Gherrity 1989) can serve as a basis for a correspondingly efficient continuous time version of the algorithm presented here (by means of Euler discretization)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "RTRL Algorithm 245\nThe algorithm starts by setting the variable t o c 0. t o represents the beginning of the current block."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 149
                            }
                        ],
                        "text": "Jurgen Schmidhuber Department of Computer Science, University of Colorado, Campus Box 430, Boulder, CO 80309 U S A\nThe real-time recurrent learning (RTRL) algorithm (Robinson and Fallside 1987; Williams and Zipser 1989) requires O(n4) computations per time step, where n is the number of noninput units."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "3 Concluding Remarks\nLike the RTRL-algorithm the method needs a fixed amount of storage of the order O(n3)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "Since it is O(n) times faster than RTRL, it should be preferred."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "Such an algorithm is the RTRL algorithm (Robinson and ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 28051649,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "006c42929dcd480490fdb367fd7478b2956dbc99",
            "isKey": true,
            "numCitedBy": 78,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "A learning algorithm for recurrent neural networks is derived. This algorithm allows a network to learn specified trajectories in state space in response to various input sequences. The network dynamics are described by a system of coupled differential equations that specify the continuous change of the unit activities and weights over time. The algorithm is nonlocal, in that a change in the connection weight between two units may depend on the values for some of the weights between different units. However, the operation of a learned network (fixed weights) is local. If the network units are specified to behave like electronic amplifiers, then an analog implementation of the learned network is straightforward. An example demonstrates the use of the algorithm in a completely connected network of four units. The network creates a limit cycle attractor in order to perform the specified task.<<ETX>>"
            },
            "slug": "A-learning-algorithm-for-analog,-fully-recurrent-Gherrity",
            "title": {
                "fragments": [],
                "text": "A learning algorithm for analog, fully recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A learning algorithm for recurrent neural networks is derived that allows a network to learn specified trajectories in state space in response to various input sequences."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 100
                            }
                        ],
                        "text": "Following the argumentation in Williams and Peng (19901, continuous time versions of BPTT and RTRL (Pearlmutter 1989; Gherrity 1989) can serve as a basis for a correspondingly efficient continuous time version of the algorithm presented here (by means of Euler discretization)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16813485,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34468c0aa95a7aea212d8738ab899a69b2fc14c6",
            "isKey": false,
            "numCitedBy": 744,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Many neural network learning procedures compute gradients of the errors on the output layer of units after they have settled to their final values. We describe a procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize E. Simulations in which networks are taught to move through limit cycles are shown. This type of recurrent network seems particularly suited for temporally continuous domains, such as signal processing, control, and speech."
            },
            "slug": "Learning-State-Space-Trajectories-in-Recurrent-Pearlmutter",
            "title": {
                "fragments": [],
                "text": "Learning State Space Trajectories in Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network, which seems particularly suited for temporally continuous domains."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30320952"
                        ],
                        "name": "F. Pineda",
                        "slug": "F.-Pineda",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pineda",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pineda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 124
                            }
                        ],
                        "text": "\u2019Pineda has described another recurrent net algorithm that, as he states, \u201chas some of the worst features of both algorithms\u201d (Pineda 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42661017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45695bf7156f5ada733863056648b4f82c5f5c38",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A comparison of algorithms that minimize error functions to train the trajectories of recurrent networks, reveals how complexity is traded off for causality. These algorithms are also related to time-independent formalisms. It is suggested that causal and scalable algorithms are possible when the activation dynamics of adaptive neurons is fast compared to the behavior to be learned. Standard continuous-time recurrent backpropagation is used in an example."
            },
            "slug": "Time-Dependent-Adaptive-Neural-Networks-Pineda",
            "title": {
                "fragments": [],
                "text": "Time Dependent Adaptive Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A comparison of algorithms that minimize error functions to train the trajectories of recurrent networks, reveals how complexity is traded off for causality."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150165387"
                        ],
                        "name": "J. Urgen Schmidhuber",
                        "slug": "J.-Urgen-Schmidhuber",
                        "structuredName": {
                            "firstName": "J",
                            "lastName": "Urgen Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Urgen Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5420632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad201d0dabb7f58730a60181eb51a36642ac6ea5",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce design principles for unsupervised detection of regularities (like causal relationships) in temporal sequences. One basic idea is to train an adaptive predictor module to predict future events from past events, and to train an additional condence module to model the reliability of the predictor's predictions. We select system states at those points in time where there are changes in prediction reliability, and use them recur-sively as inputs for higher-level predictors. This can be benecial for\u00e0daptive sub-goal generation' as well as for`conventional' goal-directed (supervised and reinforcement) learning: Systems based on these design principles were successfully tested on tasks where conventional training algorithms for recurrent nets fail. Finally we describe the principles of the rst neural sequenc\u00e8chunker' which collapses a self-organizing multi-level predictor hierarchy into a single recurrent network. This paper is based on th\u00e8principle of reduced history description': As long as an adaptive sequence processing dynamic system is able to predict future environmental inputs from previous ones, no additional knowledge can be obtained by observing these inputs in reality. Only unpredicted inputs deserve attention ([7][4][9]). This paper demonstrates that it can be very ecient to focus on unexpected inputs and ignore expected ones. First we motivate this work by describing a major problem of`conventional' learning algorithms for time-varying inputs, namely, the problem of long time lags between relevant inputs. Then we introduce a principle for unsupervised detection of causal chains in streams of input events. Short representations of`presumed causal chains' recursively serve as inputs for`higher-level' detec"
            },
            "slug": "Adaptive-Decomposition-Of-Time-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Adaptive Decomposition Of Time"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "Design principles for unsupervised detection of regularities (like causal relationships) in temporal sequences and the principles of the rst neural sequenc\u00e8chunker, which collapses a self-organizing multi-level predictor hierarchy into a single recurrent network are introduced."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In such cases, sequencecomposing algorithms ( Schmidhuber 1991a, b) can provide superior alternatives to pure gradient-based algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 45
                            }
                        ],
                        "text": "In such cases, sequencecomposing algorithms (Schmidhuber 1991a,b) can provide superior alternatives to pure gradient-based algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58601327,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f4917524024083805157dae844587e3a21d2db3",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-decomposition-of-time-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Adaptive decomposition of time"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 12
                            }
                        ],
                        "text": "Williams, R J. 1989."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complexity of exact gradient computation algorithms for recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep. NU-CCS-89-27, Boston: Northeastern University, College of Computer Science. Williams, R. J., and Peng, J. 1990. An efficient gradient-based algorithm for on-line training of recurrent network trajectories."
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 160
                            }
                        ],
                        "text": "\u2026Schmidhuber Department of Computer Science, University of Colorado, Campus Box 430, Boulder, CO 80309 U S A\nThe real-time recurrent learning (RTRL) algorithm (Robinson and Fallside 1987; Williams and Zipser 1989) requires O(n4) computations per time step, where n is the number of noninput units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 50
                            }
                        ],
                        "text": "The real-time recurrent learning (RTRL) algorithm (Robinson and Fallside 1987; Williams and Zipser 1989) requires O(n4) computations per time step, where n is the number of noninput units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The utility driven dynamic error propagation network"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep. CUED/F-INFENG/TR.l, Cambridge University Engineering Department."
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Communicated by Fernando Pineda\nA Fixed Size Storage O(n3) Time Complexity Learning"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 250
                            }
                        ],
                        "text": "His algorithm requires 1 0(n4) memory and 2 O(n4) computations per time step, if the number of time steps exceeds n.\n2Since the acceptance of this paper for publication it has come to my attention that the same algorithm was derived by Ron Williams (Williams 1989; Williams and Zipser 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complexity of exact gradient computation algorithms for recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Complexity of exact gradient computation algorithms for recurrent neural networks"
            },
            "year": 1989
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 4,
            "methodology": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 12,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/A-Fixed-Size-Storage-O(n3)-Time-Complexity-Learning-Schmidhuber/89b9a181801f32bf62c4237c4265ba036a79f9dc?sort=total-citations"
}