{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58779360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8592e46a5435d18bba70557846f47290b34c1aa5",
            "isKey": false,
            "numCitedBy": 1336,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References"
            },
            "slug": "Learning-and-relearning-in-Boltzmann-machines-Hinton-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning and relearning in Boltzmann machines"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, and an Example of the Effects of Damage."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117028612,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f251ac3347de5decd0470885aea4f415df96779",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Inspired by the information theoretic idea of minimum description length, we add a term to the usual back-propagation cost function that penalizes network complexity. From a Bayesian perspective, the complexity term can be usefully interpreted as an assumption about prior distribution of the weights. This method, called weight-elimination, is contrasted to ridge regression and to cross-validation. We apply weight-elimination to time series prediction. On the sunspot series, the network outperforms traditional statistical approaches and shows the same predictive power as multivariate adaptive regression splines."
            },
            "slug": "Generalization-through-Minimal-Networks-with-to-Weigend-Rumelhart",
            "title": {
                "fragments": [],
                "text": "Generalization through Minimal Networks with Application to Forecasting"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "Inspired by the information theoretic idea of minimum description length, a term is added to the usual back-propagation cost function that penalizes network complexity, called weight-elimination, which is applied to time series prediction."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1874331,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c3ff6424d564e004ccf1440a7d18fa93509132e",
            "isKey": false,
            "numCitedBy": 1604,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "Internal models of the environment have an important role to play in adaptive systems, in general, and are of particular importance for the supervised learning paradigm. In this article we demonstrate that certain classical problems associated with the notion of the \u201cteacher\u201d in supervised learning can be solved by judicious use of learned internal models as components of the adaptive system. In particular, we show how supervised learning algorithms can be utilized in cases in which an unknown dynamical system intervenes between actions and desired outcomes. Our approach applies to any supervised learning algorithm that is capable of learning in multilayer networks."
            },
            "slug": "Forward-Models:-Supervised-Learning-with-a-Distal-Jordan-Rumelhart",
            "title": {
                "fragments": [],
                "text": "Forward Models: Supervised Learning with a Distal Teacher"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This article demonstrates that certain classical problems associated with the notion of the \u201cteacher\u201d in supervised learning can be solved by judicious use of learned internal models as components of the adaptive system."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144473519"
                        ],
                        "name": "M. Mozer",
                        "slug": "M.-Mozer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mozer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mozer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5355536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e141d68065ce638f9fc4f006eab2f66711e89768",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning structure in temporally-extended sequences is a difficult computational problem because only a fraction of the relevant information is available at any instant. Although variants of back propagation can in principle be used to find structure in sequences, in practice they are not sufficiently powerful to discover arbitrary contingencies, especially those spanning long temporal intervals or involving high order statistics. For example, in designing a connectionist network for music composition, we have encountered the problem that the net is able to learn musical structure that occurs locally in time--e.g., relations among notes within a musical phrase--but not structure that occurs over longer time periods--e.g., relations among phrases. To address this problem, we require a means of constructing a reduced description of the sequence that makes global aspects more explicit or more readily detectable. I propose to achieve this using hidden units that operate with different time constants. Simulation experiments indicate that slower time-scale hidden units are able to pick up global structure, structure that simply can not be learned by standard back propagation."
            },
            "slug": "Induction-of-Multiscale-Temporal-Structure-Mozer",
            "title": {
                "fragments": [],
                "text": "Induction of Multiscale Temporal Structure"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Simulation experiments indicate that slower time-scale hidden units are able to pick up global structure, structure that simply can not be learned by standard back propagation, using hidden units that operate with different time constants."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57068783,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ece1bdcd12a28aa5f542898c7b5bc3c6fb23f907",
            "isKey": false,
            "numCitedBy": 833,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Explorations-In-Parallel-Distributed-Processing-McClelland",
            "title": {
                "fragments": [],
                "text": "Explorations In Parallel Distributed Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 5,
        "totalPages": 1
    },
    "page_url": "https://www.semanticscholar.org/paper/Introduction-to-the-theory-of-neural-computation-Hertz-Krogh/6c0cbbd275bb43e09f0527a31ddd61824eca295b?sort=total-citations"
}