{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "68982679"
                        ],
                        "name": "Fred J. Damerau",
                        "slug": "Fred-J.-Damerau",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Damerau",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fred J. Damerau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150444687"
                        ],
                        "name": "David E. Johnson",
                        "slug": "David-E.-Johnson",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Johnson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David E. Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 43
                            }
                        ],
                        "text": "In an earlier version of the current work (Zhang et al., 2001), in order to save memory without using a hash table, we limited the number of token feature values (words\nor punctuation) to 5000 by removing less frequent tokens."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 69
                            }
                        ],
                        "text": "Excluding results reported in the early version of the current work (Zhang et al., 2001), the previously second best system was a combination of five different WPDV models (van Halteren, 2000), with an overall F\u03b2=1 value of 93.32."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 67
                            }
                        ],
                        "text": "Consequently, timing reported in this paper is better than that of Zhang et al. (2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 42
                            }
                        ],
                        "text": "In an earlier version of the current work (Zhang et al., 2001), in order to save memory without using a hash table, we limited the number of token feature values (words"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 118
                            }
                        ],
                        "text": "A version of the new method was originally proposed by Zhang (2001), and was used in an earlier version of this work (Zhang et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 68
                            }
                        ],
                        "text": "Excluding results reported in the early version of the current work (Zhang et al., 2001), the previously second best system was a combination of five different WPDV models (van Halteren, 2000), with an overall F\u03b2=1 value of 93."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 66
                            }
                        ],
                        "text": "For consistency, we use the same set of features as those used by Zhang et al. (2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 754103,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5191bf5548e9526bedf899a6165f7e4692dc5961",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Many machine learning methods have recently been applied to natural language processing tasks. Among them, the Winnow algorithm has been argued to be particularly suitable for NLP problems, due to its robustness to irrelevant features. However in theory, Winnow may not converge for non-separable data. To remedy this problem, a modification called regularized Winnow has been proposed. In this paper, we apply this new method to text chunking. We show that this method achieves state of the art performance with significantly less computation than previous approaches."
            },
            "slug": "Text-Chunking-using-Regularized-Winnow-Zhang-Damerau",
            "title": {
                "fragments": [],
                "text": "Text Chunking using Regularized Winnow"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper applies a modification of the Winnow algorithm to text chunking, and shows that this method achieves state of the art performance with significantly less computation than previous approaches."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47137139"
                        ],
                        "name": "Yael Karov",
                        "slug": "Yael-Karov",
                        "structuredName": {
                            "firstName": "Yael",
                            "lastName": "Karov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yael Karov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 89
                            }
                        ],
                        "text": "One method that has been quite successful in such applications is the SNoW architecture (Dagan et al., 1997, Khardon et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 497031,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78686d02b1c4c3ac4fca5f27d7f562c0dcb31d58",
            "isKey": false,
            "numCitedBy": 224,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning problems in the text processing domain often map the text to a space whose dimensions are the measured features of the text, e.g., its words. Three characteristic properties of this domain are (a) very high dimensionality, (b) both the learned concepts and the instances reside very sparsely in the feature space, and (c) a high variation in the number of active features in an instance. In this work we study three mistake-driven learning algorithms for a typical task of this nature -- text categorization. We argue that these algorithms -- which categorize documents by learning a linear separator in the feature space -- have a few properties that make them ideal for this domain. We then show that a quantum leap in performance is achieved when we further modify the algorithms to better address some of the specific characteristics of the domain. In particular, we demonstrate (1) how variation in document length can be tolerated by either normalizing feature weights or by using negative weights, (2) the positive effect of applying a threshold range in training, (3) alternatives in considering feature frequency, and (4) the benefits of discarding features while training. Overall, we present an algorithm, a variation of Littlestone's Winnow, which performs significantly better than any other algorithm tested on this task using a similar feature set."
            },
            "slug": "Mistake-Driven-Learning-in-Text-Categorization-Dagan-Karov",
            "title": {
                "fragments": [],
                "text": "Mistake-Driven Learning in Text Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work studies three mistake-driven learning algorithms for a typical task of this nature -- text categorization and presents an algorithm, a variation of Littlestone's Winnow, which performs significantly better than any other algorithm tested on this task using a similar feature set."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2165139"
                        ],
                        "name": "H. V. Halteren",
                        "slug": "H.-V.-Halteren",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Halteren",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. V. Halteren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 177
                            }
                        ],
                        "text": "Excluding results reported in the early version of the current work (Zhang et al., 2001), the previously second best system was a combination of five different WPDV models (van Halteren, 2000), with an overall F\u03b2=1 value of 93.32."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 13382016,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2b16a6f2154fd2814f50d9f96283a76c2959da0",
            "isKey": true,
            "numCitedBy": 33,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper I describe the application of the WPDV algorithm to the CoNLL-2000 shared task, the identification of base chunks in English text (Tjong Kim Sang and Buchholz, 2000). For this task, I use a three-stage architecture: I first run five different base chunkers, then combine them and finally try to correct some recurring errors. Except for one base chunker, which uses the memory-based machine learning system TiMBL, all modules are based on WPDV models (van Halteren, 2000a)."
            },
            "slug": "Chunking-with-WPDV-Models-Halteren",
            "title": {
                "fragments": [],
                "text": "Chunking with WPDV Models"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "This paper describes the application of the WPDV algorithm to the CoNLL-2000 shared task, the identification of base chunks in English text, using a three-stage architecture."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL/LLL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 67
                            }
                        ],
                        "text": "Consequently, timing reported in this paper is better than that of Zhang et al. (2001). For consistency, we use the same set of features as those used by Zhang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 67
                            }
                        ],
                        "text": "Consequently, timing reported in this paper is better than that of Zhang et al. (2001). For consistency, we use the same set of features as those used by Zhang et al. (2001). The slight improvement we obtained in this paper is due to the fact that we do not limit the number of tokens to 5000."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 55
                            }
                        ],
                        "text": "A version of the new method was originally proposed by Zhang (2001), and was used in an earlier version of this work (Zhang et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 63
                            }
                        ],
                        "text": "A more general treatment of this type of duality were given by Zhang (2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 186
                            }
                        ],
                        "text": "Generalization bounds of regularized Winnow that are similar to the mistake bound of the original Winnow (in the sense of logarithmic dependent on the dimensionality) have been given by Zhang (2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 28
                            }
                        ],
                        "text": "The basic idea was given by Zhang (2001, 2002), where the original Winnow algorithm was converted into a numerical optimization problem that can handle linearly non-separable data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 65
                            }
                        ],
                        "text": "The learning rate issue has been more thoroughly investigated by Zhang (2002). In fact, results there suggest that the default choice of \u03b7 = 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11920487,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff25b22f781c495c9076770c11421d19d0b2abfb",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In theory, the Winnow multiplicative update has certain advantages over the Perceptron additive update when there are many irrelevant attributes. Recently, there has been much effort on enhancing the Perceptron algorithm by using regularization, leading to a class of linear classification methods called support vector machines. Similarly, it is also possible to apply the regularization idea to the Winnow algorithm, which gives methods we call regularized Winnows. We show that the resulting methods compare with the basic Winnows in a similar way that a support vector machine compares with the Perceptron. We investigate algorithmic issues and learning properties of the derived methods. Some experimental results will also be provided to illustrate different methods."
            },
            "slug": "Regularized-Winnow-Methods-Zhang",
            "title": {
                "fragments": [],
                "text": "Regularized Winnow Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work investigates algorithmic issues and learning properties of the derived methods of the Winnow algorithm, and shows that the resulting methods compare with the basic Winnows in a similar way that a support vector machine compares with the Perceptron."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35551590"
                        ],
                        "name": "Steven P. Abney",
                        "slug": "Steven-P.-Abney",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Abney",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven P. Abney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 67
                            }
                        ],
                        "text": "In this paper, we apply generalized Winnow to text chunking tasks (Abney, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 27
                            }
                        ],
                        "text": "The text chunking problem (Abney, 1991) is to divide text into syntactically related nonoverlapping groups of words (chunks)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 68
                            }
                        ],
                        "text": "Text Chunking and the CoNLL-2000 Chunking The text chunking problem (Abney, 1991) is to divide text into syntactically related nonoverlapping groups of words (chunks)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9716882,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "56d7826f3afaa374077f87ca3529709b1ca7e044",
            "isKey": true,
            "numCitedBy": 992,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "I begin with an intuition: when I read a sentence, I read it a chunk at a time. For example, the previous sentence breaks up something like this: \n \n(1) \n \n[I begin] [with an intuition]: [when I read] [a sentence], [I read it] [a chunk] [at a time] \n \n \n \n \n \n \nThese chunks correspond in some way to prosodic patterns. It appears, for instance, that the strongest stresses in the sentence fall one to a chunk, and pauses are most likely to fall between chunks. Chunks also represent a grammatical watershed of sorts. The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template. A simple context-free grammar is quite adequate to describe the structure of chunks. By contrast, the relationships between chunks are mediated more by lexical selection than by rigid templates. Co-occurrence of chunks is determined not just by their syntactic categories, but is sensitive to the precise words that head them; and the order in which chunks occur is much more flexible than the order of words within chunks."
            },
            "slug": "Parsing-By-Chunks-Abney",
            "title": {
                "fragments": [],
                "text": "Parsing By Chunks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template, and the relationships between chunks are mediated more by lexical selection than by rigid templates."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157221"
                        ],
                        "name": "E. T. K. Sang",
                        "slug": "E.-T.-K.-Sang",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sang",
                            "middleNames": [
                                "Tjong",
                                "Kim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. T. K. Sang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782178"
                        ],
                        "name": "S. Buchholz",
                        "slug": "S.-Buchholz",
                        "structuredName": {
                            "firstName": "Sabine",
                            "lastName": "Buchholz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Buchholz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 56
                            }
                        ],
                        "text": "Interested readers are referred to the summary paper by Sang and Buchholz (2000) for references to all systems being tested."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 105
                            }
                        ],
                        "text": "In order for us to rigorously compare our system with others, we use the CoNLL-2000 shared task dataset (Sang and Buchholz, 2000), which is publicly available from http://lcgwww.uia.ac.be/conll2000/chunking."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 45
                            }
                        ],
                        "text": "A total of eleven systems were summarized by Sang and Buchholz (2000), including a variety of statistical techniques such as maximum entropy, hidden Markov models, and transformation based rule learners."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 104
                            }
                        ],
                        "text": "In order for us to rigorously compare our system with others, we use the CoNLL-2000 shared task dataset (Sang and Buchholz, 2000), which is publicly available from http://lcgwww."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 105
                            }
                        ],
                        "text": "To facilitate the comparison among different methods, the CoNLL-2000 shared task was introduced in 2000 (Sang and Buchholz, 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8940645,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "9e85832b04cc3700c2c26d6ba93fdeae39cac04a",
            "isKey": true,
            "numCitedBy": 872,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the CoNLL-2000 shared task: dividing text into syntactically related non-overlapping groups of words, so-called text chunking. We give background information on the data sets, present a general overview of the systems that have taken part in the shared task and briefly discuss their performance."
            },
            "slug": "Introduction-to-the-CoNLL-2000-Shared-Task-Chunking-Sang-Buchholz",
            "title": {
                "fragments": [],
                "text": "Introduction to the CoNLL-2000 Shared Task Chunking"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "The CoNLL-2000 shared task: dividing text into syntactically related non-overlapping groups of words, so-called text chunking is described."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL/LLL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157221"
                        ],
                        "name": "E. T. K. Sang",
                        "slug": "E.-T.-K.-Sang",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sang",
                            "middleNames": [
                                "Tjong",
                                "Kim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. T. K. Sang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 86
                            }
                        ],
                        "text": "The third best performance was achieved by using combinations of memory-based models (Sang, 2000), with an overall F\u03b2=1 value of 92.50."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 85
                            }
                        ],
                        "text": "The third best performance was achieved by using combinations of memory-based models (Sang, 2000), with an overall F\u03b2=1 value of 92."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12841881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e8de77316a43fd8675d546880b4607433793c31",
            "isKey": true,
            "numCitedBy": 55,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We will apply a system-internal combination of memory-based learning classifiers to the CoNLL-2000 shared task: finding base chunks. Apart from testing different combination methods, we will also examine if dividing the chunking process in a boundary recognition phase and a type identification phase would aid performance."
            },
            "slug": "Text-Chunking-by-System-Combination-Sang",
            "title": {
                "fragments": [],
                "text": "Text Chunking by System Combination"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A system-internal combination of memory-based learning classifiers is applied to the CoNLL-2000 shared task: finding base chunks to examine if dividing the chunking process in a boundary recognition phase and a type identification phase would aid performance."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL/LLL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145022783"
                        ],
                        "name": "E. Brill",
                        "slug": "E.-Brill",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 94
                            }
                        ],
                        "text": "Additionally, a part-of-speech (POS) tag was assigned to each token by a standard POS tagger (Brill, 1994) that was trained on the Penn Treebank."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12309040,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "733234e097dceb9011baa8914930861996eb0b5e",
            "isKey": false,
            "numCitedBy": 624,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Most recent research in trainable part of speech taggers has explored stochastic tagging. While these taggers obtain high accuracy, linguistic information is captured indirectly, typically in tens of thousands of lexical and contextual probabilities. In (Brill 1992), a trainable rule-based tagger was described that obtained performance comparable to that of stochastic taggers, but captured relevant linguistic information in a small number of simple non-stochastic rules. In this paper, we describe a number of extensions to this rule-based tagger. First, we describe a method for expressing lexical relations in tagging that stochastic taggers are currently unable to express. Next, we show a rule-based approach to tagging unknown words. Finally, we show how the tagger-can be extended into a k-best tagger, where multiple tags can be assigned to words in some cases of uncertainty."
            },
            "slug": "Some-Advances-in-Transformation-Based-Part-of-Brill",
            "title": {
                "fragments": [],
                "text": "Some Advances in Transformation-Based Part of Speech Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A method for expressing lexical relations in tagging that stochastic taggers are currently unable to express is described and how the tagger-can be extended into a k-best tagger, where multiple tags can be assigned to words in some cases of uncertainty."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2961287"
                        ],
                        "name": "N. Littlestone",
                        "slug": "N.-Littlestone",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Littlestone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Littlestone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 460,
                                "start": 16
                            }
                        ],
                        "text": "It was shown by Littlestone (1988) that the original Winnow method was robust to irrelevant features in that the number of mistakes it makes to obtain a classifier (in the separable case) depends only logarithmically on the dimensionality of the feature space. Generalization bounds of regularized Winnow that are similar to the mistake bound of the original Winnow (in the sense of logarithmic dependent on the dimensionality) have been given by Zhang (2001). These results imply that the new method, while it can properly handle non-separable data, shares similar theoretical advantages of Winnow in that it is also robust to irrelevant features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 16
                            }
                        ],
                        "text": "It was shown by Littlestone (1988) that the original Winnow method was robust to irrelevant features in that the number of mistakes it makes to obtain a classifier (in the separable case) depends only logarithmically on the dimensionality of the feature space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 43
                            }
                        ],
                        "text": "the Winnow multiplicative update algorithm (Littlestone, 1988)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 52
                            }
                        ],
                        "text": "This architecture is based on the Winnow algorithm (Littlestone, 1988, Grove and Roth, 2001), which in theory is suitable for problems with many irrelevant attributes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 76
                            }
                        ],
                        "text": "We are especially interested in\nthe Winnow multiplicative update algorithm (Littlestone, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6334230,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1dace286582d91916fe470d08f30381cf453f20",
            "isKey": true,
            "numCitedBy": 1612,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Valiant (1984) and others have studied the problem of learning various classes of Boolean functions from examples. Here we discuss incremental learning of these functions. We consider a setting in which the learner responds to each example according to a current hypothesis. Then the learner updates the hypothesis, if necessary, based on the correct classification of the example. One natural measure of the quality of learning in this setting is the number of mistakes the learner makes. For suitable classes of functions, learning algorithms are available that make a bounded number of mistakes, with the bound independent of the number of examples seen by the learner. We present one such algorithm that learns disjunctive Boolean functions, along with variants for learning other classes of Boolean functions. The basic method can be expressed as a linear-threshold algorithm. A primary advantage of this algorithm is that the number of mistakes grows only logarithmically with the number of irrelevant attributes in the examples. At the same time, the algorithm is computationally efficient in both time and space."
            },
            "slug": "Learning-Quickly-When-Irrelevant-Attributes-Abound:-Littlestone",
            "title": {
                "fragments": [],
                "text": "Learning Quickly When Irrelevant Attributes Abound: A New Linear-Threshold Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents one such algorithm that learns disjunctive Boolean functions, along with variants for learning other classes of Boolean functions."
            },
            "venue": {
                "fragments": [],
                "text": "28th Annual Symposium on Foundations of Computer Science (sfcs 1987)"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474158"
                        ],
                        "name": "Vasin Punyakanok",
                        "slug": "Vasin-Punyakanok",
                        "structuredName": {
                            "firstName": "Vasin",
                            "lastName": "Punyakanok",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vasin Punyakanok"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 170
                            }
                        ],
                        "text": "A specific advantage of the dynamic programming approach is that constraints required in a valid prediction sequence can be handled in a principled way (for example, see Punyakanok and Roth, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14509422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3fab92869cfab684b3ffb1c16a771e9c3b774acd",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing."
            },
            "slug": "The-Use-of-Classifiers-in-Sequential-Inference-Punyakanok-Roth",
            "title": {
                "fragments": [],
                "text": "The Use of Classifiers in Sequential Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A Markovian approach is developed that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies and an extension of constraint satisfaction formalisms are extended."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3131784"
                        ],
                        "name": "Taku Kudoh",
                        "slug": "Taku-Kudoh",
                        "structuredName": {
                            "firstName": "Taku",
                            "lastName": "Kudoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Taku Kudoh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681502"
                        ],
                        "name": "Yuji Matsumoto",
                        "slug": "Yuji-Matsumoto",
                        "structuredName": {
                            "firstName": "Yuji",
                            "lastName": "Matsumoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuji Matsumoto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6953360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "923db0aeb26a6dc1cb42069c9db04e5dd2d2200a",
            "isKey": false,
            "numCitedBy": 340,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we explore the use of Support Vector Machines (SVMs) for CoNLL-2000 shared task, chunk identification. SVMs are so-called large margin classifiers and are well-known as their good generalization performance. We investigate how SVMs with a very large number of features perform with the classification task of chunk labelling."
            },
            "slug": "Use-of-Support-Vector-Learning-for-Chunk-Kudoh-Matsumoto",
            "title": {
                "fragments": [],
                "text": "Use of Support Vector Learning for Chunk Identification"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper investigates how SVMs with a very large number of features perform with the classification task of chunk labelling, CoNLL-2000 shared task, chunk identification."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL/LLL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765329"
                        ],
                        "name": "Taku Kudo",
                        "slug": "Taku-Kudo",
                        "structuredName": {
                            "firstName": "Taku",
                            "lastName": "Kudo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Taku Kudo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681502"
                        ],
                        "name": "Yuji Matsumoto",
                        "slug": "Yuji-Matsumoto",
                        "structuredName": {
                            "firstName": "Yuji",
                            "lastName": "Matsumoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuji Matsumoto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1083,
                                "start": 4
                            }
                        ],
                        "text": "91 (Kudo and Matsumoto, 2001) by voting support vector machine outputs of eight different chunk-tag sequence representations. However this is not a fair comparison to our system since it is reasonable to believe that we can achieve appreciable improvement using a similar voting approach. Excluding results reported in the early version of the current work (Zhang et al., 2001), the previously second best system was a combination of five different WPDV models (van Halteren, 2000), with an overall F\u03b2=1 value of 93.32. This system is again more complex than the regularized Winnow approach we propose (their best single classifier performance is F\u03b2=1 = 92.47). The third best performance was achieved by using combinations of memory-based models (Sang, 2000), with an overall F\u03b2=1 value of 92.50. A total of eleven systems were summarized by Sang and Buchholz (2000), including a variety of statistical techniques such as maximum entropy, hidden Markov models, and transformation based rule learners. Interested readers are referred to the summary paper by Sang and Buchholz (2000) for references to all systems being tested."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 61
                            }
                        ],
                        "text": "For example, the best performance in Table 5 was achieved by Kudoh and Matsumoto (2000), using a combination of 231 kernel support vector machines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 73
                            }
                        ],
                        "text": "We should mention that their result has been recently improved to 93.91 (Kudo and Matsumoto, 2001) by voting support vector machine outputs of eight different chunk-tag sequence representations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 3
                            }
                        ],
                        "text": "91 (Kudo and Matsumoto, 2001) by voting support vector machine outputs of eight different chunk-tag sequence representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 868,
                                "start": 4
                            }
                        ],
                        "text": "91 (Kudo and Matsumoto, 2001) by voting support vector machine outputs of eight different chunk-tag sequence representations. However this is not a fair comparison to our system since it is reasonable to believe that we can achieve appreciable improvement using a similar voting approach. Excluding results reported in the early version of the current work (Zhang et al., 2001), the previously second best system was a combination of five different WPDV models (van Halteren, 2000), with an overall F\u03b2=1 value of 93.32. This system is again more complex than the regularized Winnow approach we propose (their best single classifier performance is F\u03b2=1 = 92.47). The third best performance was achieved by using combinations of memory-based models (Sang, 2000), with an overall F\u03b2=1 value of 92.50. A total of eleven systems were summarized by Sang and Buchholz (2000), including a variety of statistical techniques such as maximum entropy, hidden Markov models, and transformation based rule learners."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3446853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6ffea7929f0e4bbee9e98755eb3d8fc09e89cf4e",
            "isKey": true,
            "numCitedBy": 586,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply Support Vector Machines (SVMs) to identify English base phrases (chunks). SVMs are known to achieve high generalization performance even with input data of high dimensional feature spaces. Furthermore, by the Kernel principle, SVMs can carry out training with smaller computational overhead independent of their dimensionality. We apply weighted voting of 8 SVMs-based systems trained with distinct chunk representations. Experimental results show that our approach achieves higher accuracy than previous approaches."
            },
            "slug": "Chunking-with-Support-Vector-Machines-Kudo-Matsumoto",
            "title": {
                "fragments": [],
                "text": "Chunking with Support Vector Machines"
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2187850"
                        ],
                        "name": "Adam J. Grove",
                        "slug": "Adam-J.-Grove",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Grove",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam J. Grove"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 71
                            }
                        ],
                        "text": "This architecture is based on the Winnow algorithm (Littlestone, 1988, Grove and Roth, 2001), which in theory is suitable for problems with many irrelevant attributes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6586239,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69f1b2bd6dad7d677a6f9769bdfd3b5484329ef2",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We study a learning problem which allows for a \u201cfair\u201d comparison between unsupervised learning methods\u2014probabilistic model construction, and more traditional algorithms that directly learn a classification. The merits of each approach are intuitively clear: inducing a model is more expensive computationally, but may support a wider range of predictions. Its performance, however, will depend on how well the postulated probabilistic model fits that data. To compare the paradigms we consider a model which postulates a single binary-valued hidden variable on which all other attributes depend. In this model, finding the most likely value of any one variable (given known values for the others) reduces to testing a linear function of the observed values. We learn the model with two techniques: the standard EM algorithm, and a new algorithm we develop based on covariances. We compare these, in a controlled fashion, against an algorithm (a version of Winnow) that attempts to find a good linear classifier directly. Our conclusions help delimit the fragility of using a model that is even \u201cslightly\u201d simpler than the distribution actually generating the data, vs. the relative robustness of directly searching for a good predictor."
            },
            "slug": "Linear-Concepts-and-Hidden-Variables-Grove-Roth",
            "title": {
                "fragments": [],
                "text": "Linear Concepts and Hidden Variables"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work studies a learning problem which allows for a \u201cfair\u201d comparison between unsupervised learning methods\u2014probabilistic model construction, and more traditional algorithms that directly learn a classification."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35551590"
                        ],
                        "name": "Steven P. Abney",
                        "slug": "Steven-P.-Abney",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Abney",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven P. Abney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3086368"
                        ],
                        "name": "R. Berwick",
                        "slug": "R.-Berwick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Berwick",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Berwick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11920722"
                        ],
                        "name": "C. Tenny",
                        "slug": "C.-Tenny",
                        "structuredName": {
                            "firstName": "Carol",
                            "lastName": "Tenny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tenny"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60863446,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a98e11a9dbff1b3d4e25e0702864fef1eee2e8a6",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Principles of principle-based parsing, R.C. Berwick deductive parsing - the use of knowledge of language, M. Johnson the computational implementation of principle-based parsers, S. Fong empty categories, chain binding, and parsing, N. Correa parsing Warlpiri - a free word order language, M.B. Kashket principle-based parsing for machine translation, B.J. Dorr principle-based interpretation of natural language quantifiers, S.S. Epstein avoid the pedestrian's paradox, E.P. Stabler Jr parsing with changing grammars - evaluating a language acquisition model, R. Kazman parsing by chunks, S.P. Abney subcategorization and sentence processing, P. Gorrell subjacency in a principle-based parser, B.L. Pritchett locating Wh traces, H.S. Kurtzman et al."
            },
            "slug": "Principle-Based-Parsing:-Computation-and-Abney-Berwick",
            "title": {
                "fragments": [],
                "text": "Principle-Based Parsing: Computation and Psycholinguistics"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "Principles of principle-based parsing, R.C. Berwick deductive parsing - the use of knowledge of language, M.S. Stabler Jr parsing with changing grammars - evaluating a language acquisition model, and the computational implementation of Principle-based parsers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746048"
                        ],
                        "name": "R. Khardon",
                        "slug": "R.-Khardon",
                        "structuredName": {
                            "firstName": "Roni",
                            "lastName": "Khardon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Khardon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 109
                            }
                        ],
                        "text": "One method that has been quite successful in such applications is the SNoW architecture (Dagan et al., 1997, Khardon et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17292236,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3ee3f666a1d6f0ddfe48acfac16e1c0a1a52bee8",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a coherent view of learning and reasoning with relational representations in the context of natural language processing. In particular, we discuss the Neuroidal Architecture, Inductive Logic Programming and the SNoW system explaining the relationships among these, and thereby offer an explanation of the theoretical basis for the SNoW system. We suggest that extensions of this system along the lines suggested by the theory may provide new levels of scalability and functionality."
            },
            "slug": "Relational-Learning-for-NLP-using-Linear-Threshold-Khardon-Roth",
            "title": {
                "fragments": [],
                "text": "Relational Learning for NLP using Linear Threshold Elements"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A coherent view of learning and reasoning with relational representations in the context of natural language processing is described and extensions of this system along the lines suggested by the theory may provide new levels of scalability and functionality."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713476"
                        ],
                        "name": "M. McCord",
                        "slug": "M.-McCord",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "McCord",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McCord"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 41
                            }
                        ],
                        "text": "The ESG (English Slot Grammar) system by McCord (1989) is not directly comparable to the phrase structure grammar implicit in the WSJ treebank."
                    },
                    "intents": []
                }
            ],
            "corpusId": 38540559,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "d6bdd8bc6e1cad210ae9efab92872b08ff5a1363",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Slot Grammar makes it easier to write practical, broad-coverage natural language grammars, for the following reasons. (a) The system has a lexicalist character; although there are grammar rules, they are fewer in number and simpler because analysis is largely data-driven through use of slots taken from lexical entries. (b) There is a modular treatment of different grammatical phenomena through different rule types, for instance rule types for expressing linear ordering constraints. This modularity also reduces the differences between the Slot Grammars of different languages. (c) Several grammatical phenomena, such as coordination and extraposition, are treated mainly in a language-independent shell provided with the system."
            },
            "slug": "Slot-Grammar:-A-System-for-Simpler-Construction-of-McCord",
            "title": {
                "fragments": [],
                "text": "Slot Grammar: A System for Simpler Construction of Practical Natural Language Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Several grammatical phenomena, such as coordination and extraposition, are treated mainly in a language-independent shell provided with the Slot Grammars system."
            },
            "venue": {
                "fragments": [],
                "text": "Natural Language and Logic"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 48
                            }
                        ],
                        "text": "Using the general on-line learning framework of Kivinen and Warmuth (1997), for each data point (xi, yi), we consider an online update rule such that the weight wi+1 after seeing the i-th example is given by the solution to\nmin wi+1"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15718458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4dc175e8f6e7ca5c40ffd6fb9c6b92323bf7daf2",
            "isKey": false,
            "numCitedBy": 281,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider two algorithms for on-line prediction based on a linear model. The algorithms are the well-known Gradient Descent (GD) algorithm and a new algorithm, which we call EG *. They both maintain a weight vector using simple updates. For the GD algorithm, the weight vector is updated by subtracting from it the gradient of the squared error made on a prediction multiplied by a parameter called the learning rate. The EG* uses the components of the gradient in the exponents of factors that are used in updating the weight vector multiplicatively. We present worst-case on-line loss bounds for EG* and compare them to previously known bounds for the GD algorithm. The bounds suggest that although the on-line losses of the algorithms are in general incomparable, EG * has a much smaller loss if only few of the input variables are relevant for the predictions. Experiments show that the worst-case upper bounds are quite tight already on simple artificial data. Our main methodological idea is using a distance function between weight vectors both in motivating the algorithms and as a potential function in an amortized analysis that leads to worst-case loss bounds. Using squared Euclidean distance leads to the GD algorithm, and using the relative entropy leads to the EG* algorithm."
            },
            "slug": "Additive-versus-exponentiated-gradient-updates-for-Kivinen-Warmuth",
            "title": {
                "fragments": [],
                "text": "Additive versus exponentiated gradient updates for linear prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The main methodological idea is using a distance function between weight vectors both in motivating the algorithms and as a potential function in an amortized analysis that leads to worst-case loss bounds."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895207"
                        ],
                        "name": "C. Gentile",
                        "slug": "C.-Gentile",
                        "structuredName": {
                            "firstName": "Claudio",
                            "lastName": "Gentile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gentile"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 398,
                                "start": 121
                            }
                        ],
                        "text": "In order to obtain a systematic solution to this problem, we shall first examine a derivation of the Winnow algorithm by Gentile and Warmuth (1998), which motivates a more general solution to be presented later. Following Gentile and Warmuth (1998), we consider a loss function max(\u2212wT xiyi, 0), often called \u201chinge loss\u201d. Using the general on-line learning framework of Kivinen and Warmuth (1997), for each data point (xi, yi), we consider an online update rule such that the weight wi+1 after seeing the i-th example is given by the solution to"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 121
                            }
                        ],
                        "text": "In order to obtain a systematic solution to this problem, we shall first examine a derivation of the Winnow algorithm by Gentile and Warmuth (1998), which motivates a more general solution to be presented later. Following Gentile and Warmuth (1998), we consider a loss function max(\u2212wT xiyi, 0), often called \u201chinge loss\u201d."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 10
                            }
                        ],
                        "text": "Following Gentile and Warmuth (1998), we consider a loss function max(\u2212wT xiyi, 0), often called \u201chinge loss\u201d."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 121
                            }
                        ],
                        "text": "In order to obtain a systematic solution to this problem, we shall first examine a derivation of the Winnow algorithm by Gentile and Warmuth (1998), which motivates a more general solution to be presented later."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2946659,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a9e441188b4a41da300c2de923a674dd5d179c5",
            "isKey": true,
            "numCitedBy": 103,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a unifying method for proving relative loss bounds for online linear threshold classification algorithms, such as the Perceptron and the Winnow algorithms. For classification problems the discrete loss is used, i.e., the total number of prediction mistakes. We introduce a continuous loss function, called the \"linear hinge loss\", that can be employed to derive the updates of the algorithms. We first prove bounds w.r.t. the linear hinge loss and then convert them to the discrete loss. We introduce a notion of \"average margin\" of a set of examples. We show how relative loss bounds based on the linear hinge loss can be converted to relative loss bounds i.t.o. the discrete loss using the average margin."
            },
            "slug": "Linear-Hinge-Loss-and-Average-Margin-Gentile-Warmuth",
            "title": {
                "fragments": [],
                "text": "Linear Hinge Loss and Average Margin"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A unifying method for proving relative loss bounds for online linear threshold classification algorithms, such as the Perceptron and the Winnow algorithms, is described and a notion of \"average margin\" of a set of examples is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 48
                            }
                        ],
                        "text": "Using the general on-line learning framework of Kivinen and Warmuth (1997), for each data point (xi, yi), we consider an online update rule such that the weight wi+1 after seeing the i-th example is given by the solution to\nmin wi+1"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6130401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98eed3f082351c4821d1edb315846207a8fefbe9",
            "isKey": false,
            "numCitedBy": 911,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider two algorithm for on-line prediction based on a linear model. The algorithms are the well-known Gradient Descent (GD) algorithm and a new algorithm, which we call EG(+/-). They both maintain a weight vector using simple updates. For the GD algorithm, the update is based on subtracting the gradient of the squared error made on a prediction. The EG(+/-) algorithm uses the components of the gradient in the exponents of factors that are used in updating the weight vector multiplicatively. We present worst-case loss bounds for EG(+/-) and compare them to previously known bounds for the GD algorithm. The bounds suggest that the losses of the algorithms are in general incomparable, but EG(+/-) has a much smaller loss if only a few components of the input are relevant for the predictions. We have performed experiments, which show that our worst-case upper bounds are quite tight already on simple artificial data."
            },
            "slug": "Exponentiated-Gradient-Versus-Gradient-Descent-for-Kivinen-Warmuth",
            "title": {
                "fragments": [],
                "text": "Exponentiated Gradient Versus Gradient Descent for Linear Predictors"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The bounds suggest that the losses of the algorithms are in general incomparable, but EG(+/-) has a much smaller loss if only a few components of the input are relevant for the predictions, which is quite tight already on simple artificial data."
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Comput."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3276863"
                        ],
                        "name": "Ion Muslea",
                        "slug": "Ion-Muslea",
                        "structuredName": {
                            "firstName": "Ion",
                            "lastName": "Muslea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ion Muslea"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 134
                            }
                        ],
                        "text": "The field of text-based information extraction has a long history in the artificial intelligence and computational linguistics areas (Muslea, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18693692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f01732c9be8ccb70b41be008e043187d40be0fdf",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Information Extraction systems rely on a set of extraction patterns that they use in order to retrieve from each document the relevant information. In this paper we survey the various types of extraction patterns that are generated by machine learning algorithms. We identify three main categories of patterns, which cover a variety of application domains, and we compare and contrast the patterns from each category."
            },
            "slug": "Extraction-Patterns-for-Information-Extraction-A-Muslea",
            "title": {
                "fragments": [],
                "text": "Extraction Patterns for Information Extraction Tasks: A Survey"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper surveys the various types of extraction patterns that are generated by machine learning algorithms and identifies three main categories of patterns, which cover a variety of application domains, and compares and contrast the patterns from each category."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 63
                            }
                        ],
                        "text": "A more general treatment of this type of duality were given by Zhang (2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 65
                            }
                        ],
                        "text": "The learning rate issue has been more thoroughly investigated by Zhang (2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 28
                            }
                        ],
                        "text": "The basic idea was given by Zhang (2001, 2002), where the original Winnow algorithm was converted into a numerical optimization problem that can handle linearly non-separable data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14958443,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "25857a9957e885008e8ca5b4f864ddb4ca32c416",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we study a general formulation of linear prediction algorithms including a number of known methods as special cases. We describe a convex duality for this class of methods and propose numerical algorithms to solve the derived dual learning problem. We show that the dual formulation is closely related to online learning algorithms. Furthermore, by using this duality, we show that new learning methods can be obtained. Numerical examples will be given to illustrate various aspects of the newly proposed algorithms."
            },
            "slug": "On-the-Dual-Formulation-of-Regularized-Linear-with-Zhang",
            "title": {
                "fragments": [],
                "text": "On the Dual Formulation of Regularized Linear Systems with Convex Risks"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A convex duality is described for linear prediction algorithms including a number of known methods as special cases and it is shown that the dual formulation is closely related to online learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 43
                            }
                        ],
                        "text": "In an earlier version of the current work (Zhang et al., 2001), in order to save memory without using a hash table, we limited the number of token feature values (words\nor punctuation) to 5000 by removing less frequent tokens."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 69
                            }
                        ],
                        "text": "Excluding results reported in the early version of the current work (Zhang et al., 2001), the previously second best system was a combination of five different WPDV models (van Halteren, 2000), with an overall F\u03b2=1 value of 93.32."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 67
                            }
                        ],
                        "text": "Consequently, timing reported in this paper is better than that of Zhang et al. (2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 118
                            }
                        ],
                        "text": "A version of the new method was originally proposed by Zhang (2001), and was used in an earlier version of this work (Zhang et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 66
                            }
                        ],
                        "text": "For consistency, we use the same set of features as those used by Zhang et al. (2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Johnson . Text chunking using regularized Winnow"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 41
                            }
                        ],
                        "text": "The ESG (English Slot Grammar) system by McCord (1989) is not directly comparable to the phrase structure grammar implicit in the WSJ treebank."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Slot grammar: a system for simple construction of practical natural language grammars. Natural Language and Logic"
            },
            "venue": {
                "fragments": [],
                "text": "Slot grammar: a system for simple construction of practical natural language grammars. Natural Language and Logic"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 94
                            }
                        ],
                        "text": "Additionally, a part-of-speech (POS) tag was assigned to each token by a standard POS tagger (Brill, 1994) that was trained on the Penn Treebank."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1308,
                                "start": 89
                            }
                        ],
                        "text": "One method that has been quite successful in such applications is the SNoW architecture (Dagan et al., 1997, Khardon et al., 1999). This architecture is based on the Winnow algorithm (Littlestone, 1988, Grove and Roth, 2001), which in theory is suitable for problems with many irrelevant attributes. In natural language processing, one often encounters a very high dimensional feature space, although most of the features are irrelevant. Therefore the robustness of Winnow to high dimensional feature space makes it attractive for NLP tasks. However, the convergence of the Winnow algorithm is only guaranteed for linearly separable data. In practical NLP applications, data may not always be linearly separable. Consequently, a direct application of Winnow can lead to numerical instability. A second problem for the original Winnow method is that it does not produce a reliable estimate of the confidence of its predictions. Such information can be very useful in statistical modeling. In fact, some kind of confidence estimate is required in our approach to text chunking. Due to the above mentioned problems, we use a modification of Winnow which can handle the linearly non-separable case, and produce a reliable probability estimate. A version of the new method was originally proposed by Zhang (2001), and was used in an earlier version of this work (Zhang et al."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some advances in rule-based part of speech tagging"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. AAAI"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 94
                            }
                        ],
                        "text": "Additionally, a part-of-speech (POS) tag was assigned to each token by a standard POS tagger (Brill, 1994) that was trained on the Penn Treebank."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some advan es in rule - based part of spee h tagging"
            },
            "venue": {
                "fragments": [],
                "text": "In Pro . AAAI"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to the conll2000 shared tasks : Chunking Chunking with wpdv models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tjong Kim Sang . Text chunking by system combination"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 10,
            "methodology": 18,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 27,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Text-Chunking-based-on-a-Generalization-of-Winnow-Zhang-Damerau/48649e3cf38d711cbaea177519becdd696c12b4c?sort=total-citations"
}