{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715066"
                        ],
                        "name": "T. Phan",
                        "slug": "T.-Phan",
                        "structuredName": {
                            "firstName": "Trung",
                            "lastName": "Phan",
                            "middleNames": [
                                "Quy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Phan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744575"
                        ],
                        "name": "P. Shivakumara",
                        "slug": "P.-Shivakumara",
                        "structuredName": {
                            "firstName": "Palaiahnakote",
                            "lastName": "Shivakumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shivakumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "The second step is motivated by our earlier work [23], which uses the Laplacian mask in the spatial domain to detect candidate text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123638,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c0c4294d517de3243fd4f8a09359e265fbf2f1ea",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose an efficient text detection method based on the Laplacian operator. The maximum gradient difference value is computed for each pixel in the Laplacian-filtered image. K-means is then used to classify all the pixels into two clusters: text and non-text. For each candidate text region, the corresponding region in the Sobel edge map of the input image undergoes projection profile analysis to determine the boundary of the text blocks. Finally, we employ empirical rules to eliminate false positives based on geometrical properties. Experimental results show that the proposed method is able to detect text of different fonts, contrast and backgrounds. Moreover, it outperforms three existing methods in terms of detection and false positive rates."
            },
            "slug": "A-Laplacian-Method-for-Video-Text-Detection-Phan-Shivakumara",
            "title": {
                "fragments": [],
                "text": "A Laplacian Method for Video Text Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The proposed text detection method outperforms three existing methods in terms of detection and false positive rates and employs empirical rules to eliminate false positives based on geometrical properties."
            },
            "venue": {
                "fragments": [],
                "text": "2009 10th International Conference on Document Analysis and Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785083"
                        ],
                        "name": "Michael R. Lyu",
                        "slug": "Michael-R.-Lyu",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lyu",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael R. Lyu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3332642"
                        ],
                        "name": "Jiqiang Song",
                        "slug": "Jiqiang-Song",
                        "structuredName": {
                            "firstName": "Jiqiang",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiqiang Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052052370"
                        ],
                        "name": "Min Cai",
                        "slug": "Min-Cai",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Cai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "Method [6], denoted as edge-based method 2, performs Sobel edge detection in the YUV color space and applies two text area enhancement filters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[6] design two filters to enhance the edges in text areas."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 102
                            }
                        ],
                        "text": "We evaluate the performance at the block level, which is a common granularity level in the literature [6], [7], [8], [10], [11], rather than the word or character level, because we have not considered text recognition in this work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "The edge-based method [6] analyzes horizontal and vertical profiles of the edge map."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12862847,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f64a1d2e366eb476be69cc431f053dcaa22935a",
            "isKey": true,
            "numCitedBy": 151,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Text detection is fundamental to video information retrieval and indexing. Existing methods cannot handle well those texts with different contrast or embedded in a complex background. To handle these difficulties, this paper proposes an efficient text detection approach, which is based on invariant features, such as edge strength, edge density, and horizontal distribution. First, it applies edge detection and uses a low threshold to filter out definitely non-text edges. Then, a local threshold is selected to both keep low-contrast text and simplify complex background of high-contrast text. Next, two text-area enhancement operators are proposed to highlight those areas with either high edge strength or high edge density. Finally, coarse-to-fine detection locates text regions efficiently. Experimental results show that this approach is robust for contrast, font-size, font-color, language, and background complexity."
            },
            "slug": "A-new-approach-for-video-text-detection-Lyu-Song",
            "title": {
                "fragments": [],
                "text": "A new approach for video text detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes an efficient text detection approach, which is based on invariant features, such as edge strength, edge density, and horizontal distribution, and it applies edge detection and uses a low threshold to filter out definitely non-text edges."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. International Conference on Image Processing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694936"
                        ],
                        "name": "Qixiang Ye",
                        "slug": "Qixiang-Ye",
                        "structuredName": {
                            "firstName": "Qixiang",
                            "lastName": "Ye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qixiang Ye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689702"
                        ],
                        "name": "Qingming Huang",
                        "slug": "Qingming-Huang",
                        "structuredName": {
                            "firstName": "Qingming",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qingming Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153706048"
                        ],
                        "name": "W. Gao",
                        "slug": "W.-Gao",
                        "structuredName": {
                            "firstName": "Wen",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725937"
                        ],
                        "name": "Debin Zhao",
                        "slug": "Debin-Zhao",
                        "structuredName": {
                            "firstName": "Debin",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Debin Zhao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[8] compute the wavelet energy features at different scales and perform adaptive thresholding to find candidate text pixels, which are then merged into candidate text lines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 112
                            }
                        ],
                        "text": "We evaluate the performance at the block level, which is a common granularity level in the literature [6], [7], [8], [10], [11], rather than the word or character level, because we have not considered text recognition in this work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17956059,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcb8cd892adbfded8373716a53787f55da89180a",
            "isKey": false,
            "numCitedBy": 363,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fast-and-robust-text-detection-in-images-and-video-Ye-Huang",
            "title": {
                "fragments": [],
                "text": "Fast and robust text detection in images and video frames"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107918894"
                        ],
                        "name": "Chunmei Liu",
                        "slug": "Chunmei-Liu",
                        "structuredName": {
                            "firstName": "Chunmei",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunmei Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683416"
                        ],
                        "name": "Chunheng Wang",
                        "slug": "Chunheng-Wang",
                        "structuredName": {
                            "firstName": "Chunheng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunheng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145841729"
                        ],
                        "name": "Ruwei Dai",
                        "slug": "Ruwei-Dai",
                        "structuredName": {
                            "firstName": "Ruwei",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruwei Dai"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[5] extract statistical features from the Sobel edge maps of four directions and use K-means to classify pixels into the text and nontext clusters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "Method [5], denoted as edge-based method 1, extracts six statistical features from four Sobel edge maps."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9821585,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04925a1e7566a1ace8a4603ef5917b5f5bcb31ff",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, an algorithm is proposed for detecting texts in images and video frames. It is performed by three steps: edge detection, text candidate detection and text refinement detection. Firstly, it applies edge detection to get four edge maps in horizontal, vertical, up-right, and up-left direction. Secondly, the feature is extracted from four edge maps to represent the texture property of text. Then k-means algorithm is applied to detect the initial text candidates. Finally, the text areas are identified by the empirical rules analysis and refined through project profile analysis. Experimental results demonstrate that the proposed approach could efficiently be used as an automatic text detection system, which is robust for font size, font color, background complexity and language."
            },
            "slug": "Text-detection-in-images-based-on-unsupervised-of-Liu-Wang",
            "title": {
                "fragments": [],
                "text": "Text detection in images based on unsupervised classification of edge-based features"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experimental results demonstrate that the proposed approach could efficiently be used as an automatic text detection system, which is robust for font size, font color, background complexity and language."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3314902"
                        ],
                        "name": "Edward K. Wong",
                        "slug": "Edward-K.-Wong",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Wong",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward K. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50133585"
                        ],
                        "name": "Minya Chen",
                        "slug": "Minya-Chen",
                        "structuredName": {
                            "firstName": "Minya",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minya Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[7] compute the maximum gradient difference to identify the potential line segments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 241
                            }
                        ],
                        "text": "There are two other performance measures commonly used in the literature, Detection Rate and False Positive Rate; however, they can also be converted to Recall and Precision: Recall 1\u20444 Detection Rate and Precision 1\u20444 1 False Positive Rate [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "difference (MD) [7], defined as the difference between the maximum value and the minimum value within a local 1 N window, is computed from the filtered image:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 107
                            }
                        ],
                        "text": "We evaluate the performance at the block level, which is a common granularity level in the literature [6], [7], [8], [10], [11], rather than the word or character level, because we have not considered text recognition in this work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 219
                            }
                        ],
                        "text": "\u2022 Recall (R) = TDB / ATB \u2022 Precision (P) = TDB / (TDB + FDB) \u2022 F-measure (F) = 2 \u00d7 P \u00d7 R / (P + R) \u2022 Misdetection Rate (MDR) = MDB / TDB There are two other performance measures commonly used in the literature, Detection Rate and False Positive Rate; however, they can also be converted to Recall and Precision: Recall = Detection Rate and Precision = 1 \u2013 False Positive Rate [7]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "Method [7], denoted as the gradient-based method, computes the maximum gradient difference to identify candidate text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 113
                            }
                        ],
                        "text": "For example, the uniformcolor method [11] performs color clustering on each row, while the gradient-based method [7] identifies candidate text segments rowwise."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20048852,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5b1f30c7f3f172c6e230a49315c8c0040a6c4b4",
            "isKey": true,
            "numCitedBy": 151,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-new-robust-algorithm-for-video-text-extraction-Wong-Chen",
            "title": {
                "fragments": [],
                "text": "A new robust algorithm for video text extraction"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32544716"
                        ],
                        "name": "Axel Wernicke",
                        "slug": "Axel-Wernicke",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Wernicke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Axel Wernicke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 143774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8387d4998f810cd2b60bd81545cb993087bc8788",
            "isKey": false,
            "numCitedBy": 467,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Many images, especially those used for page design on Web pages, as well as videos contain visible text. If these text occurrences could be detected, segmented, and recognized automatically, they would be a valuable source of high-level semantics for indexing and retrieval. We propose a novel method for localizing and segmenting text in complex images and videos. Text lines are identified by using a complex-valued multilayer feed-forward network trained to detect text at a fixed scale and position. The network's output at all scales and positions is integrated into a single text-saliency map, serving as a starting point for candidate text lines. In the case of video, these candidate text lines are refined by exploiting the temporal redundancy of text in video. Localized text lines are then scaled to a fixed height of 100 pixels and segmented into a binary image with black characters on white background. For videos, temporal redundancy is exploited to improve segmentation performance. Input images and videos can be of any size due to a true multiresolution approach. Moreover, the system is not only able to locate and segment text occurrences into large binary images, but is also able to track each text line with sub-pixel accuracy over the entire occurrence in a video, so that one text bitmap is created for all instances of that text line. Therefore, our text segmentation results can also be used for object-based video encoding such as that enabled by MPEG-4."
            },
            "slug": "Localizing-and-segmenting-text-in-images-and-videos-Lienhart-Wernicke",
            "title": {
                "fragments": [],
                "text": "Localizing and segmenting text in images and videos"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work proposes a novel method for localizing and segmenting text in complex images and videos that is not only able to locate and segment text occurrences into large binary images, but is also able to track each text line with sub-pixel accuracy over the entire occurrence in a video."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Circuits Syst. Video Technol."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109447059"
                        ],
                        "name": "Chang-Woo Lee",
                        "slug": "Chang-Woo-Lee",
                        "structuredName": {
                            "firstName": "Chang-Woo",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chang-Woo Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70261685"
                        ],
                        "name": "Hang-Joon Kim",
                        "slug": "Hang-Joon-Kim",
                        "structuredName": {
                            "firstName": "Hang-Joon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang-Joon Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[9] use support vector machines (SVM) to classify every pixel as text or nontext."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 38891128,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4fc5414666609f0d4d22d37327a8c51f853ed71",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-text-detection-and-removal-in-video-Lee-Jung",
            "title": {
                "fragments": [],
                "text": "Automatic text detection and removal in video sequences"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39319377"
                        ],
                        "name": "Yu Zhong",
                        "slug": "Yu-Zhong",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Zhong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145211604"
                        ],
                        "name": "K. Karu",
                        "slug": "K.-Karu",
                        "structuredName": {
                            "firstName": "Kalle",
                            "lastName": "Karu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Karu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 157
                            }
                        ],
                        "text": "The first approach employs color quantization and region growing (or splitting) to group neighboring pixels of similar colors into connected components (CC) [3], [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29853292,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a4af75831ed098d9fea02507f36cdbc38852fe6",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a substantial interest in retrieving images from a large database using the textual information contained in the images. An algorithm which will automatically locate the textual regions in the input image will facilitate this task; the optical character recognizer can then be applied to only those regions of the image which contain text. We present a method for automatically locating text in complex color images. The algorithm first finds the approximate locations of text lines using horizontal spatial variance, and then extracts text components in these boxes using color segmentation. The proposed method has been used to locate text in compact disc (CD) and book cover images, as well as in the images of traffic scenes captured by a video camera. Initial results are encouraging and suggest that these algorithms can be used in image retrieval applications."
            },
            "slug": "Locating-text-in-complex-color-images-Zhong-Karu",
            "title": {
                "fragments": [],
                "text": "Locating text in complex color images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The proposed algorithm has been used to locate text in compact disc and book cover images, as well as in the images of traffic scenes captured by a video camera, and initial results suggest that these algorithms can be used in image retrieval applications."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699756"
                        ],
                        "name": "K. Sobottka",
                        "slug": "K.-Sobottka",
                        "structuredName": {
                            "firstName": "Karin",
                            "lastName": "Sobottka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sobottka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36058334"
                        ],
                        "name": "H. Kronenberg",
                        "slug": "H.-Kronenberg",
                        "structuredName": {
                            "firstName": "Heino",
                            "lastName": "Kronenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kronenberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 162
                            }
                        ],
                        "text": "The first approach employs color quantization and region growing (or splitting) to group neighboring pixels of similar colors into connected components (CC) [3], [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14868685,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ba04958180cb158da0fe02a8599a7e301844456",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach to automatic text location and identification of colored book and journal covers is proposed. To reduce the amount of small variations in color, a clustering algorithm is applied in a preprocessing step. Two methods have been developed for extracting text hypotheses. One is based on a top-down analysis using successive splitting of image regions. The other is a bottom-up region growing algorithm. The results of both methods are combined to robustly distinguish between text and non-text elements. Text elements are binarized using automatically extracted information about text color. The binarized text regions can be used as input for a conventional OCR module. Results are shown for parts of book and journal covers of different complexity. The proposed method is not restricted to cover pages, but can be applied to the extraction of text from other types of color images as well."
            },
            "slug": "Identification-of-text-on-colored-book-and-journal-Sobottka-Bunke",
            "title": {
                "fragments": [],
                "text": "Identification of text on colored book and journal covers"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "An approach to automatic text location and identification of colored book and journal covers is proposed and a clustering algorithm is applied in a preprocessing step to reduce the amount of small variations in color."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785083"
                        ],
                        "name": "Michael R. Lyu",
                        "slug": "Michael-R.-Lyu",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lyu",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael R. Lyu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3332642"
                        ],
                        "name": "Jiqiang Song",
                        "slug": "Jiqiang-Song",
                        "structuredName": {
                            "firstName": "Jiqiang",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiqiang Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052052370"
                        ],
                        "name": "Min Cai",
                        "slug": "Min-Cai",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Cai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 169
                            }
                        ],
                        "text": "Multi-oriented text detection without any constraints on background, contrast, and orientation, and with high precision, and recall is still a challenging problem [14], [21], [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18648576,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e14cf92ecb1589d21324d934b2009451e602d1be",
            "isKey": false,
            "numCitedBy": 371,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Text in video is a very compact and accurate clue for video indexing and summarization. Most video text detection and extraction methods hold assumptions on text color, background contrast, and font style. Moreover, few methods can handle multilingual text well since different languages may have quite different appearances. This paper performs a detailed analysis of multilingual text characteristics, including English and Chinese. Based on the analysis, we propose a comprehensive, efficient video text detection, localization, and extraction method, which emphasizes the multilingual capability over the whole processing. The proposed method is also robust to various background complexities and text appearances. The text detection is carried out by edge detection, local thresholding, and hysteresis edge recovery. The coarse-to-fine localization scheme is then performed to identify text regions accurately. The text extraction consists of adaptive thresholding, dam point labeling, and inward filling. Experimental results on a large number of video images and comparisons with other methods are reported in detail."
            },
            "slug": "A-comprehensive-method-for-multilingual-video-text-Lyu-Song",
            "title": {
                "fragments": [],
                "text": "A comprehensive method for multilingual video text detection, localization, and extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A comprehensive, efficient video text detection, localization, and extraction method, which emphasizes the multilingual capability over the whole processing, and is also robust to various background complexities and text appearances."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Circuits and Systems for Video Technology"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784196"
                        ],
                        "name": "Datong Chen",
                        "slug": "Datong-Chen",
                        "structuredName": {
                            "firstName": "Datong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Datong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719610"
                        ],
                        "name": "J. Odobez",
                        "slug": "J.-Odobez",
                        "structuredName": {
                            "firstName": "Jean-Marc",
                            "lastName": "Odobez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Odobez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710257"
                        ],
                        "name": "J. Thiran",
                        "slug": "J.-Thiran",
                        "structuredName": {
                            "firstName": "Jean-Philippe",
                            "lastName": "Thiran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Thiran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "However, these classifiers require a large training set of text and nontext samples [10]; it is especially hard to ensure that the nontext samples are representative."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "We evaluate the performance at the block level, which is a common granularity level in the literature [6], [7], [8], [10], [11], rather than the word or character level, because we have not considered text recognition in this work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "The percentage is chosen according to [10], in which a text block is considered correctly detected if it overlaps at least 80 percent of the ground-truth block."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15702996,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e034ec59e2418f0f676c0237b7f1d953ee7fae49",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-localization/verification-scheme-for-finding-text-Chen-Odobez",
            "title": {
                "fragments": [],
                "text": "A localization/verification scheme for finding text in images and video frames based on contrast independent features and machine learning methods"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process. Image Commun."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152675651"
                        ],
                        "name": "J. Kim",
                        "slug": "J.-Kim",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Kim",
                            "middleNames": [
                                "Hyung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17901853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14bbcdc1744cc5982ffb64ea4755a72921d98d08",
            "isKey": false,
            "numCitedBy": 503,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The current paper presents a novel texture-based method for detecting texts in images. A support vector machine (SVM) is used to analyze the textural properties of texts. No external texture feature extraction module is used, but rather the intensities of the raw pixels that make up the textural pattern are fed directly to the SVM, which works well even in high-dimensional spaces. Next, text regions are identified by applying a continuously adaptive mean shift algorithm (CAMSHIFT) to the results of the texture analysis. The combination of CAMSHIFT and SVMs produces both robust and efficient text detection, as time-consuming texture analyses for less relevant pixels are restricted, leaving only a small part of the input image to be texture-analyzed."
            },
            "slug": "Texture-Based-Approach-for-Text-Detection-in-Images-Kim-Jung",
            "title": {
                "fragments": [],
                "text": "Texture-Based Approach for Text Detection in Images Using Support Vector Machines and Continuously Adaptive Mean Shift Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The combination of CAMSHIFT and SVMs produces both robust and efficient text detection, as time-consuming texture analyses for less relevant pixels are restricted, leaving only a small part of the input image to be texture-analyzed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821130"
                        ],
                        "name": "David J. Crandall",
                        "slug": "David-J.-Crandall",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Crandall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Crandall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721328"
                        ],
                        "name": "Sameer Kiran Antani",
                        "slug": "Sameer-Kiran-Antani",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Antani",
                            "middleNames": [
                                "Kiran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Kiran Antani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 163
                            }
                        ],
                        "text": "Multi-oriented text detection without any constraints on background, contrast, and orientation, and with high precision, and recall is still a challenging problem [14], [21], [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[14] propose a method for extracting multi-oriented special effects text, but this method is limited to graphics text of a few directions (0, 15, 30 degrees, and so on)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18084231,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37903a00047e1cf377408ca4119b48f2bfab89c4",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. The popularity of digital video is increasing rapidly. To help users navigate libraries of video, algorithms that automatically index video based on content are needed. One approach is to extract text appearing in video, which often reflects a scene's semantic content. This is a difficult problem due to the unconstrained nature of general-purpose video. Text can have arbitrary color, size, and orientation. Backgrounds may be complex and changing. Most work so far has made restrictive assumptions about the nature of text occurring in video. Such work is therefore not directly applicable to unconstrained, general-purpose video. In addition, most work so far has focused only on detecting the spatial extent of text in individual video frames. However, text occurring in video usually persists for several seconds. This constitutes a text event that should be entered only once in the video index. Therefore it is also necessary to determine the temporal extent of text events. This is a non-trivial problem because text may move, rotate, grow, shrink, or otherwise change over time. Such text effects are common in television programs and commercials but so far have received little attention in the literature. This paper discusses detecting, binarizing, and tracking caption text in general-purpose MPEG-1 video. Solutions are proposed for each of these problems and compared with existing work found in the literature."
            },
            "slug": "Extraction-of-special-effects-caption-text-events-Crandall-Antani",
            "title": {
                "fragments": [],
                "text": "Extraction of special effects caption text events from digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper discusses detecting, binarizing, and tracking caption text in general-purpose MPEG-1 video, and solutions are proposed for each of these problems and compared with existing work found in the literature."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal on Document Analysis and Recognition"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2673919"
                        ],
                        "name": "V. Mariano",
                        "slug": "V.-Mariano",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Mariano",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Mariano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "Method [11], denoted as the uniform-colored method, performs hierarchical clustering in the L a b color space to locate uniform-colored text strings."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 123
                            }
                        ],
                        "text": "We evaluate the performance at the block level, which is a common granularity level in the literature [6], [7], [8], [10], [11], rather than the word or character level, because we have not considered text recognition in this work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "For example, the uniformcolor method [11] performs color clustering on each row, while the gradient-based method [7] identifies candidate text segments rowwise."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195348929,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "1d4a5e70874d828bbd6c16300511fa2744f0b2fd",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper a method is proposed for locating horizontal, uniform-colored text in video frames. It was observed that when a row of pixels across such a text region is clustered in perceptually uniform L*a*b* color space, the pixels of one of these clusters would belong to the text strokes. These pixels would appeal as a line of short streaks on the row since a typical text region has many vertical and diagonal strokes. The proposed method examines every third row of the the image and checks whether this row passes through a horizontal text region. For a given row R, the pixels of R are hierarchically clustered in L*a*b* space and each cluster is tested whether similar-colored pixels in R's vicinity are possibly part of a text region. Candidate text blocks are marked by heuristics using information about the cluster's line of shell streaks. The detected text blocks are fused with the text regions. The method was tested on key frames of several video sequences and was able to locate a wide variety of text."
            },
            "slug": "Locating-uniform-colored-text-in-video-frames-Mariano-Kasturi",
            "title": {
                "fragments": [],
                "text": "Locating uniform-colored text in video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed method examines every third row of the the image and checks whether this row passes through a horizontal text region, and detects text blocks marked by heuristics using information about the cluster's line of shell streaks."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "Moreover, scene text is affected by lighting conditions and perspective distortions [1], [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 165
                            }
                        ],
                        "text": "This is because most of the nonhorizontal text lines are scene text, which is much more difficult to detect due to varying lighting and complex transformations [1], [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5999466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cedf72be1fe814ef2ee9d65633dc3226f80f0785",
            "isKey": false,
            "numCitedBy": 936,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-information-extraction-in-images-and-video:-a-Jung-Kim",
            "title": {
                "fragments": [],
                "text": "Text information extraction in images and video: a survey"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40813600"
                        ],
                        "name": "P. Roy",
                        "slug": "P.-Roy",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Roy",
                            "middleNames": [
                                "Pratim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Roy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144167309"
                        ],
                        "name": "U. Pal",
                        "slug": "U.-Pal",
                        "structuredName": {
                            "firstName": "Umapada",
                            "lastName": "Pal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Pal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143826881"
                        ],
                        "name": "J. Llad\u00f3s",
                        "slug": "J.-Llad\u00f3s",
                        "structuredName": {
                            "firstName": "Josep",
                            "lastName": "Llad\u00f3s",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Llad\u00f3s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145809751"
                        ],
                        "name": "F. Kimura",
                        "slug": "F.-Kimura",
                        "structuredName": {
                            "firstName": "Fumitaka",
                            "lastName": "Kimura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Kimura"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4392340,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "174714971cc1f31adf5dd41c6887ec6073afc88f",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a scheme towards recognition of English character in multi-scale and multi-oriented environments. Graphical document such as map consists of text lines which appear in different orientation. Sometimes, characters in a single word may follow a curvilinear way to annotate the graphical curve lines. For recognition of such multi-scale and multi-oriented characters a Support Vector Machine (SVM) based scheme is presented in this paper. The feature used here is invariant to character orientation. Circular ring and convex hull have been used along with angular information of the contour pixels of the character to make the feature rotation invariant. We tested our proposed scheme on two different datasets. Combining circular and convex hull feature we have obtained 96.73% and 99.56% accuracy in these two datasets."
            },
            "slug": "Convex-hull-based-approach-for-multi-oriented-from-Roy-Pal",
            "title": {
                "fragments": [],
                "text": "Convex hull based approach for multi-oriented character recognition from graphical documents"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A Support Vector Machine (SVM) based scheme towards recognition of English character in multi-scale and multi-oriented environments using circular ring and convex hull feature, which is invariant to character orientation."
            },
            "venue": {
                "fragments": [],
                "text": "2008 19th International Conference on Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145756253"
                        ],
                        "name": "Feng Wang",
                        "slug": "Feng-Wang",
                        "structuredName": {
                            "firstName": "Feng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Feng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977389"
                        ],
                        "name": "C. Ngo",
                        "slug": "C.-Ngo",
                        "structuredName": {
                            "firstName": "Chong-Wah",
                            "lastName": "Ngo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ngo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681882"
                        ],
                        "name": "T. Pong",
                        "slug": "T.-Pong",
                        "structuredName": {
                            "firstName": "Ting-Chuen",
                            "lastName": "Pong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 41818226,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7c155fcd0aa851c7774d90da377b8363a38e1ef",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Structuring-low-quality-videotaped-lectures-for-by-Wang-Ngo",
            "title": {
                "fragments": [],
                "text": "Structuring low-quality videotaped lectures for cross-reference browsing by video text analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772547"
                        ],
                        "name": "Xilin Chen",
                        "slug": "Xilin-Chen",
                        "structuredName": {
                            "firstName": "Xilin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xilin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155703534"
                        ],
                        "name": "Jing Zhang",
                        "slug": "Jing-Zhang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "Related works for scene text in camera-based images are [15], [16] and [17], [18] (which focus specifically on text orientation)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6109448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5295b6770ebbbc27a4651ed44b4b7e184d884f8e",
            "isKey": false,
            "numCitedBy": 330,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present an approach to automatic detection and recognition of signs from natural scenes, and its application to a sign translation task. The proposed approach embeds multiresolution and multiscale edge detection, adaptive searching, color analysis, and affine rectification in a hierarchical framework for sign detection, with different emphases at each phase to handle the text in different sizes, orientations, color distributions and backgrounds. We use affine rectification to recover deformation of the text regions caused by an inappropriate camera view angle. The procedure can significantly improve text detection rate and optical character recognition (OCR) accuracy. Instead of using binary information for OCR, we extract features from an intensity image directly. We propose a local intensity normalization method to effectively handle lighting variations, followed by a Gabor transform to obtain local features, and finally a linear discriminant analysis (LDA) method for feature selection. We have applied the approach in developing a Chinese sign translation system, which can automatically detect and recognize Chinese signs as input from a camera, and translate the recognized text into English."
            },
            "slug": "Automatic-detection-and-recognition-of-signs-from-Chen-Yang",
            "title": {
                "fragments": [],
                "text": "Automatic detection and recognition of signs from natural scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper proposes a local intensity normalization method to effectively handle lighting variations, followed by a Gabor transform to obtain local features, and finally a linear discriminant analysis (LDA) method for feature selection."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143863244"
                        ],
                        "name": "Xiansheng Hua",
                        "slug": "Xiansheng-Hua",
                        "structuredName": {
                            "firstName": "Xiansheng",
                            "lastName": "Hua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiansheng Hua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120640002"
                        ],
                        "name": "Wenyin Liu",
                        "slug": "Wenyin-Liu",
                        "structuredName": {
                            "firstName": "Wenyin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyin Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "data sets for this experiment: the Microsoft Research Asia data set [25] and the ICDAR 2003 Text Locating data set (in particular, the"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15798803,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c44fc2f6d748f54badcaf86feef8eb347d0b1c2",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Text presented in videos provides important supplemental information for video indexing and retrieval. Many efforts have been made for text detection in videos. However, there is still a lack of performance evaluation protocols for video text detection. In this paper, we propose an objective and comprehensive performance evaluation protocol for video text detection algorithms. The protocol includes a positive set and a negative set of indices at the textbox level, which evaluate the detection quality in terms of both location accuracy and fragmentation of the detected textboxes. In the protocol, we assign a detection difficulty (DD) level to each ground truth textbox. The performance indices can then be normalized with respect to the textbox DD level and are therefore tolerant to different ground-truth difficulties to a certain degree. We also assign a detectability index (DI) value to each ground-truth textbox. The overall detection rate is the DI-weighted average of the detection qualities of all ground-truth textboxes, which makes the detection rate more accurate to reveal the real performance. The automatic performance evaluation scheme has been applied to performance evaluation of a text detection approach to determine the best thresholds that can yield the best detection results. The protocol has also been employed to compare the performances of several text detection systems. Hence, we believe that the proposed protocol can be used to compare the performance of different video/image text detection algorithms/systems and can even help improve, select, and design new text detection methods."
            },
            "slug": "An-automatic-performance-evaluation-protocol-for-Hua-Liu",
            "title": {
                "fragments": [],
                "text": "An automatic performance evaluation protocol for video text detection algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An objective and comprehensive performance evaluation protocol for video text detection algorithms, which includes a positive set and a negative set of indices at the textbox level, which evaluate the detection quality in terms of both location accuracy and fragmentation of the detected textboxes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Circuits and Systems for Video Technology"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 209
                            }
                        ],
                        "text": "These methods are good for camera-based images, but they do not work well for video images because they require text to be of high resolution in order to fully extract the clear shape of each single character [19], [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15034932,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93c8a2963be6088dd55959bfe8cd92916891fb66",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-network-based-text-location-in-color-images-Jung",
            "title": {
                "fragments": [],
                "text": "Neural network-based text location in color images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50295995"
                        ],
                        "name": "Xiaoou Tang",
                        "slug": "Xiaoou-Tang",
                        "structuredName": {
                            "firstName": "Xiaoou",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoou Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10699750"
                        ],
                        "name": "Xinbo Gao",
                        "slug": "Xinbo-Gao",
                        "structuredName": {
                            "firstName": "Xinbo",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinbo Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7137861"
                        ],
                        "name": "Jianzhuang Liu",
                        "slug": "Jianzhuang-Liu",
                        "structuredName": {
                            "firstName": "Jianzhuang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianzhuang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 215
                            }
                        ],
                        "text": "These methods are good for camera-based images, but they do not work well for video images because they require text to be of high resolution in order to fully extract the clear shape of each single character [19], [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15021030,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1697bfbb2c701fba6032d63309a904d21b4f0d09",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a video caption detection and recognition system based on a fuzzy-clustering neural network (FCNN) classifier. Using a novel caption-transition detection scheme we locate both spatial and temporal positions of video captions with high precision and efficiency. Then employing several new character segmentation and binarization techniques, we improve the Chinese video-caption recognition accuracy from 13% to 86% on a set of news video captions. As the first attempt on Chinese video-caption recognition, our experiment results are very encouraging."
            },
            "slug": "A-spatial-temporal-approach-for-video-caption-and-Tang-Gao",
            "title": {
                "fragments": [],
                "text": "A spatial-temporal approach for video caption detection and recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A video caption detection and recognition system based on a fuzzy-clustering neural network (FCNN) classifier that improves the Chinese video-caption recognition accuracy from 13% to 86% on a set of news video captions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40813600"
                        ],
                        "name": "P. Roy",
                        "slug": "P.-Roy",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Roy",
                            "middleNames": [
                                "Pratim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Roy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144167309"
                        ],
                        "name": "U. Pal",
                        "slug": "U.-Pal",
                        "structuredName": {
                            "firstName": "Umapada",
                            "lastName": "Pal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Pal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143826881"
                        ],
                        "name": "J. Llad\u00f3s",
                        "slug": "J.-Llad\u00f3s",
                        "structuredName": {
                            "firstName": "Josep",
                            "lastName": "Llad\u00f3s",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Llad\u00f3s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2313892"
                        ],
                        "name": "Mathieu Delalandre",
                        "slug": "Mathieu-Delalandre",
                        "structuredName": {
                            "firstName": "Mathieu",
                            "lastName": "Delalandre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mathieu Delalandre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "Related works for scene text in camera-based images are [15], [16] and [17], [18] (which focus specifically on text orientation)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4382794,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "113c362f743b5b824d463feedda1ebabb322abe0",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a scheme towards the segmentation of English multi-oriented touching strings into individual characters. When two or more characters touch, they generate a big cavity region at the background portion. Using Convex Hull information, we use these background information to find some initial points to segment a touching string into possible primitive segments (a primitive segment consists of a single character or a part of a character). Next these primitive segments are merged to get optimum segmentation and dynamic programming is applied using total likelihood of characters as the objective function. SVM classifier is used to find the likelihood of a character. To consider multi-oriented touching strings the features used in the SVM are invariant to character orientation. Circular ring and convex hull ring based approach has been used along with angular information of the contour pixels of the character to make the feature rotation invariant. From the experiment, we obtained encouraging results."
            },
            "slug": "Multi-Oriented-and-Multi-Sized-Touching-Character-Roy-Pal",
            "title": {
                "fragments": [],
                "text": "Multi-Oriented and Multi-Sized Touching Character Segmentation Using Dynamic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A scheme towards the segmentation of English multi-oriented touching strings into individual characters using Convex Hull information, and dynamic programming is applied using total likelihood of characters as the objective function."
            },
            "venue": {
                "fragments": [],
                "text": "2009 10th International Conference on Document Analysis and Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155699044"
                        ],
                        "name": "Jing Zhang",
                        "slug": "Jing-Zhang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "Moreover, scene text is affected by lighting conditions and perspective distortions [1], [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 160
                            }
                        ],
                        "text": "This is because most of the nonhorizontal text lines are scene text, which is much more difficult to detect due to varying lighting and complex transformations [1], [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28161769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0567609da19ae90f1742800f1ff873b9f1bd411",
            "isKey": false,
            "numCitedBy": 195,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Text extraction in video documents, as an important research field of content-based information indexing and retrieval, has been developing rapidly since 1990s. This has led to much progress in text extraction, performance evaluation, and related applications. By reviewing the approaches proposed during the past five years, this paper introduces the progress made in this area and discusses promising directions for future research."
            },
            "slug": "Extraction-of-Text-Objects-in-Video-Documents:-Zhang-Kasturi",
            "title": {
                "fragments": [],
                "text": "Extraction of Text Objects in Video Documents: Recent Progress"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The progress made in text extraction in video documents is introduced and promising directions for future research are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "2008 The Eighth IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2435807"
                        ],
                        "name": "U. Bhattacharya",
                        "slug": "U.-Bhattacharya",
                        "structuredName": {
                            "firstName": "Ujjwal",
                            "lastName": "Bhattacharya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Bhattacharya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702114"
                        ],
                        "name": "S. K. Parui",
                        "slug": "S.-K.-Parui",
                        "structuredName": {
                            "firstName": "Swapan",
                            "lastName": "Parui",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. K. Parui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31968222"
                        ],
                        "name": "Srikanta Mondal",
                        "slug": "Srikanta-Mondal",
                        "structuredName": {
                            "firstName": "Srikanta",
                            "lastName": "Mondal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Srikanta Mondal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14041586,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a916e623ef3136e96c4fd918629d3f57b665808a",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "With the increasing popularity of digital cameras attached with various handheld devices, many new computational challenges have gained significance. One such problem is extraction of texts from natural scene images captured by such devices. The extracted text can be sent to OCR or to a text-to-speech engine for recognition. In this article, we propose a novel and effective scheme based on analysis of connected components for extraction of Devanagari and Bangla texts from camera captured scene images. A common unique feature of these two scripts is the presence of headline and the proposed scheme uses mathematical morphology operations for their extraction. Additionally, we consider a few criteria for robust filtering of text components from such scene images. Moreover, we studied the problem of binarization of such scene images and observed that there are situations when repeated binarization by a well-known global thresholding approach is effective. We tested our algorithm on a repository of 100 scene images containing texts of Devanagari and / or Bangla."
            },
            "slug": "Devanagari-and-Bangla-Text-Extraction-from-Natural-Bhattacharya-Parui",
            "title": {
                "fragments": [],
                "text": "Devanagari and Bangla Text Extraction from Natural Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel and effective scheme based on analysis of connected components for extraction of Devanagari and Bangla texts from camera captured scene images and it is observed that there are situations when repeated binarization by a well-known global thresholding approach is effective."
            },
            "venue": {
                "fragments": [],
                "text": "2009 10th International Conference on Document Analysis and Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1842569,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbf50fe5622253f401e892ed943a18033e18b7b9",
            "isKey": false,
            "numCitedBy": 318,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the results of the ICDAR 2005 competition for locating text in camera captured scenes. For this we used the same data as the ICDAR 2003 competition, which has been kept private until now. This allows a direct comparison with the 2003 entries. The main result is that the leading 2005 entry has improved significantly on the leading 2003 entry, with an increase in average f-score from 0.5 to 0.62, where the f-score is the same adapted information retrieval measure used for the 2003 competition. The paper also discusses the Web-based deployment and evaluation of text locating systems, and one of the leading entries has now been deployed in this way. This mode of usage could lead to more complete and more immediate knowledge of the strengths and weaknesses of each newly developed system."
            },
            "slug": "ICDAR-2005-text-locating-competition-results-Lucas",
            "title": {
                "fragments": [],
                "text": "ICDAR 2005 text locating competition results"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The main result is that the leading 2005 entry has improved significantly on the leading 2003 entry, with an increase in average f- score from 0.5 to 0.62, where the f-score is the same adapted information retrieval measure used for the 2003 competition."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "Related works for scene text in camera-based images are [15], [16] and [17], [18] (which focus specifically on text orientation)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Devenagari and Bangla Text Extraction from Natural Scene Images,"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Int\u2019l Conf. Document Analysis and Recognition,"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "However, the results may not be directly comparable because we use a more relaxed definition of recall than in [27] and perform evaluation at the block level instead of the word level, as explained in Section 3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 178
                            }
                        ],
                        "text": "The performance of the top two entries (denoted as ICDAR 1 and ICDAR 2) of the ICDAR 2005 Competition, which used the same data set as the 2003 competition, is also included for [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Text Locating Competition Results,"
            },
            "venue": {
                "fragments": [],
                "text": "\u201cICDAR"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib"
            },
            "venue": {
                "fragments": [],
                "text": "For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 175
                            }
                        ],
                        "text": "Multi-oriented text detection without any constraints on background, contrast, and orientation, and with high precision, and recall is still a challenging problem [14], [21], [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Wernickle, \u201cLocalizing and Segmenting Text in Images and Videos,"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Circuits and Systems for Video Technology,"
            },
            "year": 2002
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 15,
            "methodology": 12
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 29,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/A-Laplacian-Approach-to-Multi-Oriented-Text-in-Shivakumara-Phan/1755d99d89dee5915df1df4a7991b87138e93c78?sort=total-citations"
}