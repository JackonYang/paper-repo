{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14878957,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "17808fdaa56ca0227cea4990ecf36f5a1bc9bb58",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Hill climbing is used to maximize an information theoretic measure of the difference \nbetween the actual behavior of a unit and the behavior that would be predicted by a \nstatistician who knew the first order statistics of the inputs but believed them to be \nindependent. This causes the unit to detect higher order correlations among its inputs. \nInitial simulations are presented, and seem encouraging. We describe an extension of the \nbasic idea which makes it resemble competitive learning and which causes members of a \npopulation of these units to differentiate, each extracting different structure from the input."
            },
            "slug": "G-maximization:-An-unsupervised-learning-procedure-Pearlmutter-Hinton",
            "title": {
                "fragments": [],
                "text": "G-maximization: An unsupervised learning procedure for discovering regularities"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An extension of the basic idea which makes it resemble competitive learning and which causes members of a population of these units to differentiate, each extracting different structure from the input is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 107
                            }
                        ],
                        "text": "In other words, this method will tend to throw away redundant temporal information much as the systems in (Schmidhuber, 1992a) and (Schmidhuber, 1992b) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14426348,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f7c4048a03281e976f28d35c2f9fef3a58346e6",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Do you want your neural net algorithm to learn sequences? Do not limit yourself to conventional gradient descent (or approximations thereof). Instead, use your sequence learning algorithm (any will do) to implement the following method for history compression. No matter what your final goals are, train a network to predict its next input from the previous ones. Since only unpredictable inputs convey new information, ignore all predictable inputs but let all unexpected inputs (plus information about the time step at which they occurred) become inputs to a higher-level network of the same kind (working on a slower, self-adjusting time scale). Go on building a hierarchy of such networks. This principle reduces the descriptions of event sequences without loss of information, thus easing supervised or reinforcement learning tasks. Alternatively, you may use two recurrent networks to collapse a multi-level predictor hierarchy into a single recurrent net. Experiments show that systems based on these principles can require less computation per time step and many fewer training sequences than conventional training algorithms for recurrent nets. Finally you can modify the above method such that predictability is not defined in a yes-or-no fashion but in a continuous fashion."
            },
            "slug": "Learning-Unambiguous-Reduced-Sequence-Descriptions-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning Unambiguous Reduced Sequence Descriptions"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experiments show that systems based on these principles can require less computation per time step and many fewer training sequences than conventional training algorithms for recurrent nets."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144246983"
                        ],
                        "name": "H. Barlow",
                        "slug": "H.-Barlow",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Barlow",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Barlow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2315855"
                        ],
                        "name": "T. P. Kaushal",
                        "slug": "T.-P.-Kaushal",
                        "structuredName": {
                            "firstName": "Tej",
                            "lastName": "Kaushal",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. P. Kaushal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145666307"
                        ],
                        "name": "G. Mitchison",
                        "slug": "G.-Mitchison",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Mitchison",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mitchison"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 123
                            }
                        ],
                        "text": "Note that this method seemingly works diametrically opposite to the sequential, heuristic, non-neural methods described by Barlow et al. (1989), where the sum of bit entropies is minimized instead of being maximized."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 179
                            }
                        ],
                        "text": "It is interesting to note that with non-factorial codes predictability minimization prefers to reduce the number of used units instead of minimizing the sum of bit-entropies a la Barlow et al. (1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 77
                            }
                        ],
                        "text": "The focus of this paper, however, is on situations like the ones studied in (Barlow et. al, 1989): Noise-free environments and suucient representational capacity in the representational units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 3
                            }
                        ],
                        "text": "As Barlow, Kaushal, and Mitchison (1989) point out , with factorial codes the detection of dependence between two symbols indicates hitherto undiscovered associations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 72
                            }
                        ],
                        "text": "This notion is captured by the concept of`discovering factorial codes' (Barlow et al., 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 182
                            }
                        ],
                        "text": "Also, the purpose of this report is not to compare the performance of algorithms based on the novel principle to the performance of existing sequentia\u00ecnon-neural' heuristic methods (Barlow et. al, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 28107770,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5074f2219ddac70995f0a5e81ee2e892cb884b56",
            "isKey": true,
            "numCitedBy": 203,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "To determine whether a particular sensory event is a reliable predictor of reward or punishment it is necessary to know the prior probability of that event. If the variables of a sensory representation normally occur independently of each other, then it is possible to derive the prior probability of any logical function of the variables from the prior probabilities of the individual variables, without any additional knowledge; hence such a representation enormously enlarges the scope of definable events that can be searched for reliable predictors. Finding a Minimum Entropy Code is a possible method of forming such a representation, and methods for doing this are explored in this paper. The main results are (1) to show how to find such a code when the probabilities of the input states form a geometric progression, as is shown to be nearly true for keyboard characters in normal text; (2) to show how a Minimum Entropy Code can be approximated by repeatedly recoding pairs, triples, etc. of an original 7-bit code for keyboard characters; (3) to prove that in some cases enlarging the capacity of the output channel can lower the entropy."
            },
            "slug": "Finding-Minimum-Entropy-Codes-Barlow-Kaushal",
            "title": {
                "fragments": [],
                "text": "Finding Minimum Entropy Codes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Find a Minimum Entropy Code is a possible method of forming such a representation, and methods for doing this are explored, and the main results are to show how to find such a code when the probabilities of the input states form a geometric progression."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6999498"
                        ],
                        "name": "J. Rubner",
                        "slug": "J.-Rubner",
                        "structuredName": {
                            "firstName": "Jeanne",
                            "lastName": "Rubner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rubner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144340430"
                        ],
                        "name": "K. Schulten",
                        "slug": "K.-Schulten",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Schulten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schulten"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 136
                            }
                        ],
                        "text": "\u2026methods for decreasing output redundancy (e.g. Linsker (1988), Zemel and Hinton (1991), Oja (1989), Sanger (1989), FF oldii ak (1990), Rubner and Schulten (1990), Silva and Almeida (1991)) are restricted to the linear case and do not aim at the ambitious general goal of statistical\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31506754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "48d91d9dfb5cd4037565524c081e8718b1eb65fc",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a two-layered network of linear neurons that organizes itself as to extract the complete information contained in a set of presented patterns. The weights between layers obey a Hebbian rule. We propose a local anti-Hebbian rule for lateral, hierarchically organized weights within the output layer. This rule forces the activities of the output units to become uncorrelated and the lateral weights to vanish. The weights between layers converge to the eigenvectors of the covariance matrix of input patterns, i.e., the network performs a principal component analysis, yielding all principal components. As a consequence of the proposed learning scheme, the output units become detectors of orthogonal features, similar to ones found in the brain of mammals."
            },
            "slug": "Development-of-feature-detectors-by-A-network-Rubner-Schulten",
            "title": {
                "fragments": [],
                "text": "Development of feature detectors by self-organization. A network model."
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A two-layered network of linear neurons that organizes itself as to extract the complete information contained in a set of presented patterns, and the output units become detectors of orthogonal features, similar to ones found in the brain of mammals."
            },
            "venue": {
                "fragments": [],
                "text": "Biological cybernetics"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 107
                            }
                        ],
                        "text": "In other words, this method will tend to throw away redundant temporal information much as the systems in (Schmidhuber, 1992a) and (Schmidhuber, 1992b) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18271205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50c770b425a5bb25c77387f687a9910a9d130722",
            "isKey": false,
            "numCitedBy": 428,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous neural network learning algorithms for sequence processing are computationally expensive and perform poorly when it comes to long time lags. This paper first introduces a simple principle for reducing the descriptions of event sequences without loss of information. A consequence of this principle is that only unexpected inputs can be relevant. This insight leads to the construction of neural architectures that learn to divide and conquer by recursively decomposing sequences. I describe two architectures. The first functions as a self-organizing multilevel hierarchy of recurrent networks. The second, involving only two recurrent networks, tries to collapse a multilevel predictor hierarchy into a single recurrent net. Experiments show that the system can require less computation per time step and many fewer training sequences than conventional training algorithms for recurrent nets."
            },
            "slug": "Learning-Complex,-Extended-Sequences-Using-the-of-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning Complex, Extended Sequences Using the Principle of History Compression"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A simple principle for reducing the descriptions of event sequences without loss of information is introduced and this insight leads to the construction of neural architectures that learn to divide and conquer by recursively decomposing sequences."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 80
                            }
                        ],
                        "text": "Existing`neural' methods for decreasing output redundancy (e.g. Linsker (1988), Zemel and Hinton (1991), Oja (1989), Sanger (1989), FF oldii ak (1990), Rubner and Schulten (1990), Silva and Almeida (1991)) are restricted to the linear case and do not aim at the ambitious general goal of statistical\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16472149,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "852d3055ce81ef75b51dd9a406c0f5d056d270da",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Using an unsupervised learning procedure, a network is trained on an ensemble of images of the same two-dimensional object at different positions, orientations and sizes. Each half of the network \"sees\" one fragment of the object, and tries to produce as output a set of 4 parameters that have high mutual information with the 4 parameters output by the other half of the network. Given the ensemble of training patterns, the 4 parameters on which the two halves of the network can agree are the position, orientation, and size of the whole object, or some recoding of them. After training, the network can reject instances of other shapes by using the fact that the predictions made by its two halves disagree. If two competing networks are trained on an unlabelled mixture of images of two objects, they cluster the training cases on the basis of the objects' shapes, independently of the position, orientation, and size."
            },
            "slug": "Discovering-Viewpoint-Invariant-Relationships-That-Zemel-Hinton",
            "title": {
                "fragments": [],
                "text": "Discovering Viewpoint-Invariant Relationships That Characterize Objects"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "Using an unsupervised learning procedure, a network is trained on an ensemble of images of the same two-dimensional object at different positions, orientations and sizes, and can reject instances of other shapes by using the fact that the predictions made by its two halves disagree."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50333420"
                        ],
                        "name": "S. Becker",
                        "slug": "S.-Becker",
                        "structuredName": {
                            "firstName": "Suzanna",
                            "lastName": "Becker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Becker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 3
                            }
                        ],
                        "text": "As Becker (1991) observes, if a representation with uncorrelated components is used as the input to a higher-level linear supervised learning network, then the Hessian of the supervised network's error function is diagonal, thus allowing eecient methods for speeding up learning (note that\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42902861,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e44fdadb28105e07b7ba9e2294a09c240d170a9",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Supervised learning procedures for neural networks have recently met with considerable success in learning difficult mappings. However, their range of applicability is limited by their poor scaling behavior, lack of biological plausibility, and restriction to problems for which an external teacher is available. A promising alternative is to develop unsupervised learning algorithms which can adaptively learn to encode the statistical regularities of the input patterns, without being told explicitly the correct response for each pattern. In this paper, we describe the major approaches that have been taken to model unsupervised learning, and give an in-depth review of several examples of each approach."
            },
            "slug": "Unsupervised-Learning-Procedures-for-Neural-Becker",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning Procedures for Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper describes the major approaches that have been taken to model unsupervised learning, and gives an in-depth review of several examples of each approach."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Neural Syst."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 105
                            }
                        ],
                        "text": "Existing`neural' methods for decreasing output redundancy (e.g. Linsker (1988), Zemel and Hinton (1991), Oja (1989), Sanger (1989), FF oldii ak (1990), Rubner and Schulten (1990), Silva and Almeida (1991)) are restricted to the linear case and do not aim at the ambitious general goal of statistical\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207107700,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0eddb03e19bcf7555042508145426451da1d5c7f",
            "isKey": false,
            "numCitedBy": 907,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A single neuron with Hebbian-type learning for the connection weights, and with nonlinear internal feedback, has been shown to extract the statistical principal components of its stationary input pattern sequence. A generalization of this model to a layer of neuron units is given, called the Subspace Network, which yields a multi-dimensional, principal component subspace. This can be used as an associative memory for the input vectors or as a module in nonsupervised learning of data clusters in the input space. It is also able to realize a powerful pattern classifier based on projections on class subspaces. Some classification results for natural textures are given."
            },
            "slug": "Neural-Networks,-Principal-Components,-and-Oja",
            "title": {
                "fragments": [],
                "text": "Neural Networks, Principal Components, and Subspaces"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A single neuron with Hebbian-type learning for the connection weights, and with nonlinear internal feedback, has been shown to extract the statistical principal components of its stationary input pattern sequence, which yields a multi-dimensional, principal component subspace."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Neural Syst."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 281
                            }
                        ],
                        "text": "\u2026if the independence criterion is met: H = 1 2 X i X p P p i ? y i ] 2 : (2) This term for mutual predictability minimization aims at making the outputs independent { similar to the goal of a term for maximizing the determinant of the covariance matrix under Gaussian assumptions (Linsker, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 64
                            }
                        ],
                        "text": "In the latter case, reversibility is equivalent to Infomax a la Linsker (1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 64
                            }
                        ],
                        "text": "Existing`neural' methods for decreasing output redundancy (e.g. Linsker (1988), Zemel and Hinton (1991), Oja (1989), Sanger (1989), FF oldii ak (1990), Rubner and Schulten (1990), Silva and Almeida (1991)) are restricted to the linear case and do not aim at the ambitious general goal of statistical\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1527671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16d70e8af45ca0ae2c1bb73f3be6628518d40b8f",
            "isKey": false,
            "numCitedBy": 1417,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The emergence of a feature-analyzing function from the development rules of simple, multilayered networks is explored. It is shown that even a single developing cell of a layered network exhibits a remarkable set of optimization properties that are closely related to issues in statistics, theoretical physics, adaptive signal processing, the formation of knowledge representation in artificial intelligence, and information theory. The network studied is based on the visual system. These results are used to infer an information-theoretic principle that can be applied to the network as a whole, rather than a single cell. The organizing principle proposed is that the network connections develop in such a way as to maximize the amount of information that is preserved when signals are transformed at each processing stage, subject to certain constraints. The operation of this principle is illustrated for some simple cases.<<ETX>>"
            },
            "slug": "Self-organization-in-a-perceptual-network-Linsker",
            "title": {
                "fragments": [],
                "text": "Self-organization in a perceptual network"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "It is shown that even a single developing cell of a layered network exhibits a remarkable set of optimization properties that are closely related to issues in statistics, theoretical physics, adaptive signal processing, the formation of knowledge representation in artificial intelligence, and information theory."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807117"
                        ],
                        "name": "T. Sanger",
                        "slug": "T.-Sanger",
                        "structuredName": {
                            "firstName": "Terence",
                            "lastName": "Sanger",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sanger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 117
                            }
                        ],
                        "text": "Existing`neural' methods for decreasing output redundancy (e.g. Linsker (1988), Zemel and Hinton (1991), Oja (1989), Sanger (1989), FF oldii ak (1990), Rubner and Schulten (1990), Silva and Almeida (1991)) are restricted to the linear case and do not aim at the ambitious general goal of statistical\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 707975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3975117d907c0e582a35c34137231c87956aa93b",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an optimality principle for training an unsupervised feedforward neural network based upon maximal ability to reconstruct the input data from the network outputs. We describe an algorithm which can be used to train either linear or nonlinear networks with certain types of nonlinearity. Examples of applications to the problems of image coding, feature detection, and analysis of random-dot stereograms are presented."
            },
            "slug": "An-Optimality-Principle-for-Unsupervised-Learning-Sanger",
            "title": {
                "fragments": [],
                "text": "An Optimality Principle for Unsupervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "An optimality principle is proposed for training an unsupervised feedforward neural network based upon maximal ability to reconstruct the input data from the network outputs and an algorithm which can be used to train either linear or nonlinear networks with certain types of nonlinearity."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319833"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 172
                            }
                        ],
                        "text": "Each of the n predictors, the n representational modules, and the potentially available auto-associator can be implemented as a feed-forward back-propagation network (e.g. Werbos, 1974)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207975157,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "56623a496727d5c71491850e04512ddf4152b487",
            "isKey": false,
            "numCitedBy": 4468,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Beyond-Regression-:-\"New-Tools-for-Prediction-and-Werbos",
            "title": {
                "fragments": [],
                "text": "Beyond Regression : \"New Tools for Prediction and Analysis in the Behavioral Sciences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 164
                            }
                        ],
                        "text": "\u2026methods for decreasing output redundancy (e.g. Linsker (1988), Zemel and Hinton (1991), Oja (1989), Sanger (1989), FF oldii ak (1990), Rubner and Schulten (1990), Silva and Almeida (1991)) are restricted to the linear case and do not aim at the ambitious general goal of statistical independence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A distributed decorrelation algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Erol Gelenbe, editor, Neural Networks, Advances and Applications"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 130
                            }
                        ],
                        "text": "Daniel Prelinger and Jee Rink implemented on-line and oo-line systems based on section 6 (see details in (Schmidhuber, 1991) and (Prelinger, 1992))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 112
                            }
                        ],
                        "text": "An additional modiication for escaping certain cases of local minima was introduced (see Schmidhuber (1991) and Prelinger (1992) )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Diploma thesis, in preparation"
            },
            "venue": {
                "fragments": [],
                "text": "Diploma thesis, in preparation"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 206
                            }
                        ],
                        "text": "In addition, some of these methods require Gaussian assumptions about the input and output signals, as well as the explicit calculation of the derivative of the determinant of the output covariance matrix (Shannon, 1948)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A mathematical theory of communication (part III)"
            },
            "venue": {
                "fragments": [],
                "text": "Bell System Technical Journal"
            },
            "year": 1948
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Rink for implementing and testing the algorithms. Thanks to Mike Mozer Rich Zemel, and Clayton McMillan for valuable comments and suggestions which helped to improve the paper"
            },
            "venue": {
                "fragments": [],
                "text": "This research was supported by NSF PYI award IRI{9058450, grant 90{21 from the James S. McDonnell Foundation, and DEC external research grant 1250 to Michael C. Mozer. References"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 4,
            "methodology": 11
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 15,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-Factorial-Codes-by-Predictability-Schmidhuber/675d381653da0d2825ae37ab06069a1525fafb79?sort=total-citations"
}