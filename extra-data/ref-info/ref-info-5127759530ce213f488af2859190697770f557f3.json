{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736245"
                        ],
                        "name": "Laurenz Wiskott",
                        "slug": "Laurenz-Wiskott",
                        "structuredName": {
                            "firstName": "Laurenz",
                            "lastName": "Wiskott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laurenz Wiskott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3245558"
                        ],
                        "name": "P. Berkes",
                        "slug": "P.-Berkes",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Berkes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Berkes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13293531,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "48b7ad1e8131c37c07761768a84264ee7d30cd3b",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Slow feature analysis is an algorithm for extracting slowly varying features from a quickly varying signal. It has been shown in network simulations on one-dimensional stimuli that visual invariances to shift and other transformations can be learned in an unsupervised fashion based on slow feature analysis. More recently, we have shown that slow feature analysis applied to image sequences generated from natural images using a range of spatial transformations results in units that share many properties with complex and hypercomplex cells of the primary visual cortex. We find cells responsive to Gabor stimuli with phase invariance, sharpened or widened orientation or frequency tuning, secondary response lobes, end-stopping, and cells selective for direction of motion. These results indicate that slowness may be an important principle of self-organization in the visual cortex."
            },
            "slug": "Is-slowness-a-learning-principle-of-the-visual-Wiskott-Berkes",
            "title": {
                "fragments": [],
                "text": "Is slowness a learning principle of the visual cortex?"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The results indicate that slowness may be an important principle of self-organization in the visual cortex and applied to image sequences generated from natural images using a range of spatial transformations results in units that share many properties with complex and hypercomplex cells of the primary visual cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Zoology"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706109"
                        ],
                        "name": "H. Wersing",
                        "slug": "H.-Wersing",
                        "structuredName": {
                            "firstName": "Heiko",
                            "lastName": "Wersing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wersing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115105"
                        ],
                        "name": "E. K\u00f6rner",
                        "slug": "E.-K\u00f6rner",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "K\u00f6rner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. K\u00f6rner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3645746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d44f85fcc2eaa17e3c08313ad3ce70f8accf46fb",
            "isKey": false,
            "numCitedBy": 203,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "There is an ongoing debate over the capabilities of hierarchical neural feedforward architectures for performing real-world invariant object recognition. Although a variety of hierarchical models exists, appropriate supervised and unsupervised learning methods are still an issue of intense research. We propose a feedforward model for recognition that shares components like weight sharing, pooling stages, and competitive nonlinearities with earlier approaches but focuses on new methods for learning optimal feature-detecting cells in intermediate stages of the hierarchical network. We show that principles of sparse coding, which were previously mostly applied to the initial feature detection stages, can also be employed to obtain optimized intermediate complex features. We suggest a new approach to optimize the learning of sparse features under the constraints of a weight-sharing or convolutional architecture that uses pooling operations to achieve gradual invariance in the feature hierarchy. The approach explicitly enforces symmetry constraints like translation invariance on the feature set. This leads to a dimension reduction in the search space of optimal features and allows determining more efficiently the basis representatives, which achieve a sparse decomposition of the input. We analyze the quality of the learned feature representation by investigating the recognition performance of the resulting hierarchical network on object and face databases. We show that a hierarchy with features learned on a single object data set can also be applied to face recognition without parameter changes and is competitive with other recent machine learning recognition approaches. To investigate the effect of the interplay between sparse coding and processing nonlinearities, we also consider alternative feedforward pooling nonlinearities such as presynaptic maximum selection and sum-of-squares integration. The comparison shows that a combination of strong competitive nonlinearities with sparse coding offers the best recognition performance in the difficult scenario of segmentation-free recognition in cluttered surround. We demonstrate that for both learning and recognition, a precise segmentation of the objects is not necessary."
            },
            "slug": "Learning-Optimized-Features-for-Hierarchical-Models-Wersing-K\u00f6rner",
            "title": {
                "fragments": [],
                "text": "Learning Optimized Features for Hierarchical Models of Invariant Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a feedforward model for recognition that shares components like weight sharing, pooling stages, and competitive nonlinearities with earlier approaches but focuses on new methods for learning optimal feature-detecting cells in intermediate stages of the hierarchical network."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736245"
                        ],
                        "name": "Laurenz Wiskott",
                        "slug": "Laurenz-Wiskott",
                        "structuredName": {
                            "firstName": "Laurenz",
                            "lastName": "Wiskott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laurenz Wiskott"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 885423,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d294311809d993f8a4c1eb30d17e9e7bf221829",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Temporal slowness is a learning principle that allows learning of invariant representations by extracting slowly varying features from quickly varying input signals. Slow feature analysis (SFA) is an efficient algorithm based on this principle and has been applied to the learning of translation, scale, and other invariances in a simple model of the visual system. Here, a theoretical analysis of the optimization problem solved by SFA is presented, which provides a deeper understanding of the simulation results obtained in previous studies."
            },
            "slug": "Slow-Feature-Analysis:-A-Theoretical-Analysis-of-Wiskott",
            "title": {
                "fragments": [],
                "text": "Slow Feature Analysis: A Theoretical Analysis of Optimal Free Responses"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A theoretical analysis of the optimization problem solved by SFA is presented, which provides a deeper understanding of the simulation results obtained in previous studies."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804982"
                        ],
                        "name": "James V. Stone",
                        "slug": "James-V.-Stone",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Stone",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James V. Stone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2832052"
                        ],
                        "name": "Alistair J. Bray",
                        "slug": "Alistair-J.-Bray",
                        "structuredName": {
                            "firstName": "Alistair",
                            "lastName": "Bray",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alistair J. Bray"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Stone and Bray (1995) have presented a learning rule that is based on an objective function similar to that of Becker and Hinton (1992) or equation 2.1 and which includes a memory trace mechanism as in (Fo\u0308ldia\u0301k 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 17
                            }
                        ],
                        "text": "The examples in (Stone and Bray 1995) are linearly solvable so that only linear units are used."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121620192,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3f93b18ae4cb75d73019b2bf267f2861e02f4cd6",
            "isKey": true,
            "numCitedBy": 59,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The inputs to photoreceptors tend to change rapidly over time, whereas physical parameters (e.g. surface depth) underlying these changes vary more slowly. Accordingly, if a neuron codes for a physical parameter then its output should also change slowly, despite its rapidly fluctuating inputs. We demonstrate that a model neuron which adapts to make its output vary smoothly over time can learn to extract invariances implicit in its input. This learning consists of a linear combination of Hebbian and anti-Hebbian synaptic changes, operating simultaneously upon the same connection weights but at different time scales. This is shown to be sufficient for the unsupervised learning of simple spatio-temporal invariances."
            },
            "slug": "A-learning-rule-for-extracting-spatio-temporal-Stone-Bray",
            "title": {
                "fragments": [],
                "text": "A learning rule for extracting spatio-temporal invariances"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is demonstrated that a model neuron which adapts to make its output vary smoothly over time can learn to extract invariances implicit in its input, using a linear combination of Hebbian and anti-Hebbian synaptic changes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34993891"
                        ],
                        "name": "W. Konen",
                        "slug": "W.-Konen",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Konen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Konen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055501916"
                        ],
                        "name": "Thomas Maurer",
                        "slug": "Thomas-Maurer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Maurer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Maurer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704573"
                        ],
                        "name": "C. Malsburg",
                        "slug": "C.-Malsburg",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Malsburg",
                            "middleNames": [
                                "von",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Malsburg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 266
                            }
                        ],
                        "text": "These two types of systems are trained or applied to static images, and the invariances, such as translation or size invariance, need to be known in advance by the designer of the system (dynamic link matching is less strict in specifying the invariance in advance; Konen et al., 1994)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 676612,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76662392a856beb879cc4378cac6282c38c12e45",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-fast-dynamic-link-matching-algorithm-for-pattern-Konen-Maurer",
            "title": {
                "fragments": [],
                "text": "A fast dynamic link matching algorithm for invariant pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1917767"
                        ],
                        "name": "W. Einh\u00e4user",
                        "slug": "W.-Einh\u00e4user",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Einh\u00e4user",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Einh\u00e4user"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144679212"
                        ],
                        "name": "C. Kayser",
                        "slug": "C.-Kayser",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Kayser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kayser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3282030"
                        ],
                        "name": "Konrad Paul Kording",
                        "slug": "Konrad-Paul-Kording",
                        "structuredName": {
                            "firstName": "Konrad",
                            "lastName": "Kording",
                            "middleNames": [
                                "Paul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Konrad Paul Kording"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40089171"
                        ],
                        "name": "P. K\u00f6nig",
                        "slug": "P.-K\u00f6nig",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "K\u00f6nig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. K\u00f6nig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18293373,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "eaa519bdb742ec2e9a91a6e58646a4be3988841f",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Many biological and artificial neural networks require the parallel extraction of multiple features, and meet this requirement with distinct populations of neurons that are selective to one property of the stimulus while being non-selective to another property. In this way, several populations can resolve a set of features independently of each other, and thus achieve a parallel mode of processing. This raises the question how an initially homogeneous population of neurons segregates into groups with distinct and complementary response properties. Using a colour image sequence recorded from a camera mounted on the head of a freely behaving cat, we train a network of neurons to achieve optimally stable responses, that is, responses that change minimally over time. This objective leads to the development of colour-selective neurons. Adding a second objective, decorrelating activity within the network, a subpopulation of neurons develops with achromatic response properties. Colour selective neurons tend to be non-oriented while achromatic neurons are orientation-tuned. The proposed objective thus successfully leads to the segregation of neurons into complementary populations that are either selective for colour or orientation."
            },
            "slug": "Learning-Distinct-and-Complementary-Feature-from-Einh\u00e4user-Kayser",
            "title": {
                "fragments": [],
                "text": "Learning Distinct and Complementary Feature Selectivities from Natural Colour Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work uses a colour image sequence recorded from a camera mounted on the head of a freely behaving cat to train a network of neurons to achieve optimally stable responses, that is, responses that change minimally over time, and develops colour-selective neurons."
            },
            "venue": {
                "fragments": [],
                "text": "Reviews in the neurosciences"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1858054"
                        ],
                        "name": "P. F\u00f6ldi\u00e1k",
                        "slug": "P.-F\u00f6ldi\u00e1k",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "F\u00f6ldi\u00e1k",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. F\u00f6ldi\u00e1k"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2175819,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2da4e9984a75ffe28c5364662807996ac5bb2662",
            "isKey": false,
            "numCitedBy": 698,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The visual system can reliably identify objects even when the retinal image is transformed considerably by commonly occurring changes in the environment. A local learning rule is proposed, which allows a network to learn to generalize across such transformations. During the learning phase, the network is exposed to temporal sequences of patterns undergoing the transformation. An application of the algorithm is presented in which the network learns invariance to shift in retinal position. Such a principle may be involved in the development of the characteristic shift invariance property of complex cells in the primary visual cortex, and also in the development of more complicated invariance properties of neurons in higher visual areas."
            },
            "slug": "Learning-Invariance-from-Transformation-Sequences-F\u00f6ldi\u00e1k",
            "title": {
                "fragments": [],
                "text": "Learning Invariance from Transformation Sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An application of the algorithm is presented in which the network learns invariance to shift in retinal position, which may be involved in the development of the characteristic shift invariance property of complex cells in the primary visual cortex and also in theDevelopment of more complicated invariance properties of neurons in higher visual areas."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Comput."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721722"
                        ],
                        "name": "N. Kr\u00fcger",
                        "slug": "N.-Kr\u00fcger",
                        "structuredName": {
                            "firstName": "Norbert",
                            "lastName": "Kr\u00fcger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kr\u00fcger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714016"
                        ],
                        "name": "F. W\u00f6rg\u00f6tter",
                        "slug": "F.-W\u00f6rg\u00f6tter",
                        "structuredName": {
                            "firstName": "Florentin",
                            "lastName": "W\u00f6rg\u00f6tter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. W\u00f6rg\u00f6tter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 221288079,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbe4f866408e3678ce7cba95f8dda385876b2115",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we address the statistics of second-order relations of feature vectors derived from image sequences. We compute the individual vector components corresponding to the visual modalities orientation, contrast transition, optic flow, and colour by conventional low-level early vision algorithms. As a main result, we observe that collinear (or parallel) line pairs are, with very great likelihood, also associated with other identical features, for example sharing the same flow pattern, or colour or even sharing multiple feature combinations.It is known that low level processes, such as edge detection, optic flow estimation and stereo are ambiguous. Our results provide support for the assumption that the ambiguity of low level processes can be substantially reduced by integrating information across visual modalities. Furthermore, the attempt to model the application of Gestalt laws in computer vision systems based on statistical measurements, as suggested recently by some researchers (Kruger N 1998 ..."
            },
            "slug": "Multi-modal-estimation-of-collinearity-and-in-image-Kr\u00fcger-W\u00f6rg\u00f6tter",
            "title": {
                "fragments": [],
                "text": "Multi-modal estimation of collinearity and parallelism in natural image sequences."
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This paper addresses the statistics of second-order relations of feature vectors derived from image sequences and observes that collinear line pairs are, with very great likelihood, also associated with other identical features, for example sharing the same flow pattern."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50333420"
                        ],
                        "name": "S. Becker",
                        "slug": "S.-Becker",
                        "structuredName": {
                            "firstName": "Suzanna",
                            "lastName": "Becker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Becker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15063686,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f1c69f4dcd7ceea6363367822ed845015e5836a",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Supervised learning procedures for neural networks have recently met with considerable success in learning di cult mappings. So far, however, they have been limited by their poor scaling behaviour, particularly for networks with many hidden layers. A promising alternative is to develop unsupervised learning algorithms by de ning objective functions that characterize the quality of an internal representation without requiring knowledge of the desired outputs of the system. Our major goal is to build self-organizing network modules which capture important regularities in the environment in a simple form. A layered hierarchy of such modules should be able to learn in a time roughly linear in the number of layers. We propose that a good objective for perceptual learning is to extract higher-order features that exhibit simple coherence across time or space. This can be done by transforming the input representation into an underlying representation in which the mutual information between adjacent patches of the input can be expressed in a simple way. We have applied this basic idea to develop several interesting learning algorithms for discovering spatially coherent features in images. Our simulations show that a network can discover depth of surfaces when trained on binary random dot stereograms with discrete global shifts, as well as on real-valued stereograms of surfaces with continuously varying disparities. Once a module of depth-tuned units has developed, we show that units in a higher layer can discover a simple form of surface interpolation of curved surfaces, by learning to predict the depth of one image region based on depth measurements in surrounding regions."
            },
            "slug": "Spatial-coherence-as-an-internal-teacher-for-a-Becker-Hinton",
            "title": {
                "fragments": [],
                "text": "Spatial coherence as an internal teacher for a neural network"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes that a good objective for perceptual learning is to extract higher-order features that exhibit simple coherence across time or space, and applies this basic idea to develop several interesting learning algorithms for discovering spatially coherent features in images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804982"
                        ],
                        "name": "James V. Stone",
                        "slug": "James-V.-Stone",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Stone",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James V. Stone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 182
                            }
                        ],
                        "text": "The approach described in this article belongs to a third class of systems based on learning invariances from temporal input sequences (Fo\u0308ldia\u0301k, 1991; Mitchison, 1991; Becker, 1993; Stone, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 13
                            }
                        ],
                        "text": "Examples in (Stone 1996) are concerned with disparity estimation, which is not linearly solvable and requires a multilayer network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 12
                            }
                        ],
                        "text": "Examples in (Stone 1996) are concerned with disparity estimation, which isnot linearly solvable and requires a multilayer network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 135
                            }
                        ],
                        "text": "The approach described in this article belongs to a third class of systems based on learning invariances from temporal input sequences (F\u00f6ldi \u00c2 ak, 1991; Mitchison, 1991; Becker, 1993; Stone, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41064508,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0146ae3caabdcc083ce903ae34d92f4acaa3b0b",
            "isKey": true,
            "numCitedBy": 88,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "A model is presented for unsupervised learning of low level vision tasks, such as the extraction of surface depth. A key assumption is that perceptually salient visual parameters (e.g., surface depth) vary smoothly over time. This assumption is used to derive a learning rule that maximizes the long-term variance of each unit's outputs, whilst simultaneously minimizing its short-term variance. The length of the half-life associated with each of these variances is not critical to the success of the algorithm. The learning rule involves a linear combination of anti-Hebbian and Hebbian weight changes, over short and long time scales, respectively. This maximizes the information throughput with respect to low-frequency parameters implicit in the input sequence. The model is used to learn stereo disparity from temporal sequences of random-dot and gray-level stereograms containing synthetically generated subpixel disparities. The presence of temporal discontinuities in disparity does not prevent learning or generalization to previously unseen image sequences. The implications of this class of unsupervised methods for learning in perceptual systems are discussed."
            },
            "slug": "Learning-Perceptually-Salient-Visual-Parameters-Stone",
            "title": {
                "fragments": [],
                "text": "Learning Perceptually Salient Visual Parameters Using Spatiotemporal Smoothness Constraints"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A model is presented for unsupervised learning of low level vision tasks, such as the extraction of surface depth, that involves a linear combination of anti-Hebbian and Hebbian weight changes, over short and long time scales, respectively, which maximizes the information throughput with respect to low-frequency parameters implicit in the input sequence."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2218905"
                        ],
                        "name": "M. Bartlett",
                        "slug": "M.-Bartlett",
                        "structuredName": {
                            "firstName": "Marian",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "Stewart"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 208
                            }
                        ],
                        "text": "Closely related models were also presented in (Barrow and Bray 1992, O\u2019Reilly and Johnson 1994, and Fukushima 1999) and applied to learning lighting and orientation invariances in face recognition in (Stewart Bartlett and Sejnowski 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6014414,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a300b160559ca80962e5e895956185e08baf52da",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": "In natural visual experience, different views of an object or face tend to appear in close temporal proximity as an animal manipulates the object or navigates around it, or as a face changes expression or pose. A set of simulations is presented which demonstrate how viewpoint-invariant representations of faces can be developed from visual experience by capturing the temporal relationships among the input patterns. The simulations explored the interaction of temporal smoothing of activity signals with Hebbian learning in both a feedforward layer and a second, recurrent layer of a network. The feedforward connections were trained by competitive Hebbian learning with temporal smoothing of the post-synaptic unit activities. The recurrent layer was a generalization of a Hopfield network with a low-pass temporal filter on all unit activities. The combination of basic Hebbian learning with temporal smoothing of unit activities produced an attractor network learning rule that associated temporally proximal input patterns into basins of attraction. These two mechanisms were demonstrated in a model that took grey-level images of faces as input. Following training on image sequences of faces as they changed pose, multiple views of a given face fell into the same basin of attraction, and the system acquired representations of faces that were approximately viewpoint-invariant."
            },
            "slug": "Learning-viewpoint-invariant-face-representations-Bartlett-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning viewpoint-invariant face representations from visual experience in an attractor network."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A set of simulations is presented which demonstrate how viewpoint-invariant representations of faces can be developed from visual experience by capturing the temporal relationships among the input patterns."
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50333420"
                        ],
                        "name": "S. Becker",
                        "slug": "S.-Becker",
                        "structuredName": {
                            "firstName": "Suzanna",
                            "lastName": "Becker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Becker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 22
                            }
                        ],
                        "text": "For an overview, see (Becker 1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3523845,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "705ac0caa9b3704b09758cb12a0d2b9e34f27767",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Unsupervised learning procedures based on Hebbian principles have been successful at modelling low-level feature extraction, but are insufficient for learning to recognize higher- order features and complex objects. In this paper we explore a class of unsupervised learning algorithms called Imax (Becker and Hinton 1992 Nature 355 161-3) that are derived from information-theoretic principles. The Imax algorithms are based on the idea of maximizing the mutual information between the outputs of different network modules, and are capable of extracting higher-order features from data. They are therefore well suited to modelling intermediate-to-high-level perceptual processing stages. We substantiate this claim with some novel results for two signal classification problems, as well as by reviewing some previously published results and several related approaches. Finally, Imax is evaluated with respect to computational costs and biological plausibility."
            },
            "slug": "Mutual-information-maximization:-models-of-cortical-Becker",
            "title": {
                "fragments": [],
                "text": "Mutual information maximization: models of cortical self-organization."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The Imax algorithms are based on the idea of maximizing the mutual information between the outputs of different network modules, and are capable of extracting higher-order features from data and are well suited to modelling intermediate-to-high-level perceptual processing stages."
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3352381"
                        ],
                        "name": "W. Hashimoto",
                        "slug": "W.-Hashimoto",
                        "structuredName": {
                            "firstName": "Wakako",
                            "lastName": "Hashimoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hashimoto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 221263824,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90a8293a7d2d373de7fdcb190ec946db4f55b709",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Several studies have succeeded in correlating natural image statistics with receptive field properties of neurons in the primary visual cortex. If we determine the parameters of linear transformations that make their output values as independent as possible when input data are natural images, we obtain parameter values that correspond to simple cell characteristics. It was also proved that, by making output values as temporally coherent as possible, simple cell characteristics also emerge. However, complex cell properties have not been fully explained by previous studies of natural image statistics. In this study, we examine whether we could reproduce complex cell properties by determining the parameters of two-layer networks that make their outputs as independent and sparse as possible or as temporally coherent as possible. Input\u2013output functions of two-layer networks correspond to quadratic forms and they form a class of functions that includes complex cell responses and many other functions. Therefore,..."
            },
            "slug": "Quadratic-forms-in-natural-images.-Hashimoto",
            "title": {
                "fragments": [],
                "text": "Quadratic forms in natural images."
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This study examines whether it could be possible to reproduce complex cell properties by determining the parameters of two-layer networks that make their outputs as independent and sparse as possible or as temporally coherent as possible."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390067049"
                        ],
                        "name": "R. O\u2019Reilly",
                        "slug": "R.-O\u2019Reilly",
                        "structuredName": {
                            "firstName": "Randall",
                            "lastName": "O\u2019Reilly",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. O\u2019Reilly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145546235"
                        ],
                        "name": "Mark H. Johnson",
                        "slug": "Mark-H.-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark H. Johnson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 69
                            }
                        ],
                        "text": "Closely related models were also presented in (Barrow and Bray 1992, O\u2019Reilly and Johnson 1994, and Fukushima 1999) and applied to learning lighting and orientation invariances in face recognition in (Stewart Bartlett and Sejnowski 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20614446,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "73964bd03426b577adb13e0c447554db33ab714f",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Using neural and behavioral constraints from a relatively simple biological visual system, we evaluate the mechanism and behavioral implications of a model of invariant object recognition. Evidence from a variety of methods suggests that a localized portion of the domestic chick brain, the intermediate and medial hyperstriatum ventrale (IMHV), is critical for object recognition. We have developed a neural network model of translation-invariant object recognition that incorporates features of the neural circuitry of IMHV, and exhibits behavior qualitatively similar to a range of findings in the filial imprinting paradigm. We derive several counter-intuitive behavioral predictions that depend critically upon the biologically derived features of the model. In particular, we propose that the recurrent excitatory and lateral inhibitory circuitry in the model, and observed in IMHV, produces hysteresis on the activation state of the units in the model and the principal excitatory neurons in IMHV. Hysteresis, when combined with a simple Hebbian covariance learning mechanism, has been shown in this and earlier work (Fldik 1991; O'Reilly and McClelland 1992) to produce translation-invariant visual representations. The hysteresis and learning rule are responsible for a sensitive period phenomenon in the network, and for a series of novel temporal blending phenomena. These effects are empirically testable. Further, physiological and anatomical features of mammalian visual cortex support a hysteresis-based mechanism, arguing for the generality of the algorithm."
            },
            "slug": "Object-Recognition-and-Sensitive-Periods:-A-of-O\u2019Reilly-Johnson",
            "title": {
                "fragments": [],
                "text": "Object Recognition and Sensitive Periods: A Computational Analysis of Visual Imprinting"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A neural network model of translation-invariant object recognition that incorporates features of the neural circuitry of IMHV, and exhibits behavior qualitatively similar to a range of findings in the filial imprinting paradigm is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "80317385"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Olshausen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143718183"
                        ],
                        "name": "C. Anderson",
                        "slug": "C.-Anderson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Anderson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1392063491"
                        ],
                        "name": "D. van Essen",
                        "slug": "D.-van-Essen",
                        "structuredName": {
                            "firstName": "D. C.",
                            "lastName": "van Essen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. van Essen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1118263,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "700bbcd3518ca8cb3dac50a89fc69cad3dc1a579",
            "isKey": false,
            "numCitedBy": 948,
            "numCiting": 146,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a biologically plausible model of an attentional mechanism for forming position- and scale-invariant representations of objects in the visual world. The model relies on a set of control neurons to dynamically modify the synaptic strengths of intracortical connections so that information from a windowed region of primary visual cortex (V1) is selectively routed to higher cortical areas. Local spatial relationships (i.e., topography) within the attentional window are preserved as information is routed through the cortex. This enables attended objects to be represented in higher cortical areas within an object-centered reference frame that is position and scale invariant. We hypothesize that the pulvinar may provide the control signals for routing information through the cortex. The dynamics of the control neurons are governed by simple differential equations that could be realized by neurobiologically plausible circuits. In preattentive mode, the control neurons receive their input from a low-level \u201csaliency map\u201d representing potentially interesting regions of a scene. During the pattern recognition phase, control neurons are driven by the interaction between top-down (memory) and bottom-up (retinal input) sources. The model respects key neurophysiological, neuroanatomical, and psychophysical data relating to attention, and it makes a variety of experimentally testable predictions."
            },
            "slug": "A-neurobiological-model-of-visual-attention-and-on-Olshausen-Anderson",
            "title": {
                "fragments": [],
                "text": "A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A biologically plausible model of an attentional mechanism for forming position- and scale-invariant representations of objects in the visual world that respects key neurophysiological, neuroanatomical, and psychophysical data relating to attention, and it makes a variety of experimentally testable predictions."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of neuroscience : the official journal of the Society for Neuroscience"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3160228"
                        ],
                        "name": "K. Fukushima",
                        "slug": "K.-Fukushima",
                        "structuredName": {
                            "firstName": "Kunihiko",
                            "lastName": "Fukushima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukushima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096049539"
                        ],
                        "name": "Kazuya Yoshimoto",
                        "slug": "Kazuya-Yoshimoto",
                        "structuredName": {
                            "firstName": "Kazuya",
                            "lastName": "Yoshimoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kazuya Yoshimoto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 99
                            }
                        ],
                        "text": "Closely related models were also presented in (Barrow and Bray 1992, O\u2019Reilly and Johnson 1994, and Fukushima 1999) and applied to learning lighting and orientation invariances in face recognition in (Stewart Bartlett and Sejnowski 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15859673,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f46f42b7c76bfc59423aaafd72de34411c91eaa7",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new learning rule by which cells with shift-invariant receptive fields are self-organized. Namely, cells similar to simple and complex cells in the primary visual cortex are generated in a network trained by the new leaning rule. To demonstrate the new learning rule, we simulate a three-layered network that consists of an input layer (retina), a layer of S-cells (simple cells), and a layer of C-cells (complex cells). During the learning, straight lines of various orientations sweep across the input layer Both S- and C-cells are created through competition. Although S-cells compete depending on their instantaneous outputs, C-cells compete depending on the traces (or temporal averages) of their outputs. For the self-organization of S-cells, only winner S-cells have LTP (long term potentiation) in their input connections. For the self-organization of S-cells, however, loser S-cells have LTD (long term depression) in their input connections, while winners have LTP. Both S- and C-cells are accompanied by inhibitory cells. Modification of inhibitory connections together with excitatory connections is important for creation of C-cells as well as S-cells."
            },
            "slug": "Self-organization-of-shift-invariant-receptive-Fukushima-Yoshimoto",
            "title": {
                "fragments": [],
                "text": "Self-organization of shift-invariant receptive fields"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Cells similar to simple and complex cells in the primary visual cortex are generated in a network trained by the new leaning rule, by which cells with shift-invariant receptive fields are self-organized."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50333420"
                        ],
                        "name": "S. Becker",
                        "slug": "S.-Becker",
                        "structuredName": {
                            "firstName": "Suzanna",
                            "lastName": "Becker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Becker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Becker and Hinton (1992) trained a network to discover disparity as an invariant variable of random-dot stereograms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 111
                            }
                        ],
                        "text": "Stone and Bray (1995) have presented a learning rule that is based on an objective function similar to that of Becker and Hinton (1992) or equation 2.1 and which includes a memory trace mechanism as in (Fo\u0308ldia\u0301k 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 33
                            }
                        ],
                        "text": "This is the formulation used in (Becker and Hinton 1992)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4332326,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c85b7fe70dda0adbbd7630e2a341a904c74fbd2",
            "isKey": true,
            "numCitedBy": 408,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "THE standard form of back-propagation learning1 is implausible as a model of perceptual learning because it requires an external teacher to specify the desired output of the network. We show how the external teacher can be replaced by internally derived teaching signals. These signals are generated by using the assumption that different parts of the perceptual input have common causes in the external world. Small modules that look at separate but related parts of the perceptual input discover these common causes by striving to produce outputs that agree with each other (Fig. la). The modules may look at different modalities (such as vision and touch), or the same modality at different times (for example, the consecutive two-dimensional views of a rotating three-dimensional object), or even spatially adjacent parts of the same image. Our simulations show that when our learning procedure is applied to adjacent patches of two-dimensional images, it allows a neural network that has no prior knowledge of the third dimension to discover depth in random dot stereograms of curved surfaces."
            },
            "slug": "Self-organizing-neural-network-that-discovers-in-Becker-Hinton",
            "title": {
                "fragments": [],
                "text": "Self-organizing neural network that discovers surfaces in random-dot stereograms"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The authors' simulations show that when the learning procedure is applied to adjacent patches of two-dimensional images, it allows a neural network that has no prior knowledge of the third dimension to discover depth in random dot stereograms of curved surfaces."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145666307"
                        ],
                        "name": "G. Mitchison",
                        "slug": "G.-Mitchison",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Mitchison",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mitchison"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 151
                            }
                        ],
                        "text": "The approach described in this article belongs to a third class of systems based on learning invariances from temporal input sequences (Fo\u0308ldia\u0301k, 1991; Mitchison, 1991; Becker, 1993; Stone, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Mitchison (1991) presented a learning rule for linear units that is also derived from an objective function like that of equation 2.1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 102
                            }
                        ],
                        "text": "The solution is the normed eigenvector of matrix \u3008z\u0307z\u0307T\u3009 that corresponds to the smallest eigenvalue (cf. Mitchison, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 135
                            }
                        ],
                        "text": "The approach described in this article belongs to a third class of systems based on learning invariances from temporal input sequences (F\u00f6ldi \u00c2 ak, 1991; Mitchison, 1991; Becker, 1993; Stone, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13531615,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2eadcb6fb0c285ef361fcec080979dc336e1df79",
            "isKey": true,
            "numCitedBy": 76,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "I describe a local synaptic learning rule that can be used to remove the effects of certain types of systematic temporal variation in the inputs to a unit. According to this rule, changes in synaptic weight result from a conjunction of short-term temporal changes in the inputs and the output. Formally, This is like the differential rule proposed by Klopf (1986) and Kosko (1986), except for a change of sign, which gives it an anti-Hebbian character. By itself this rule is insufficient. A weight conservation condition is needed to prevent the weights from collapsing to zero, and some further constraintimplemented here by a biasing termto select particular sets of weights from the subspace of those which give minimal variation. As an example, I show that this rule will generate center-surround receptive fields that remove temporally varying linear gradients from the inputs."
            },
            "slug": "Removing-Time-Variation-with-the-Anti-Hebbian-Mitchison",
            "title": {
                "fragments": [],
                "text": "Removing Time Variation with the Anti-Hebbian Differential Synapse"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A local synaptic learning rule is described that can be used to remove the effects of certain types of systematic temporal variation in the inputs to a unit and will generate center-surround receptive fields that remove temporally varying linear gradients from the inputs."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 104
                            }
                        ],
                        "text": "It would be interesting to investigate whether SFA can be combined with independent component analysis (Bell & Sejnowski, 1995) to extract the truly independent slow features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "It would be interesting to investigate whether SFA can be combined with independent component analysis ( Bell & Sejnowski, 1995 ) to extract the truly independent slow features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1701422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d7d0e8c4791700defd4b0df82a26b50055346e0",
            "isKey": false,
            "numCitedBy": 8757,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing."
            },
            "slug": "An-Information-Maximization-Approach-to-Blind-and-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that information maximization provides a unifying framework for problems in \"blind\" signal processing and dependencies of information transfer on time delays are derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793739"
                        ],
                        "name": "David J. Fleet",
                        "slug": "David-J.-Fleet",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fleet",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Fleet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723930"
                        ],
                        "name": "A. Jepson",
                        "slug": "A.-Jepson",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Jepson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Jepson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 219
                            }
                        ],
                        "text": "Just as the phase difference between corresponding simple cells of different eyes can be used to estimate disparity, phase changes over time can be used to estimate direction and speed of motion of the visual stimulus (Fleet & Jepson, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 27667388,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1e91e17ee1135bcdfc03a0c56fae3affe7812d8b",
            "isKey": false,
            "numCitedBy": 831,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a technique for the computation of 2D component velocity from image sequences. Initially, the image sequence is represented by a family of spatiotemporal velocity-tuned linear filters. Component velocity, computed from spatiotemporal responses of identically tuned filters, is expressed in terms of the local first-order behavior of surfaces of constant phase. Justification for this definition is discussed from the perspectives of both 2D image translation and deviations from translation that are typical in perspective projections of 3D scenes. The resulting technique is predominantly linear, efficient, and suitable for parallel processing. Moreover, it is local in space-time, robust with respect to noise, and permits multiple estimates within a single neighborhood. Promising quantiative results are reported from experiments with realistic image sequences, including cases with sizeable perspective deformation."
            },
            "slug": "Computation-of-component-image-velocity-from-local-Fleet-Jepson",
            "title": {
                "fragments": [],
                "text": "Computation of component image velocity from local phase information"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The resulting technique is predominantly linear, efficient, and suitable for parallel processing, and is local in space-time, robust with respect to noise, and permits multiple estimates within a single neighborhood."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3346683"
                        ],
                        "name": "J. Hurri",
                        "slug": "J.-Hurri",
                        "structuredName": {
                            "firstName": "Jarmo",
                            "lastName": "Hurri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hurri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7867966,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "53678a7d6ed45901071fc49c863870dec6b7c782",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, statistical models of natural images have shown the emergence of several properties of the visual cortex. Most models have considered the nongaussian properties of static image patches, leading to sparse coding or independent component analysis. Here we consider the basic time dependencies of image sequences instead of their nongaussianity. We show that simple-cell-type receptive fields emerge when temporal response strength correlation is maximized for natural image sequences. Thus, temporal response strength correlation, which is a nonlinear measure of temporal coherence, provides an alternative to sparseness in modeling simple-cell receptive field properties. Our results also suggest an interpretation of simple cells in terms of invariant coding principles, which have previously been used to explain complex-cell receptive fields."
            },
            "slug": "Simple-Cell-Like-Receptive-Fields-Maximize-Temporal-Hurri-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "Simple-Cell-Like Receptive Fields Maximize Temporal Coherence in Natural Video"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that simple-cell-type receptive fields emerge when temporal response strength correlation is maximized for natural image sequences, and suggests an interpretation of simple cells in terms of invariant coding principles, which have previously been used to explain complex-cell receptive fields."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144316140"
                        ],
                        "name": "G. Wallis",
                        "slug": "G.-Wallis",
                        "structuredName": {
                            "firstName": "Guy",
                            "lastName": "Wallis",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wallis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144663088"
                        ],
                        "name": "E. Rolls",
                        "slug": "E.-Rolls",
                        "structuredName": {
                            "firstName": "Edmund",
                            "lastName": "Rolls",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rolls"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 142
                            }
                        ],
                        "text": "These kinds of learning rules have also been applied to\nSlow Feature Analysis 765\nhierarchical networks with several layers, most clearly in (Wallis and Rolls 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 59
                            }
                        ],
                        "text": "hierarchical networks with several layers, most clearly in (Wallis and Rolls 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 45
                            }
                        ],
                        "text": "Some work in this direction has been done by Wallis and Rolls (1997)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 26095952,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "6895894bbb476182da8b43185c273f476576776c",
            "isKey": true,
            "numCitedBy": 546,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "INVARIANT-FACE-AND-OBJECT-RECOGNITION-IN-THE-VISUAL-Wallis-Rolls",
            "title": {
                "fragments": [],
                "text": "INVARIANT FACE AND OBJECT RECOGNITION IN THE VISUAL SYSTEM"
            },
            "venue": {
                "fragments": [],
                "text": "Progress in Neurobiology"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50333420"
                        ],
                        "name": "S. Becker",
                        "slug": "S.-Becker",
                        "structuredName": {
                            "firstName": "Suzanna",
                            "lastName": "Becker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Becker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 70
                            }
                        ],
                        "text": "An application to the temporal domain, for instance, can be found in (Becker 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 168
                            }
                        ],
                        "text": "The approach described in this article belongs to a third class of systems based on learning invariances from temporal input sequences (Fo\u0308ldia\u0301k, 1991; Mitchison, 1991; Becker, 1993; Stone, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026Computational Neurobiology Laboratory, Salk Institute for Biological Studies, San Diego, CA 92168, U.S.A.; Institute for Advanced Studies, D-14193, Berlin, Germany; and Innovationskolleg Theoretische Biologie, Institute for Biology, Humboldt-University Berlin, D-10115 Berlin, Germany"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1845235,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "337c0508d6c3553084ec346ad8d2d18722b1117e",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The invariance of an objects' identity as it transformed over time provides a powerful cue for perceptual learning. We present an unsupervised learning procedure which maximizes the mutual information between the representations adopted by a feed-forward network at consecutive time steps. We demonstrate that the network can learn, entirely unsupervised, to classify an ensemble of several patterns by observing pattern trajectories, even though there are abrupt transitions from one object to another between trajectories. The same learning procedure should be widely applicable to a variety of perceptual learning tasks."
            },
            "slug": "Learning-to-Categorize-Objects-Using-Temporal-Becker",
            "title": {
                "fragments": [],
                "text": "Learning to Categorize Objects Using Temporal Coherence"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is demonstrated that the network can learn, entirely unsupervised, to classify an ensemble of several patterns by observing pattern trajectories, even though there are abrupt transitions from one object to another between trajectories."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107584303"
                        ],
                        "name": "J. P. Jones",
                        "slug": "J.-P.-Jones",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Jones",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. P. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3459632"
                        ],
                        "name": "L. Palmer",
                        "slug": "L.-Palmer",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Palmer",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Palmer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Consider five monocular simple cells for the left and right eyes with receptive fields, as indicated in Figure 4. The simple cells are modeled by spatial Gabor wavelets ( Jones & Palmer, 1987 ), whose responses Q x.t/ to a visual stimulus smoothly moving across the receptive field are given by a combination of nonnegative amplitudeQ a.t/ and phase Q `.t/ both varying in time:Q x.t/ :DQ a.t/ sin.Q `.t//."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 56
                            }
                        ],
                        "text": "The simple cells are modeled by spatial Gabor wavelets (Jones & Palmer, 1987), whose responses x\u0303(t) to a visual stimulus smoothly moving across the receptive field are given by a combination of nonnegative amplitude a\u0303(t) and phase \u03c6\u0303(t) both varying in time: x\u0303(t) := a\u0303(t) sin(\u03c6\u0303(t))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16809045,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "0dbf797d5b34f40d16eeadfa7a5b4543c2af2c11",
            "isKey": false,
            "numCitedBy": 1709,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Using the two-dimensional (2D) spatial and spectral response profiles described in the previous two reports, we test Daugman's generalization of Marcelja's hypothesis that simple receptive fields belong to a class of linear spatial filters analogous to those described by Gabor and referred to here as 2D Gabor filters. 2. In the space domain, we found 2D Gabor filters that fit the 2D spatial response profile of each simple cell in the least-squared error sense (with a simplex algorithm), and we show that the residual error is devoid of spatial structure and statistically indistinguishable from random error. 3. Although a rigorous statistical approach was not possible with our spectral data, we also found a Gabor function that fit the 2D spectral response profile of each simple cell and observed that the residual errors are everywhere small and unstructured. 4. As an assay of spatial linearity in two dimensions, on which the applicability of Gabor theory is dependent, we compare the filter parameters estimated from the independent 2D spatial and spectral measurements described above. Estimates of most parameters from the two domains are highly correlated, indicating that assumptions about spatial linearity are valid. 5. Finally, we show that the functional form of the 2D Gabor filter provides a concise mathematical expression, which incorporates the important spatial characteristics of simple receptive fields demonstrated in the previous two reports. Prominent here are 1) Cartesian separable spatial response profiles, 2) spatial receptive fields with staggered subregion placement, 3) Cartesian separable spectral response profiles, 4) spectral response profiles with axes of symmetry not including the origin, and 5) the uniform distribution of spatial phase angles. 6. We conclude that the Gabor function provides a useful and reasonably accurate description of most spatial aspects of simple receptive fields. Thus it seems that an optimal strategy has evolved for sampling images simultaneously in the 2D spatial and spatial frequency domains."
            },
            "slug": "An-evaluation-of-the-two-dimensional-Gabor-filter-Jones-Palmer",
            "title": {
                "fragments": [],
                "text": "An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It seems that an optimal strategy has evolved for sampling images simultaneously in the 2D spatial and spatial frequency domains and the Gabor function provides a useful and reasonably accurate description of most spatial aspects of simple receptive fields."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5182756"
                        ],
                        "name": "H. Bartsch",
                        "slug": "H.-Bartsch",
                        "structuredName": {
                            "firstName": "Hauke",
                            "lastName": "Bartsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bartsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743272"
                        ],
                        "name": "K. Obermayer",
                        "slug": "K.-Obermayer",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Obermayer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Obermayer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15198963,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "c500c4251a8ff2ed0206380ac35e037cca61e4d0",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Second-Order-Statistics-of-Natural-Images-Bartsch-Obermayer",
            "title": {
                "fragments": [],
                "text": "Second-Order Statistics of Natural Images"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2051110"
                        ],
                        "name": "L. Schwabe",
                        "slug": "L.-Schwabe",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Schwabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Schwabe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743272"
                        ],
                        "name": "K. Obermayer",
                        "slug": "K.-Obermayer",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Obermayer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Obermayer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 24468150,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c23e9d18693a66200f1b0f50fdf62938ffdbe1fb",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modeling-the-adaptive-visual-system:-a-survey-of-Schwabe-Obermayer",
            "title": {
                "fragments": [],
                "text": "Modeling the adaptive visual system: a survey of principled approaches"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808846"
                        ],
                        "name": "M. Oram",
                        "slug": "M.-Oram",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Oram",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Oram"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695074"
                        ],
                        "name": "D. Perrett",
                        "slug": "D.-Perrett",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Perrett",
                            "middleNames": [
                                "Ian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Perrett"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 232
                            }
                        ],
                        "text": "It is clear from the architecture that the receptive field size increases from bottom to top, and therefore the units become potentially able to respond to more complex features, two properties characteristic for the visual system (Oram & Perrett, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41905851,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "d9bd4208edcf0cfbf7a8cd2fb6e3bbf72460d670",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 182,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modeling-visual-recognition-from-neurobiological-Oram-Perrett",
            "title": {
                "fragments": [],
                "text": "Modeling visual recognition from neurobiological constraints"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144679212"
                        ],
                        "name": "C. Kayser",
                        "slug": "C.-Kayser",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Kayser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kayser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3282030"
                        ],
                        "name": "Konrad Paul Kording",
                        "slug": "Konrad-Paul-Kording",
                        "structuredName": {
                            "firstName": "Konrad",
                            "lastName": "Kording",
                            "middleNames": [
                                "Paul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Konrad Paul Kording"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40089171"
                        ],
                        "name": "P. K\u00f6nig",
                        "slug": "P.-K\u00f6nig",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "K\u00f6nig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. K\u00f6nig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9185200,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "6470a1b2ecefe33968225d8cc4bf5f9a2ca11ccf",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning in neural networks is usually applied to parameters related to linear kernels and keeps the nonlinearity of the model fixed. Thus, for successful models, properties and parameters of the nonlinearity have to be specified using a priori knowledge, which often is missing. Here, we investigate adapting the nonlinearity simultaneously with the linear kernel. We use natural visual stimuli for training a simple model of the visual system. Many of the neurons converge to an energy detector matching existing models of complex cells. The overall distribution of the parameter describing the nonlinearity well matches recent physiological results. Controls with randomly shuffled natural stimuli and pink noise demonstrate that the match of simulation and experimental results depends on the higher-order statistical properties of natural stimuli."
            },
            "slug": "Learning-the-Nonlinearity-of-Neurons-from-Natural-Kayser-Kording",
            "title": {
                "fragments": [],
                "text": "Learning the Nonlinearity of Neurons from Natural Visual Stimuli"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work uses natural visual stimuli for training a simple model of the visual system and investigates adapting the nonlinearity simultaneously with the linear kernel."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Zemel and Hinton (1992)  have generalized this approach to extract several invariant variables."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Zemel and Hinton (1992) have generalized this approach to extract several invariant variables. a and b became vectors, and V(\u00b7) became the determinant of the covariance matrix."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16472149,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "852d3055ce81ef75b51dd9a406c0f5d056d270da",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Using an unsupervised learning procedure, a network is trained on an ensemble of images of the same two-dimensional object at different positions, orientations and sizes. Each half of the network \"sees\" one fragment of the object, and tries to produce as output a set of 4 parameters that have high mutual information with the 4 parameters output by the other half of the network. Given the ensemble of training patterns, the 4 parameters on which the two halves of the network can agree are the position, orientation, and size of the whole object, or some recoding of them. After training, the network can reject instances of other shapes by using the fact that the predictions made by its two halves disagree. If two competing networks are trained on an unlabelled mixture of images of two objects, they cluster the training cases on the basis of the objects' shapes, independently of the position, orientation, and size."
            },
            "slug": "Discovering-Viewpoint-Invariant-Relationships-That-Zemel-Hinton",
            "title": {
                "fragments": [],
                "text": "Discovering Viewpoint-Invariant Relationships That Characterize Objects"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "Using an unsupervised learning procedure, a network is trained on an ensemble of images of the same two-dimensional object at different positions, orientations and sizes, and can reject instances of other shapes by using the fact that the predictions made by its two halves disagree."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3346683"
                        ],
                        "name": "J. Hurri",
                        "slug": "J.-Hurri",
                        "structuredName": {
                            "firstName": "Jarmo",
                            "lastName": "Hurri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hurri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39065866"
                        ],
                        "name": "Jaakko J. V\u00e4yrynen",
                        "slug": "Jaakko-J.-V\u00e4yrynen",
                        "structuredName": {
                            "firstName": "Jaakko",
                            "lastName": "V\u00e4yrynen",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jaakko J. V\u00e4yrynen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 268506,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "818ef40d226bddbdadde4fb2ffaaa2f939f24f6b",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, different models of the statistical structure of natural images have been proposed. These models predict properties of biological visual systems and can be used as priors in Bayesian inference. The fundamental model is independent component analysis, which can be estimated by maximization of the sparsenesses of linear filter outputs. This leads to the emergence of principal simple cell properties. Alternatively, simple cell properties are obtained by maximizing the temporal coherence in natural image sequences. Taking account of the basic dependencies of linear filter outputs permit modeling of complex cells and topographic organization as well. We propose a unifying framework for these statistical properties, based on the concept of spatiotemporal activity \"bubbles.\"A bubble means here an activation of simple cells (linear filters) that is contiguous both in space (the cortical surface) and in time."
            },
            "slug": "Bubbles:-a-unifying-framework-for-low-level-of-Hyv\u00e4rinen-Hurri",
            "title": {
                "fragments": [],
                "text": "Bubbles: a unifying framework for low-level statistical properties of natural image sequences."
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes a unifying framework for statistical properties of biological visual systems, based on the concept of spatiotemporal activity \"bubbles\", which can be estimated by maximization of the sparsenesses of linear filter outputs."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics, image science, and vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31080870"
                        ],
                        "name": "M. Eisele",
                        "slug": "M.-Eisele",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Eisele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Eisele"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 102
                            }
                        ],
                        "text": "A more detailed biological model of learning invariances also based on memory traces was presented by Eisele (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120033420,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "5abf2678223fa4f32ae5a01b1b41fd69401efc5f",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "An unsupervised learning principle is proposed for individual neurons with complex synaptic structure and dynamical input. The learning goal is a neuronal response to temporal constancies: If some input patterns often occur in close temporal succession, then the neuron should respond either to all of them or to none. It is shown that linear threshold neurons can achieve this learning goal, if each synapse stores not only a weight, but also a short-term memory trace. The online learning process requires no biologically implausible interactions. The sequence of temporal associations can be interpreted as a random walk on the state transition graph of the input dynamics. In numerical simulations the learning process turned out to be robust against parameter changes."
            },
            "slug": "Unsupervised-learning-of-temporal-constancies-by-Eisele",
            "title": {
                "fragments": [],
                "text": "Unsupervised learning of temporal constancies by pyramidal-type neurons"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "An unsupervised learning principle is proposed for individual neurons with complex synaptic structure and dynamical input and it is shown that linear threshold neurons can achieve this learning goal, if each synapse stores not only a weight, but also a short-term memory trace."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145833095"
                        ],
                        "name": "S. Kothari",
                        "slug": "S.-Kothari",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Kothari",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kothari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681982"
                        ],
                        "name": "H. Oh",
                        "slug": "H.-Oh",
                        "structuredName": {
                            "firstName": "Heekuck",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Oh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 177751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbc0a468ab103ae29717703d4aa9f682f6a2b664",
            "isKey": false,
            "numCitedBy": 15338,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-Networks-for-Pattern-Recognition-Kothari-Oh",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3160228"
                        ],
                        "name": "K. Fukushima",
                        "slug": "K.-Fukushima",
                        "structuredName": {
                            "firstName": "Kunihiko",
                            "lastName": "Fukushima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukushima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3126340"
                        ],
                        "name": "S. Miyake",
                        "slug": "S.-Miyake",
                        "structuredName": {
                            "firstName": "Sei",
                            "lastName": "Miyake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Miyake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145214184"
                        ],
                        "name": "Takayuki Ito",
                        "slug": "Takayuki-Ito",
                        "structuredName": {
                            "firstName": "Takayuki",
                            "lastName": "Ito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takayuki Ito"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In both the neocognitron ( Fukushima, Miyake, & Ito, 1983 ) and the weight-sharing backpropagation network (LeCun et al., 1989), for instance, translation invariance is implemented by replicating a common but plastic synaptic weight pattern at all shifted locations and then spatially subsampling the output signal, possibly blurred over a certain region."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8235461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71ea46c9266f5104f79ea27fdfb4c5686677695a",
            "isKey": false,
            "numCitedBy": 755,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A recognition with a large-scale network is simulated on a PDP-11/34 minicomputer and is shown to have a great capability for visual pattern recognition. The model consists of nine layers of cells. The authors demonstrate that the model can be trained to recognize handwritten Arabic numerals even with considerable deformations in shape. A learning-with-a-teacher process is used for the reinforcement of the modifiable synapses in the new large-scale model, instead of the learning-without-a-teacher process applied to a previous model. The authors focus on the mechanism for pattern recognition rather than that for self-organization."
            },
            "slug": "Neocognitron:-A-neural-network-model-for-a-of-Fukushima-Miyake",
            "title": {
                "fragments": [],
                "text": "Neocognitron: A neural network model for a mechanism of visual pattern recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A recognition with a large-scale network is simulated on a PDP-11/34 minicomputer and is shown to have a great capability for visual pattern recognition and can be trained to recognize handwritten Arabic numerals even with considerable deformations in shape."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696931"
                        ],
                        "name": "H. Barrow",
                        "slug": "H.-Barrow",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Barrow",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Barrow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2832052"
                        ],
                        "name": "Alistair J. Bray",
                        "slug": "Alistair-J.-Bray",
                        "structuredName": {
                            "firstName": "Alistair",
                            "lastName": "Bray",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alistair J. Bray"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 47
                            }
                        ],
                        "text": "Closely related models were also presented in (Barrow and Bray 1992, O\u2019Reilly and Johnson 1994, and Fukushima 1999) and applied to learning lighting and orientation invariances in face recognition in (Stewart Bartlett and Sejnowski 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59702659,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "49ed3f0f4e089438eab4b50922b0050d578caf15",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Model-of-Adaptive-Development-of-Complex-Cortical-Barrow-Bray",
            "title": {
                "fragments": [],
                "text": "A Model of Adaptive Development of Complex Cortical Cells"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50333420"
                        ],
                        "name": "S. Becker",
                        "slug": "S.-Becker",
                        "structuredName": {
                            "firstName": "Suzanna",
                            "lastName": "Becker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Becker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804703"
                        ],
                        "name": "Mark D. Plumbley",
                        "slug": "Mark-D.-Plumbley",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Plumbley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark D. Plumbley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 223
                            }
                        ],
                        "text": "\u2026T 2\u03c0\n\u221a 1(y) = T\n2\u03c0\n\u221a n24\u03c02\nT2 = n.\nSlow Feature Analysis 727\nunits gj, either only from lower to higher units as shown in the figure or between all units, are needed to decorrelate the output signal components by some variant of anti-Hebbian learning (see Becker & Plumbley, 1996, for an overview)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11105114,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e60ea228d307abd287d7315473510777dddb3c47",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 132,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article, we review unsupervised neural network learning procedures which can be applied to the task of preprocessing raw data to extract useful features for subsequent classification. The learning algorithms reviewed here are grouped into three sections: information-preserving methods, density estimation methods, and feature extraction methods. Each of these major sections concludes with a discussion of successful applications of the methods to real-world problems."
            },
            "slug": "Unsupervised-neural-network-learning-procedures-for-Becker-Plumbley",
            "title": {
                "fragments": [],
                "text": "Unsupervised neural network learning procedures for feature extraction and classification"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The learning algorithms reviewed here are grouped into three sections: information-preserving methods, density estimation methods, and feature extraction methods, which are applied to the task of preprocessing raw data to extract useful features for subsequent classification."
            },
            "venue": {
                "fragments": [],
                "text": "Applied Intelligence"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 106
                            }
                        ],
                        "text": "In both the neocognitron (Fukushima, Miyake, & Ito, 1983) and the weight-sharing backpropagation network (LeCun et al., 1989), for instance, translation invariance is implemented by replicating a common but plastic synaptic weight pattern at all shifted locations and then spatially subsampling the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 105
                            }
                        ],
                        "text": "In both the neocognitron (Fukushima, Miyake, & Ito, 1983) and the weight-sharing backpropagation network (LeCun et al., 1989), for instance, translation invariance is implemented by replicating a common but plastic synaptic weight pattern at all shifted locations and then spatially subsampling the output signal, possibly blurred over a certain region."
                    },
                    "intents": []
                }
            ],
            "corpusId": 41312633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27",
            "isKey": false,
            "numCitedBy": 7830,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification."
            },
            "slug": "Backpropagation-Applied-to-Handwritten-Zip-Code-LeCun-Boser",
            "title": {
                "fragments": [],
                "text": "Backpropagation Applied to Handwritten Zip Code Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper demonstrates how constraints from the task domain can be integrated into a backpropagation network through the architecture of the network, successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143814637"
                        ],
                        "name": "Berthold K. P. Horn",
                        "slug": "Berthold-K.-P.-Horn",
                        "structuredName": {
                            "firstName": "Berthold",
                            "lastName": "Horn",
                            "middleNames": [
                                "K.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Berthold K. P. Horn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728401"
                        ],
                        "name": "M. Brooks",
                        "slug": "M.-Brooks",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Brooks",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brooks"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 127217532,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "a1c45a16a17a94fbb1fb794f07237eabaddbe5e6",
            "isKey": false,
            "numCitedBy": 703,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Understanding how the shape of a three dimensional object may be recovered from shading in a two-dimensional image of the object is one of the most important - and still unresolved - problems in machine vision. Although this important subfield is now in its second decade, this book is the first to provide a comprehensive review of shape from shading. It brings together all of the seminal papers on the subject, shows how recent work relates to more traditional approaches, and provides a comprehensive annotated bibliography.The book's 17 chapters cover: Surface Descriptions from Stereo and Shading. Shape and Source from Shading. The Eikonal Equation: some Results Applicable to Computer Vision. A Method for Enforcing Integrability in Shape from Shading Algorithms. Obtaining Shape from Shading Information. The Variational Approach to Shape from Shading. Calculating the Reflectance Map. Numerical Shape from Shading and Occluding Boundaries. Photometric Invariants Related to Solid Shape. Improved Methods of Estimating Shape from Shading Using the Light Source Coordinate System. A Provably Convergent Algorithm for Shape from Shading. Recovering Three Dimensional Shape from a Single Image of Curved Objects. Perception of Solid Shape from Shading. Local Shading Analysis Pentland. Radarclinometry for the Venus Radar Mapper. Photometric Method for Determining Surface Orientation from Multiple Images.Berthold K. P. Horn is Professor of Electrical Engineering and Computer Science at MIT. He has presided over the field of machine vision for more than a decade and is the author of \"Robot Vision. \"Michael Brooks is Reader in Computer Science at The Flinders University of South Australia. \"Shape from Shading\" is included in the Artificial Intelligence series, edited by Michael Brady, Daniel Bobrow, and Randall Davis."
            },
            "slug": "Shape-from-shading-Horn-Brooks",
            "title": {
                "fragments": [],
                "text": "Shape from shading"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2302763"
                        ],
                        "name": "W. Theimer",
                        "slug": "W.-Theimer",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Theimer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Theimer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3038637"
                        ],
                        "name": "H. Mallot",
                        "slug": "H.-Mallot",
                        "structuredName": {
                            "firstName": "Hanspeter",
                            "lastName": "Mallot",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mallot"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 105
                            }
                        ],
                        "text": "This phase difference between simple cell responses of different eyes can be used to estimate disparity (Theimer & Mallot, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 64142044,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "4f2ac80621b8214374d41ce33e1a20df9a3c4a06",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We present a technique for guiding vergence movements for an active stereo camera system and for calculating dense disparity maps. Both processes are described in the same theoretical framework based on phase differences in complex Gabor filter responses, modeling receptive field properties in the visual cortex. While the camera movements are computed with input images of coarse spatial resolution, the disparity map calculation uses a finer resolution in the scale space. The correspondence problem is solved implicitly by restricting the disparity range around zero disparity (Panum\u2032s area in the human visual system). The vergence process is interpreted as a mechanism to minimize global disparity, thereby setting a 3D region of interest for subsequent disparity detection. The disparity map represents smaller local disparities as an important cue for depth perception. Experimental data for the integrated performance of vergence in natural scenes followed by disparity map calculations are presented."
            },
            "slug": "Phase-based-binocular-vergence-control-and-depth-Theimer-Mallot",
            "title": {
                "fragments": [],
                "text": "Phase-based binocular vergence control and depth reconstruction using active vision"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A technique for guiding vergence movements for an active stereo camera system and for calculating dense disparity maps described in the same theoretical framework based on phase differences in complex Gabor filter responses, modeling receptive field properties in the visual cortex."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2138443786"
                        ],
                        "name": "Hanchuan Peng",
                        "slug": "Hanchuan-Peng",
                        "structuredName": {
                            "firstName": "Hanchuan",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hanchuan Peng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070223415"
                        ],
                        "name": "Lifeng Sha",
                        "slug": "Lifeng-Sha",
                        "structuredName": {
                            "firstName": "Lifeng",
                            "lastName": "Sha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lifeng Sha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2349808"
                        ],
                        "name": "Q. Gan",
                        "slug": "Q.-Gan",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Gan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Q. Gan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143619255"
                        ],
                        "name": "Yu Wei",
                        "slug": "Yu-Wei",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Wei"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Stone and Bray (1995) have presented a learning rule that is based on an objective function similar to that of Becker andHinton (1992) or equation 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62526387,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fcd8a99546cb515655cfdb615d560182f86b85f7",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A new energy function is proposed for forming self-adapting ordered representations of input samples in a multilayer perceptron. Simulation results on unconstrained handwritten digit recognition give a better invariance extraction for this model than for several other models."
            },
            "slug": "Energy-function-for-learning-invariance-in-Peng-Sha",
            "title": {
                "fragments": [],
                "text": "Energy function for learning invariance in multilayer perceptron"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A new energy function is proposed for forming self-adapting ordered representations of input samples in a multilayer perceptron that gives better invariance extraction than several other models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 52
                            }
                        ],
                        "text": "A well-known example is the support vector machine (Vapnik, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38756,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 820779,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "e03d550b23e45d1c5da474574914d91bee77101f",
            "isKey": false,
            "numCitedBy": 839,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Classes of PDP Models, Specific Versions of the General Parallel Activation Model, Sigma-Pi Units, Conclusion, Acknowledgments"
            },
            "slug": "A-general-framework-for-parallel-distributed-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "A general framework for parallel distributed processing"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter contains sections titled: Classes of PDP Models, Specific Versions of the General Parallel Activation Model, Sigma-Pi Units, Conclusion, Acknowledgments."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 151
                            }
                        ],
                        "text": "The approach described in this article belongs to a third class of systems based on learning invariances from temporal input sequences (Fo\u0308ldia\u0301k, 1991; Mitchison, 1991; Becker, 1993; Stone, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Mitchison (1991) presented a learning rule for linear units that is also derived from an objective function like that of equation 2.1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 102
                            }
                        ],
                        "text": "The solution is the normed eigenvector of matrix \u3008z\u0307z\u0307T\u3009 that corresponds to the smallest eigenvalue (cf. Mitchison, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 135
                            }
                        ],
                        "text": "The approach described in this article belongs to a third class of systems based on learning invariances from temporal input sequences (F\u00f6ldi \u00c2 ak, 1991; Mitchison, 1991; Becker, 1993; Stone, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1991).Removing time variationwith the anti-Hebbiandifferential synapse"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Becker and Hinton (1992) trained a network to discover disparity as an invariant variable of random-dot stereograms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 111
                            }
                        ],
                        "text": "Stone and Bray (1995) have presented a learning rule that is based on an objective function similar to that of Becker and Hinton (1992) or equation 2.1 and which includes a memory trace mechanism as in (Fo\u0308ldia\u0301k 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 33
                            }
                        ],
                        "text": "This is the formulation used in (Becker and Hinton 1992)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1992).A self-organizing neural network that discovers surfaces in random-dot stereograms"
            },
            "venue": {
                "fragments": [],
                "text": "Nature,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Slow Feature Analysis 769"
            },
            "venue": {
                "fragments": [],
                "text": "Slow Feature Analysis 769"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 56
                            }
                        ],
                        "text": "The simple cells are modeled by spatial Gabor wavelets (Jones & Palmer, 1987), whose responses x\u0303(t) to a visual stimulus smoothly moving across the receptive field are given by a combination of nonnegative amplitude a\u0303(t) and phase \u03c6\u0303(t) both varying in time: x\u0303(t) := a\u0303(t) sin(\u03c6\u0303(t))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An evaluation of the two-dimensional Gabor \u008elter model of simple receptive \u008eelds in cat striate cortex"
            },
            "venue": {
                "fragments": [],
                "text": "J. of Neurophysiology,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 238
                            }
                        ],
                        "text": "The general mechanism by which the network learns approximate translation invariance is closely related to higher-order networks in which translation invariance is achieved by weight sharing among monomials of same degree and spread (see Bishop, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 142
                            }
                        ],
                        "text": "The radial basis function network with nonadaptable basis functions is an example of this interpretation (cf. Becker & Hinton, 1995; see also Bishop, 1995, for an introduction to radial basis function networks)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1995).Neural networks for pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 56
                            }
                        ],
                        "text": "The simple cells are modeled by spatial Gabor wavelets (Jones & Palmer, 1987), whose responses x\u0303(t) to a visual stimulus smoothly moving across the receptive field are given by a combination of nonnegative amplitude a\u0303(t) and phase \u03c6\u0303(t) both varying in time: x\u0303(t) := a\u0303(t) sin(\u03c6\u0303(t))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An evaluation of the two-dimensional Gabor lter model of simple receptive elds in cat striate cortex"
            },
            "venue": {
                "fragments": [],
                "text": "J. of Neurophysiology"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 99
                            }
                        ],
                        "text": "Closely related models were also presented in (Barrow and Bray 1992, O\u2019Reilly and Johnson 1994, and Fukushima 1999) and applied to learning lighting and orientation invariances in face recognition in (Stewart Bartlett and Sejnowski 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1999).Self-organization of shift-invariant receptive \u008eelds"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 266
                            }
                        ],
                        "text": "These two types of systems are trained or applied to static images, and the invariances, such as translation or size invariance, need to be known in advance by the designer of the system (dynamic link matching is less strict in specifying the invariance in advance; Konen et al., 1994)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1994).A fast dynamic link matching algorithm for invariant pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 102
                            }
                        ],
                        "text": "A more detailed biological model of learning invariances also based on memory traces was presented by Eisele (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1997).Unsupervised learning of temporal constancies by pyramidaltype neurons"
            },
            "venue": {
                "fragments": [],
                "text": "Mathematics of neural networks (pp. 171\u2013175)"
            },
            "year": 1997
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 10
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 50,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Slow-Feature-Analysis:-Unsupervised-Learning-of-Wiskott-Sejnowski/5127759530ce213f488af2859190697770f557f3?sort=total-citations"
}