{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1776507"
                        ],
                        "name": "Michael Gleicher",
                        "slug": "Michael-Gleicher",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Gleicher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Gleicher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 111
                            }
                        ],
                        "text": "Examples include intelligent scissor [Mortensen and Barrett 1995; Mortensen and Barrett 1999], image snapping [Gleicher 1995] and Jetstream [Perez and Blake 2001]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207193994,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5530d54f83d18531a697bc34da8952bbda15ad80",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Cursor snapping is a standard method for providing precise pointing in direct manipulation graphical interfaces. In this paper, we introduce image snapping, a variant of cursor snapping that works in image-based programs such as paint systems. Image snapping moves the cursor location to nearby features in the image, such as edges. It is implemented by using gradient descent on blurred versions of feature maps made from the images. Interaction techniques using cursor snapping for image segmentation and curve tracing are presented CR Descriptors: I.4.3 [Image Processing]: image processing software. I.3.6 [Computer Graphics]: interaction techniques."
            },
            "slug": "Image-snapping-Gleicher",
            "title": {
                "fragments": [],
                "text": "Image snapping"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper introduces image snapping, a variant of cursor snapping that works in image-based programs such as paint systems that moves the cursor location to nearby features in the image, such as edges."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39226969"
                        ],
                        "name": "Eric N. Mortensen",
                        "slug": "Eric-N.-Mortensen",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Mortensen",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric N. Mortensen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144055367"
                        ],
                        "name": "W. Barrett",
                        "slug": "W.-Barrett",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Barrett",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Barrett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 38
                            }
                        ],
                        "text": "Examples include intelligent scissor [Mortensen and Barrett 1995; Mortensen and Barrett 1999], image snapping [Gleicher 1995] and Jetstream [Perez and Blake 2001]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4374741,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41093fe0f19cb37b239a62adb5f2c0cd058fec83",
            "isKey": false,
            "numCitedBy": 878,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new, interactive tool called Intelligent Scissors which we use for image segmentation and composition. Fully automated segmentation is an unsolved problem, while manual tracing is inaccurate and laboriously unacceptable. However, Intelligent Scissors allow objects within digital images to be extracted quickly and accurately using simple gesture motions with a mouse. When the gestured mouse position comes in proximity to an object edge, a live-wire boundary \u201csnaps\u201d to, and wraps around the object of interest. Live-wire boundary detection formulates discrete dynamic programming (DP) as a two-dimensional graph searching problem. DP provides mathematically optimal boundaries while greatly reducing sensitivity to local noise or other intervening structures. Robustness is further enhanced with on-the-fly training which causes the boundary to adhere to the specific type of edge currently being followed, rather than simply the strongest edge in the neighborhood. Boundary cooling automatically freezes unchanging segments and automates input of additional seed points. Cooling also allows the user to be much more free with the gesture path, thereby increasing the efficiency and finesse with which boundaries can be extracted. Extracted objects can be scaled, rotated, and composited using live-wire masks and spatial frequency equivalencing. Frequency equivalencing is performed by applying a Butterworth filter which matches the lowest frequency spectra to all other image components. Intelligent Scissors allow creation of convincing compositions from existing images while dramatically increasing the speed and precision with which objects can be extracted."
            },
            "slug": "Intelligent-scissors-for-image-composition-Mortensen-Barrett",
            "title": {
                "fragments": [],
                "text": "Intelligent scissors for image composition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Intelligent Sc scissors allows objects within digital images to be extracted quickly and accurately using simple gesture motions with a mouse, and allows creation of convincing compositions from existing images while dramatically increasing the speed and precision with which objects can be extracted."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39226969"
                        ],
                        "name": "Eric N. Mortensen",
                        "slug": "Eric-N.-Mortensen",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Mortensen",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric N. Mortensen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144055367"
                        ],
                        "name": "W. Barrett",
                        "slug": "W.-Barrett",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Barrett",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Barrett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 66
                            }
                        ],
                        "text": "Examples include intelligent scissor [Mortensen and Barrett 1995; Mortensen and Barrett 1999], image snapping [Gleicher 1995] and Jetstream [Perez and Blake 2001]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2404991,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b35dcf1b73a6c295f94b09996785e79918ecebb7",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Intelligent Scissors is an interactive image segmentation tool that allows a user to select piece-wise globally optimal contour segments that correspond to a desired object boundary. We present a new and faster method of computing the optimal path by over-segmenting the image using tobogganing and then imposing a weighted planar graph on top of the resulting region boundaries. The resulting region-based graph is many times smaller than the previous pixel-based graph, thus providing faster graph searches and immediate user interaction. Further tobogganing provides an new systematic and predictable framework for computing edge model parameters, allowing subpixel localization as well as a measure of edge blur."
            },
            "slug": "Toboggan-based-intelligent-scissors-with-a-edge-Mortensen-Barrett",
            "title": {
                "fragments": [],
                "text": "Toboggan-based intelligent scissors with a four-parameter edge model"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new and faster method of computing the optimal path by over-segmenting the image using tobogganing and then imposing a weighted planar graph on top of the resulting region boundaries, thus providing faster graph searches and immediate user interaction."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653004"
                        ],
                        "name": "V. Kolmogorov",
                        "slug": "V.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kolmogorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 15
                            }
                        ],
                        "text": "2003], GrabCut [Rother et al. 2004] and Photomontage [Agarwala et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 217
                            }
                        ],
                        "text": "\u2026paint [Reese and Barrett 2002; Barrett and Cheney 2002], marker drawing [Falcao et al. 2000], sketch-based interaction [Tan and Ahuja 2001], interactive graph cut image segmentation [Boykov and Jolly 2001], GrabCut [Rother et al. 2004]) and interactive image Photomontage [Agarwala et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 128
                            }
                        ],
                        "text": "2000], sketch-based interaction [Tan and Ahuja 2001], interactive graph cut image segmentation [Boykov and Jolly 2001], GrabCut [Rother et al. 2004]) and interactive image Photomontage [Agarwala et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 137
                            }
                        ],
                        "text": "The graph cut algorithm has also been used in the computer graphics community, such as Graph Cut Textures [Kwatra et al. 2003], GrabCut [Rother et al. 2004] and Photomontage [Agarwala et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 347,
                                "start": 340
                            }
                        ],
                        "text": "Recently, researchers have managed to improve image cutout by using region-based methods, e.g., magic wand in Photoshop, intelligent paint [Reese and Barrett 2002; Barrett and Cheney 2002], marker drawing [Falcao et al. 2000], sketch-based interaction [Tan and Ahuja 2001], interactive graph cut image segmentation [Boykov and Jolly 2001], GrabCut [Rother et al. 2004]) and interactive image Photomontage [Agarwala et al. 2004]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6202829,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a953aaf29ef67ee094943d4be50d753b3744573",
            "isKey": true,
            "numCitedBy": 5202,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of efficient, interactive foreground/background segmentation in still images is of great practical importance in image editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) information, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a robust algorithm for \"border matting\" has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show that for moderately difficult examples the proposed method outperforms competitive tools."
            },
            "slug": "\"GrabCut\":-interactive-foreground-extraction-using-Rother-Kolmogorov",
            "title": {
                "fragments": [],
                "text": "\"GrabCut\": interactive foreground extraction using iterated graph cuts"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A more powerful, iterative version of the optimisation of the graph-cut approach is developed and the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144055367"
                        ],
                        "name": "W. Barrett",
                        "slug": "W.-Barrett",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Barrett",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Barrett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32674745"
                        ],
                        "name": "Alan S. Cheney",
                        "slug": "Alan-S.-Cheney",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Cheney",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alan S. Cheney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 143
                            }
                        ],
                        "text": "\u2026have managed to improve image cutout by using region-based methods, e.g., magic wand in Photoshop, intelligent paint [Reese and Barrett 2002; Barrett and Cheney 2002], marker drawing [Falcao et al. 2000], sketch-based interaction [Tan and Ahuja 2001], interactive graph cut image\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 45
                            }
                        ],
                        "text": ", magic wand in Photoshop, intelligent paint [Reese and Barrett 2002; Barrett and Cheney 2002], marker drawing [Falcao et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6307032,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "b7d7239c9a949ed911b66f5ed40b4394be21fa41",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Object-Based Image Editing (OBIE) for real-time animation and manipulation of static digital photographs. Individual image objects (such as an arm or nose, Figure 1) are selected, scaled, stretched, bent, warped or even deleted (with automatic hole filling) - at the object, rather than the pixel level - using simple gesture motions with a mouse. OBIE gives the user direct, local control over object shape, size, and placement while dramatically reducing the time required to perform image editing tasks.Object selection is performed by manually collecting (subobject) regions detected by a watershed algorithm. Objects are tessellated into a triangular mesh, allowing shape modification to be performed in real time using OpenGL's texture mapping hardware.Through the use of anchor points, the user is able to interactively perform editing operations on a whole object, or just part(s) of an object - including moving, scaling, rotating, stretching, bending, and deleting. Indirect manipulation of object shape is also provided through the use of sliders and Bezier curves. Holes created by movement are filled in real-time based on surrounding texture.When objects stretch or scale, we provide a method for preserving texture granularity or scale. We also present a texture brush, which allows the user to \"paint\" texture into different parts of an image, using existing image texture(s).OBIE allows the user to perform interactive, high-level editing of image objects in a few seconds to a few ten's of seconds"
            },
            "slug": "Object-based-image-editing-Barrett-Cheney",
            "title": {
                "fragments": [],
                "text": "Object-based image editing"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "OBIE allows the user to perform interactive, high-level editing of image objects in a few seconds to a few ten's of seconds while dramatically reducing the time required to perform image editing tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2441498"
                        ],
                        "name": "K. Tan",
                        "slug": "K.-Tan",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Tan",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237406"
                        ],
                        "name": "N. Ahuja",
                        "slug": "N.-Ahuja",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Ahuja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ahuja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5075023,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b55e3da12b16c22395fc79e132a6c6565a525a8b",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present in this paper the design of an interactive tool for selecting objects using simple freehand sketches. The objective is to extract object boundaries precisely while requiring little skill and time from the user. The tool proposed achieves this objective by integrating user input and image computation in a two-phase algorithm. In the first phase, the input sketch is used along with a coarse global segmentation of the image to derive an initial selection and a triangulation of the region around the boundary. The triangles are used to formulate subproblems of local finer-grained segmentation and selection. Each of the subproblems is processed independently in the second phase, where a linear approximation of the local boundary as well as a local, finer-grained segmentation are computed. The approximate boundary is then used with the local segmentation to compute a final selection, represented with an alpha channel to fully capture diffused object boundaries. Experimental results show that the tool allows very simple sketches to be used to select objects with complex boundaries. Therefore, the tool has immediate applications in graphics systems for image editing, manipulation, synthesis, retrieval and processing."
            },
            "slug": "Selecting-objects-with-freehand-sketches-Tan-Ahuja",
            "title": {
                "fragments": [],
                "text": "Selecting objects with freehand sketches"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Experimental results show that the interactive tool proposed allows very simple sketches to be used to select objects with complex boundaries, and has immediate applications in graphics systems for image editing, manipulation, synthesis, retrieval and processing."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1581919528"
                        ],
                        "name": "L. Reese",
                        "slug": "L.-Reese",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Reese",
                            "middleNames": [
                                "Jack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Reese"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144055367"
                        ],
                        "name": "W. Barrett",
                        "slug": "W.-Barrett",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Barrett",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Barrett"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 45
                            }
                        ],
                        "text": ", magic wand in Photoshop, intelligent paint [Reese and Barrett 2002; Barrett and Cheney 2002], marker drawing [Falcao et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 140
                            }
                        ],
                        "text": "Recently, researchers have managed to improve image cutout by using region-based methods, e.g., magic wand in Photoshop, intelligent paint [Reese and Barrett 2002; Barrett and Cheney 2002], marker drawing [Falcao et al. 2000], sketch-based interaction [Tan and Ahuja 2001], interactive graph cut\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14051276,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29bcf82d5465404739c93c746178d29d93f37e5f",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Intelligent Paint provides a new tool for interactively selecting image objects or regions of interest, while simultaneously applying filters or effects directly to the selection(s). By coupling adaptive cost-ordered region growing with the high-level visual expertise of the user to indicate which objects are of interest, we are able to accurately select and edit complex, non-homogeneous objects in only a few seconds using simple mouse input gestures. Prior to selection, the image is oversegmented using a watershed (tobogganing) algorithm. Watershed catchment basins are assembled into an hierarchical image partition by grouping basins using the student\u2019s t-distribution. Grouped basins, referred to as TRAPs (Tobogganed Region Accumulation Plateaus), reasonably capture sub-object detail. Object selection is accomplished by user-steered, adaptive, cost-ordered collection of TRAPs. Tinting or application of filters during object selection provides immediate visual feedback as to what is being selected as well as the suitability of the chosen digital effect. Intelligent Paint selection compares favorably with popular commercial tools in terms of efficiency, accuracy and reproducibility with the added benefit of on-the-fly filtering, while raising the granularity of image editing from the pixel to the object-level. Novel tools for object-centered image editing such as Intelligent Eraser and Intelligent Clone tools are also possible with this technique."
            },
            "slug": "Image-Editing-with-Intelligent-Paint-Reese-Barrett",
            "title": {
                "fragments": [],
                "text": "Image Editing with Intelligent Paint"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Intelligent Paint provides a new tool for interactively selecting image objects or regions of interest, while simultaneously applying filters or effects directly to the selection(s) by coupling adaptive cost-ordered region growing with the high-level visual expertise of the user to indicate which objects are of interest."
            },
            "venue": {
                "fragments": [],
                "text": "Eurographics"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696487"
                        ],
                        "name": "A. Agarwala",
                        "slug": "A.-Agarwala",
                        "structuredName": {
                            "firstName": "Aseem",
                            "lastName": "Agarwala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Agarwala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2875493"
                        ],
                        "name": "Mira Dontcheva",
                        "slug": "Mira-Dontcheva",
                        "structuredName": {
                            "firstName": "Mira",
                            "lastName": "Dontcheva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mira Dontcheva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1820412"
                        ],
                        "name": "Maneesh Agrawala",
                        "slug": "Maneesh-Agrawala",
                        "structuredName": {
                            "firstName": "Maneesh",
                            "lastName": "Agrawala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maneesh Agrawala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2311676"
                        ],
                        "name": "S. Drucker",
                        "slug": "S.-Drucker",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Drucker",
                            "middleNames": [
                                "Mark"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Drucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143792576"
                        ],
                        "name": "Alex Colburn",
                        "slug": "Alex-Colburn",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Colburn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Colburn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800609"
                        ],
                        "name": "B. Curless",
                        "slug": "B.-Curless",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Curless",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Curless"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745260"
                        ],
                        "name": "D. Salesin",
                        "slug": "D.-Salesin",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Salesin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Salesin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400248273"
                        ],
                        "name": "Michael F. Cohen",
                        "slug": "Michael-F.-Cohen",
                        "structuredName": {
                            "firstName": "Michael F.",
                            "lastName": "Cohen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael F. Cohen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 175
                            }
                        ],
                        "text": "The graph cut algorithm has also been used in the computer graphics community, such as Graph Cut Textures [Kwatra et al. 2003], GrabCut [Rother et al. 2004] and Photomontage [Agarwala et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 404,
                                "start": 392
                            }
                        ],
                        "text": "Recently, researchers have managed to improve image cutout by using region-based methods, e.g., magic wand in Photoshop, intelligent paint [Reese and Barrett 2002; Barrett and Cheney 2002], marker drawing [Falcao et al. 2000], sketch-based interaction [Tan and Ahuja 2001], interactive graph cut image segmentation [Boykov and Jolly 2001], GrabCut [Rother et al. 2004]) and interactive image Photomontage [Agarwala et al. 2004]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 42
                            }
                        ],
                        "text": "2004]) and interactive image Photomontage [Agarwala et al. 2004]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 274
                            }
                        ],
                        "text": "\u2026paint [Reese and Barrett 2002; Barrett and Cheney 2002], marker drawing [Falcao et al. 2000], sketch-based interaction [Tan and Ahuja 2001], interactive graph cut image segmentation [Boykov and Jolly 2001], GrabCut [Rother et al. 2004]) and interactive image Photomontage [Agarwala et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5699077,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "82b9ec4ab1d3488ed4af81216810ec20a60177b3",
            "isKey": true,
            "numCitedBy": 489,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an interactive, computer-assisted framework for combining parts of a set of photographs into a single composite picture, a process we call \"digital photomontage.\" Our framework makes use of two techniques primarily: graph-cut optimization, to choose good seams within the constituent images so that they can be combined as seamlessly as possible; and gradient-domain fusion, a process based on Poisson equations, to further reduce any remaining visible artifacts in the composite. Also central to the framework is a suite of interactive tools that allow the user to specify a variety of high-level image objectives, either globally across the image, or locally through a painting-style interface. Image objectives are applied independently at each pixel location and generally involve a function of the pixel values (such as \"maximum contrast\") drawn from that same location in the set of source images. Typically, a user applies a series of image objectives iteratively in order to create a finished composite. The power of this framework lies in its generality; we show how it can be used for a wide variety of applications, including \"selective composites\" (for instance, group photos in which everyone looks their best), relighting, extended depth of field, panoramic stitching, clean-plate production, stroboscopic visualization of movement, and time-lapse mosaics."
            },
            "slug": "Interactive-digital-photomontage-Agarwala-Dontcheva",
            "title": {
                "fragments": [],
                "text": "Interactive digital photomontage"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The framework makes use of two techniques primarily: graph-cut optimization, to choose good seams within the constituent images so that they can be combined as seamlessly as possible; and gradient-domain fusion, a process based on Poisson equations, to further reduce any remaining visible artifacts in the composite."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706355"
                        ],
                        "name": "Vivek Kwatra",
                        "slug": "Vivek-Kwatra",
                        "structuredName": {
                            "firstName": "Vivek",
                            "lastName": "Kwatra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vivek Kwatra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39685930"
                        ],
                        "name": "A. Sch\u00f6dl",
                        "slug": "A.-Sch\u00f6dl",
                        "structuredName": {
                            "firstName": "Arno",
                            "lastName": "Sch\u00f6dl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sch\u00f6dl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21472040"
                        ],
                        "name": "Irfan Essa",
                        "slug": "Irfan-Essa",
                        "structuredName": {
                            "firstName": "Irfan",
                            "lastName": "Essa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Irfan Essa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713189"
                        ],
                        "name": "Greg Turk",
                        "slug": "Greg-Turk",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Turk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Turk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688328"
                        ],
                        "name": "A. Bobick",
                        "slug": "A.-Bobick",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Bobick",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bobick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 107
                            }
                        ],
                        "text": "The graph cut algorithm has also been used in the computer graphics community, such as Graph Cut Textures [Kwatra et al. 2003], GrabCut [Rother et al. 2004] and Photomontage [Agarwala et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 106
                            }
                        ],
                        "text": "The graph cut algorithm has also been used in the computer graphics community, such as Graph Cut Textures [Kwatra et al. 2003], GrabCut [Rother et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6175301,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55c3ecb47aebba55a9de761c0e4dc0fd6d8d28b0",
            "isKey": false,
            "numCitedBy": 1556,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce a new algorithm for image and video texture synthesis. In our approach, patch regions from a sample image or video are transformed and copied to the output and then stitched together along optimal seams to generate a new (and typically larger) output. In contrast to other techniques, the size of the patch is not chosen a-priori, but instead a graph cut technique is used to determine the optimal patch region for any given offset between the input and output texture. Unlike dynamic programming, our graph cut technique for seam optimization is applicable in any dimension. We specifically explore it in 2D and 3D to perform video texture synthesis in addition to regular image synthesis. We present approximative offset search techniques that work well in conjunction with the presented patch size optimization. We show results for synthesizing regular, random, and natural images and videos. We also demonstrate how this method can be used to interactively merge different images to generate new scenes."
            },
            "slug": "Graphcut-textures:-image-and-video-synthesis-using-Kwatra-Sch\u00f6dl",
            "title": {
                "fragments": [],
                "text": "Graphcut textures: image and video synthesis using graph cuts"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A new algorithm for image and video texture synthesis where patch regions from a sample image or video are transformed and copied to the output and then stitched together along optimal seams to generate a new (and typically larger) output."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144565371"
                        ],
                        "name": "P. P\u00e9rez",
                        "slug": "P.-P\u00e9rez",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "P\u00e9rez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. P\u00e9rez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752496"
                        ],
                        "name": "M. Gangnet",
                        "slug": "M.-Gangnet",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Gangnet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gangnet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7209802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c32c7919f844f01819110354dd5f3fe46155b813",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of extracting continuous structures from noisy or cluttered images is a difficult one. Successful extraction depends critically on the ability to balance prior constraints on continuity and smoothness against evidence garnered from image analysis. Exact, deterministic optimisation algorithms, based on discretized functionals, suffer from severe limitations on the form of prior constraint that can be imposed tractably. This paper proposes a sequential Monte-Carlo technique, termed JetStream, that enables constraints on curvature, corners, and contour parallelism. To be mobilized, all of which are infeasible under exact optimization. The power of JetStream is demonstrated in two contexts: (1) interactive cut-out in photo-editing applications, and (2) the recovery of roads in aerial photographs."
            },
            "slug": "JetStream:-probabilistic-contour-extraction-with-P\u00e9rez-Blake",
            "title": {
                "fragments": [],
                "text": "JetStream: probabilistic contour extraction with particles"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A sequential Monte-Carlo technique, termed JetStream, is proposed that enables constraints on curvature, corners, and contour parallelism to be mobilized, all of which are infeasible under exact optimization."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692688"
                        ],
                        "name": "Yuri Boykov",
                        "slug": "Yuri-Boykov",
                        "structuredName": {
                            "firstName": "Yuri",
                            "lastName": "Boykov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuri Boykov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144197071"
                        ],
                        "name": "M. Jolly",
                        "slug": "M.-Jolly",
                        "structuredName": {
                            "firstName": "Marie-Pierre",
                            "lastName": "Jolly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jolly"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 95
                            }
                        ],
                        "text": "2000], sketch-based interaction [Tan and Ahuja 2001], interactive graph cut image segmentation [Boykov and Jolly 2001], GrabCut [Rother et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 95
                            }
                        ],
                        "text": "Similar marking UI to separate object from background is also presented in[Falcao et al. 2000; Boykov and Jolly 2001; Fails and Olsen 2003] for image segmentation or gesture tracking for camera-based interaction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 13
                            }
                        ],
                        "text": "Inspired by [Boykov and Jolly 2001], we also formulate image cutout as a graph cut problem in both steps."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 184
                            }
                        ],
                        "text": "\u2026paint [Reese and Barrett 2002; Barrett and Cheney 2002], marker drawing [Falcao et al. 2000], sketch-based interaction [Tan and Ahuja 2001], interactive graph cut image segmentation [Boykov and Jolly 2001], GrabCut [Rother et al. 2004]) and interactive image Photomontage [Agarwala et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 21
                            }
                        ],
                        "text": "We refer readers to [Boykov and Jolly 2001] for a detailed formulation of energy minimization as a graph cut problem and how to solve it."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2245438,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d3b177e8d027d44c191e739a3a70ccacc2eac82",
            "isKey": true,
            "numCitedBy": 4175,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe a new technique for general purpose interactive segmentation of N-dimensional images. The user marks certain pixels as \"object\" or \"background\" to provide hard constraints for segmentation. Additional soft constraints incorporate both boundary and region information. Graph cuts are used to find the globally optimal segmentation of the N-dimensional image. The obtained solution gives the best balance of boundary and region properties among all segmentations satisfying the constraints. The topology of our segmentation is unrestricted and both \"object\" and \"background\" segments may consist of several isolated parts. Some experimental results are presented in the context of photo/video editing and medical image segmentation. We also demonstrate an interesting Gestalt example. A fast implementation of our segmentation method is possible via a new max-flow algorithm."
            },
            "slug": "Interactive-graph-cuts-for-optimal-boundary-&-of-in-Boykov-Jolly",
            "title": {
                "fragments": [],
                "text": "Interactive graph cuts for optimal boundary & region segmentation of objects in N-D images"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "A new technique for general purpose interactive segmentation of N-dimensional images where the user marks certain pixels as \"object\" or \"background\" to provide hard constraints for segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1820595"
                        ],
                        "name": "L. Vincent",
                        "slug": "L.-Vincent",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Vincent",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2987794"
                        ],
                        "name": "P. Soille",
                        "slug": "P.-Soille",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Soille",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Soille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 35
                            }
                        ],
                        "text": "We choose the watershed algorithm [Vincent and Soille 1991], which locates boundaries well, and preserves small differences inside each small region."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15436061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a3819dda9a5f00dbb8cd3413ca7422e37a0d5794",
            "isKey": false,
            "numCitedBy": 5733,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "A fast and flexible algorithm for computing watersheds in digital gray-scale images is introduced. A review of watersheds and related motion is first presented, and the major methods to determine watersheds are discussed. The algorithm is based on an immersion process analogy, in which the flooding of the water in the picture is efficiently simulated using of queue of pixel. It is described in detail provided in a pseudo C language. The accuracy of this algorithm is proven to be superior to that of the existing implementations, and it is shown that its adaptation to any kind of digital grid and its generalization to n-dimensional images (and even to graphs) are straightforward. The algorithm is reported to be faster than any other watershed algorithm. Applications of this algorithm with regard to picture segmentation are presented for magnetic resonance (MR) imagery and for digital elevation models. An example of 3-D watershed is also provided. >"
            },
            "slug": "Watersheds-in-Digital-Spaces:-An-Efficient-Based-on-Vincent-Soille",
            "title": {
                "fragments": [],
                "text": "Watersheds in Digital Spaces: An Efficient Algorithm Based on Immersion Simulations"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A fast and flexible algorithm for computing watersheds in digital gray-scale images is introduced, based on an immersion process analogy, which is reported to be faster than any other watershed algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144154486"
                        ],
                        "name": "H. Shum",
                        "slug": "H.-Shum",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Shum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Shum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2200826"
                        ],
                        "name": "Shuntaro Yamazaki",
                        "slug": "Shuntaro-Yamazaki",
                        "structuredName": {
                            "firstName": "Shuntaro",
                            "lastName": "Yamazaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuntaro Yamazaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111190090"
                        ],
                        "name": "Yin Li",
                        "slug": "Yin-Li",
                        "structuredName": {
                            "firstName": "Yin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yin Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088295"
                        ],
                        "name": "Chi-Keung Tang",
                        "slug": "Chi-Keung-Tang",
                        "structuredName": {
                            "firstName": "Chi-Keung",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chi-Keung Tang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 24
                            }
                        ],
                        "text": "We use Coherent matting [Shum et al. 2004], an extended Bayesian"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 25
                            }
                        ],
                        "text": "We use Coherent matting [Shum et al. 2004], an extended Bayesian\nmatting [Chuang et al. 2001] with alpha prior, to compute the opacity around the object boundary before compositing the cutout object on a new background."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14439030,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "7510dadeb0d64d8dbb8e3c0237683db6e1454e01",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article, we present an image-based modeling and rendering system, which we call pop-up light field, that models a sparse light field using a set of coherent layers. In our system, the user specifies how many coherent layers should be modeled or popped up according to the scene complexity. A coherent layer is defined as a collection of corresponding planar regions in the light field images. A coherent layer can be rendered free of aliasing all by itself, or against other background layers. To construct coherent layers, we introduce a Bayesian approach, coherence matting, to estimate alpha matting around segmented layer boundaries by incorporating a coherence prior in order to maintain coherence across images.We have developed an intuitive and easy-to-use user interface (UI) to facilitate pop-up light field construction. The key to our UI is the concept of human-in-the-loop where the user specifies where aliasing occurs in the rendered image. The user input is reflected in the input light field images where pop-up layers can be modified. The user feedback is instant through a hardware-accelerated real-time pop-up light field renderer. Experimental results demonstrate that our system is capable of rendering anti-aliased novel views from a sparse light field."
            },
            "slug": "Pop-up-light-field:-An-interactive-image-based-and-Shum-Sun",
            "title": {
                "fragments": [],
                "text": "Pop-up light field: An interactive image-based modeling and rendering system"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "An image-based modeling and rendering system that models a sparse light field using a set of coherent layers, and introduces a Bayesian approach, coherence matting, to estimate alpha matting around segmented layer boundaries by incorporating a coherence prior in order to maintain coherence across images."
            },
            "venue": {
                "fragments": [],
                "text": "TOGS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143708263"
                        ],
                        "name": "Yung-Yu Chuang",
                        "slug": "Yung-Yu-Chuang",
                        "structuredName": {
                            "firstName": "Yung-Yu",
                            "lastName": "Chuang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yung-Yu Chuang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800609"
                        ],
                        "name": "B. Curless",
                        "slug": "B.-Curless",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Curless",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Curless"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745260"
                        ],
                        "name": "D. Salesin",
                        "slug": "D.-Salesin",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Salesin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Salesin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 74
                            }
                        ],
                        "text": "We use Coherent matting [Shum et al. 2004], an extended Bayesian\nmatting [Chuang et al. 2001] with alpha prior, to compute the opacity around the object boundary before compositing the cutout object on a new background."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 8
                            }
                        ],
                        "text": "matting [Chuang et al. 2001] with alpha prior, to compute the opacity around the object boundary before compositing the cutout object on a new background."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2062421,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03b2a8c90b9e5068bb05bfc885588e647f97356d",
            "isKey": false,
            "numCitedBy": 900,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new Bayesian framework for solving the matting problem, i.e. extracting a foreground element from a background image by estimating an opacity for each pixel of the foreground element. Our approach models both the foreground and background color distributions with spatially-varying sets of Gaussians, and assumes a fractional blending of the foreground and background colors to produce the final output. It then uses a maximum-likelihood criterion to estimate the optimal opacity, foreground and background simultaneously. In addition to providing a principled approach to the matting problem, our algorithm effectively handles objects with intricate boundaries, such as hair strands and fur, and provides an improvement over existing techniques for these difficult cases."
            },
            "slug": "A-Bayesian-approach-to-digital-matting-Chuang-Curless",
            "title": {
                "fragments": [],
                "text": "A Bayesian approach to digital matting"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "This paper proposes a new Bayesian framework for solving the matting problem, i.e. extracting a foreground element from a background image by estimating an opacity for each pixel of the foreground element by using a maximum-likelihood criterion."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3141061"
                        ],
                        "name": "Jerry Alan Fails",
                        "slug": "Jerry-Alan-Fails",
                        "structuredName": {
                            "firstName": "Jerry",
                            "lastName": "Fails",
                            "middleNames": [
                                "Alan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jerry Alan Fails"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733794"
                        ],
                        "name": "D. Olsen",
                        "slug": "D.-Olsen",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Olsen",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Olsen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 118
                            }
                        ],
                        "text": "Similar marking UI to separate object from background is also presented in[Falcao et al. 2000; Boykov and Jolly 2001; Fails and Olsen 2003] for image segmentation or gesture tracking for camera-based interaction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1758223,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "b1e394ce7a072bbb7b986a3d1bd7c296cc4aa60e",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Cameras provide an appealing new input medium for interaction. The creation of camera-based interfaces is outside the skill-set of most programmers and completely beyond the skills of most interface designers. Image Processing with Crayons is a tool for creating new camera-based interfaces using a simple painting metaphor. A transparent layers model is used to present the designer with all of the necessary information. Traditional machine learning algorithms have been modified to accommodate the rapid response time required of an interactive design tool."
            },
            "slug": "A-design-tool-for-camera-based-interaction-Fails-Olsen",
            "title": {
                "fragments": [],
                "text": "A design tool for camera-based interaction"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Image Processing with Crayons is a tool for creating new camera-based interfaces using a simple painting metaphor and a transparent layers model is used to present the designer with all of the necessary information."
            },
            "venue": {
                "fragments": [],
                "text": "CHI '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692688"
                        ],
                        "name": "Yuri Boykov",
                        "slug": "Yuri-Boykov",
                        "structuredName": {
                            "firstName": "Yuri",
                            "lastName": "Boykov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuri Boykov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653004"
                        ],
                        "name": "V. Kolmogorov",
                        "slug": "V.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kolmogorov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 78
                            }
                        ],
                        "text": "To minimize the energy E(X) in Equation (1), we use the maxflow algorithm in [Boykov and Kolmogorov 2001]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5324521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c62fdf1e6a520d9fee8ca9981fb588d07f2c6fa",
            "isKey": false,
            "numCitedBy": 3563,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Minimum cut/maximum flow algorithms on graphs have emerged as an increasingly useful tool for exactor approximate energy minimization in low-level vision. The combinatorial optimization literature provides many min-cut/max-flow algorithms with different polynomial time complexity. Their practical efficiency, however, has to date been studied mainly outside the scope of computer vision. The goal of this paper is to provide an experimental comparison of the efficiency of min-cut/max flow algorithms for applications in vision. We compare the running times of several standard algorithms, as well as a new algorithm that we have recently developed. The algorithms we study include both Goldberg-Tarjan style \"push -relabel\" methods and algorithms based on Ford-Fulkerson style \"augmenting paths.\" We benchmark these algorithms on a number of typical graphs in the contexts of image restoration, stereo, and segmentation. In many cases, our new algorithm works several times faster than any of the other methods, making near real-time performance possible. An implementation of our max-flow/min-cut algorithm is available upon request for research purposes."
            },
            "slug": "An-experimental-comparison-of-min-cut/max-flow-for-Boykov-Kolmogorov",
            "title": {
                "fragments": [],
                "text": "An experimental comparison of min-cut/max- flow algorithms for energy minimization in vision"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The goal of this paper is to provide an experimental comparison of the efficiency of min-cut/max flow algorithms for applications in vision, comparing the running times of several standard algorithms, as well as a new algorithm that is recently developed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 72
                            }
                        ],
                        "text": "The solution X = {xi} can be obtained by minimizing a Gibbs energy E(X) [Geman and Geman. 1984]:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5837272,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "459b30a9a960080f3b313e41886b1aa0e51e882c",
            "isKey": false,
            "numCitedBy": 18711,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios."
            },
            "slug": "Stochastic-Relaxation,-Gibbs-Distributions,-and-the-Geman-Geman",
            "title": {
                "fragments": [],
                "text": "Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The analogy between images and statistical mechanics systems is made and the analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations, creating a highly parallel ``relaxation'' algorithm for MAP estimation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35674406"
                        ],
                        "name": "Shigeo Abe DrEng",
                        "slug": "Shigeo-Abe-DrEng",
                        "structuredName": {
                            "firstName": "Shigeo",
                            "lastName": "DrEng",
                            "middleNames": [
                                "Abe"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shigeo Abe DrEng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 42
                            }
                        ],
                        "text": "The likelihood energy E1 is defined as in Equation (2) in the object marking step."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 44
                            }
                        ],
                        "text": "The likelihood energy E1 is also similar to Equation (2) while the color C(i) is computed as the mean color of the small region i ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 234
                            }
                        ],
                        "text": "In addition to the gradient term, E2 uses the polygon locations as soft constraints, in order to deal with ambiguous and low contrast gradient boundaries:\nE2(xi, xj) = |xi \u2212 xj | \u00b7 g ( (1 \u2212 \u03b2) \u00b7 Cij + \u03b2 \u00b7 \u03b7 \u00b7 g(D2ij) ) (4)\nwhere g(\u00b7) is the same as in Equation (3), Dij is the distance from the center of arc (i, j) to the polygon and \u03b7 is the scale to unify the units of the two terms (typical value is 10)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 3
                            }
                        ],
                        "text": "In Equation (4), \u03b2 \u2208 [0, 1] is used to control the influence of D(i, j)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 27
                            }
                        ],
                        "text": "For the prior energy E2 in Equation (3), we compared two definitions of Cij : 1) Cij is the mean color difference between the two regions i and j; 2) Similarly defined Cij , but it is weighted by the shared boundary length between regions i and j."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 31
                            }
                        ],
                        "text": "To minimize the energy E(X) in Equation (1), we use the maxflow algorithm in [Boykov and Kolmogorov 2001]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 86
                            }
                        ],
                        "text": "To compute E1, first the colors in seeds F and B are clustered by the K-means method [Duda et al. 2000]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 13
                            }
                        ],
                        "text": "In Equation (1), E1 encodes the color similarity of a node, indicating if it belongs to the foreground or background."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9384346,
            "fieldsOfStudy": [
                "Mathematics",
                "Environmental Science"
            ],
            "id": "65a69968bb8c41aad0113cec4c2d981bddf50bc8",
            "isKey": true,
            "numCitedBy": 13095,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Classification \u2022 Supervised \u2013 parallelpiped \u2013 minimum distance \u2013 maximum likelihood (Bayes Rule) > non-parametric > parametric \u2013 support vector machines \u2013 neural networks \u2013 context classification \u2022 Unsupervised (clustering) \u2013 K-Means \u2013 ISODATA \u2022 Pattern recognition in remote sensing has been based on the intuitive notion that pixels belonging to the same class should have similar gray values in a given band. \u2013 Given two spectral bands, pixels from the same class plotted in a two-dimensional histogram should appear as a localized cluster. \u2013 If n images, each in a different spectral band, are available, pixels from the same class should form a localized cluster in n-space."
            },
            "slug": "Pattern-Classification-DrEng",
            "title": {
                "fragments": [],
                "text": "Pattern Classification"
            },
            "venue": {
                "fragments": [],
                "text": "Springer London"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586918"
                        ],
                        "name": "D. Stork",
                        "slug": "D.-Stork",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Stork",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stork"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 86
                            }
                        ],
                        "text": "To compute E1, first the colors in seeds F and B are clustered by the K-means method [Duda et al. 2000]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 361680,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "e75fceff79fefa063d00ebc56a20c7df5485cf2b",
            "isKey": false,
            "numCitedBy": 4146,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-classification,-2nd-Edition-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification, 2nd Edition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716806"
                        ],
                        "name": "A. Falc\u00e3o",
                        "slug": "A.-Falc\u00e3o",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Falc\u00e3o",
                            "middleNames": [
                                "Xavier"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Falc\u00e3o"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 75
                            }
                        ],
                        "text": "Similar marking UI to separate object from background is also presented in[Falcao et al. 2000; Boykov and Jolly 2001; Fails and Olsen 2003] for image segmentation or gesture tracking for camera-based interaction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 148
                            }
                        ],
                        "text": "\u2026by using region-based methods, e.g., magic wand in Photoshop, intelligent paint [Reese and Barrett 2002; Barrett and Cheney 2002], marker drawing [Falcao et al. 2000], sketch-based interaction [Tan and Ahuja 2001], interactive graph cut image segmentation [Boykov and Jolly 2001], GrabCut [Rother\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 111
                            }
                        ],
                        "text": ", magic wand in Photoshop, intelligent paint [Reese and Barrett 2002; Barrett and Cheney 2002], marker drawing [Falcao et al. 2000], sketch-based interaction [Tan and Ahuja 2001], interactive graph cut image segmentation [Boykov and Jolly 2001], GrabCut [Rother et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17610422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2350c3785f09cabce24283fb26d4ced5b2ffb5b0",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Image-Foresting-Transformation-Falc\u00e3o",
            "title": {
                "fragments": [],
                "text": "The Image Foresting Transformation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "User Input Result Border Matting Hard Segmentation  Band of Uncertainty  Soft Segmentation  Extract \u03b1-values along border F B Bayes Matting"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 11,
            "methodology": 14
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 21,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Lazy-snapping-Li-Sun/4f51d84009a1ec30dffa5ab8a1c3214f669086cf?sort=total-citations"
}