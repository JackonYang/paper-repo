{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577358"
                        ],
                        "name": "P. Srinivasan",
                        "slug": "P.-Srinivasan",
                        "structuredName": {
                            "firstName": "Praveen",
                            "lastName": "Srinivasan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Srinivasan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16635607,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49927656ede0c75af22ca73dcf4abba028839650",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Analyzing videos of human activities involves not only recognizing actions (typically based on their appearances), but also determining the story/plot of the video. The storyline of a video describes causal relationships between actions. Beyond recognition of individual actions, discovering causal relationships helps to better understand the semantic meaning of the activities. We present an approach to learn a visually grounded storyline model of videos directly from weakly labeled data. The storyline model is represented as an AND-OR graph, a structure that can compactly encode storyline variation across videos. The edges in the AND-OR graph correspond to causal relationships which are represented in terms of spatio-temporal constraints. We formulate an Integer Programming framework for action recognition and storyline extraction using the storyline model and visual groundings learned from training data."
            },
            "slug": "Understanding-videos,-constructing-plots-learning-a-Gupta-Srinivasan",
            "title": {
                "fragments": [],
                "text": "Understanding videos, constructing plots learning a visually grounded storyline model from annotated videos"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An Integer Programming framework for action recognition and storyline extraction using the storyline model and visual groundings learned from training data is formulated."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2601166"
                        ],
                        "name": "E. Tzoukermann",
                        "slug": "E.-Tzoukermann",
                        "structuredName": {
                            "firstName": "Evelyne",
                            "lastName": "Tzoukermann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Tzoukermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144660077"
                        ],
                        "name": "J. Neumann",
                        "slug": "J.-Neumann",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Neumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743020"
                        ],
                        "name": "J. Kosecka",
                        "slug": "J.-Kosecka",
                        "structuredName": {
                            "firstName": "Jana",
                            "lastName": "Kosecka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kosecka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759899"
                        ],
                        "name": "C. Ferm\u00fcller",
                        "slug": "C.-Ferm\u00fcller",
                        "structuredName": {
                            "firstName": "Cornelia",
                            "lastName": "Ferm\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ferm\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145007714"
                        ],
                        "name": "Ian Perera",
                        "slug": "Ian-Perera",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Perera",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian Perera"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064957124"
                        ],
                        "name": "Francis Ferraro",
                        "slug": "Francis-Ferraro",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Ferraro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francis Ferraro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9133363"
                        ],
                        "name": "Benjamin Sapp",
                        "slug": "Benjamin-Sapp",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Sapp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Sapp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36899698"
                        ],
                        "name": "Rizwan Ahmed Chaudhry",
                        "slug": "Rizwan-Ahmed-Chaudhry",
                        "structuredName": {
                            "firstName": "Rizwan",
                            "lastName": "Chaudhry",
                            "middleNames": [
                                "Ahmed"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rizwan Ahmed Chaudhry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34674427"
                        ],
                        "name": "Gautam Singh",
                        "slug": "Gautam-Singh",
                        "structuredName": {
                            "firstName": "Gautam",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gautam Singh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 236
                            }
                        ],
                        "text": "\u2026far been mostly explored\nby the computer vision community, where different methods for improving action recognition by exploiting linguistic data have been proposed (Gupta and Mooney, 2010; Motwani and Mooney, 2012; Cour et al., 2008; Tzoukermann et al., 2011; Rohrbach et al., 2012b, among others)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 144
                            }
                        ],
                        "text": "\u2026and granularity of action descriptions in context\n\u2022 A high-quality alignment of sentences with video segments, supporting the grounding of action descriptions in visual information\n\u2022 Collections of paraphrases describing the same scene, which result as a by-product from the text-video\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8513914,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef373d0f3eea73230edf39ab246c1ec96ccfe22d",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper addresses the following issues: (a) how to represent semantic information from natural language so that a vision model can utilize it? (b) how to extract the salient textual information relevant to vision? For a given domain, we present a new model of semantic extraction that takes into account word relatedness as well as word disambiguation in order to apply to a vision model. We automatically process the text transcripts and perform syntactic analysis to extract dependency relations. We then perform semantic extraction on the output to filter semantic entities related to actions. The resulting data are used to populate a matrix of co-occurrences utilized by the vision processing modules. Results show that explicitly modeling the co-occurrence of actions and tools significantly improved performance."
            },
            "slug": "Language-Models-for-Semantic-Extraction-and-in-Tzoukermann-Neumann",
            "title": {
                "fragments": [],
                "text": "Language Models for Semantic Extraction and Filtering in Video Action Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new model of semantic extraction is presented that takes into account word relatedness as well as word disambiguation in order to apply to a vision model and shows that explicitly modeling the co-occurrence of actions and tools significantly improved performance."
            },
            "venue": {
                "fragments": [],
                "text": "Language-Action Tools for Cognitive Artificial Agents"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34176020"
                        ],
                        "name": "Jesse Dodge",
                        "slug": "Jesse-Dodge",
                        "structuredName": {
                            "firstName": "Jesse",
                            "lastName": "Dodge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jesse Dodge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46479604"
                        ],
                        "name": "Amit Goyal",
                        "slug": "Amit-Goyal",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amit Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682965"
                        ],
                        "name": "Xufeng Han",
                        "slug": "Xufeng-Han",
                        "structuredName": {
                            "firstName": "Xufeng",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xufeng Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40614240"
                        ],
                        "name": "Alyssa C. Mensch",
                        "slug": "Alyssa-C.-Mensch",
                        "structuredName": {
                            "firstName": "Alyssa",
                            "lastName": "Mensch",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alyssa C. Mensch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49501003"
                        ],
                        "name": "Margaret Mitchell",
                        "slug": "Margaret-Mitchell",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Margaret Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714215"
                        ],
                        "name": "K. Stratos",
                        "slug": "K.-Stratos",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Stratos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Stratos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721910"
                        ],
                        "name": "Kota Yamaguchi",
                        "slug": "Kota-Yamaguchi",
                        "structuredName": {
                            "firstName": "Kota",
                            "lastName": "Yamaguchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kota Yamaguchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699545"
                        ],
                        "name": "Yejin Choi",
                        "slug": "Yejin-Choi",
                        "structuredName": {
                            "firstName": "Yejin",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yejin Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722360"
                        ],
                        "name": "Hal Daum\u00e9",
                        "slug": "Hal-Daum\u00e9",
                        "structuredName": {
                            "firstName": "Hal",
                            "lastName": "Daum\u00e9",
                            "middleNames": [],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hal Daum\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3570838,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d5e1395e1ace37d9d5b7ce6854d518e7f128e79",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "When people describe a scene, they often include information that is not visually apparent; sometimes based on background knowledge, sometimes to tell a story. We aim to separate visual text---descriptions of what is being seen---from non-visual text in natural images and their descriptions. To do so, we first concretely define what it means to be visual, annotate visual text and then develop algorithms to automatically classify noun phrases as visual or non-visual. We find that using text alone, we are able to achieve high accuracies at this task, and that incorporating features derived from computer vision algorithms improves performance. Finally, we show that we can reliably mine visual nouns and adjectives from large corpora and that we can use these effectively in the classification task."
            },
            "slug": "Detecting-Visual-Text-Dodge-Goyal",
            "title": {
                "fragments": [],
                "text": "Detecting Visual Text"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work concretely defines what it means to be visual, annotate visual text and develops algorithms to automatically classify noun phrases as visual or non-visual, and finds that using text alone, it is able to achieve high accuracies at this task, and that incorporating features derived from computer vision algorithms improves performance."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37710702"
                        ],
                        "name": "Stefan Mathe",
                        "slug": "Stefan-Mathe",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Mathe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Mathe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775745"
                        ],
                        "name": "A. Fazly",
                        "slug": "A.-Fazly",
                        "structuredName": {
                            "firstName": "Afsaneh",
                            "lastName": "Fazly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fazly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779136"
                        ],
                        "name": "S. Dickinson",
                        "slug": "S.-Dickinson",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Dickinson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dickinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145584212"
                        ],
                        "name": "S. Stevenson",
                        "slug": "S.-Stevenson",
                        "structuredName": {
                            "firstName": "Suzanne",
                            "lastName": "Stevenson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Stevenson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 33
                            }
                        ],
                        "text": "An interesting in-depth study by Mathe et al. (2008) automatically learnt the semantics of motion verbs as abstract features from videos."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "3.3)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14415724,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6cd2d1058023fbf9dc9fb12e5c2bbd88037e8a0a",
            "isKey": true,
            "numCitedBy": 10,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an algorithm for learning the semantics of a (motion) verb from videos depicting the action expressed by the verb, paired with sentences describing the action participants and their roles. Acknowledging that commonalities among example videos may not exist at the level of the input features, our approximation algorithm efficiently searches the space of more abstract features for a common solution. We test our algorithm by using it to learn the semantics of a sample set of verbs; results demonstrate the usefulness of the proposed framework, while identifying directions for further improvement."
            },
            "slug": "Learning-the-abstract-motion-semantics-of-verbs-Mathe-Fazly",
            "title": {
                "fragments": [],
                "text": "Learning the abstract motion semantics of verbs from captioned videos"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "An algorithm for learning the semantics of a (motion) verb from videos depicting the action expressed by the verb, paired with sentences describing the action participants and their roles is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118343423"
                        ],
                        "name": "Sonal Gupta",
                        "slug": "Sonal-Gupta",
                        "structuredName": {
                            "firstName": "Sonal",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sonal Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 215540570,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ac56ce34776bda8541e5773699a0419780f9b5c",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognizing activities in real-world videos is a difficult problem exacerbated by background clutter, changes in camera angle & zoom, and rapid camera movements. Large corpora of labeled videos can be used to train automated activity recognition systems, but this requires expensive human labor and time. This paper explores how closed captions that naturally accompany many videos can act as weak supervision that allows automatically collecting 'labeled' data for activity recognition. We show that such an approach can improve activity retrieval in soccer videos. Our system requires no manual labeling of video clips and needs minimal human supervision. We also present a novel caption classifier that uses additional linguistic information to determine whether a specific comment refers to an ongoing activity. We demonstrate that combining linguistic analysis and automatically trained activity recognizers can significantly improve the precision of video retrieval."
            },
            "slug": "Using-closed-captions-as-supervision-for-video-Gupta-Mooney",
            "title": {
                "fragments": [],
                "text": "Using closed captions as supervision for video activity recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is demonstrated that combining linguistic analysis and automatically trained activity recognizers can significantly improve the precision of video retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI 2010"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2490430"
                        ],
                        "name": "Carina Silberer",
                        "slug": "Carina-Silberer",
                        "structuredName": {
                            "firstName": "Carina",
                            "lastName": "Silberer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carina Silberer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8871184,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "d4149dbef949644dad4833012e2def98529c0241",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two."
            },
            "slug": "Grounded-Models-of-Semantic-Representation-Silberer-Lapata",
            "title": {
                "fragments": [],
                "text": "Grounded Models of Semantic Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2485529"
                        ],
                        "name": "Michaela Regneri",
                        "slug": "Michaela-Regneri",
                        "structuredName": {
                            "firstName": "Michaela",
                            "lastName": "Regneri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michaela Regneri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1906895"
                        ],
                        "name": "M. Andriluka",
                        "slug": "M.-Andriluka",
                        "structuredName": {
                            "firstName": "Mykhaylo",
                            "lastName": "Andriluka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Andriluka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40404576"
                        ],
                        "name": "S. Amin",
                        "slug": "S.-Amin",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Amin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717560"
                        ],
                        "name": "Manfred Pinkal",
                        "slug": "Manfred-Pinkal",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Pinkal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred Pinkal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 60
                            }
                        ],
                        "text": "For the experiments reported below we use the same setup as Rohrbach et al. (2012b) and use all videos in MPII Composites and MPII Cooking (Rohrbach et al., 2012a), excluding the 127 videos used during evaluation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 106
                            }
                        ],
                        "text": "For our setting this feature representation has been shown to be superior to human pose-based approaches (Rohrbach et al., 2012a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 34
                            }
                        ],
                        "text": "Starting from the video corpus of Rohrbach et al. (2012b), which contains highresolution video recordings of basic cooking tasks, we collected multiple textual descriptions of each video via Mechanical Turk."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 21
                            }
                        ],
                        "text": ") Visual Classifiers (Rohrbach et al. 2012): Cosine similarity of two classifier output vectors (components = likelihood of an activity or object to have appeared in a video segment); supervised Task: Action similarity evaluation"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 82
                            }
                        ],
                        "text": "We build our corpus on top of the \u201cMPII Cooking Composite Activities\u201d video corpus (Rohrbach et al., 2012b, MPII Composites), which contains videos of different activities in the cooking domain, e.g., preparing carrots or separating eggs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Rohrbach et al. (2012b) showed that using such an attribute classifier representation can significantly improve per-\nformance for composite activity recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 103
                            }
                        ],
                        "text": "The second approach builds upon the first by additionally learning higher level attribute classifiers (Rohrbach et al., 2012b) on a held out training set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 262
                            }
                        ],
                        "text": "\u2026far been mostly explored\nby the computer vision community, where different methods for improving action recognition by exploiting linguistic data have been proposed (Gupta and Mooney, 2010; Motwani and Mooney, 2012; Cour et al., 2008; Tzoukermann et al., 2011; Rohrbach et al., 2012b, among others)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10175213,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8623fe8b087cedcaac276e313f8fed6f0dfccc33",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art human activity recognition methods build on discriminative learning which requires a representative training set for good performance. This leads to scalability issues for the recognition of large sets of highly diverse activities. In this paper we leverage the fact that many human activities are compositional and that the essential components of the activities can be obtained from textual descriptions or scripts. To share and transfer knowledge between composite activities we model them by a common set of attributes corresponding to basic actions and object participants. This attribute representation allows to incorporate script data that delivers new variations of a composite activity or even to unseen composite activities. In our experiments on 41 composite cooking tasks, we found that script data to successfully capture the high variability of composite activities. We show improvements in a supervised case where training data for all composite cooking tasks is available, but we are also able to recognize unseen composites by just using script data and without any manual video annotation."
            },
            "slug": "Script-Data-for-Attribute-Based-Recognition-of-Rohrbach-Regneri",
            "title": {
                "fragments": [],
                "text": "Script Data for Attribute-Based Recognition of Composite Activities"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper leverage the fact that many human activities are compositional and that the essential components of the activities can be obtained from textual descriptions or scripts to incorporate script data that delivers new variations of a composite activity or even to unseen composite activities."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717629"
                        ],
                        "name": "Yansong Feng",
                        "slug": "Yansong-Feng",
                        "structuredName": {
                            "firstName": "Yansong",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yansong Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Feng and Lapata (2010) were the first to enrich topic models for newspaper articles with visual information, by incorporating features from article illustrations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 211
                            }
                        ],
                        "text": "In the last few years, a growing amount of work has been devoted to the task of\ngrounding meaning in visual information, in particular by extending the distributional approach to jointly cover texts and images (Feng and Lapata, 2010; Bruni et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 210
                            }
                        ],
                        "text": "In the last few years, a growing amount of work has been devoted to the task of grounding meaning in visual information, in particular by extending the distributional approach to jointly cover texts and images (Feng and Lapata, 2010; Bruni et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14826028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d84082b1b86be38decd388f955cd9a884e0311f",
            "isKey": true,
            "numCitedBy": 116,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "The question of how meaning might be acquired by young children and represented by adult speakers of a language is one of the most debated topics in cognitive science. Existing semantic representation models are primarily amodal based on information provided by the linguistic input despite ample evidence indicating that the cognitive system is also sensitive to perceptual information. In this work we exploit the vast resource of images and associated documents available on the web and develop a model of multimodal meaning representation which is based on the linguistic and visual context. Experimental results show that a closer correspondence to human data can be obtained by taking the visual modality into account."
            },
            "slug": "Visual-Information-in-Semantic-Representation-Feng-Lapata",
            "title": {
                "fragments": [],
                "text": "Visual Information in Semantic Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experimental results show that a closer correspondence to human data can be obtained by taking the visual modality into account and a model of multimodal meaning representation which is based on the linguistic and visual context is developed."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2807482"
                        ],
                        "name": "Timoth\u00e9e Cour",
                        "slug": "Timoth\u00e9e-Cour",
                        "structuredName": {
                            "firstName": "Timoth\u00e9e",
                            "lastName": "Cour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timoth\u00e9e Cour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053283925"
                        ],
                        "name": "Christopher T. Jordan",
                        "slug": "Christopher-T.-Jordan",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Jordan",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher T. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759302"
                        ],
                        "name": "Eleni Miltsakaki",
                        "slug": "Eleni-Miltsakaki",
                        "structuredName": {
                            "firstName": "Eleni",
                            "lastName": "Miltsakaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eleni Miltsakaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 217
                            }
                        ],
                        "text": "\u2026far been mostly explored\nby the computer vision community, where different methods for improving action recognition by exploiting linguistic data have been proposed (Gupta and Mooney, 2010; Motwani and Mooney, 2012; Cour et al., 2008; Tzoukermann et al., 2011; Rohrbach et al., 2012b, among others)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 145
                            }
                        ],
                        "text": "\u2026e.g., the selection and granularity of action descriptions in context\n\u2022 A high-quality alignment of sentences with video segments, supporting the grounding of action descriptions in visual information\n\u2022 Collections of paraphrases describing the same scene, which result as a by-product from\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14415655,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c980b058f98dc1904ad328c2341a47c31479d076",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Movies and TV are a rich source of diverse and complex video of people, objects, actions and locales \"in the wild\". Harvesting automatically labeled sequences of actions from video would enable creation of large-scale and highly-varied datasets. To enable such collection, we focus on the task of recovering scene structure in movies and TV series for object tracking and action retrieval. We present a weakly supervised algorithm that uses the screenplay and closed captions to parse a movie into a hierarchy of shots and scenes. Scene boundaries in the movie are aligned with screenplay scene labels and shots are reordered into a sequence of long continuous tracks or threads which allow for more accurate tracking of people, actions and objects. Scene segmentation, alignment, and shot threading are formulated as inference in a unified generative model and a novel hierarchical dynamic programming algorithm that can handle alignment and jump-limited reorderings in linear time is presented. We present quantitative and qualitative results on movie alignment and parsing, and use the recovered structure to improve character naming and retrieval of common actions in several episodes of popular TV series."
            },
            "slug": "Movie/Script:-Alignment-and-Parsing-of-Video-and-Cour-Jordan",
            "title": {
                "fragments": [],
                "text": "Movie/Script: Alignment and Parsing of Video and Text Transcription"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A weakly supervised algorithm is presented that uses the screenplay and closed captions to parse a movie into a hierarchy of shots and scenes, and the recovered structure is used to improve character naming and retrieval of common actions in several episodes of popular TV series."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46506697"
                        ],
                        "name": "Heng Wang",
                        "slug": "Heng-Wang",
                        "structuredName": {
                            "firstName": "Heng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2909350"
                        ],
                        "name": "Alexander Kl\u00e4ser",
                        "slug": "Alexander-Kl\u00e4ser",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Kl\u00e4ser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Kl\u00e4ser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689269"
                        ],
                        "name": "Cheng-Lin Liu",
                        "slug": "Cheng-Lin-Liu",
                        "structuredName": {
                            "firstName": "Cheng-Lin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Lin Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13537104,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3afbb0e64fcb70496b44b30b76fac9456cc51e34",
            "isKey": false,
            "numCitedBy": 2230,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Feature trajectories have shown to be efficient for representing videos. Typically, they are extracted using the KLT tracker or matching SIFT descriptors between frames. However, the quality as well as quantity of these trajectories is often not sufficient. Inspired by the recent success of dense sampling in image classification, we propose an approach to describe videos by dense trajectories. We sample dense points from each frame and track them based on displacement information from a dense optical flow field. Given a state-of-the-art optical flow algorithm, our trajectories are robust to fast irregular motions as well as shot boundaries. Additionally, dense trajectories cover the motion information in videos well. We, also, investigate how to design descriptors to encode the trajectory information. We introduce a novel descriptor based on motion boundary histograms, which is robust to camera motion. This descriptor consistently outperforms other state-of-the-art descriptors, in particular in uncontrolled realistic videos. We evaluate our video description in the context of action classification with a bag-of-features approach. Experimental results show a significant improvement over the state of the art on four datasets of varying difficulty, i.e. KTH, YouTube, Hollywood2 and UCF sports."
            },
            "slug": "Action-recognition-by-dense-trajectories-Wang-Kl\u00e4ser",
            "title": {
                "fragments": [],
                "text": "Action recognition by dense trajectories"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work introduces a novel descriptor based on motion boundary histograms, which is robust to camera motion and consistently outperforms other state-of-the-art descriptors, in particular in uncontrolled realistic videos."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804885"
                        ],
                        "name": "M. Steyvers",
                        "slug": "M.-Steyvers",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Steyvers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Steyvers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Steyvers (2010) and later Silberer and Lapata (2012) present an alternative approach to incorporating visual information directly: they use so-called feature norms, which consist of human associations for many given words, as a proxy for general perceptual information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 746449,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "a4cfe2c3f914f9b9c1570244d8ef491c2d44d670",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Combining-feature-norms-and-text-data-with-topic-Steyvers",
            "title": {
                "fragments": [],
                "text": "Combining feature norms and text data with topic models."
            },
            "venue": {
                "fragments": [],
                "text": "Acta psychologica"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2552871"
                        ],
                        "name": "Elia Bruni",
                        "slug": "Elia-Bruni",
                        "structuredName": {
                            "firstName": "Elia",
                            "lastName": "Bruni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elia Bruni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34710580"
                        ],
                        "name": "G. Tran",
                        "slug": "G.-Tran",
                        "structuredName": {
                            "firstName": "Giang",
                            "lastName": "Tran",
                            "middleNames": [
                                "Binh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1939935,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1003bccaddda851c19a1127b23691e17bc6a334b",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a distributional semantic model combining text- and image-based features. We evaluate this multimodal semantic model on simulating similarity judgments, concept clustering and the BLESS benchmark. When integrated with the same core text-based model, image-based features are at least as good as further text-based features, and they capture different qualitative aspects of the tasks, suggesting that the two sources of information are complementary."
            },
            "slug": "Distributional-semantics-from-text-and-images-Bruni-Tran",
            "title": {
                "fragments": [],
                "text": "Distributional semantics from text and images"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This multimodal semantic model combining text- and image-based features is evaluated on simulating similarity judgments, concept clustering and the BLESS benchmark, suggesting that the two sources of information are complementary."
            },
            "venue": {
                "fragments": [],
                "text": "GEMS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1631318420"
                        ],
                        "name": "Tanvi S. Motwani",
                        "slug": "Tanvi-S.-Motwani",
                        "structuredName": {
                            "firstName": "Tanvi",
                            "lastName": "Motwani",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tanvi S. Motwani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 191
                            }
                        ],
                        "text": "\u2026far been mostly explored\nby the computer vision community, where different methods for improving action recognition by exploiting linguistic data have been proposed (Gupta and Mooney, 2010; Motwani and Mooney, 2012; Cour et al., 2008; Tzoukermann et al., 2011; Rohrbach et al., 2012b, among others)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 215542366,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47af965b9567bb0389cea161c9bd43a23a2aedd6",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognizing activities in real-world videos is a challenging AI problem. We present a novel combination of standard activity classification, object recognition, and text mining to learn effective activity recognizers without ever explicitly labeling training videos. We cluster verbs used to describe videos to automatically discover classes of activities and produce a labeled training set. This labeled data is then used to train an activity classifier based on spatio-temporal features. Next, text mining is employed to learn the correlations between these verbs and related objects. This knowledge is then used together with the outputs of an off-the-shelf object recognizer and the trained activity classifier to produce an improved activity recognizer. Experiments on a corpus of YouTube videos demonstrate the effectiveness of the overall approach."
            },
            "slug": "Improving-Video-Activity-Recognition-using-Object-Motwani-Mooney",
            "title": {
                "fragments": [],
                "text": "Improving Video Activity Recognition using Object Recognition and Text Mining"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This work presents a novel combination of standard activity classification, object recognition, and text mining to learn effective activity recognizers without ever explicitly labeling training videos."
            },
            "venue": {
                "fragments": [],
                "text": "ECAI"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2314929"
                        ],
                        "name": "Hilke Reckman",
                        "slug": "Hilke-Reckman",
                        "structuredName": {
                            "firstName": "Hilke",
                            "lastName": "Reckman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hilke Reckman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633360"
                        ],
                        "name": "Jeff Orkin",
                        "slug": "Jeff-Orkin",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Orkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Orkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145364504"
                        ],
                        "name": "D. Roy",
                        "slug": "D.-Roy",
                        "structuredName": {
                            "firstName": "Deb",
                            "lastName": "Roy",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 197858,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "6e6339fb08b6990b9a118d055218fd62579bb481",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We use data from a virtual world game for automated learning of words and grammatical constructions and their meanings. The language data are an integral part of the social interaction in the game and consist of chat dialogue, which is only constrained by the cultural context, as set by the nature of the provided virtual environment. Building on previous work, where we extracted a vocabulary for concrete objects in the game by making use of the non-linguistic context, we now target NP/DP grammar, in particular determiners. We assume that we have captured the meanings of a set of determiners if we can predict which determiner will be used in a particular context. To this end we train a classifier that predicts the choice of a determiner on the basis of features from the linguistic and non-linguistic context."
            },
            "slug": "Extracting-aspects-of-determiner-meaning-from-in-a-Reckman-Orkin",
            "title": {
                "fragments": [],
                "text": "Extracting aspects of determiner meaning from dialogue in a virtual world environment"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work uses data from a virtual world game for automated learning of words and grammatical constructions and their meanings and trains a classifier that predicts the choice of a determiner on the basis of features from the linguistic and non-linguistic context."
            },
            "venue": {
                "fragments": [],
                "text": "IWCS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2258966"
                        ],
                        "name": "A. Glenberg",
                        "slug": "A.-Glenberg",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Glenberg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Glenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2871004"
                        ],
                        "name": "Michael P. Kaschak",
                        "slug": "Michael-P.-Kaschak",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kaschak",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael P. Kaschak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 85
                            }
                        ],
                        "text": "Psychological studies have shown the connection between action semantics and videos (Glenberg, 2002; Howell et al., 2005), but to our knowledge, we are the first to provide a suitable data source and to implement such a model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Submitted 10/2012; Published 3/2013. c\u00a92013 Association for Computational Linguistics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1274984,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "6b04c7901672615a2b6fa13d5821cd52771bcc86",
            "isKey": false,
            "numCitedBy": 1900,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We report a new phenomenon associated with language comprehension: theaction\u2014sentence compatibility effect (ACE). Participants judged whether sentences were sensible by making a response that required moving toward or away from their bodies. When a sentence implied action in one direction (e.g., \u201cClose the drawer\u201d implies action away from the body), the participants had difficulty making a sensibility judgment requiring a response in the opposite direction. The ACE was demonstrated for three sentences types: imperative sentences, sentences describing the transfer of concrete objects, and sentences describing the transfer of abstract entities, such as \u201cLiz told you the story.\u201d These data are inconsistent with theories of language comprehension in which meaning is represented as a set of relations among nodes. Instead, the data support an embodied theory of meaning that relates the meaning of sentences to human action."
            },
            "slug": "Grounding-language-in-action-Glenberg-Kaschak",
            "title": {
                "fragments": [],
                "text": "Grounding language in action"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The data support an embodied theory of meaning that relates the meaning of sentences to human action, and are inconsistent with theories of language comprehension in which meaning is represented as a set of relations among nodes."
            },
            "venue": {
                "fragments": [],
                "text": "Psychonomic bulletin & review"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1856025"
                        ],
                        "name": "Carl Vondrick",
                        "slug": "Carl-Vondrick",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Vondrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl Vondrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143712289"
                        ],
                        "name": "Donald J. Patterson",
                        "slug": "Donald-J.-Patterson",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Patterson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald J. Patterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 104
                            }
                        ],
                        "text": "We published these assignments (HITs) on MTurk, using an adapted version3 of the annotation tool Vatic (Vondrick et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 11
                            }
                        ],
                        "text": "Overall, we had to filter out 13% of the text instances, which left us with 2206 textual video descriptions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2315620,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a061e7eab865fc8d2ef00e029b7070719ad2e9a",
            "isKey": false,
            "numCitedBy": 508,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an extensive three year study on economically annotating video with crowdsourced marketplaces. Our public framework has annotated thousands of real world videos, including massive data sets unprecedented for their size, complexity, and cost. To accomplish this, we designed a state-of-the-art video annotation user interface and demonstrate that, despite common intuition, many contemporary interfaces are sub-optimal. We present several user studies that evaluate different aspects of our system and demonstrate that minimizing the cognitive load of the user is crucial when designing an annotation platform. We then deploy this interface on Amazon Mechanical Turk and discover expert and talented workers who are capable of annotating difficult videos with dense and closely cropped labels. We argue that video annotation requires specialized skill; most workers are poor annotators, mandating robust quality control protocols. We show that traditional crowdsourced micro-tasks are not suitable for video annotation and instead demonstrate that deploying time-consuming macro-tasks on MTurk is effective. Finally, we show that by extracting pixel-based features from manually labeled key frames, we are able to leverage more sophisticated interpolation strategies to maximize performance given a fixed budget. We validate the power of our framework on difficult, real-world data sets and we demonstrate an inherent trade-off between the mix of human and cloud computing used vs. the accuracy and cost of the labeling. We further introduce a novel, cost-based evaluation criteria that compares vision algorithms by the budget required to achieve an acceptable performance. We hope our findings will spur innovation in the creation of massive labeled video data sets and enable novel data-driven computer vision applications."
            },
            "slug": "Efficiently-Scaling-up-Crowdsourced-Video-Vondrick-Patterson",
            "title": {
                "fragments": [],
                "text": "Efficiently Scaling up Crowdsourced Video Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is argued that video annotation requires specialized skill; most workers are poor annotators, mandating robust quality control protocols and an inherent trade-off between the mix of human and cloud computing used vs. the accuracy and cost of the labeling."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708114"
                        ],
                        "name": "Katrin Erk",
                        "slug": "Katrin-Erk",
                        "structuredName": {
                            "firstName": "Katrin",
                            "lastName": "Erk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katrin Erk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145586618"
                        ],
                        "name": "Diana McCarthy",
                        "slug": "Diana-McCarthy",
                        "structuredName": {
                            "firstName": "Diana",
                            "lastName": "McCarthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diana McCarthy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1817941"
                        ],
                        "name": "Nicholas Gaylord",
                        "slug": "Nicholas-Gaylord",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Gaylord",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicholas Gaylord"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 123
                            }
                        ],
                        "text": "We call it the \u201cAction Similarity Dataset\u201d (ASim) in analogy to the Usage Similarity dataset (USim) of Erk et al. (2009) and Erk et al. (2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 693326,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f75e3453679b8373bae81a66db008b2577dba795",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 101,
            "paperAbstract": {
                "fragments": [],
                "text": "Word sense disambiguation (WSD) is an old and important task in computational linguistics that still remains challenging, to machines as well as to human annotators. Recently there have been several proposals for representing word meaning in context that diverge from the traditional use of a single best sense for each occurrence. They represent word meaning in context through multiple paraphrases, as points in vector space, or as distributions over latent senses. New methods of evaluating and comparing these different representations are needed.In this paper we propose two novel annotation schemes that characterize word meaning in context in a graded fashion. In WSsim annotation, the applicability of each dictionary sense is rated on an ordinal scale. Usim annotation directly rates the similarity of pairs of usages of the same lemma, again on a scale. We find that the novel annotation schemes show good inter-annotator agreement, as well as a strong correlation with traditional single-sense annotation and with annotation of multiple lexical paraphrases. Annotators make use of the whole ordinal scale, and give very fine-grained judgments that \u201cmix and match\u201d senses for each individual usage. We also find that the Usim ratings obey the triangle inequality, justifying models that treat usage similarity as metric.There has recently been much work on grouping senses into coarse-grained groups. We demonstrate that graded WSsim and Usim ratings can be used to analyze existing coarse-grained sense groupings to identify sense groups that may not match intuitions of untrained native speakers. In the course of the comparison, we also show that the WSsim ratings are not subsumed by any static sense grouping."
            },
            "slug": "Measuring-Word-Meaning-in-Context-Erk-McCarthy",
            "title": {
                "fragments": [],
                "text": "Measuring Word Meaning in Context"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is demonstrated that graded WSsim and Usim ratings can be used to analyze existing coarse-grained sense groupings to identify sense groups that may not match intuitions of untrained native speakers, and that the WSsim ratings are not subsumed by any static sense grouping."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633360"
                        ],
                        "name": "Jeff Orkin",
                        "slug": "Jeff-Orkin",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Orkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Orkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145364504"
                        ],
                        "name": "D. Roy",
                        "slug": "D.-Roy",
                        "structuredName": {
                            "firstName": "Deb",
                            "lastName": "Roy",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "3.3)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 23
                            }
                        ],
                        "text": "The Restaurant Game by Orkin and Roy (2009) grounds written chat dialogues in actions carried out in a computer game."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 25219626,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e36d23675c934510b95335c7e77333f3c620d3e4",
            "isKey": true,
            "numCitedBy": 82,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Current approaches to authoring behavior and dialogue for agents that interact with humans in virtual environments are labor intensive, yet often yield less robust results than desired in the face of the incredible variance possible in human input. The growing number of people playing multiplayer games online provides a potentially better alternative to hand-authored content -- capturing behavior and dialogue from human-human interactions, and automating agents with this data. This paper documents promising results from the first iteration of a Collective Artificial Intelligence system that generates behavior and dialogue in real-time from data captured from over 11,000 players of The Restaurant Game. We first describe the game, the collective memory system, and the proposal-critique driven agent architecture, and then demonstrate quantitatively that our system preserves the texture, or meaningful local coherence, of human social interaction."
            },
            "slug": "Automatic-learning-and-generation-of-social-from-Orkin-Roy",
            "title": {
                "fragments": [],
                "text": "Automatic learning and generation of social behavior from collective human gameplay"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Promising results are documents from the first iteration of a Collective Artificial Intelligence system that generates behavior and dialogue in real-time from data captured from over 11,000 players of The Restaurant Game."
            },
            "venue": {
                "fragments": [],
                "text": "AAMAS"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3328108"
                        ],
                        "name": "Luis von Ahn",
                        "slug": "Luis-von-Ahn",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Ahn",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis von Ahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784365"
                        ],
                        "name": "Laura A. Dabbish",
                        "slug": "Laura-A.-Dabbish",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Dabbish",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laura A. Dabbish"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 102
                            }
                        ],
                        "text": "A large multimodal resource combining language and visual information resulted from the ESP game (von Ahn and Dabbish, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 338469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2d4a6e4900ec0f096c87bb2b1272eeceaa584a6",
            "isKey": false,
            "numCitedBy": 2386,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained."
            },
            "slug": "Labeling-images-with-a-computer-game-Ahn-Dabbish",
            "title": {
                "fragments": [],
                "text": "Labeling images with a computer game"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new interactive system: a game that is fun and can be used to create valuable output that addresses the image-labeling problem and encourages people to do the work by taking advantage of their desire to be entertained."
            },
            "venue": {
                "fragments": [],
                "text": "CHI"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40404576"
                        ],
                        "name": "S. Amin",
                        "slug": "S.-Amin",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Amin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1906895"
                        ],
                        "name": "M. Andriluka",
                        "slug": "M.-Andriluka",
                        "structuredName": {
                            "firstName": "Mykhaylo",
                            "lastName": "Andriluka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Andriluka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 60
                            }
                        ],
                        "text": "For the experiments reported below we use the same setup as Rohrbach et al. (2012b) and use all videos in MPII Composites and MPII Cooking (Rohrbach et al., 2012a), excluding the 127 videos used during evaluation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Rohrbach et al. (2012b) showed that using such an attribute classifier representation can significantly improve per-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 106
                            }
                        ],
                        "text": "For our setting this feature representation has been shown to be superior to human pose-based approaches (Rohrbach et al., 2012a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 34
                            }
                        ],
                        "text": "Starting from the video corpus of Rohrbach et al. (2012b), which contains highresolution video recordings of basic cooking tasks, we collected multiple textual descriptions of each video via Mechanical Turk."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 82
                            }
                        ],
                        "text": "We build our corpus on top of the \u201cMPII Cooking Composite Activities\u201d video corpus (Rohrbach et al., 2012b, MPII Composites), which contains videos of different activities in the cooking domain, e.g., preparing carrots or separating eggs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Rohrbach et al. (2012b) showed that using such an attribute classifier representation can significantly improve per-\nformance for composite activity recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "(2012b) and use all videos in MPII Composites and MPII Cooking (Rohrbach et al., 2012a), excluding the 127 videos used during evaluation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For the experiments reported below we use the same setup as Rohrbach et al. (2012b) and use all videos in MPII Composites and MPII Cooking (Rohrbach et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 103
                            }
                        ],
                        "text": "The second approach builds upon the first by additionally learning higher level attribute classifiers (Rohrbach et al., 2012b) on a held out training set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 262
                            }
                        ],
                        "text": "\u2026far been mostly explored\nby the computer vision community, where different methods for improving action recognition by exploiting linguistic data have been proposed (Gupta and Mooney, 2010; Motwani and Mooney, 2012; Cour et al., 2008; Tzoukermann et al., 2011; Rohrbach et al., 2012b, among others)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9349950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49435aab7cdf259335725acc96691f755e436f55",
            "isKey": false,
            "numCitedBy": 479,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "While activity recognition is a current focus of research the challenging problem of fine-grained activity recognition is largely overlooked. We thus propose a novel database of 65 cooking activities, continuously recorded in a realistic setting. Activities are distinguished by fine-grained body motions that have low inter-class variability and high intra-class variability due to diverse subjects and ingredients. We benchmark two approaches on our dataset, one based on articulated pose tracks and the second using holistic video features. While the holistic approach outperforms the pose-based approach, our evaluation suggests that fine-grained activities are more difficult to detect and the body model can help in those cases. Providing high-resolution videos as well as an intermediate pose representation we hope to foster research in fine-grained activity recognition."
            },
            "slug": "A-database-for-fine-grained-activity-detection-of-Rohrbach-Amin",
            "title": {
                "fragments": [],
                "text": "A database for fine grained activity detection of cooking activities"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A novel database of 65 cooking activities, continuously recorded in a realistic setting, is proposed, suggesting that fine-grained activities are more difficult to detect and the body model can help in those cases."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153642390"
                        ],
                        "name": "David L. Chen",
                        "slug": "David-L.-Chen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David L. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "83415753"
                        ],
                        "name": "W. Dolan",
                        "slug": "W.-Dolan",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Dolan",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Dolan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 215717103,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "554a31ce91189cf6022ac677413ef2f8b9b40ca7",
            "isKey": false,
            "numCitedBy": 650,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "A lack of standard datasets and evaluation metrics has prevented the field of paraphrasing from making the kind of rapid progress enjoyed by the machine translation community over the last 15 years. We address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale. The highly parallel nature of this data allows us to use simple n-gram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates. In addition to being simple and efficient to compute, experiments show that these metrics correlate highly with human judgments."
            },
            "slug": "Collecting-Highly-Parallel-Data-for-Paraphrase-Chen-Dolan",
            "title": {
                "fragments": [],
                "text": "Collecting Highly Parallel Data for Paraphrase Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel data collection framework is presented that produces highly parallel text data relatively inexpensively and on a large scale that allows for simple n-gram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2807469"
                        ],
                        "name": "Oren Glickman",
                        "slug": "Oren-Glickman",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Glickman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Glickman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712352"
                        ],
                        "name": "B. Magnini",
                        "slug": "B.-Magnini",
                        "structuredName": {
                            "firstName": "Bernardo",
                            "lastName": "Magnini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Magnini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8587959,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de794d50713ea5f91a7c9da3d72041e2f5ef8452",
            "isKey": false,
            "numCitedBy": 1763,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems. In creating this year's dataset, a number of longer texts were introduced to make the challenge more oriented to realistic scenarios. Additionally, a pool of resources was offered so that the participants could share common tools. A pilot task was also set up, aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions. 26 participants submitted 44 runs, using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges."
            },
            "slug": "The-PASCAL-Recognising-Textual-Entailment-Challenge-Dagan-Glickman",
            "title": {
                "fragments": [],
                "text": "The PASCAL Recognising Textual Entailment Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems."
            },
            "venue": {
                "fragments": [],
                "text": "MLCW"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727272"
                        ],
                        "name": "Stefan Thater",
                        "slug": "Stefan-Thater",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Thater",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Thater"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683588"
                        ],
                        "name": "Hagen F\u00fcrstenau",
                        "slug": "Hagen-F\u00fcrstenau",
                        "structuredName": {
                            "firstName": "Hagen",
                            "lastName": "F\u00fcrstenau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hagen F\u00fcrstenau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717560"
                        ],
                        "name": "Manfred Pinkal",
                        "slug": "Manfred-Pinkal",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Pinkal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred Pinkal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7893614,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83d787a13aca79c833d11718da9ab6243117cf47",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a model that represents word meaning in context by vectors which are modified according to the words in the target\u2019s syntactic context. Contextualization of a vector is realized by reweighting its components, based on distributional information about the context words. Evaluation on a paraphrase ranking task derived from the SemEval 2007 Lexical Substitution Task shows that our model outperforms all previous models on this task. We show that our model supports a wider range of applications by evaluating it on a word sense disambiguation task. Results show that our model achieves state-of-the-art performance."
            },
            "slug": "Word-Meaning-in-Context:-A-Simple-and-Effective-Thater-F\u00fcrstenau",
            "title": {
                "fragments": [],
                "text": "Word Meaning in Context: A Simple and Effective Vector Model"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A model that represents word meaning in context by vectors which are modified according to the words in the target\u2019s syntactic context, which outperforms all previous models on a word sense disambiguation task."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNLP"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34902160"
                        ],
                        "name": "Jeff Mitchell",
                        "slug": "Jeff-Mitchell",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 199
                            }
                        ],
                        "text": "We compute inter-annotator agreement (and the forthcoming evaluation scores) using Spearman\u2019s rank correlation coefficient (\u03c1), a non-parametric test which is widely used for similar evaluation tasks (Mitchell and Lapata, 2008; Bruni et al., 2011; Erk and McCarthy, 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18597583,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5d67d1dc671bce42a9daac0c3605adb3fcfc697",
            "isKey": false,
            "numCitedBy": 730,
            "numCiting": 133,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a framework for representing the meaning of phrases and sentences in vector space. Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments."
            },
            "slug": "Vector-based-Models-of-Semantic-Composition-Mitchell-Lapata",
            "title": {
                "fragments": [],
                "text": "Vector-based Models of Semantic Composition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Under this framework, a wide range of composition models are introduced which are evaluated empirically on a sentence similarity task and demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40943456"
                        ],
                        "name": "Steve R. Howell",
                        "slug": "Steve-R.-Howell",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Howell",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steve R. Howell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51897276"
                        ],
                        "name": "Damian Jankowicz",
                        "slug": "Damian-Jankowicz",
                        "structuredName": {
                            "firstName": "Damian",
                            "lastName": "Jankowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Damian Jankowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50333420"
                        ],
                        "name": "S. Becker",
                        "slug": "S.-Becker",
                        "structuredName": {
                            "firstName": "Suzanna",
                            "lastName": "Becker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Becker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 101
                            }
                        ],
                        "text": "Psychological studies have shown the connection between action semantics and videos (Glenberg, 2002; Howell et al., 2005), but to our knowledge, we are the first to provide a suitable data source and to implement such a model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Submitted 10/2012; Published 3/2013. c\u00a92013 Association for Computational Linguistics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17142612,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "4e679d0946d68663303278005b766fbdbc8f5649",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Model-of-Grounded-Language-Acquisition:-Features-Howell-Jankowicz",
            "title": {
                "fragments": [],
                "text": "A Model of Grounded Language Acquisition: Sensorimotor Features Improve Lexical and Grammatical Learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708114"
                        ],
                        "name": "Katrin Erk",
                        "slug": "Katrin-Erk",
                        "structuredName": {
                            "firstName": "Katrin",
                            "lastName": "Erk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katrin Erk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145586618"
                        ],
                        "name": "Diana McCarthy",
                        "slug": "Diana-McCarthy",
                        "structuredName": {
                            "firstName": "Diana",
                            "lastName": "McCarthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diana McCarthy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1817941"
                        ],
                        "name": "Nicholas Gaylord",
                        "slug": "Nicholas-Gaylord",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Gaylord",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicholas Gaylord"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 81
                            }
                        ],
                        "text": "The dataset has been designed as analogous to the Usage Similarity dataset of\n25\nErk et al. (2009) and contains pairs of naturallanguage action descriptions plus their associated video segments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We call it the \u201cAction Similarity Dataset\u201d (ASim) in analogy to the Usage Similarity dataset (USim) of Erk et al. (2009) and Erk et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 101
                            }
                        ],
                        "text": "We call it the \u201cAction Similarity Dataset\u201d (ASim) in analogy to the Usage Similarity dataset (USim) of Erk et al. (2009) and Erk et al. (2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2123239,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "477804ac9dbf871b4fe5e5ac80467413dd619a63",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "The vast majority of work on word senses has relied on predefined sense inventories and an annotation schema where each word instance is tagged with the best fitting sense. This paper examines the case for a graded notion of word meaning in two experiments, one which uses WordNet senses in a graded fashion, contrasted with the \"winner takes all\" annotation, and one which asks annotators to judge the similarity of two usages. We find that the graded responses correlate with annotations from previous datasets, but sense assignments are used in a way that weakens the case for clear cut sense boundaries. The responses from both experiments correlate with the overlap of paraphrases from the English lexical substitution task which bodes well for the use of substitutes as a proxy for word sense. This paper also provides two novel datasets which can be used for evaluating computational systems."
            },
            "slug": "Investigations-on-Word-Senses-and-Word-Usages-Erk-McCarthy",
            "title": {
                "fragments": [],
                "text": "Investigations on Word Senses and Word Usages"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The case for a graded notion of word meaning is examined in two experiments, one which uses WordNet senses in a graded fashion, contrasted with the \"winner takes all\" annotation, and one which asks annotators to judge the similarity of two usages."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708114"
                        ],
                        "name": "Katrin Erk",
                        "slug": "Katrin-Erk",
                        "structuredName": {
                            "firstName": "Katrin",
                            "lastName": "Erk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katrin Erk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145586618"
                        ],
                        "name": "Diana McCarthy",
                        "slug": "Diana-McCarthy",
                        "structuredName": {
                            "firstName": "Diana",
                            "lastName": "McCarthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diana McCarthy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11182883,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "9875ca912adcd4ba97e0eefeeb064c87073a69a5",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Word sense disambiguation is typically phrased as the task of labeling a word in context with the best-fitting sense from a sense inventory such as WordNet. While questions have often been raised over the choice of sense inventory, computational linguists have readily accepted the best-fitting sense methodology despite the fact that the case for discrete sense boundaries is widely disputed by lexical semantics researchers. This paper studies graded word sense assignment, based on a recent dataset of graded word sense annotation."
            },
            "slug": "Graded-Word-Sense-Assignment-Erk-McCarthy",
            "title": {
                "fragments": [],
                "text": "Graded Word Sense Assignment"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "Grading word sense assignment is studied based on a recent dataset of graded word sense annotation and finds the task of labeling a word in context with the best-fitting sense from a sense inventory such as WordNet is difficult."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1500900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a0e788268fafb23ab20da0e98bb578b06830f7d",
            "isKey": false,
            "numCitedBy": 2724,
            "numCiting": 208,
            "paperAbstract": {
                "fragments": [],
                "text": "Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field."
            },
            "slug": "From-Frequency-to-Meaning:-Vector-Space-Models-of-Turney-Pantel",
            "title": {
                "fragments": [],
                "text": "From Frequency to Meaning: Vector Space Models of Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs, and to provide pointers into the literature for those who are less familiar with the field."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644043304"
                        ],
                        "name": "D. TurneyPeter",
                        "slug": "D.-TurneyPeter",
                        "structuredName": {
                            "firstName": "D",
                            "lastName": "TurneyPeter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. TurneyPeter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643872837"
                        ],
                        "name": "PantelPatrick",
                        "slug": "PantelPatrick",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "PantelPatrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "PantelPatrick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 133
                            }
                        ],
                        "text": "Such distributional models are attractive because they are conceptually simple, easy to implement and relevant for various NLP tasks (Turney and Pantel, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 220956987,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "c572ae1432b5146cd5fdfd7788f141e0993b69a8",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and..."
            },
            "slug": "From-frequency-to-meaning-TurneyPeter-PantelPatrick",
            "title": {
                "fragments": [],
                "text": "From frequency to meaning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Grounding Action Descriptions in Videos"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 13,
            "methodology": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 30,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Grounding-Action-Descriptions-in-Videos-Regneri-Rohrbach/21b3007f967d39e1346bc91e0fc8b3f16121300c?sort=total-citations"
}