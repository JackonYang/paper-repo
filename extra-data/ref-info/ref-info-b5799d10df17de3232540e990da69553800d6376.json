{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2912641"
                        ],
                        "name": "I. Kavasidis",
                        "slug": "I.-Kavasidis",
                        "structuredName": {
                            "firstName": "Isaak",
                            "lastName": "Kavasidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Kavasidis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46792082"
                        ],
                        "name": "S. Palazzo",
                        "slug": "S.-Palazzo",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Palazzo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Palazzo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2441118"
                        ],
                        "name": "C. Spampinato",
                        "slug": "C.-Spampinato",
                        "structuredName": {
                            "firstName": "Concetto",
                            "lastName": "Spampinato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Spampinato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145587660"
                        ],
                        "name": "C. Pino",
                        "slug": "C.-Pino",
                        "structuredName": {
                            "firstName": "Carmelo",
                            "lastName": "Pino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Pino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144027622"
                        ],
                        "name": "D. Giordano",
                        "slug": "D.-Giordano",
                        "structuredName": {
                            "firstName": "Daniela",
                            "lastName": "Giordano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Giordano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076542103"
                        ],
                        "name": "D. Giuffrida",
                        "slug": "D.-Giuffrida",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Giuffrida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Giuffrida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19324186"
                        ],
                        "name": "P. Messina",
                        "slug": "P.-Messina",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Messina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Messina"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4899629,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03672cfa599950f208d424d5298cdc12b72c2492",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep Convolutional Neural Networks (DCNNs) have recently been applied successfully to a variety of vision and multimedia tasks, thus driving development of novel solutions in several application domains. Document analysis is a particularly promising area for DCNNs: indeed, the number of available digital documents has reached unprecedented levels, and humans are no longer able to discover and retrieve all the information contained in these documents without the help of automation. Under this scenario, DCNNs offers a viable solution to automate the information extraction process from digital documents. Within the realm of information extraction from documents, detection of tables and charts is particularly needed as they contain a visual summary of the most valuable information contained in a document. For a complete automation of visual information extraction process from tables and charts, it is necessary to develop techniques that localize them and identify precisely their boundaries. In this paper we aim at solving the table/chart detection task through an approach that combines deep convolutional neural networks, graphical models and saliency concepts. In particular, we propose a saliency-based fully-convolutional neural network performing multi-scale reasoning on visual cues followed by a fully-connected conditional random field (CRF) for localizing tables and charts in digital/digitized documents. Performance analysis carried out on an extended version of ICDAR 2013 (with annotated charts as well as tables) shows that our approach yields promising results, outperforming existing models."
            },
            "slug": "A-Saliency-based-Convolutional-Neural-Network-for-Kavasidis-Palazzo",
            "title": {
                "fragments": [],
                "text": "A Saliency-based Convolutional Neural Network for Table and Chart Detection in Digitized Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A saliency-based fully-convolutional neural network performing multi-scale reasoning on visual cues followed by a fully-connected conditional random field (CRF) for localizing tables and charts in digital/digitized documents is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ICIAP"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402440928"
                        ],
                        "name": "Azka Gilani",
                        "slug": "Azka-Gilani",
                        "structuredName": {
                            "firstName": "Azka",
                            "lastName": "Gilani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Azka Gilani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39404123"
                        ],
                        "name": "S. Qasim",
                        "slug": "S.-Qasim",
                        "structuredName": {
                            "firstName": "Shah",
                            "lastName": "Qasim",
                            "middleNames": [
                                "Rukh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Qasim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49012494"
                        ],
                        "name": "M. I. Malik",
                        "slug": "M.-I.-Malik",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Malik",
                            "middleNames": [
                                "Imran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. I. Malik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206777650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d6931dd0ba9e492b0ceab00268ca4be62ef663a",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Table detection is a crucial step in many document analysis applications as tables are used for presenting essential information to the reader in a structured manner. It is a hard problem due to varying layouts and encodings of the tables. Researchers have proposed numerous techniques for table detection based on layout analysis of documents. Most of these techniques fail to generalize because they rely on hand engineered features which are not robust to layout variations. In this paper, we have presented a deep learning based method for table detection. In the proposed method, document images are first pre-processed. These images are then fed to a Region Proposal Network followed by a fully connected neural network for table detection. The proposed method works with high precision on document images with varying layouts that include documents, research papers, and magazines. We have done our evaluations on publicly available UNLV dataset where it beats Tesseract's state of the art table detection system by a significant margin."
            },
            "slug": "Table-Detection-Using-Deep-Learning-Gilani-Qasim",
            "title": {
                "fragments": [],
                "text": "Table Detection Using Deep Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The proposed method works with high precision on document images with varying layouts that include documents, research papers, and magazines and beats Tesseract's state of the art table detection system by a significant margin."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057440295"
                        ],
                        "name": "Sebastian Schreiber",
                        "slug": "Sebastian-Schreiber",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Schreiber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Schreiber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2582412"
                        ],
                        "name": "S. Agne",
                        "slug": "S.-Agne",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Agne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Agne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144651288"
                        ],
                        "name": "I. Wolf",
                        "slug": "I.-Wolf",
                        "structuredName": {
                            "firstName": "Ivo",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734717217"
                        ],
                        "name": "Sheraz Ahmed",
                        "slug": "Sheraz-Ahmed",
                        "structuredName": {
                            "firstName": "Sheraz",
                            "lastName": "Ahmed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sheraz Ahmed"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "The fine tuned F-RCNN model achieves the state-of-the-art performance reported in [6], where the F-RCNN model was fine tuned with 1600 samples from a pre-trained object detection model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 156
                            }
                        ],
                        "text": "More recently, image analytics methods based on deep learning are becoming available [5] and are used to train document layout understanding pipelines [1], [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10191334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8bead3ae810cd3f7427d3004e45b4158da9b744",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel end-to-end system for table understanding in document images called DeepDeSRT. In particular, the contribution of DeepDeSRT is two-fold. First, it presents a deep learning-based solution for table detection in document images. Secondly, it proposes a novel deep learning-based approach for table structure recognition, i.e. identifying rows, columns, and cell positions in the detected tables. In contrast to existing rule-based methods, which rely on heuristics or additional PDF metadata (like, for example, print instructions, character bounding boxes, or line segments), the presented system is data-driven and does not need any heuristics or metadata to detect as well as to recognize tabular structures in document images. Furthermore, in contrast to most existing table detection and structure recognition methods, which are applicable only to PDFs, DeepDeSRT processes document images, which makes it equally suitable for born-digital PDFs (as they can automatically be converted into images) as well as even harder problems, e.g. scanned documents. To gauge the performance of DeepDeSRT, the system is evaluated on the publicly available ICDAR 2013 table competition dataset containing 67 documents with 238 pages overall. Evaluation results reveal that DeepDeSRT outperforms state-of-the-art methods for table detection and structure recognition and achieves F1-measures of 96.77% and 91.44% for table detection and structure recognition, respectively. Additionally, DeepDeSRT is evaluated on a closed dataset from a real use case of a major European aviation company comprising documents which are highly unlike those in ICDAR 2013. Tested on a randomly selected sample from this dataset, DeepDeSRT achieves high detection accuracy for tables which demonstrates the sound generalization capabilities of our system."
            },
            "slug": "DeepDeSRT:-Deep-Learning-for-Detection-and-of-in-Schreiber-Agne",
            "title": {
                "fragments": [],
                "text": "DeepDeSRT: Deep Learning for Detection and Structure Recognition of Tables in Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "In contrast to most existing table detection and structure recognition methods, which are applicable only to PDFs, DeepDeSRT processes document images, which makes it equally suitable for born-digital PDFs as well as even harder problems, e.g. scanned documents."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803149"
                        ],
                        "name": "A. Antonacopoulos",
                        "slug": "A.-Antonacopoulos",
                        "structuredName": {
                            "firstName": "Apostolos",
                            "lastName": "Antonacopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Antonacopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144322499"
                        ],
                        "name": "D. Bridson",
                        "slug": "D.-Bridson",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Bridson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bridson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070747525"
                        ],
                        "name": "C. Papadopoulos",
                        "slug": "C.-Papadopoulos",
                        "structuredName": {
                            "firstName": "Christos",
                            "lastName": "Papadopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papadopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1980669"
                        ],
                        "name": "S. Pletschacher",
                        "slug": "S.-Pletschacher",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Pletschacher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pletschacher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "Examples of these efforts are available in several ICDAR challenges [7], which cover as well complex layouts [8], [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 158
                            }
                        ],
                        "text": "neovascular glaucoma; 10% are combined with branch retinal arterial occlusion (usually cilioretinal artery due to low perfusion pressure of choroidal system) [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11850358,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4288ec46736acbe7ca1fc54d43f94b19b602450",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a significant need for a realistic dataset on which to evaluate layout analysis methods and examine their performance in detail. This paper presents a new dataset (and the methodology used to create it) based on a wide range of contemporary documents. Strong emphasis is placed on comprehensive and detailed representation of both complex and simple layouts, and on colour originals. In-depth information is recorded both at the page and region level. Ground truth is efficiently created using a new semi-automated tool and stored in a new comprehensive XML representation, the PAGE format. The dataset can be browsed and searched via a web-based front end to the underlying database and suitable subsets (relevant to specific evaluation goals) can be selected and downloaded."
            },
            "slug": "A-Realistic-Dataset-for-Performance-Evaluation-of-Antonacopoulos-Bridson",
            "title": {
                "fragments": [],
                "text": "A Realistic Dataset for Performance Evaluation of Document Layout Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents a new dataset (and the methodology used to create it) based on a wide range of contemporary documents, with strong emphasis on comprehensive and detailed representation of both complex and simple layouts, and on colour originals."
            },
            "venue": {
                "fragments": [],
                "text": "2009 10th International Conference on Document Analysis and Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2656315"
                        ],
                        "name": "Leipeng Hao",
                        "slug": "Leipeng-Hao",
                        "structuredName": {
                            "firstName": "Leipeng",
                            "lastName": "Hao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leipeng Hao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777642"
                        ],
                        "name": "Liangcai Gao",
                        "slug": "Liangcai-Gao",
                        "structuredName": {
                            "firstName": "Liangcai",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liangcai Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3412797"
                        ],
                        "name": "Xiaohan Yi",
                        "slug": "Xiaohan-Yi",
                        "structuredName": {
                            "firstName": "Xiaohan",
                            "lastName": "Yi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaohan Yi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143830636"
                        ],
                        "name": "Zhi Tang",
                        "slug": "Zhi-Tang",
                        "structuredName": {
                            "firstName": "Zhi",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi Tang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2016 [15] 0."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2870724,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06bf934004b6f93711298f905b1e447683a8d0b9",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Because of the better performance of deep learning on many computer vision tasks, researchers in the area of document analysis and recognition begin to adopt this technique into their work. In this paper, we propose a novel method for table detection in PDF documents based on convolutional neutral networks, one of the most popular deep learning models. In the proposed method, some table-like areas are selected first by some loose rules, and then the convolutional networks are built and refined to determine whether the selected areas are tables or not. Besides, the visual features of table areas are directly extracted and utilized through the convolutional networks, while the non-visual information (e.g. characters, rendering instructions) contained in original PDF documents is also taken into consideration to help achieve better recognition results. The primary experimental results show that the approach is effective in table detection."
            },
            "slug": "A-Table-Detection-Method-for-PDF-Documents-Based-on-Hao-Gao",
            "title": {
                "fragments": [],
                "text": "A Table Detection Method for PDF Documents Based on Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel method for table detection in PDF documents based on convolutional neutral networks, one of the most popular deep learning models, which shows that the approach is effective in table detection."
            },
            "venue": {
                "fragments": [],
                "text": "2016 12th IAPR Workshop on Document Analysis Systems (DAS)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2764871"
                        ],
                        "name": "C. Clausner",
                        "slug": "C.-Clausner",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Clausner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Clausner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803149"
                        ],
                        "name": "A. Antonacopoulos",
                        "slug": "A.-Antonacopoulos",
                        "structuredName": {
                            "firstName": "Apostolos",
                            "lastName": "Antonacopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Antonacopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1980669"
                        ],
                        "name": "S. Pletschacher",
                        "slug": "S.-Pletschacher",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Pletschacher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pletschacher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 114
                            }
                        ],
                        "text": "Examples of these efforts are available in several ICDAR challenges [7], which cover as well complex layouts [8], [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "serum lipids levels can improve visual acuity or the complications of RVO [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4763136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c69b8f83a73c216b89df24d66ad0e58f0eda4b8e",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an objective comparative evaluation of page segmentation and region classification methods for documents with complex layouts. It describes the competition (modus operandi, dataset and evaluation methodology) held in the context of ICDAR2017, presenting the results of the evaluation of seven methods \u2013 five submitted, two state-of-the-art systems (commercial and open-source). Three scenarios are reported in this paper, one evaluating the ability of methods to accurately segment regions and two evaluating both segmentation and region classification (one focusing only on text regions). For the first time, nested region content (table cells, chart labels etc.) are evaluated in addition to the top-level page content. Text recognition was a bonus challenge and was not taken up by all participants. The results indicate that an innovative approach has a clear advantage but there is still a considerable need to develop robust methods that deal with layout challenges, especially with the non-textual content."
            },
            "slug": "ICDAR2017-Competition-on-Recognition-of-Documents-Clausner-Antonacopoulos",
            "title": {
                "fragments": [],
                "text": "ICDAR2017 Competition on Recognition of Documents with Complex Layouts - RDCL2017"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results indicate that an innovative approach has a clear advantage but there is still a considerable need to develop robust methods that deal with layout challenges, especially with the non-textual content."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115379063"
                        ],
                        "name": "Jing Fang",
                        "slug": "Jing-Fang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Fang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Fang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070898875"
                        ],
                        "name": "Xin Tao",
                        "slug": "Xin-Tao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Tao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Tao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087321561"
                        ],
                        "name": "Zhi Tang",
                        "slug": "Zhi-Tang",
                        "structuredName": {
                            "firstName": "Zhi",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29035564"
                        ],
                        "name": "Ruiheng Qiu",
                        "slug": "Ruiheng-Qiu",
                        "structuredName": {
                            "firstName": "Ruiheng",
                            "lastName": "Qiu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruiheng Qiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49420283"
                        ],
                        "name": "Y. Liu",
                        "slug": "Y.-Liu",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[10,11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[10], [11]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 23786594,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f7a62139dfd09bfad667691332bd55e31736887",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Table detection is an important task in the field of document analysis. It has been extensively studied since a couple of decades. Various kinds of document mediums are involved, from scanned images to web pages, from plain texts to PDF files. Numerous algorithms published bring up a challenging issue: how to evaluate algorithms in different context. Currently, most work on table detection conducts experiments on their in-house dataset. Even the few sources of online datasets are targeted at image documents only. Moreover, Precision and recall measurement are usual practice in order to account performance based on human evaluation. In this paper, we provide a dataset that is representative, large and most importantly, publicly available. The compatible format of the ground truth makes evaluation independent of document medium. We also propose a set of new measures, implement them, and open the source code. Finally, three existing table detection algorithms are evaluated to demonstrate the reliability of the dataset and metrics."
            },
            "slug": "Dataset,-Ground-Truth-and-Performance-Metrics-for-Fang-Tao",
            "title": {
                "fragments": [],
                "text": "Dataset, Ground-Truth and Performance Metrics for Table Detection Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A dataset that is representative, large and most importantly, publicly available, and the compatible format of the ground truth makes evaluation independent of document medium is provided."
            },
            "venue": {
                "fragments": [],
                "text": "2012 10th IAPR International Workshop on Document Analysis Systems"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2655325,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a7809043fe4f50da53163d1fd318754c5f259e6",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, I summarize research in document layout analysis carried out over the last few years in our laboratory. Correct document layout analysis is a key step in document capture conversions into electronic formats, optical character recognition (OCR), information retrieval from scanned documents, appearance-based document retrieval, and reformatting of documents for on-screen display. We have developed a number of novel geometric algorithms and statistical methods. Layout analysis systems built from these algorithms are applicable to a wide variety of languages and layouts, and have proven to be robust to the presence of noise and spurious features in a page image. The system itself consists of reusable and independent software modules that can be reconfigured to be adapted to different languages and applications. Currently, we are using them for electronic book and document capture applications. If there is commercial or government demand, we are interested in adapting these tools to information retrieval and intelligence applications."
            },
            "slug": "High-Performance-Document-Layout-Analysis-Breuel",
            "title": {
                "fragments": [],
                "text": "High Performance Document Layout Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This paper summarize research in document layout analysis carried out over the last few years in the laboratory, which has developed a number of novel geometric algorithms and statistical methods that are applicable to a wide variety of languages and layouts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3493665"
                        ],
                        "name": "Dafang He",
                        "slug": "Dafang-He",
                        "structuredName": {
                            "firstName": "Dafang",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dafang He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145823372"
                        ],
                        "name": "Scott D. Cohen",
                        "slug": "Scott-D.-Cohen",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Cohen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott D. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31844147"
                        ],
                        "name": "Brian L. Price",
                        "slug": "Brian-L.-Price",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Price",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian L. Price"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1852261"
                        ],
                        "name": "Daniel Kifer",
                        "slug": "Daniel-Kifer",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Kifer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Kifer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 144
                            }
                        ],
                        "text": "Examples include table detection from document images using heuristics [12], vertical arrangement of text blocks [13] and deep learning methods [14]\u2013[17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29473897,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3cf063c3e25b2ab30e44ba49920b811d40f7702",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Page segmentation and table detection play an important role in understanding the structure of documents. We present a page segmentation algorithm that incorporates state-of-the-art deep learning methods for segmenting three types of document elements: text blocks, tables, and figures. We propose a multi-scale, multi-task fully convolutional neural network (FCN) for the tasks of semantic page segmentation and element contour detection. The semantic segmentation network accurately predicts the probability at each pixel of the three element classes. The contour detection network accurately predicts instance level \"edges\" around each element occurrence. We propose a conditional random field (CRF) that uses features output from the semantic segmentation and contour networks to improve upon the semantic segmentation network output. Given the semantic segmentation output, we also extract individual table instances from the page using some heuristic rules and a verification network to remove false positives. We show that although we only consider a page image as input, we produce comparable results with other methods that relies on PDF file information and heuristics and hand crafted features tailored to specific types of documents. Our approach learns the representative features for page segmentation from real and synthetic training data. %, and produces good results on real documents. The learning-based property makes it a more general method than existing methods in terms of document types and element appearances. For example, our method reliably detects sparsely lined tables which are hard for rule-based or heuristic methods."
            },
            "slug": "Multi-Scale-Multi-Task-FCN-for-Semantic-Page-and-He-Cohen",
            "title": {
                "fragments": [],
                "text": "Multi-Scale Multi-Task FCN for Semantic Page Segmentation and Table Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a page segmentation algorithm that incorporates state-of-the-art deep learning methods for segmenting three types of document elements: text blocks, tables, and figures and proposes a conditional random field (CRF) that uses features output from the semantic segmentsation and contour networks to improve upon the semantic segmentation network output."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2764871"
                        ],
                        "name": "C. Clausner",
                        "slug": "C.-Clausner",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Clausner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Clausner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070747525"
                        ],
                        "name": "C. Papadopoulos",
                        "slug": "C.-Papadopoulos",
                        "structuredName": {
                            "firstName": "Christos",
                            "lastName": "Papadopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papadopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1980669"
                        ],
                        "name": "S. Pletschacher",
                        "slug": "S.-Pletschacher",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Pletschacher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pletschacher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803149"
                        ],
                        "name": "A. Antonacopoulos",
                        "slug": "A.-Antonacopoulos",
                        "structuredName": {
                            "firstName": "Apostolos",
                            "lastName": "Antonacopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Antonacopoulos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 109
                            }
                        ],
                        "text": "Examples of these efforts are available in several ICDAR challenges [7], which cover as well complex layouts [8], [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 267,
                                "start": 264
                            }
                        ],
                        "text": "Other risk factors are glaucoma, syphilis, sarcoidosis, vasculitis, increased intraorbital or intraocular pressure, hyphema, hyperviscosity syndromes (multiple myeloma, Waldenstrom\u2019s macroglobulinemia, and leukemia), high homocysteine levels, sickle cell, and HIV [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1269750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d23049c9047484b9f13e53c1af30055b1ad6e440",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a research dataset of historical newspapers comprising over 500 page images, uniquely representative of European cultural heritage from the digitization projects of 12 national and major European libraries, created within the scope of the large-scale digitisation Europeana Newspapers Project (ENP). Every image is accompanied by comprehensive ground truth (Unicode encoded full-text, layout information with precise region outlines, type labels, and reading order) in PAGE format and searchable metadata about document characteristics and artefacts. The first part of the paper describes the nature of the dataset, how it was built, and the challenges encountered. In the second part, a baseline for two state-of-the-art OCR systems (ABBYY FineReader Engine 11 and Tesseract 3.03) is given with regard to both text recognition and segmentation/layout analysis performance."
            },
            "slug": "The-ENP-image-and-ground-truth-dataset-of-Clausner-Papadopoulos",
            "title": {
                "fragments": [],
                "text": "The ENP image and ground truth dataset of historical newspapers"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A baseline for two state-of-the-art OCR systems (ABBYY FineReader Engine 11 and Tesseract 3.03) is given with regard to both text recognition and segmentation/layout analysis performance."
            },
            "venue": {
                "fragments": [],
                "text": "2015 13th International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3200914"
                        ],
                        "name": "Asif Shahab",
                        "slug": "Asif-Shahab",
                        "structuredName": {
                            "firstName": "Asif",
                            "lastName": "Shahab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Asif Shahab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3320897"
                        ],
                        "name": "T. Kieninger",
                        "slug": "T.-Kieninger",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Kieninger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kieninger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[10], [11]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16140787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bff7592f28354b3be632f4bf678b622f765a2b8d",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Table spotting and structural analysis are just a small fraction of tasks relevant when speaking of table analysis. Today, quite a large number of different approaches facing these tasks have been described in literature or are available as part of commercial OCR systems that claim to deal with tables on the scanned documents and to treat them accordingly.\n However, the problem of detecting tables is not yet solved at all. Different approaches have different strengths and weak points. Some fail in certain situations or layouts where others perform better. How shall one know, which approach or system is the best for his specific job? The answer to this question raises the demand for an objective comparison of different approaches which address the same task of spotting tables and recognizing their structure.\n This paper describes our approach towards establishing a complete and publicly available, hence open environment for the benchmarking of table spotting and structural analysis. We provide free access to the ground truthing tool and evaluation mechanism described in this paper, describe the ideas behind and we also provide ground truth for the 547 documents of the UNLV and UW-3 datasets that contain tables.\n In addition, we applied the quality measures to the results that were generated by the T-Recs system which we developed some years ago and which we started to further advance since a few months."
            },
            "slug": "An-open-approach-towards-the-benchmarking-of-table-Shahab-Shafait",
            "title": {
                "fragments": [],
                "text": "An open approach towards the benchmarking of table structure recognition systems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The approach towards establishing a complete and publicly available, hence open environment for the benchmarking of table spotting and structural analysis is described and free access to the ground truthing tool and evaluation mechanism described is provided."
            },
            "venue": {
                "fragments": [],
                "text": "DAS '10"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047516905"
                        ],
                        "name": "Max C. G\u00f6bel",
                        "slug": "Max-C.-G\u00f6bel",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "G\u00f6bel",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max C. G\u00f6bel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "18592170"
                        ],
                        "name": "Tamir Hassan",
                        "slug": "Tamir-Hassan",
                        "structuredName": {
                            "firstName": "Tamir",
                            "lastName": "Hassan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamir Hassan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759801"
                        ],
                        "name": "Ermelinda Oro",
                        "slug": "Ermelinda-Oro",
                        "structuredName": {
                            "firstName": "Ermelinda",
                            "lastName": "Oro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ermelinda Oro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35203250"
                        ],
                        "name": "G. Orsi",
                        "slug": "G.-Orsi",
                        "structuredName": {
                            "firstName": "Giorgio",
                            "lastName": "Orsi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Orsi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "The ICDAR 2013 Table Competition [21] is one of the most prestigious competitions on table detection in PDF documents from European government sources."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206777311,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7cbc1582175b5c03b8203ec38bb25bae9d66397d",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Table understanding is a well studied problem in document analysis, and many academic and commercial approaches have been developed to recognize tables in several document formats, including plain text, scanned page images and born-digital, object-based formats such as PDF. Despite the abundance of these techniques, an objective comparison of their performance is still missing. The Table Competition held in the context of ICDAR 2013 is our first attempt at objectively evaluating these techniques against each other in a standardized way, across several input formats. The competition independently addresses three problems: (i) table location, (ii) table structure recognition, and (iii) these two tasks combined. We received results from seven academic systems, which we have also compared against four commercial products. This paper presents our findings."
            },
            "slug": "ICDAR-2013-Table-Competition-G\u00f6bel-Hassan",
            "title": {
                "fragments": [],
                "text": "ICDAR 2013 Table Competition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The Table Competition held in the context of ICDAR 2013 is the first attempt at objectively evaluating these techniques against each other in a standardized way, across several input formats."
            },
            "venue": {
                "fragments": [],
                "text": "2013 12th International Conference on Document Analysis and Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2672708"
                        ],
                        "name": "D. Tran",
                        "slug": "D.-Tran",
                        "structuredName": {
                            "firstName": "Dieu",
                            "lastName": "Tran",
                            "middleNames": [
                                "Ni"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072578366"
                        ],
                        "name": "T. A. Tran",
                        "slug": "T.-A.-Tran",
                        "structuredName": {
                            "firstName": "Tuan",
                            "lastName": "Tran",
                            "middleNames": [
                                "Anh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. A. Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31704596"
                        ],
                        "name": "A. Oh",
                        "slug": "A.-Oh",
                        "structuredName": {
                            "firstName": "Aran",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Oh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2355626"
                        ],
                        "name": "Soohyung Kim",
                        "slug": "Soohyung-Kim",
                        "structuredName": {
                            "firstName": "Soohyung",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soohyung Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31725329"
                        ],
                        "name": "In Seop Na",
                        "slug": "In-Seop-Na",
                        "structuredName": {
                            "firstName": "In",
                            "lastName": "Na",
                            "middleNames": [
                                "Seop"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "In Seop Na"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "Examples include table detection from document images using heuristics [12], vertical arrangement of text blocks [13] and deep learning methods [14]\u2013[17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61442517,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca02ddb5febf92ec87744d6ce3bc7a96a3b89702",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Table detection is a challenging problem and plays an important role in document layout analysis. In this paper, we propose an effective method to identify the table region from document images. First, the regions of interest (ROIs) are recognized as the table candidates. In each ROI, we locate text components and extract text blocks. After that, we check all text blocks to determine if they are arranged horizontally or vertically and compare the height of each text block with the average height. If the text blocks satisfy a series of rules, the ROI is regarded as a table. Experiments on the ICDAR 2013 dataset show that the results obtained are very encouraging. This proves the effectiveness and superiority of our proposed method."
            },
            "slug": "Table-Detection-from-Document-Image-using-Vertical-Tran-Tran",
            "title": {
                "fragments": [],
                "text": "Table Detection from Document Image using Vertical Arrangement of Text Blocks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experiments on the ICDAR 2013 dataset show that the results obtained are very encouraging and proves the effectiveness and superiority of the proposed method."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27086451"
                        ],
                        "name": "R. Cattoni",
                        "slug": "R.-Cattoni",
                        "structuredName": {
                            "firstName": "Roldano",
                            "lastName": "Cattoni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cattoni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2820668"
                        ],
                        "name": "T. Coianiz",
                        "slug": "T.-Coianiz",
                        "structuredName": {
                            "firstName": "Tarcisio",
                            "lastName": "Coianiz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Coianiz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472153"
                        ],
                        "name": "S. Messelodi",
                        "slug": "S.-Messelodi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Messelodi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Messelodi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2389886"
                        ],
                        "name": "C. M. Modena",
                        "slug": "C.-M.-Modena",
                        "structuredName": {
                            "firstName": "Carla",
                            "lastName": "Modena",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. M. Modena"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 144
                            }
                        ],
                        "text": "Geometric layout analysis techniques based on an image representation of the document combined with optical character recognition (OCR) methods [2]\u2013[4] were firstly used to understand these documents."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10885035,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc71c69bb1eb5ed3b82a10347a841cb3a27e2d58",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 149,
            "paperAbstract": {
                "fragments": [],
                "text": "Document Image Understanding (DIU) is an interesting research area with a large variety of challenging applications. Researchers have worked from decades on this topic, as witnessed by the scientific literature. The main purpose of the present report is to describe the current status of DIU with particular attention to two subprocesses: document skew angle estimation and page decomposition. Several algorithms proposed in the literature are synthetically described. They are included in a novel classification scheme. Some methods proposed for the evaluation of page decomposition algorithms are described. Critical discussions are reported about the current status of the field and about the open problems. Some considerations about the logical layout analysis are also reported."
            },
            "slug": "Geometric-Layout-Analysis-Techniques-for-Document-a-Cattoni-Coianiz",
            "title": {
                "fragments": [],
                "text": "Geometric Layout Analysis Techniques for Document Image Understanding: a Review"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The main purpose of the present report is to describe the current status of DIU with particular attention to two subprocesses: document skew angle estimation and page decomposition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2632608"
                        ],
                        "name": "B. Yildiz",
                        "slug": "B.-Yildiz",
                        "structuredName": {
                            "firstName": "Burcu",
                            "lastName": "Yildiz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yildiz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804028"
                        ],
                        "name": "K. Kaiser",
                        "slug": "K.-Kaiser",
                        "structuredName": {
                            "firstName": "Katharina",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692521"
                        ],
                        "name": "S. Miksch",
                        "slug": "S.-Miksch",
                        "structuredName": {
                            "firstName": "Silvia",
                            "lastName": "Miksch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Miksch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "Examples include table detection from document images using heuristics [12], vertical arrangement of text blocks [13] and deep learning methods [14]\u2013[17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17242556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5dc08add84ef4070b00c2b43dac037bcfd6df460",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Tables are a common structuring element in many documents, such as PDF files. To reuse such tables, appropriate methods need to be develop, which capture the structure and the content information. We have developed several heuristics which together recognize and decompose tables in PDF files and store the extracted data in a structured data format (XML) for easier reuse. Additionally, we implemented a prototype, which gives the user the ability of making adjustments on the extracted data. Our work shows that purely heuristic-based approaches can achieve good results, especially for lucid tables."
            },
            "slug": "pdf2table:-A-Method-to-Extract-Table-Information-Yildiz-Kaiser",
            "title": {
                "fragments": [],
                "text": "pdf2table: A Method to Extract Table Information from PDF Files"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work developed several heuristics which together recognize and decompose tables in PDF files and store the extracted data in a structured data format (XML) for easier reuse and shows that purely heuristic-based approaches can achieve good results, especially for lucid tables."
            },
            "venue": {
                "fragments": [],
                "text": "IICAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2032184078"
                        ],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 111
                            }
                        ],
                        "text": "Three experiments are designed to investigate 1) how well the established object detection models Faster-RCNN (F-RCNN) [19] and Mask-RCNN (M-RCNN) [5] can\nrecognize document layout of PubLayNet; 2) if the F-RCNN and M-RCNN models pre-trained on PubLayNet can be fine-tuned to tackle the ICDAR 2013 Table Recognition Competition6; 3) if the F-RCNN and M-RCNN models pre-trained on PubLayNet are better initializations than those pre-trained on the ImageNet and COCO datasets for analyzing documents in a different domain."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 105
                            }
                        ],
                        "text": "Both models can generate accurate (MAP > 0.9) document layout, where M-RCNN shows a small advantage over F-RCNN."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 56
                            }
                        ],
                        "text": "The only exception is that fine tuning pre-trained COCO F-RCNN model detects tables more accurately than fine tuning pre-trained PubLayNet F-RCNN model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 23
                            }
                        ],
                        "text": "The performance of the F-RCNN and the M-RCNN models on our development and testing sets are depicted in Table III."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 13
                            }
                        ],
                        "text": "We trained a F-RCNN model and a M-RCNN model on PubLayNet using the Detectron implementation [20]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 114
                            }
                        ],
                        "text": "As implied by the high MAP, the model is able to generate accurate layout.\ntaken to compare different pre-trained F-RCNN and M-RCNN models for fine tuning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "Three experiments are designed to investigate 1) how well the established object detection models Faster-RCNN (F-RCNN) [19] and Mask-RCNN (M-RCNN) [5] can"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 15
                            }
                        ],
                        "text": "The fine tuned F-RCNN model achieves the state-of-the-art performance reported in [6], where the F-RCNN model was fine tuned with 1600 samples from a pre-trained object detection model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 13
                            }
                        ],
                        "text": "We trained a F-RCNN model and a M-RCNN model on this table detection dataset under the same configuration described in Section IV-A."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10328909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "isKey": true,
            "numCitedBy": 32561,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available"
            },
            "slug": "Faster-R-CNN:-Towards-Real-Time-Object-Detection-Ren-He",
            "title": {
                "fragments": [],
                "text": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals and further merge RPN and Fast R-CNN into a single network by sharing their convolutionAL features."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1411363,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32004665dd4cba02bef7f1f3b3ddaf79f0e2eafc",
            "isKey": false,
            "numCitedBy": 176,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents geometric algorithms for solving two key problems in layout analysis: finding a cover of the background whitespace of a document in terms of maximal empty rectangles, and finding constrained maximum likelihood matches of geometric text line models in the presence of geometric obstacles. The algorithms are considerably easier to implement than prior methods, they return globally optimal solutions, and they require no heuristics. The paper also introduces an evaluation function that reliably identifies maximal empty rectangles corresponding to column boundaries. Combining this evaluation function with the two geometric algorithms results in an easy-to-implement layout analysis system. Reliability of the system is demonstrated on documents from the UW3 database."
            },
            "slug": "Two-Geometric-Algorithms-for-Layout-Analysis-Breuel",
            "title": {
                "fragments": [],
                "text": "Two Geometric Algorithms for Layout Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "Geometric algorithms for solving two key problems in layout analysis: finding a cover of the background whitespace of a document in terms of maximal empty rectangles and finding constrained maximum likelihood matches of geometric text line models in the presence of geometric obstacles are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Document Analysis Systems"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46383755"
                        ],
                        "name": "D. B. Powell",
                        "slug": "D.-B.-Powell",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Powell",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. B. Powell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690704"
                        ],
                        "name": "Edward A. Lee",
                        "slug": "Edward-A.-Lee",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Lee",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward A. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144688191"
                        ],
                        "name": "W. C. Newman",
                        "slug": "W.-C.-Newman",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Newman",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. C. Newman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144629409"
                        ],
                        "name": "S. Bhattacharyya",
                        "slug": "S.-Bhattacharyya",
                        "structuredName": {
                            "firstName": "Shuvra",
                            "lastName": "Bhattacharyya",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bhattacharyya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39969030"
                        ],
                        "name": "S. Ha",
                        "slug": "S.-Ha",
                        "structuredName": {
                            "firstName": "Soonhoi",
                            "lastName": "Ha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1815712"
                        ],
                        "name": "D. Messerschmitt",
                        "slug": "D.-Messerschmitt",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Messerschmitt",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Messerschmitt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66891077"
                        ],
                        "name": "D. Genin",
                        "slug": "D.-Genin",
                        "structuredName": {
                            "firstName": "Dominique",
                            "lastName": "Genin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Genin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2865600"
                        ],
                        "name": "J. D. Moortel",
                        "slug": "J.-D.-Moortel",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Moortel",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. D. Moortel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1897422"
                        ],
                        "name": "D. Desmet",
                        "slug": "D.-Desmet",
                        "structuredName": {
                            "firstName": "Dirk",
                            "lastName": "Desmet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Desmet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2298414"
                        ],
                        "name": "E. V. D. Velde",
                        "slug": "E.-V.-D.-Velde",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Velde",
                            "middleNames": [
                                "F.",
                                "van",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. V. D. Velde"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699786"
                        ],
                        "name": "L. Hendren",
                        "slug": "L.-Hendren",
                        "structuredName": {
                            "firstName": "Laurie",
                            "lastName": "Hendren",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Hendren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50318619"
                        ],
                        "name": "G. Gao",
                        "slug": "G.-Gao",
                        "structuredName": {
                            "firstName": "Guang",
                            "lastName": "Gao",
                            "middleNames": [
                                "Rong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682451"
                        ],
                        "name": "E. Altman",
                        "slug": "E.-Altman",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Altman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Altman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40590306"
                        ],
                        "name": "C. Mukherjee",
                        "slug": "C.-Mukherjee",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Mukherjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mukherjee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082414"
                        ],
                        "name": "W. Ho",
                        "slug": "W.-Ho",
                        "structuredName": {
                            "firstName": "W.-H.",
                            "lastName": "Ho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Ho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16892037,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47a9844bd5316af3fbd87bafceccedc8053b4066",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Managing the buffering of data along arcs is a critical part of compiling a synchronous dataflow (SDF) program. This paper shows how dataflow properties can be analyzed at compile-time to make buffering more efficient. Since the target code corresponding to each node of an SDF graph is normally obtained from a hand-optimized library of predefined blocks, the efficiency of data transfer between blocks is often the limiting factor in how closely an SDF compiler can approximate meticulous manual coding. Furthermore, in the presence of large sample-rate changes, straightforward buffering techniques can quickly exhaust limited on-chip data memory, necessitating the use of slower external memory. The techniques presented in this paper address both of these problems in a unified manner."
            },
            "slug": "Parts-that-add-up-to-a-whole-:-a-framework-for-the-Powell-Lee",
            "title": {
                "fragments": [],
                "text": "Parts that add up to a whole : a framework for the analysis of tables"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper shows how dataflow properties can be analyzed at compile-time to make buffering more efficient in a synchronous dataflow (SDF) program."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082991"
                        ],
                        "name": "Georgia Gkioxari",
                        "slug": "Georgia-Gkioxari",
                        "structuredName": {
                            "firstName": "Georgia",
                            "lastName": "Gkioxari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Georgia Gkioxari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 50
                            }
                        ],
                        "text": "5 illustrates some of the rare errors made by the M-RCNN model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 104
                            }
                        ],
                        "text": "4 shows some representative examples of the document layout analysis results of testing pages using the M-RCNN model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 139
                            }
                        ],
                        "text": "Three experiments are designed to investigate 1) how well the established object detection models Faster-RCNN (F-RCNN) [19] and Mask-RCNN (M-RCNN) [5] can\nrecognize document layout of PubLayNet; 2) if the F-RCNN and M-RCNN models pre-trained on PubLayNet can be fine-tuned to tackle the ICDAR 2013 Table Recognition Competition6; 3) if the F-RCNN and M-RCNN models pre-trained on PubLayNet are better initializations than those pre-trained on the ImageNet and COCO datasets for analyzing documents in a different domain."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "More recently, image analytics methods based on deep learning are becoming available [5] and are used to train document layout understanding pipelines [1], [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 77
                            }
                        ],
                        "text": "4: Representative examples of the document layout analysis results using the M-RCNN model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 202
                            }
                        ],
                        "text": "5) Generation of instance segmentation: For text, title, and list instances, we automatically generate segmentations from the texelines of the PDF elements, which allows us to train the Mask-RCNN model [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 69
                            }
                        ],
                        "text": "Both models can generate accurate (MAP > 0.9) document layout, where M-RCNN shows a small advantage over F-RCNN."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 38
                            }
                        ],
                        "text": "The performance of the F-RCNN and the M-RCNN models on our development and testing sets are depicted in Table III."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 32
                            }
                        ],
                        "text": "We trained a F-RCNN model and a M-RCNN model on PubLayNet using the Detectron implementation [20]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 125
                            }
                        ],
                        "text": "As implied by the high MAP, the model is able to generate accurate layout.\ntaken to compare different pre-trained F-RCNN and M-RCNN models for fine tuning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 147
                            }
                        ],
                        "text": "Three experiments are designed to investigate 1) how well the established object detection models Faster-RCNN (F-RCNN) [19] and Mask-RCNN (M-RCNN) [5] can"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 32
                            }
                        ],
                        "text": "We trained a F-RCNN model and a M-RCNN model on this table detection dataset under the same configuration described in Section IV-A."
                    },
                    "intents": []
                }
            ],
            "corpusId": 54465873,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "022dd244f2e25525eb37e9dda51abb9cd8ca8c30",
            "isKey": true,
            "numCitedBy": 9771,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron."
            },
            "slug": "Mask-R-CNN-He-Gkioxari",
            "title": {
                "fragments": [],
                "text": "Mask R-CNN"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work presents a conceptually simple, flexible, and general framework for object instance segmentation that outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2154179"
                        ],
                        "name": "V. Levenshtein",
                        "slug": "V.-Levenshtein",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Levenshtein",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Levenshtein"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 165
                            }
                        ],
                        "text": "We use the fuzzysearch5 package to search for the closest match to a target string in a source string, where string distance is measured by the Levenshtein distance [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60827152,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2f8876482c97e804bb50a5e2433881ae31d0cdd",
            "isKey": false,
            "numCitedBy": 10967,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Binary-codes-capable-of-correcting-deletions,-and-Levenshtein",
            "title": {
                "fragments": [],
                "text": "Binary codes capable of correcting deletions, insertions, and reversals"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "We trained a F-RCNN model and a M-RCNN model on PubLayNet using the Detectron implementation [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Detectron"
            },
            "venue": {
                "fragments": [],
                "text": "https://github.com/facebookresearch/detectron, 2018."
            },
            "year": 2018
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 6,
            "methodology": 8
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 21,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/PubLayNet:-Largest-Dataset-Ever-for-Document-Layout-Zhong-Tang/b5799d10df17de3232540e990da69553800d6376?sort=total-citations"
}