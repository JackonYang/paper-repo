{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87277825"
                        ],
                        "name": "J. Stutz",
                        "slug": "J.-Stutz",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Stutz",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stutz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40484982"
                        ],
                        "name": "P. Cheeseman",
                        "slug": "P.-Cheeseman",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Cheeseman",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cheeseman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118378659,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "7a4e63b762e4046191cd4b818c3620228e3e700a",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a Bayesian approach to the unsupervised discovery of classes in a set of cases, sometimes called finite mixture separation or clustering. The main difference between clustering and our approach is that we search for the \u201cbest\u201d set of class descriptions rather than grouping the cases themselves. We describe our classes in terms of probability distribution or density functions, and the locally maximal posterior probability parameters. We rate our classifications with an approximate posterior probability of the distribution function w.r.t. the data, obtained by marginalizing over all the parameters. Approximation is necessitated by the computational complexity of the joint probability, and our marginalization is w.r.t. a local maxima in the parameter space. This posterior probability rating allows direct comparison of alternate density functions that differ in number of classes and/or individual class density functions."
            },
            "slug": "Autoclass-\u2014-A-Bayesian-Approach-to-Classification-Stutz-Cheeseman",
            "title": {
                "fragments": [],
                "text": "Autoclass \u2014 A Bayesian Approach to Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A Bayesian approach to the unsupervised discovery of classes in a set of cases, sometimes called finite mixture separation or clustering, which allows direct comparison of alternate density functions that differ in number of classes and/or individual class density functions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067844876"
                        ],
                        "name": "Robin Hanson",
                        "slug": "Robin-Hanson",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Hanson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robin Hanson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87277825"
                        ],
                        "name": "J. Stutz",
                        "slug": "J.-Stutz",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Stutz",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stutz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40484982"
                        ],
                        "name": "P. Cheeseman",
                        "slug": "P.-Cheeseman",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Cheeseman",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cheeseman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14975902,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4110a8aff2e92964187f888bbaa1312e0b39fd1d",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The task of inferring a set of classes and class descriptions most likely to explain a given data set can be placed on a firm theoretical foundation using Bayesian statistics. Within this framework, and using various mathematical and algorithmic approximations, the Auto Class system searches for the most probable classifications, automatically choosing the number of classes and complexity of class descriptions. Simpler versions of AutoClass have been applied to many large real data sets, have discovered new independently-verified phenomena, and have been released as a robust software package. Recent extensions allow attributes to be selectively correlated within particular classes, and allow classes to inherit, or share, model parameters though a class hierarchy."
            },
            "slug": "Bayesian-Classification-with-Correlation-and-Hanson-Stutz",
            "title": {
                "fragments": [],
                "text": "Bayesian Classification with Correlation and Inheritance"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The task of inferring a set of classes and class descriptions most likely to explain a given data set can be placed on a firm theoretical foundation using Bayesian statistics, and using various mathematical and algorithmic approximations the Auto Class system searches for the most probable classifications."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145153956"
                        ],
                        "name": "J. Goebel",
                        "slug": "J.-Goebel",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Goebel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Goebel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49282476"
                        ],
                        "name": "K. Volk",
                        "slug": "K.-Volk",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Volk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Volk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50752976"
                        ],
                        "name": "H. Walker",
                        "slug": "H.-Walker",
                        "structuredName": {
                            "firstName": "Helen",
                            "lastName": "Walker",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Walker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7438176"
                        ],
                        "name": "F. Gerbault",
                        "slug": "F.-Gerbault",
                        "structuredName": {
                            "firstName": "Florence",
                            "lastName": "Gerbault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gerbault"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40484982"
                        ],
                        "name": "P. Cheeseman",
                        "slug": "P.-Cheeseman",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Cheeseman",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cheeseman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144308823"
                        ],
                        "name": "Matthew Self",
                        "slug": "Matthew-Self",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Self",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Self"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87277825"
                        ],
                        "name": "J. Stutz",
                        "slug": "J.-Stutz",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Stutz",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stutz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114766082"
                        ],
                        "name": "Will Taylor",
                        "slug": "Will-Taylor",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Will Taylor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117259268,
            "fieldsOfStudy": [
                "Physics",
                "Computer Science"
            ],
            "id": "5822c2742d603e8e3d6504bd5c1ead2fbe0cb9f8",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The availability of a reclassification of the IRAS LRS Atlas of spectra using a new Bayesian classification procedure (AutoClass) is announced. The classes of objects which result from the application of the AutoClass algorithm include many of the previously known LRS classes. New classes which have interesting astronomical and astrophysical interpretations were also found. Techniques, such as the AutoClass algorithm, have a bright future in the arena of astronomical classification problems."
            },
            "slug": "A-Bayesian-classification-of-the-IRAS-LRS-Atlas-Goebel-Volk",
            "title": {
                "fragments": [],
                "text": "A Bayesian classification of the IRAS LRS Atlas"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "The availability of a reclassification of the IRAS LRS Atlas of spectra using a new Bayesian classification procedure (AutoClass) is announced, which includes many of the previously known LRS classes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "65793277"
                        ],
                        "name": "T. Loredo",
                        "slug": "T.-Loredo",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Loredo",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Loredo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6698730,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2b220e238a4ce46df0430838fb0aaa55cc26677c",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 127,
            "paperAbstract": {
                "fragments": [],
                "text": "The \"frequentist\" approach to statistics, currently domi\u00ad nating statistical practice in astrophysics, is compared to the historically older Bayesian approach, which is now growing in popularity in other scien\u00ad tific disciplines, and which provides unique, optimal solutions to well-posed problems. The two approaches address the same questions with very dif\u00ad ferent calculations, but in simple cases often give the same final results, confusing the issue of whether one is superior to the other. Here frequentist and Bayesian methods are applied to problems where such a mathemati\u00ad cal coincidence does not occur, allowing assessment of their relative merits based on their performance, rather than philosophical argument. Emphasis is placed on a key distinction between the two approaches: Bayesian meth\u00ad ods, based on comparisons among alternative hypotheses using the single observed data set, consider averages over hypotheses; frequentist methods, in contrast, average over hypothetical alternative data samples and consider hypothesis averaging to be irrelevant. Simple problems are presented that magnify the consequences of this distinction to where common sense can confidently judge between the methods. These demonstrate the irrelevance of sample averaging, and the necessity of hypothesis averaging, revealing frequentist methods to be fundamentally flawed. Bayesian methods are then presented for astrophysically relevant problems using the Poisson distribu\u00ad tion, including the analysis of \"on/off\" measurements of a weak source in a strong background. Weaknesses of the presently used frequentist methods for these problems are straightforwardly overcome using Bayesian meth\u00ad ods. Additional existing applications of Bayesian inference to astrophysical problems are noted."
            },
            "slug": "Promise-of-Bayesian-Inference-for-Astrophysics-Loredo",
            "title": {
                "fragments": [],
                "text": "Promise of Bayesian Inference for Astrophysics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144277223"
                        ],
                        "name": "B. Kanefsky",
                        "slug": "B.-Kanefsky",
                        "structuredName": {
                            "firstName": "Bob",
                            "lastName": "Kanefsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kanefsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87277825"
                        ],
                        "name": "J. Stutz",
                        "slug": "J.-Stutz",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Stutz",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stutz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40484982"
                        ],
                        "name": "P. Cheeseman",
                        "slug": "P.-Cheeseman",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Cheeseman",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cheeseman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114766082"
                        ],
                        "name": "Will Taylor",
                        "slug": "Will-Taylor",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Will Taylor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 128132273,
            "fieldsOfStudy": [
                "Mathematics",
                "Environmental Science"
            ],
            "id": "d3a95f82e34c06f5d6bdf6dfb5a7b9b7d055eebd",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This research note shows the results of applying a new massively parallel version of the automatic classification program (AutoClass IV) to a particular Landsat/TM image. The previous results for this image were produced using a \"subsampling\" technique because of the image size. The new massively parallel version of AutoClass allows the complete image to be classified without \"subsampling\", thus yielding improved results. The area in question is the FIFE study area in Kansas, and the classes AutoClass found show many interesting subtle variations in types of ground cover. Displays of the spatial distributions of these classes make up the bulk of this report. While the spatial distribution of some of these classes make their interpretation easy, most of the classes require detailed knowledge of the area for their full interpretation. We hope that some who receive this document can help us in understanding these classes. One of the motivations of this exercise was to test the new version of AutoClass (IV) that allows for correlation among the variables within a class. The scatter plots associated with the classes show that this correlation information is important in separating the classes. The fact that the spatial distribution of each of these classes is far from uniform, even though AutoClass was not given information about positions of pixels, shows that the classes are due to real differences in the image."
            },
            "slug": "An-Improved-Automatic-Classification-of-a-Image-Kanefsky-Stutz",
            "title": {
                "fragments": [],
                "text": "An Improved Automatic Classification of a Landsat/TM Image from Kansas (FIFE)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145948621"
                        ],
                        "name": "G. Box",
                        "slug": "G.-Box",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Box",
                            "middleNames": [
                                "E.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Box"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36184409"
                        ],
                        "name": "G. C. Tiao",
                        "slug": "G.-C.-Tiao",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Tiao",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. C. Tiao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122028907,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a205103d4f25ae39f417bac7bd5142302d7f448c",
            "isKey": false,
            "numCitedBy": 4326,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Nature of Bayesian Inference Standard Normal Theory Inference Problems Bayesian Assessment of Assumptions: Effect of Non-Normality on Inferences About a Population Mean with Generalizations Bayesian Assessment of Assumptions: Comparison of Variances Random Effect Models Analysis of Cross Classification Designs Inference About Means with Information from More than One Source: One-Way Classification and Block Designs Some Aspects of Multivariate Analysis Estimation of Common Regression Coefficients Transformation of Data Tables References Indexes."
            },
            "slug": "Bayesian-inference-in-statistical-analysis-Box-Tiao",
            "title": {
                "fragments": [],
                "text": "Bayesian inference in statistical analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This chapter discusses Bayesian Assessment of Assumptions, which investigates the effect of non-Normality on Inferences about a Population Mean with Generalizations in the context of a Bayesian inference model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2046035534"
                        ],
                        "name": "\u65e5\u91ce \u5bdb\u4e09",
                        "slug": "\u65e5\u91ce-\u5bdb\u4e09",
                        "structuredName": {
                            "firstName": "\u65e5\u91ce",
                            "lastName": "\u5bdb\u4e09",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u65e5\u91ce \u5bdb\u4e09"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102846495"
                        ],
                        "name": "K. Hino",
                        "slug": "K.-Hino",
                        "structuredName": {
                            "firstName": "Kanzo",
                            "lastName": "Hino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hino"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 116991854,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "331cc35674210acb4b564f944bee071f848ceec0",
            "isKey": false,
            "numCitedBy": 527,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Ecological data are often lognormally distributed. Nutrient concentrations, population densities and biomasses, rates of production and other flows are always positive, and generally have standard deviations that increase as the mean increases. Lognormally distributed variables have these characteristics, whereas normally distributed variables can be negative and have a standard deviation that does not change as the mean changes. Lognormal errors arise when sources of variation accumulate multiplicatively, whereas normal errors arise when sources of variation are additive. Given a normally distributed random variable Y, one can calculate a lognormally distributed random variable Z = exp(Y) where exp stands for the exponential function (exp(x) = e x). The mean Z and the standard deviation s Z of the lognormal variable are related to the mean Y and standard deviation s Y of the normal variable by) 2 / exp() exp(2 Y s Y Z = [1] 5. 0 2 ] 1) [exp(\u2212 = Y Z s Z s [2] Equation 1 can be used to correct for transformation bias in logarithmic regression. Suppose that lognormally-distributed observations Z have been log transformed as Y = log(Z) to fit a regression model such as \u03b5 + =) , (\u02c6 b X f Y [3] where Y is the log-transformed response variable which is predicted to be Y \u02c6 computed from the function f, X is a matrix of predictors, b is a vector of parameters, and the errors \u03b5 are normally distributed with mean zero and standard deviation s \u03b5. Predictions Z \u02c6 in the original units are calculated using equation 1 as ] 2) \u02c6 exp[(\u02c6 2 \u03b5 s Y Z + = [4] Note that estimates the median prediction of Z, which will be smaller than the mean for a lognormally distributed variate. Thus it makes sense to adjust the median upward, as in equation 4.) \u02c6 exp(Y Equation 1 is also used in drawing random numbers from a lognormal distribution. Generators for normally-distributed random variables Y are common. Suppose we draw many values of Y with mean zero and standard deviation s Y. Then from equation 1, the mean of exp(Y) will not be 1 = e 0 ; instead the mean of exp(Y) will be. Generally, however, one would prefer to have the mean of a set of lognormally distributed random numbers be 1. This can be accomplished by shifting the random numbers to Y) 2 / exp(2 Y \u2026"
            },
            "slug": "\u5bfe\u6570\u6b63\u898f\u5206\u5e03(Lognormal-Distribution)\u306e\u3042\u3066\u306f\u3081\u306b\u3064\u3044\u3066-\u65e5\u91ce-Hino",
            "title": {
                "fragments": [],
                "text": "\u5bfe\u6570\u6b63\u898f\u5206\u5e03(Lognormal Distribution)\u306e\u3042\u3066\u306f\u3081\u306b\u3064\u3044\u3066"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152591573"
                        ],
                        "name": "D. Titterington",
                        "slug": "D.-Titterington",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Titterington",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Titterington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15974963"
                        ],
                        "name": "A. F. Smith",
                        "slug": "A.-F.-Smith",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Smith",
                            "middleNames": [
                                "F.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. F. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580190"
                        ],
                        "name": "U. Makov",
                        "slug": "U.-Makov",
                        "structuredName": {
                            "firstName": "Udi",
                            "lastName": "Makov",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Makov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124992180,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "54a1f6ab4cc6cb749c2b8d15c1dd3449e072362f",
            "isKey": false,
            "numCitedBy": 3447,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical Problems. Applications of Finite Mixture Models. Mathematical Aspects of Mixtures. Learning About the Parameters of a Mixture. Learning About the Components of a Mixture. Sequential Problems and Procedures."
            },
            "slug": "Statistical-analysis-of-finite-mixture-Titterington-Smith",
            "title": {
                "fragments": [],
                "text": "Statistical analysis of finite mixture distributions"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This course discusses Mathematical Aspects of Mixtures, Sequential Problems and Procedures, and Applications of Finite Mixture Models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 78
                            }
                        ],
                        "text": "Integer count valued attributes | Poisson distribution with uniform prior per Loredo (1992). No covariant form has been developed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3165,
                                "start": 3162
                            }
                        ],
                        "text": "The magnitude of this scaling is usually small relative to that of P X jT S I but may be signi cant when comparing T with very di erent numbers of classes\nThus we rate the various models T by their best P X jTS I and report on them in terms of the corresponding map parameterizations bV If one model s marginal dominates all others it is our single best choice for classifying the database Otherwise we report the several that do dominate\nAutoClass Attribute Models\nEach class model is a product of conditionally independent probability distributions over singleton and or covariant subsets of the attributes For the medical example given in section blood type is a discrete valued attribute which we model with a Bernoulli distribution while age and weight are both scalar real numbers that we model with a log Gaussian density\nThe only hard constraint is that all class models used in any classi cations that are to be compared must model the same attribute set Attributes deemed irrelevant to a particular classi cation cannot simply be ignored since this would a ect the marginal probabilities as is shown below\nAutoClass provides basic models for simple discrete nominal and several types of nu\nmerical data We have not yet identi ed a satisfactory distribution function for ordered discrete ordinal data In each case we adopt a minimum or near minimum informa\ntion prior the choice being limited among those providing integrable marginals This limitation has seriously retarded development of the more speci c models but numerical integration is considered to be too costly for EM convergence\nIn the following we describe in detail the basic elements of the independent Bernoulli and Gaussian models and note other attribute probability distributions that we use to assemble the class models\nDiscrete valued attributes sex blood type Bernoulli distributions with uniform Dirichlet conjugate prior For the singleton case with Lk possible values\nthe parameters are Vjk fqjk qjkLkg such that qjkl PLk l qjkl where\nP Xik l jXi Cj Vjk Tjk S I qjkl\nP qjk qjkLk jTjk S I Lk\nLk\nLk LkY l q Lk jkl\nbqjkl wjkl Lk wj\nFor the covariant case say sex and blood type jointly we apply the above model to the cross product of individual attribute values Thus female and type A would form a single value in the cross product The number of such values and thus the number of parameters required is the product of the individual attribute s Lk However the prior of equation severely limits the probability of large covariant p d f s as discussed in section\nReal valued location attributes spatial locations Gaussian densities with either a uniform or Gaussian prior on the means We use a Je reys prior on a singleton attribute s standard deviation and the inverse Wishart distribution Box\nTiao as the variance prior of covariant attribute subsets For a single\nattribute with uniform priors using the statistics de ned in equation\nP Xik jXi Cj jk jk Tjk S I p jk e\nXik jk\njk\nP jk jTjk S I kmax kmin b jk mjk P jk jTjk S I jk log\nkmax kmin\nb jk s jk wjwj\nReal valued scalar attributes age weight Log Gaussian density model obtained by applying the Gaussian model to log Xik mink See Aitchison Brown\nBounded real valued attributes probabilities Gaussian Log Odds obtained by applying the Gaussian to log Xik mink maxk Xik under development\nCircular or angular real valued attributes von Mises Fisher distributions on the circle and n sphere under development See Mardia et al\nInteger count valued attributes Poisson distribution with uniform prior per Loredo No covariant form has been developed\nMissing values Discrete valued attribute sets are extended to include missing as an additional attribute value to be modeled as any other value Numerical attributes use a binary discrete probability qjk for missing and qjk for known with the standard numerical model conditioned on the known side With the Gaussian model this gives\nP Xik missing jXi Cj qjk jk jk Tjk S I qjk\nP Xik r jXi Cj qjk jk jk Tjk S I qjk p jk e\nr jk\njk\nHierarchical models represent a reorganization of the standard mixture model from a at structure where each class is fully independent to a tree structure where multiple classes can share one or more model terms A class is then described by the attribute model nodes along the branch between root and leaf This makes it possible to avoid duplicating essentially identical attribute distributions common to several classes The advantage of such hierarchical models lies in eliminating excess parameters thereby increasing the model posterior See Hanson et al for a full description of our approach Other approaches are possible see Boulton Wallace\nIrrelevant attributes Irrelevant attributes pose a problem which we have only recently recognized If an attribute is deemed irrelevant to all classi cation models under consideration it can simply be deleted from the database If an attribute is deemed irrelevant to only some of the models one is tempted to simply eliminate it from consideration by those models and to model it in the others This is what we have done in AutoClass but it is an error\nConsider two models Vj Tj and Vj T j identical in both form and parameter values except that the latter includes an additional Vjk T jk modeling one additional\nattribute k Let T jk be any appropriate p d f except a delta function Then for any instance Xi\nP Xi jXi Cj Vj Tj S I P Xi jXi C j Vj T j S I\nThis is a simple consequence of the fact that a non delta p d f cannot predict any value with probability Taken to the limit we nd that a class model which ignores all attributes will always be more probable than one which models any attributes Obviously the results of modeling with di erent attribute sets are incommensurable\nHow should we handle irrelevant attributes For a classi er an attribute is irrele vant when all classes possess identical p d f s for that attribute In the hierarchical model described above this can be achieved by pushing the attribute model Vjk Tjk up to the root node where it is inherited by all leaves In an ordinary mixture model the same e ect can be obtained by using a common Tjk with every Vjk xed at the map values estimated from a single class classi cation model This will suf\nce for the case when all classes within a classi cation ignore the attribute and\nallow comparison between classi cations that deem di erent attribute subsets irrel evant The case where only some classes within a classi cation ignore an attribute is yet undecided\nIn principle our classi cation model should also include a prior distribution P T jS I on the number of classes present and the individual class model forms Tj Currently we take this distribution to be uniform and drop it from our calculations Thus we ignore any prior information on alternate classi cation model probabilities relying solely on our parameter priors for the Occam factor preventing over tting of the models We nd this quite su\"cient\nThe Occam Factor\nWe have several times mentioned an Occam Factor implying that Bayesian parameter priors can somehow prevent the over tting that is a problem with maximum likelihood optimization of any kind of probabilistic model Consider that every single parameter introduced into a Bayesian model brings its own multiplicative prior to the joint prob ability which always lowers the marginal If a parameter fails to raise the marginal by increasing the direct probability by a greater factor than the prior lowers the marginal we reject the model incorporating that parameter In the mixture models used by Au toClass each class requires a full set of attribute model parameters each with its own prior Those priors always favor classi cations with smaller numbers of classes and do so\noverwhelmingly once the number of classes exceeds some small fraction of the database size\nSimilar e ects limit model complexity within the classes Simple independent attribute\nmodels are usually favored simply because they require fewer parameters than the corre sponding covariant models Ten real valued attributes require parameters for modeling with independent Gaussians and for the full covariant Gaussian Ten binary discrete attributes also require parameters for modeling with independent Bernoulli distribu tions but are needed for a fully covariant Bernoulli distribution One needs a great many very highly covariant instances to raise a fully covariant model s marginal above the independent model s\nBoth of the foregoing e ects are con rmed throughout our experience with AutoClass\nFor data sets of a few hundred to a few thousand instances class models with large order covariant terms are generally rated far lower than those combining independent and or small order covariant terms We have yet to nd a case where the most probable number of classes was not a small fraction of the number of instances classi ed Nor have we found a case where the most probable number of model parameters was more than a small fraction of the total number of attribute values Over tting simply does not occur when Bayesian mixture models are correctly applied\nThe AutoClass Implementation\nAutoClass was written in Lisp taking full advantage of the extraordinary programming environment provided by the Symbolics Lisp Machine It has been adapted to operate in most Lisp environments and a data parallel version exists for star Lisp on the CM A C translation is in preparation Some details regarding the computational considerations encountered in implementing AutoClass will be found in Stutz Cheeseman A NASA technical report giving fuller details of the mathematics and implementation is in preparation\nCase Studies\nInfrared Astronomical Satellite IRAS Data\nThe rst major test of AutoClass on a large scale real world database was the application of AutoClass to the IRAS Low Resolution Spectral Atlas This atlas consisted of mean spectra of IRAS point sources Each spectrum consists of blue channels in the range to microns and another red channels in the range from to\nTypically of order but potentially larger for very distinct classes\nmicrons Of these channels only contain usable data These point source spectra covered a wide range of intensities and showed many di erent spectral distributions We applied AutoClass to this spectral database by treating each of the spectral channels\nintensities as an independent normally distributed single real value The log normal model is preferable for such scalar data but several percent of the reported intensity values were negative Also adjacent spectral values are expected to be highly correlated but it was not obvious how to incorporate neighbor correlation information Thus we knew from the beginning that we were missing important information but we were curious how well AutoClass would do despite this handicap\nOur very rst attempts to apply AutoClass to the spectral data did not produce very good results as was immediately apparent from visual inspection Fortunately inspection also exposed the cause of the problem The spectra we were given had been\nnormalized in this case normalization meant scaling the spectra so that all had the same peak height This normalization meant that noisy spectra were arti cially scaled up or down depending on whether the noise at the peak was higher or lower than the average Since all values in a single spectrum were scaled by the same constant an incorrect scaling constant distorted all spectral values Also spectra with a single strong peak were scaled so that the rest of the spectrum was close to the noise level We solved the normalization problem by renormalizing the data ourselves so that area under the all curves is the same This method of normalization is much less sensitive to noise than the peak normalization method\nThe experts who provided us with this data tried to make life easy for us by only giving\nus the brightest spectra from of the sky without telling us about this sampling bias When we found this out we requested all the spectra in the atlas to work with Because this larger atlas included much noisier spectra we found a new problem some spectral intensities were negative A negative intensity or measurement is physically impossible so these values were a mystery After much investigation we nally found out that the processing software had subtracted a background value from all spectra This pre processing of course violates the basic maxim that analysis should be performed on the data actually measured and all corrections should be done in the statistical modeling step\nOnce these problems had been removed we used AutoClass II to classify all spec\ntra The results of this classi cation are presented in Cheeseman et al and it revealed many interesting new discoveries The rst observation is that the AutoClass classes of them gave a signi cantly di erent classi cation than that previously pro vided with the atlas This earlier IRAS spectral classi cation was based on expected features and human generated criteria based on visual inspection AutoClass was able to make many subtle distinctions between spectra that super cially look similar and\nthese distinctions were not previously known Some of the classes such as the blackbody emission classes and the silicate emission classes were known from previous studies but the ne distinctions within these broad classes were not previously known\nThe IRAS spectral classi cation also revealed other astronomically signi cant results\nFor example by nding which classes the few known carbon stars occured in we were able to triple the number of known or suspected carbon stars AutoClass also revealed a large number of blackbody stars with a signi cant IR excess presumably due to dust surrounding the star Another indirect result of the classi cation is that the average spectral intensities of a class cancel the noise present in single spectra making ner detail visible For example this noise suppression revealed a very weak spectral bump at microns in some classes that is completely invisible in the individual spectra Many of these discoveries are discussed in Goebel et al\nThe AutoClass classi cation was su\"ciently good that it revealed problems with the data that had been previously missed In particular some of the blackbody sub classes showed an excess of energy at the blue end of the spectrum There is no plausible physical mechanism that could produce such a blue excess in blackbody stars so this result was a mystery Eventually we discovered that this excess was the result of incorrect calibration Originally Vega a degree star was chosen as the reference star but later in the mission the reference star was switched to Tau a degree star Unfortunately the software was not altered to re ect this change thus causing the calibration error Of course none of this change information was documented so it took considerable search for us to nd the cause\nOther calibration errors and artifacts of the data processing also gradually came to light as we discussed our results with the domain experts In particular we found out that the spectra were often contaminated with cosmic ray spikes and that a lter in the software removed these spikes from the data before averaging the di erent spectra together Unfortunately this lter could not tell the di erence between a strong spectral line and a cosmic ray hit so it often eliminated perfectly good spectra and yet still passed contaminated spectra Again the lesson to be drawn from this experience is that the raw observation data should be made available and e ects such as cosmic rays and background noise should be statistically modeled the data itself should not be modi ed!"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 127
                            }
                        ],
                        "text": "Circular or angular real valued attributes | von Mises-Fisher distributions on the circle and n-sphere (under development) See Mardia et al. (1979). Integer count valued attributes | Poisson distribution with uniform prior per Loredo (1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An Improved Automatic Classi cation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 4
                            }
                        ],
                        "text": "See Hanson et al. (1991) for a full description of our approach. Other approaches are possible: see Boulton & Wallace (1973). Irrelevant attributes | Irrelevant attributes pose a problem which we have only recently recognized."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 71
                            }
                        ],
                        "text": "The theory behind this correlated version of AutoClass is presented in Hanson et al. (1991). This model still assumes that the pixels are independent of their neighbors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 4
                            }
                        ],
                        "text": "See Hanson et al. (1991) for a full description of our approach."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Classi cation with Correlation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143698811"
                        ],
                        "name": "A. Cohen",
                        "slug": "A.-Cohen",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Cohen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cohen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125431157,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3ef90a62796c6c8c88439bd116ca7472e96254b7",
            "isKey": false,
            "numCitedBy": 669,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Finite-Mixture-Distributions-Cohen",
            "title": {
                "fragments": [],
                "text": "Finite Mixture Distributions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4110643"
                        ],
                        "name": "D. Boulton",
                        "slug": "D.-Boulton",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Boulton",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Boulton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8635127"
                        ],
                        "name": "C. S. Wallace",
                        "slug": "C.-S.-Wallace",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Wallace"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 41661355,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0396beddfbba4c20825019fd84c5fccb8a69c115",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-Information-Measure-for-Hierarchic-Boulton-Wallace",
            "title": {
                "fragments": [],
                "text": "An Information Measure for Hierarchic Classification"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Classiication (AutoClass): Theory and Results 83"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian Classiication (AutoClass): Theory and Results 83"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic Classi cation of Spectra From the Infrared Astronomical Satellite (IRAS), NASA Reference Publication #1217"
            },
            "venue": {
                "fragments": [],
                "text": "National Technical Information Service"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic Classiication of Spectra From the Infrared Astronomical Satellite (IRAS), NASA Reference Publication #1217, National Technical Information Service"
            },
            "venue": {
                "fragments": [],
                "text": "Automatic Classiication of Spectra From the Infrared Astronomical Satellite (IRAS), NASA Reference Publication #1217, National Technical Information Service"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiavariant Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Multiavariant Analysis"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An Information Measure of Hierarchic Classi"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 18,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Bayesian-Classification-(AutoClass):-Theory-and-Cheeseman-Stutz/42f75b297aed474599c8e598dd211a1999804138?sort=total-citations"
}