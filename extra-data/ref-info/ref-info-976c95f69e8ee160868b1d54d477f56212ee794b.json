{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118681"
                        ],
                        "name": "J. Hockenmaier",
                        "slug": "J.-Hockenmaier",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hockenmaier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hockenmaier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 25
                            }
                        ],
                        "text": "Furthermore, as noted by Hockenmaier (2001), it does not penalize equivalent analyses of multiple modi-\nfiers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 39
                            }
                        ],
                        "text": "This model was originally described in Hockenmaier (2001), where it was applied to a preliminary version of CCGbank, and its definition is repeated here in the top row of Table 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14066502,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "995ecb82a54cc17cb4cd68805acf20709fca917b",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a statistical parser for a wide-coverage Combinatory Categorial Grammar (CCG) derived from the Penn Treebank. The Treebank is translated to a corpus of canonical CCG derivations. We de ne a generative statistical model over CCG derivations and train it on the transformed Treebank. This model is evaluated using Parseval measures and the accuracy of recovery of word-word dependencies. The impact of lexical coverage on parsing accuracy is also investigated."
            },
            "slug": "Statistical-Parsing-for-CCG-with-Simple-Generative-Hockenmaier",
            "title": {
                "fragments": [],
                "text": "Statistical Parsing for CCG with Simple Generative Models"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This paper presents a statistical parser for a wide-coverage Combinatory Categorial Grammar (CCG) derived from the Penn Treebank using a generative statistical model over CCG derivations and train it on the transformed Treebank."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 130
                            }
                        ],
                        "text": "Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 153
                            }
                        ],
                        "text": "Distance measures for CCG Our distance measures are related to those proposed by Goodman (1997), which are appropriate for binary trees (unlike those of Collins (1997))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 73
                            }
                        ],
                        "text": "Gildea (2001) shows that removing the lexical dependencies in Model 1 of Collins (1997) (that is, not conditioning onwh when generatingws) decreases labeled precision and recall by only 0.5%."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ffa423a5283396c88ff3d4033d541796bd039cc",
            "isKey": true,
            "numCitedBy": 873,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96)."
            },
            "slug": "Three-Generative,-Lexicalised-Models-for-Parsing-Collins",
            "title": {
                "fragments": [],
                "text": "Three Generative, Lexicalised Models for Statistical Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A new statistical parsing model is proposed, which is a generative model of lexicalised context-free grammar and extended to include a probabilistic treatment of both subcategorisation and wh-movement."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145287425"
                        ],
                        "name": "David Chiang",
                        "slug": "David-Chiang",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Chiang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1471139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "351752e0080d7635aef227b5d0bd6461cf0b14bd",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance. We find that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG."
            },
            "slug": "Statistical-Parsing-with-an-Automatically-Extracted-Chiang",
            "title": {
                "fragments": [],
                "text": "Statistical Parsing with an Automatically-Extracted Tree Adjoining Grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work describes the induction of a probabilistic LTAG model from the Penn Treebank and finds that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118681"
                        ],
                        "name": "J. Hockenmaier",
                        "slug": "J.-Hockenmaier",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hockenmaier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hockenmaier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145332819"
                        ],
                        "name": "Mark Steedman",
                        "slug": "Mark-Steedman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Steedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Steedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 131
                            }
                        ],
                        "text": "CCGbank is a corpus of CCG normal-form derivations obtained by translating the Penn Treebank trees using an algorithm described by Hockenmaier and Steedman (2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8667731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5332134b56af552515548e7e2ea1923cd63a5805",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We present an algorithm which translates the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations. To do this we have needed to make several systematic changes to the Treebank which have to effect of cleaning up a number of errors and inconsistencies. This process has yielded a cleaner treebank that can potentially be used in any framework. We also show how unary type-changing rules for certain types of modifiers can be introduced in a CCG grammar to ensure a compact lexicon without augmenting the generative power of the system. We demonstrate how the combination of preprocessing and type-changing rules minimizes the lexical coverage problem."
            },
            "slug": "Acquiring-Compact-Lexicalized-Grammars-from-a-Hockenmaier-Steedman",
            "title": {
                "fragments": [],
                "text": "Acquiring Compact Lexicalized Grammars from a Cleaner Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "An algorithm which translates the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations is presented and it is demonstrated how the combination of preprocessing and type-changing rules minimizes the lexical coverage problem."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 82
                            }
                        ],
                        "text": "Therefore, we also evaluate performance using a dependency evaluation reported by Collins (1999), which counts wordword dependencies as determined by local trees and their labels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 72
                            }
                        ],
                        "text": "These figures are surprisingly close to the figure of 90.9% reported by Collins (1999) on section 00, given that, in order to allow a direct comparison, we have used the same interpolation technique and beam strategy as Collins (1999), which are very unlikely to be as well-tuned to our kind of\u2026"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 76
                            }
                        ],
                        "text": "The hP;H;Si labeled dependencies we report are not directly comparable with Collins (1999), since CCG categories encode subcategorization frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 147
                            }
                        ],
                        "text": "According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to the figures given by Collins (1999) for a linguistically less expressive grammar."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 31
                            }
                        ],
                        "text": "2We compute\u03bb in the same way as Collins (1999), p. 185."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 215
                            }
                        ],
                        "text": "\u2026figures are surprisingly close to the figure of 90.9% reported by Collins (1999) on section 00, given that, in order to allow a direct comparison, we have used the same interpolation technique and beam strategy as Collins (1999), which are very unlikely to be as well-tuned to our kind of grammar."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 31
                            }
                        ],
                        "text": "5We compute\u03bb in the same way as Collins (1999), p. 185."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In contrast to Gildea (2001), we find a significant improvement from modeling wordword dependencies."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 149
                            }
                        ],
                        "text": "As is to be expected, a statistical model of a CCG extracted from the Treebank is less robust than a model with an overly permissive grammar such as Collins (1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 98
                            }
                        ],
                        "text": "However, our model does not yet contain anything like the hard constraint on punctuation marks in Collins (1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 5
                            }
                        ],
                        "text": "Like Collins (1999), we assume that the test data is POStagged, and can therefore replace unknown words in the test data with their POS-tag, which is more appropriate for a formalism like CCG with a large set of lexical categories than one generic token for all unknown words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 217
                            }
                        ],
                        "text": "This results in a smaller and less overgenerating grammar than standard PCFGs (ca. 3,000 rules when instantiated with the above categories in sections 02-21, instead of>12,400 in the original Treebank representation (Collins, 1999))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7901127,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3fc44ff7f37ec5585310666c183c65e0a0bb2446",
            "isKey": true,
            "numCitedBy": 2062,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models."
            },
            "slug": "Head-Driven-Statistical-Models-for-Natural-Language-Collins",
            "title": {
                "fragments": [],
                "text": "Head-Driven Statistical Models for Natural Language Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "Three statistical models for natural language parsing are described, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118681"
                        ],
                        "name": "J. Hockenmaier",
                        "slug": "J.-Hockenmaier",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hockenmaier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hockenmaier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145332819"
                        ],
                        "name": "Mark Steedman",
                        "slug": "Mark-Steedman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Steedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Steedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 121
                            }
                        ],
                        "text": "We present a number of models over syntactic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al. (2002), this conference, for introduction), estimated from\nand tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 55
                            }
                        ],
                        "text": "In order to compare our performance with the parser of Clark et al. (2002), we also evaluate our best model according to the dependency evaluation introduced for that parser."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 7
                            }
                        ],
                        "text": "Unlike Clark et al. (2002), our parser does not always model the dependencies in the logical form."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 167
                            }
                        ],
                        "text": "4Due to the smaller grammar and lexicon of Clark et al., our parser can only be evaluated on slightly over 94% of the sentences in section 23, whereas the figures for Clark et al. (2002) are on 97%.\nmodel,HWDep, predicts the correct analyses, using the development section 00."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Clark et al. (2002) presents another statistical CCG parser, which is based on a conditional (rather than generative) model of the derived dependency structure, including non-surface dependencies."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 115
                            }
                        ],
                        "text": "The following table compares the two parsers according to the evaluation of surface and deep dependencies given in Clark et al. (2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6673978,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "760836263a766c71901e8e92417da34654d627df",
            "isKey": false,
            "numCitedBy": 125,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures. The parser differs from most existing wide-coverage treebank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies. A set of dependency structures used for training and testing the parser is obtained from a treebank of CCG normal-form derivations, which have been derived (semi-) automatically from the Penn Treebank. The parser correctly recovers over 80% of labelled dependencies, and around 90% of unlabelled dependencies."
            },
            "slug": "Building-Deep-Dependency-Structures-using-a-CCG-Clark-Hockenmaier",
            "title": {
                "fragments": [],
                "text": "Building Deep Dependency Structures using a Wide-Coverage CCG Parser"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures and correctly recovers over 80% of labelled dependencies, and around 90% of unlabelled dependencies."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793218"
                        ],
                        "name": "D. Gildea",
                        "slug": "D.-Gildea",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Gildea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gildea"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 15
                            }
                        ],
                        "text": "In contrast to Gildea (2001), we find a significant improvement from modeling wordword dependencies."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Gildea (2001) shows that removing the lexical dependencies in Model 1 of Collins (1997) (that is, not conditioning onwh when generatingws) decreases labeled precision and recall by only 0.5%."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 196105,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee7e21dd09949a5a53b39c13fca9cd3d55e2bc50",
            "isKey": true,
            "numCitedBy": 326,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank. While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might a ect parser performance, and how portable parsing models are across corpora. We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained. This leads us to a technique for pruning parameters to reduce the size of the parsing model."
            },
            "slug": "Corpus-Variation-and-Parser-Performance-Gildea",
            "title": {
                "fragments": [],
                "text": "Corpus Variation and Parser Performance"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work examines how other types of text might a ect parser performance, and how portable parsing models are across corpora by comparing results for the Brown and WSJ corpora, and considers which parts of the parser's probability model are particularly tuned to the corpus on which it was trained."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145177220"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 38
                            }
                        ],
                        "text": "The impact of the grandparent feature Johnson (1998) showed that a PCFG estimated from a version of the Penn Treebank in which the label of a node\u2019s parent is attached to the node\u2019s own label yields a substantial improvement (LP/LR: from 73.5%/69.7% to 80.0%/79.2%)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7978249,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c9f553e723a40a6713453b734b552c1928bf52b",
            "isKey": true,
            "numCitedBy": 441,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The kinds of tree representations used in a treebank corpus can have a dramatic effect on performance of a parser based on the PCFG estimated from that corpus, causing the estimated likelihood of a tree to differ substantially from its frequency in the training corpus. This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today. This performance variation comes about because any PCFG, and hence the corpus of trees from which the PCFG is induced, embodies independence assumptions about the distribution of words and phrases. The particular independence assumptions implicit in a tree representation can be studied theoretically and investigated empirically by means of a tree transformation / detransformation process."
            },
            "slug": "PCFG-Models-of-Linguistic-Tree-Representations-Johnson",
            "title": {
                "fragments": [],
                "text": "PCFG Models of Linguistic Tree Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A simple node relabeling transformation is described that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 57
                            }
                        ],
                        "text": "The inclusion of an additional grandparent feature gives Charniak (1999) a slight improvement in the Maximum Entropy inspired model, but a slight decrease in performance for an MLE model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 80
                            }
                        ],
                        "text": "Instead, it is quite likely that we are facing an estimation problem similar to Charniak (1999), who reports that the inclusion of the grandparent feature worsens performance of an MLE model, but improves performance if the individual distributions are modelled using Maximum Entropy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 52
                            }
                        ],
                        "text": "This paper compares a number of generative probability models for a widecoverage Combinatory Categorial Grammar (CCG) parser."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 52
                            }
                        ],
                        "text": "The currently best single-model statistical parser (Charniak, 1999) achieves Parseval scores of over 89% on the Penn Treebank."
                    },
                    "intents": []
                }
            ],
            "corpusId": 538122,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76d5e3fa888bee872b7adb7fa810089aa8ab1d58",
            "isKey": true,
            "numCitedBy": 1855,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5, 9, 10, 15, 17] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is the use of a \"maximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head."
            },
            "slug": "A-Maximum-Entropy-Inspired-Parser-Charniak",
            "title": {
                "fragments": [],
                "text": "A Maximum-Entropy-Inspired Parser"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less and 89.5% when trained and tested on the previously established sections of the Wall Street Journal treebank is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17446277"
                        ],
                        "name": "J. Goodman",
                        "slug": "J.-Goodman",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Goodman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Goodman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 19
                            }
                        ],
                        "text": "Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 81
                            }
                        ],
                        "text": "Distance measures for CCG Our distance measures are related to those proposed by Goodman (1997), which are appropriate for binary trees (unlike those of Collins (1997))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18774233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2588593c42126e059fb8aad7673fa1736755f1e1",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new formalism, probabilistic feature grammar (PFG). PFGs combine most of the best properties of several other formalisms, including those of Collins, Magerman, and Charniak, and in experiments have comparable or better performance. PFGs generate features one at a time, probabilistically, conditioning the probabilities of each feature on other features in a local context. Because the conditioning is local, efficient polynomial time parsing algorithms exist for computing inside, outside, and Viterbi parses. PFGs can produce probabilities of strings, making them potentially useful for language modeling. Precision and recall results are comparable to the state of the art with words, and the best reported without words."
            },
            "slug": "Probabilistic-Feature-Grammars-Goodman",
            "title": {
                "fragments": [],
                "text": "Probabilistic Feature Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "Probabilistic feature grammar combines most of the best properties of several other formalisms, including those of Collins, Magerman, and Charniak, and in experiments have comparable or better performance."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144372201"
                        ],
                        "name": "T. Kiss",
                        "slug": "T.-Kiss",
                        "structuredName": {
                            "firstName": "Tibor",
                            "lastName": "Kiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kiss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153331212"
                        ],
                        "name": "R. Levine",
                        "slug": "R.-Levine",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Levine",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Levine"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121759027,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "ae0c28e49afd694b10b16489dbb2f480305465f5",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The second volume in the Vancouver Studies in Cognitive Science series, this collection presents recent work in the fields of phonology, morphology, semantics, and neurolinguistics. Its overall theme is the relationship between the contents of grammatical formalisms and their real-time realizations in machine or biological systems. Individual essays address such topics as learnability, implementability, computational issues, parameter setting, and neurolinguistic issues. Contributors include Janet Dean Fodor, Richard T. Oehrle, Bob Carpenter, Edward P. Stabler, Elan Dresher, Arnold Zwicky, Mary-Louis Kean, and Lewis P. Shapiro."
            },
            "slug": "Formal-grammar-:-theory-and-implementation-Kiss-Levine",
            "title": {
                "fragments": [],
                "text": "Formal grammar : theory and implementation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145493610"
                        ],
                        "name": "M. Kay",
                        "slug": "M.-Kay",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Kay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 101
                            }
                        ],
                        "text": "We present a number of models over syntactic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al. (2002), this conference, for introduction), estimated from\nand tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In contrast to Gildea (2001), we find a significant improvement from modeling wordword dependencies."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58106824,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "ce8cca19455e8d3055c57a9bafe882984c95a201",
            "isKey": false,
            "numCitedBy": 911,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "In computational linguistics, which began in the 1950's with machine translation, systems that are based mainly on the lexicon have a longer t r a d i t i o n than anything e l se f o r these purposes, twenty f i ve years must be allowed to count as a tradition. The bulk of many of the early translation systems was made up by a d ic t ionary whose ent r ies consisted of a rb i t ra ry ins t ruc t ions In machine language. In the early 60's, computational llnsulsts---at least those with theoretical pretentlons---abandoned this way of doing business for at least three related reasons:"
            },
            "slug": "Syntactic-Process-Kay",
            "title": {
                "fragments": [],
                "text": "Syntactic Process"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "In computational linguistics, which began in the 1950's with machine translation, systems that are based mainly on the lexicon have a longer lifespan than anything else, so twenty years must be allowed to count as a tradition."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35371521"
                        ],
                        "name": "Kristina Toutanvoa",
                        "slug": "Kristina-Toutanvoa",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanvoa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanvoa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 85
                            }
                        ],
                        "text": "We therefore ran the\ndependency model on a test corpus tagged with the POS-tagger of Ratnaparkhi (1996), which is trained on the original Penn Treebank (seeHWDep (+ tagger) in Table 3)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10807721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1504a9d5829033a8cb4cf37b8bb13dfd4baddc7b",
            "isKey": false,
            "numCitedBy": 1574,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents results for a maximum-entropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging. In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs. The best resulting accuracy for the tagger on the Penn Treebank is 96.86% overall, and 86.91% on previously unseen words."
            },
            "slug": "Enriching-the-Knowledge-Sources-Used-in-a-Maximum-Toutanvoa-Manning",
            "title": {
                "fragments": [],
                "text": "Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "This paper presents results for a maximum-entropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging by incorporating these features: more extensive treatment of capitalization for unknown words, and features for the disambiguation of the tense forms of verbs."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35551590"
                        ],
                        "name": "Steven P. Abney",
                        "slug": "Steven-P.-Abney",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Abney",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven P. Abney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 18
                            }
                        ],
                        "text": "As is well known (Abney, 1997), DAG-like dependencies cannot in general be modeled with a generative approach of the kind taken here3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5361885,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61dffff2116f3543e71d536a18308fa4fc5e53c3",
            "isKey": false,
            "numCitedBy": 236,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Probabilistic analogues of regular and context-free grammars are well known in computational linguistics, and currently the subject of intensive research. To date, however, no satisfactory probabilistic analogue of attribute-value grammars has been proposed: previous attempts have failed to define an adequate parameter-estimation algorithm.In the present paper, I define stochastic attribute-value grammars and give an algorithm for computing the maximum-likelihood estimate of their parameters. The estimation algorithm is adapted from Della Pietra, Della Pietra, and Lafferty (1995). To estimate model parameters, it is necessary to compute the expectations of certain functions under random fields. In the application discussed by Della Pietra, Della Pietra, and Lafferty (representing English orthographic constraints), Gibbs sampling can be used to estimate the needed expectations. The fact that attribute-value grammars generate constrained languages makes Gibbs sampling inapplicable, but I show that sampling can be done using the more general Metropolis-Hastings algorithm."
            },
            "slug": "Stochastic-Attribute-Value-Grammars-Abney",
            "title": {
                "fragments": [],
                "text": "Stochastic Attribute-Value Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Stochastic attribute-value grammars are defined and an algorithm for computing the maximum-likelihood estimate of their parameters is given and it is shown that sampling can be done using the more general Metropolis-Hastings algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 85
                            }
                        ],
                        "text": "We therefore ran the\ndependency model on a test corpus tagged with the POS-tagger of Ratnaparkhi (1996), which is trained on the original Penn Treebank (seeHWDep (+ tagger) in Table 3)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5914287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a574e320d899e7e82e341eb64baef7dfe8a24642",
            "isKey": false,
            "numCitedBy": 1545,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a statistical model which trains from a corpus annotated with Part Of Speech tags and assigns them to previously unseen text with state of the art accuracy The model can be classi ed as a Maximum Entropy model and simultaneously uses many contextual features to predict the POS tag Furthermore this paper demonstrates the use of specialized fea tures to model di cult tagging decisions discusses the corpus consistency problems discovered during the implementation of these features and proposes a training strategy that mitigates these problems"
            },
            "slug": "A-Maximum-Entropy-Model-for-Part-Of-Speech-Tagging-Ratnaparkhi",
            "title": {
                "fragments": [],
                "text": "A Maximum Entropy Model for Part-Of-Speech Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A statistical model which trains from a corpus annotated with Part Of Speech tags and assigns them to previously unseen text with state of the art accuracy and discusses the corpus consistency problems discovered during the implementation of these features."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 82
                            }
                        ],
                        "text": "Therefore, we also evaluate performance using a dependency evaluation reported by Collins (1999), which counts wordword dependencies as determined by local trees and their labels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 130
                            }
                        ],
                        "text": "Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 73
                            }
                        ],
                        "text": "Gildea (2001) shows that removing the lexical dependencies in Model 1 of Collins (1997) (that is,"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 39
                            }
                        ],
                        "text": "9%, comparable to the figures given by Collins (1999) for a linguistically less expressive grammar."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 32
                            }
                        ],
                        "text": "2We compute\u03bb in the same way as Collins (1999), p."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 153
                            }
                        ],
                        "text": "Distance measures for CCG Our distance measures are related to those proposed by Goodman (1997), which are appropriate for binary trees (unlike those of Collins (1997))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 39
                            }
                        ],
                        "text": "9%, comparable to the figures given by Collins (1999) for a linguistically less expressive grammar. In contrast to Gildea (2001), we find a significant improvement from modeling wordword dependencies."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 73
                            }
                        ],
                        "text": "Gildea (2001) shows that removing the lexical dependencies in Model 1 of Collins (1997) (that is, not conditioning onwh when generatingws) decreases labeled precision and recall by only 0.5%."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 98
                            }
                        ],
                        "text": "However, our model does not yet contain anything like the hard constraint on punctuation marks in Collins (1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 5
                            }
                        ],
                        "text": "Like Collins (1999), we assume that the test data is POStagged, and can therefore replace unknown words in the test data with their POS-tag, which is more appropriate for a formalism like CCG with a large set of lexical categories than one generic token for all unknown words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 559,
                                "start": 82
                            }
                        ],
                        "text": "Therefore, we also evaluate performance using a dependency evaluation reported by Collins (1999), which counts wordword dependencies as determined by local trees and their labels. According to this metric, a local tree with parent nodeP, head daughter H and non-head daughterS (and position ofS relative toP, ie. left or right, which is implicit in CCG categories) defines ahP;H;Si dependency between the head word of S, wS, and the head word of H, wH. This measure is neutral with respect to the branching factor. Furthermore, as noted by Hockenmaier (2001), it does not penalize equivalent analyses of multiple modi-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Three Generative Lexicalized Model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 131
                            }
                        ],
                        "text": "CCGbank is a corpus of CCG normal-form derivations obtained by translating the Penn Treebank trees using an algorithm described by Hockenmaier and Steedman (2002). Almost all types of construction\u2014with the exception of gapping andUCP (\u201cUnlike Coordinate Phrases\u201d) are covered by the translation procedure, which processes 98."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 15
                            }
                        ],
                        "text": "In contrast to Gildea (2001), we find a significant improvement from modeling wordword dependencies."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Gildea (2001) shows that removing the lexical dependencies in Model 1 of Collins (1997) (that is, not conditioning onwh when generatingws) decreases labeled precision and recall by only 0.5%."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Corpus Variation and Parser Performance.EMNLP"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2579894"
                        ],
                        "name": "B. Carpenter",
                        "slug": "B.-Carpenter",
                        "structuredName": {
                            "firstName": "Bob",
                            "lastName": "Carpenter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Carpenter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 92
                            }
                        ],
                        "text": "The grammar contains a set of type-changing rules similar to the lexical rules described in Carpenter (1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 54149055,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "73ef8e49d79d158d47da1904aaaa8d38c85847d8",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Categorial-Grammars,-Lexical-Rules-and-the-English-Carpenter",
            "title": {
                "fragments": [],
                "text": "Categorial Grammars, Lexical Rules and the English Predicative"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 19
                            }
                        ],
                        "text": "Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 81
                            }
                        ],
                        "text": "Distance measures for CCG Our distance measures are related to those proposed by Goodman (1997), which are appropriate for binary trees (unlike those of Collins (1997))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic Feature Grammars. IWPT"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 85
                            }
                        ],
                        "text": "We therefore ran the\ndependency model on a test corpus tagged with the POS-tagger of Ratnaparkhi (1996), which is trained on the original Penn Treebank (seeHWDep (+ tagger) in Table 3)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 64
                            }
                        ],
                        "text": "dependency model on a test corpus tagged with the POS-tagger of Ratnaparkhi (1996), which is trained on the original Penn Treebank (see HWDep (+ tagger) in Table 3)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Maximum Entropy Part-OfSpeech Tagger"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 9,
            "methodology": 15,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 20,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Generative-Models-for-Statistical-Parsing-with-Hockenmaier-Steedman/976c95f69e8ee160868b1d54d477f56212ee794b?sort=total-citations"
}