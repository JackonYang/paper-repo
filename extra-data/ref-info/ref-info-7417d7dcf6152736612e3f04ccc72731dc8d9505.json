{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145266088"
                        ],
                        "name": "T. Leung",
                        "slug": "T.-Leung",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Leung",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Leung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the previous section,the number of models was the same as the number of training images (and in effect [ 13 ] used 20 models/images for every texture)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In all three experiments we follow both [2] and [ 13 ],and learn the texton dictionary from 20 textures (using the procedure outlined before in subsection 2.3)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We present results on the Columbia-Utrecht database [6],the same database used by [2, 13 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The second experiment,where 40 textures are classified, is modeled on the setup of Leung and Malik [ 13 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This is a considerable difference in approach to that of [ 13 ] to which we compare our results,and it is worth elaborating on the difference at this point."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The sets include those used by Schmid [18] and Leung and Malik [ 13 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Therefore,classifying materials by their textural appearance in single images photographed under unknown viewing and illumination conditions is still quite an outstanding problem,though significant progress has been made recently [2,3,9,12, 13 ,17,20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Here we present a texture classification method with superior performance to [ 13 ],but requiring only a single image as input and with no information (implicit or explicit) about the illumination and viewing conditions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The particular textures used are specified in figure 7 of [ 13 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For example,it would not be possible to use this approach with the 48 dimensional filter space used by [ 13 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "These results compare very favourably with those reported in [2] and [ 13 ]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We have demonstrated that with a handful of models per texture,classification rates superior to [2, 13 ] can be achieved."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In the second experiment,the 40 textures specified in figure 7 of [ 13 ] are chosen."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular,Leung and Malik [ 13 ] made an important innovation in giving an operational definition of a texton."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14915716,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90d6e7f2202f754d8588f9536e3f5b4a24701f24",
            "isKey": true,
            "numCitedBy": 1713,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the recognition of surfaces made from different materials such as concrete, rug, marble, or leather on the basis of their textural appearance. Such natural textures arise from spatial variation of two surface attributes: (1) reflectance and (2) surface normal. In this paper, we provide a unified model to address both these aspects of natural texture. The main idea is to construct a vocabulary of prototype tiny surface patches with associated local geometric and photometric properties. We call these 3D textons. Examples might be ridges, grooves, spots or stripes or combinations thereof. Associated with each texton is an appearance vector, which characterizes the local irradiance distribution, represented as a set of linear Gaussian derivative filter outputs, under different lighting and viewing conditions.Given a large collection of images of different materials, a clustering approach is used to acquire a small (on the order of 100) 3D texton vocabulary. Given a few (1 to 4) images of any material, it can be characterized using these textons. We demonstrate the application of this representation for recognition of the material viewed under novel lighting and viewing conditions. We also illustrate how the 3D texton model can be used to predict the appearance of materials under novel conditions."
            },
            "slug": "Representing-and-Recognizing-the-Visual-Appearance-Leung-Malik",
            "title": {
                "fragments": [],
                "text": "Representing and Recognizing the Visual Appearance of Materials using Three-dimensional Textons"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A unified model to construct a vocabulary of prototype tiny surface patches with associated local geometric and photometric properties, represented as a set of linear Gaussian derivative filter outputs, under different lighting and viewing conditions is provided."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2672681"
                        ],
                        "name": "G. O. Cula",
                        "slug": "G.-O.-Cula",
                        "structuredName": {
                            "firstName": "Gabriela",
                            "lastName": "Cula",
                            "middleNames": [
                                "Oana"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. O. Cula"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710772"
                        ],
                        "name": "Kristin J. Dana",
                        "slug": "Kristin-J.-Dana",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Dana",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristin J. Dana"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These results compare very favourably with those reported in [ 2 ] and [13]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We present results on the Columbia-Utrecht database [6],the same database used by [ 2 ,13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We have demonstrated that with a handful of models per texture,classification rates superior to [ 2 ,13] can be achieved."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Dana [ 2 ,3]. Schmid\u2019s approach is rotationally invariant but the invariance is achieved in a different manner and texton clustering is in a higher dimensional space than here."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The first experiment, where we classify images from 20 textures,corresponds to the setup employed by Cula and Dana [ 2 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In all three experiments we follow both [ 2 ] and [13],and learn the texton dictionary from 20 textures (using the procedure outlined before in subsection 2.3)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "rate of roughly 87% (see figure 6 and table 2 in [ 2 ])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Therefore,classifying materials by their textural appearance in single images photographed under unknown viewing and illumination conditions is still quite an outstanding problem,though significant progress has been made recently [ 2 ,3,9,12,13,17,20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In the first experiment,20 novel textures are chosen (see figure 5 in [ 2 ] for a list of the novel textures) and 20 \u00d7 46 = 920 novel images are classified in all."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7356365,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b45dc2f11ed201f192d9bec153fcca1ca95e460",
            "isKey": true,
            "numCitedBy": 196,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A bidirectional texture function (BTF) describes image texture as it varies with viewing and illumination direction. Many real world surfaces such as skin, fur, gravel, etc. exhibit fine-scale geometric surface detail. Accordingly, variations in appearance with viewing and illumination direction may be quite complex due to local foreshortening, masking and shadowing. Representations of surface texture that support robust recognition must account for these effects. We construct a representation which captures the underlying statistical distribution of features in the image texture as well as the variations in this distribution with viewing and illumination direction. The representation combines clustering to learn characteristic image features and principle components analysis to reduce the space of feature histograms. This representation is based on a core image set as determined by a quantitative evaluation of importance of individual images in the overall representation. The result is a compact representation and a recognition method where a single novel image of unknown viewing and illumination direction can be classified efficiently. The CUReT (Columbia-Utrecht reflectance and texture) database is used as a test set for evaluation of these methods."
            },
            "slug": "Compact-representation-of-bidirectional-texture-Cula-Dana",
            "title": {
                "fragments": [],
                "text": "Compact representation of bidirectional texture functions"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A representation is constructed which captures the underlying statistical distribution of features in the image texture as well as the variations in this distribution with viewing and illumination direction and is a compact representation and a recognition method where a single novel image of unknown viewing and illuminated direction can be classified efficiently."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145266088"
                        ],
                        "name": "T. Leung",
                        "slug": "T.-Leung",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Leung",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Leung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2166325,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "191c3c15cc4c957ee3437fc27ba3178bae292e7f",
            "isKey": false,
            "numCitedBy": 257,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the recognition of surfaces made from different materials such as concrete, rug, marble or leather on the basis of their textural appearance. Such natural textures arise from spatial variation of two surface attributes: (1) reflectance and (2) surface normal. In this paper, we provide a unified model to address both these aspects of natural texture. The main idea is to construct a vocabulary of prototype tiny surface patches with associated local geometric and photometric properties. We call these 3D textons. Examples might be ridges, grooves, spots or stripes or combinations thereof Associated with each texton is an appearance vector, which characterizes the local irradiance distribution, represented as a set of linear Gaussian derivative filter outputs, under different lighting and viewing conditions. Given a large collection of images of different materials, a clustering approach is used to acquire a small (on the order of 100) 3D texton vocabulary. Given a few (1 to 4) images of any material, it can be characterized using these textons. We demonstrate the application of this representation for recognition of the material viewed under novel lighting and viewing conditions."
            },
            "slug": "Recognizing-surfaces-using-three-dimensional-Leung-Malik",
            "title": {
                "fragments": [],
                "text": "Recognizing surfaces using three-dimensional textons"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A unified model to address both the reflectance and surface normal aspects of natural texture and to construct a vocabulary of prototype tiny surface patches with associated local geometric and photometric properties is provided."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49544028"
                        ],
                        "name": "Xinguo Liu",
                        "slug": "Xinguo-Liu",
                        "structuredName": {
                            "firstName": "Xinguo",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinguo Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1841911"
                        ],
                        "name": "Yizhou Yu",
                        "slug": "Yizhou-Yu",
                        "structuredName": {
                            "firstName": "Yizhou",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yizhou Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144154486"
                        ],
                        "name": "H. Shum",
                        "slug": "H.-Shum",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Shum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Shum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The need to seriously address \u201c3D effects\u201d \u2013 where the appearance varies considerably with viewpoint and lighting \u2013 is illustrated in figure 1. The importance of such effects for classification [1,4,5] and synthesis [ 15 ,19] has also been noted by other researchers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5320148,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9757d66da0c62c506e5ded914ce7c8fd7fbce070",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a novel approach to synthetically generating bidirectional texture functions (BTFs) of real-world surfaces. Unlike a conventional two-dimensional texture, a BTF is a six-dimensional function that describes the appearance of texture as a function of illumination and viewing directions. The BTF captures the appearance change caused by visible small-scale geometric details on surfaces. From a sparse set of images under different viewing/lighting settings, our approach generates BTFs in three steps. First, it recovers approximate 3D geometry of surface details using a shape-from-shading method. Then, it generates a novel version of the geometric details that has the same statistical properties as the sample surface with a non-parametric sampling method. Finally, it employs an appearance preserving procedure to synthesize novel images for the recovered or generated geometric details under various viewing/lighting settings, which then define a BTF. Our experimental results demonstrate the effectiveness of our approach."
            },
            "slug": "Synthesizing-bidirectional-texture-functions-for-Liu-Yu",
            "title": {
                "fragments": [],
                "text": "Synthesizing bidirectional texture functions for real-world surfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A novel approach to synthetically generating bidirectional texture functions (BTFs) of real-world surfaces by employing an appearance preserving procedure to synthesize novel images for the recovered or generated geometric details under various viewing/lighting settings."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2672681"
                        ],
                        "name": "G. O. Cula",
                        "slug": "G.-O.-Cula",
                        "structuredName": {
                            "firstName": "Gabriela",
                            "lastName": "Cula",
                            "middleNames": [
                                "Oana"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. O. Cula"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710772"
                        ],
                        "name": "Kristin J. Dana",
                        "slug": "Kristin-J.-Dana",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Dana",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristin J. Dana"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Therefore, classifying materials by their textural appearance in single images photographed under unknown viewing and illumination conditions is still quite an outstanding problem, though significant progress has been made recently [2,3,9,12,13,17,20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Our approach is most closely related to that of Schmid [18] and Cula and Dana [2,3]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 14911986,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9005468d00d217d52e30f5c39cb6acaa62a1e9a",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Texture as a surface representation is the subject of a wide body of computer vision and computer graphics literature. While texture is always associated with a form of repetition in the image, the repeating quantity may vary. The texture may be a color or albedo variation as in a checkerboard, a paisley print or zebra stripes. Very often in real-world scenes, texture is instead due to a surface height variation, e.g. pebbles, gravel, foliage and any rough surface. Such surfaces are referred to here as 3D textured surfaces. Standard texture recognition algorithms are not appropriate for 3D textured surfaces because the appearance of these surfaces changes in a complex manner with viewing direction and illumination direction. Recent methods have been developed for recognition of 3D textured surfaces using a database of surfaces observed under varied imaging parameters. One of these methods is based on 3D textons obtained using K-means clustering of multiscale feature vectors. Another method uses eigen-analysis originally developed for appearance-based object recognition. In this work we develop a hybrid approach that employs both feature grouping and dimensionality reduction. The method is tested using the Columbia-Utrecht texture database and provides excellent recognition rates. The method is compared with existing recognition methods for 3D textured surfaces. A direct comparison is facilitated by empirical recognition rates from the same texture data set. The current method has key advantages over existing methods including requiring less prior information on both the training and novel images."
            },
            "slug": "Recognition-methods-for-3D-textured-surfaces-Cula-Dana",
            "title": {
                "fragments": [],
                "text": "Recognition methods for 3D textured surfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A hybrid approach that employs both feature grouping and dimensionality reduction is developed that has key advantages over existing methods including requiring less prior information on both the training and novel images."
            },
            "venue": {
                "fragments": [],
                "text": "IS&T/SPIE Electronic Imaging"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710772"
                        ],
                        "name": "Kristin J. Dana",
                        "slug": "Kristin-J.-Dana",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Dana",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristin J. Dana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8038506"
                        ],
                        "name": "B. Ginneken",
                        "slug": "B.-Ginneken",
                        "structuredName": {
                            "firstName": "Bram",
                            "lastName": "Ginneken",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ginneken"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750470"
                        ],
                        "name": "S. Nayar",
                        "slug": "S.-Nayar",
                        "structuredName": {
                            "firstName": "Shree",
                            "lastName": "Nayar",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nayar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716904"
                        ],
                        "name": "J. Koenderink",
                        "slug": "J.-Koenderink",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Koenderink",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Koenderink"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We present results on the Columbia-Utrecht database [6], the same database used by [2,13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 622815,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "16ba88cb3c3a0438bd9e5ace9096f9655ddc63df",
            "isKey": false,
            "numCitedBy": 1074,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we investigate the visual appearance of real-world surfaces and the dependence of appearance on imaging conditions. We present a BRDF (bidirectional reflectance distribution function) database with reflectance measurements for over 60 different samples, each observed with over 200 different combinations of viewing and source directions. We fit the BRDF measurements to two recent models to obtain a BRDF parameter database. These BRDF parameters can be directly used for both image analysis and image synthesis. Finally, we present a BTF (bidirectional texture function) database with image textures from over 60 different samples, each observed with over 200 different combinations of viewing and source directions. Each of these unique databases has important implications for a variety of vision algorithms and each is made publicly available."
            },
            "slug": "Reflectance-and-texture-of-real-world-surfaces-Dana-Ginneken",
            "title": {
                "fragments": [],
                "text": "Reflectance and texture of real-world surfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "The visual appearance of real-world surfaces and the dependence of appearance on imaging conditions is investigated and a BRDF (bidirectional reflectance distribution function) database with reflectance measurements for over 60 different samples, each observed with over 200 different combinations of viewing and source directions is presented."
            },
            "venue": {
                "fragments": [],
                "text": "TOGS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775242"
                        ],
                        "name": "Frederik Schaffalitzky",
                        "slug": "Frederik-Schaffalitzky",
                        "structuredName": {
                            "firstName": "Frederik",
                            "lastName": "Schaffalitzky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frederik Schaffalitzky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Therefore,classifying materials by their textural appearance in single images photographed under unknown viewing and illumination conditions is still quite an outstanding problem,though significant progress has been made recently [2,3,9,12,13, 17 ,20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In [ 17 ] it was demonstrated that,provided a texture has sufficient directional variation,it can be pose normalized by maximizing weak isotropy of the second moment gradient matrix (a method originally suggested in [14])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2964260,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aac66ac5e90cc4c187a5aa063b522e5193ef8834",
            "isKey": false,
            "numCitedBy": 236,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe and demonstrate a texture region descriptor which is invariant to affine geometric and photometric transformations, and insensitive to the shape of the texture region. It is applicable to texture patches which are locally planar and have stationary statistics. The novelty of the descriptor is that it is based on statistics aggregated over the region, resulting in richer and more stable descriptors than those computed at a point. Two texture matching applications of this descriptor are demonstrated: (1) it is used to automatically identify, regions of the same type of texture, but with varying surface pose, within a single image; (2) it is used to support wide baseline stereo, i.e. to enable the automatic computation of the epipolar geometry between two images acquired from quite separated viewpoints. Results are presented on several sets of real images."
            },
            "slug": "Viewpoint-invariant-texture-matching-and-wide-Schaffalitzky-Zisserman",
            "title": {
                "fragments": [],
                "text": "Viewpoint invariant texture matching and wide baseline stereo"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A texture region descriptor is described and demonstrated which is invariant to affine geometric and photometric transformations, and insensitive to the shape of the texture region, resulting in richer and more stable descriptors than those computed at a point."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710772"
                        ],
                        "name": "Kristin J. Dana",
                        "slug": "Kristin-J.-Dana",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Dana",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristin J. Dana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750470"
                        ],
                        "name": "S. Nayar",
                        "slug": "S.-Nayar",
                        "structuredName": {
                            "firstName": "Shree",
                            "lastName": "Nayar",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nayar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The need to seriously address \u201c3D effects\u201d \u2013 where the appearance varies considerably with viewpoint and lighting \u2013 is illustrated in figure 1. The importance of such effects for classification [1, 4 ,5] and synthesis [15,19] has also been noted by other researchers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1744493,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd2ef7dbd1dbe42e2dfeefa6b03f3b689a8f3f08",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Image texture can arise not only from surface albedo variations (2D texture) but also from surface height variations (3D texture). Since the appearance of 3D texture depends on the illumination and viewing direction in a complicated manner, such image texture can be called a bidirectional texture function. A fundamental representation of image texture is the histogram of pixel intensities. Since the histogram of 3D texture also depends on the illumination and viewing directions in a complex fashion, we refer to it as a bidirectional histogram. In this work, we present a concise analytical model for the bidirectional histogram of Lambertian, isotropic, randomly rough surfaces, which are common in real-world scenes. We demonstrate the accuracy of the histogram model by fitting to several samples from the Columbia-Utrecht texture database. The parameters obtained from the model fits are roughness measures which can be used in texture recognition schemes. In addition, the model has potential application in estimating illumination direction in scenes where surfaces of known tilt and roughness are visible. We demonstrate the usefulness of our model by employing it in a novel 3D texture synthesis procedure."
            },
            "slug": "Histogram-model-for-3D-textures-Dana-Nayar",
            "title": {
                "fragments": [],
                "text": "Histogram model for 3D textures"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents a concise analytical model for the bidirectional histogram of Lambertian, isotropic, randomly rough surfaces, which are common in real-world scenes and demonstrates the usefulness of the model by employing it in a novel 3D texture synthesis procedure."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712844"
                        ],
                        "name": "M. Chantler",
                        "slug": "M.-Chantler",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Chantler",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Chantler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46365515"
                        ],
                        "name": "Jiahua Wu",
                        "slug": "Jiahua-Wu",
                        "structuredName": {
                            "firstName": "Jiahua",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiahua Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15039697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "169b7a5c6597dbf7d2abb470e30467b91de27e2c",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Many image-rotation invariant texture classification approaches have been presented. However, image rotation is not necessarily the same as surface rotation. This paper proposes a novel scheme that is surface-rotation invariant. It uses magnitude spectra of the partial derivatives of the surface obtained using photometric stereo. Unfortunately the partial derivative operator is directional. It is therefore not suited for direct use as a rotation invariant feature. We present a simple frequency domain method of removing the directional artefacts. Polarograms (polar functions of spectra) are extracted from resulting spectra. Classification is performed by comparing training and classification polarograms over a range of rotations (1\u00b0 steps over the range 0\u00b0 to 180\u00b0). Thus the system both classifies the test texture and estimates its orientation relative to the relevant training texture. A proof for the removal of directional artefacts from partial derivative spectra is provided. Results obtained using the classification scheme on synthetic and real textures are presented."
            },
            "slug": "Rotation-Invariant-Classification-of-3D-Surface-and-Chantler-Wu",
            "title": {
                "fragments": [],
                "text": "Rotation Invariant Classification of 3D Surface Textures using Photometric Stereo and Surface Magnitude Spectra"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper proposes a novel scheme that is surface-rotation invariant texture classification scheme that uses magnitude spectra of the partial derivatives of the surface obtained using photometric stereo and presents a simple frequency domain method of removing the directional artefacts."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We also plan to investigate how the co-occurrence of textons (as in [18]) can lead to further improvements."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The sets include those used by Schmid [18] and Leung and Malik [13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The use of such filters has already been espoused by Schmid [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Our approach is most closely related to that of Schmid [18] and Cula and Dana [2,3]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 5927402,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db7159af39b66cde32cec502a17d48436b541d00",
            "isKey": true,
            "numCitedBy": 423,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new method for constructing models from a set of positive and negative sample images; the method requires no manual extraction of significant objects or features. Our model representation is based on two layers. The first one consists of \"generic\" descriptors which represent sets of similar rotational invariant feature vectors. Rotation invariance allows to group similar, but rotated patterns and makes the method robust to model deformations. The second layer is the joint probability on the frequencies of the \"generic\" descriptors over neighborhoods. This probability is multi-modal and is represented by a set of \"spatial-frequency\" clusters. It adds a statistical spatial constraint which is rotationally invariant. Our two-layer representation is novel; it allows to efficiently capture \"texture-like\" visual structure. The selection of distinctive structure determines characteristic model features (common to the positive and rare in the negative examples) and increases the performance of the model. Models are retrieved and localized using a probabilistic score. Experimental results for \"textured\" animals and faces show a very good performance for retrieval as well as localization."
            },
            "slug": "Constructing-models-for-content-based-image-Schmid",
            "title": {
                "fragments": [],
                "text": "Constructing models for content-based image retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A new method for constructing models from a set of positive and negative sample images; the method requires no manual extraction of significant objects or features and allows to efficiently capture \"texture-like\" visual structure."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710772"
                        ],
                        "name": "Kristin J. Dana",
                        "slug": "Kristin-J.-Dana",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Dana",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristin J. Dana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750470"
                        ],
                        "name": "S. Nayar",
                        "slug": "S.-Nayar",
                        "structuredName": {
                            "firstName": "Shree",
                            "lastName": "Nayar",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nayar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The need to seriously address \u201c3D effects\u201d \u2013 where the appearance varies considerably with viewpoint and lighting \u2013 is illustrated in figure 1. The importance of such effects for classification [1,4, 5 ] and synthesis [15,19] has also been noted by other researchers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17056703,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "7cbaf0f7d65686c5829fc08daa940eb8e3d3b21b",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "While an exact definition of texture is somewhat elusive, texture can be qualitatively described as a distribution of color, albedo or local normal on a surface. In the literature, the word texture is often used to describe a color or albedo variation on a smooth surface. We refer to such texture as 2D texture. In real world scenes, texture is often due to surface height variations and can be termed 3D texture. Because of local foreshortening and masking, oblique views of 3D texture are not simple transformations of the frontal view. Consequently, texture representations such as the correlation function or power spectrum are also affected by local foreshortening and masking. This work presents a correlation model for a particular class of 3D textures. The model characterizes the spatial relationship among neighboring pixels in an image of 3D texture and the change of this spatial relationship with viewing direction."
            },
            "slug": "Correlation-model-for-3D-texture-Dana-Nayar",
            "title": {
                "fragments": [],
                "text": "Correlation model for 3D texture"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work presents a correlation model for a particular class of 3D textures that characterizes the spatial relationship among neighboring pixels in an image of3D texture and the change of this spatial relationship with viewing direction."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39092098"
                        ],
                        "name": "Y. Wu",
                        "slug": "Y.-Wu",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Wu",
                            "middleNames": [
                                "Nian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117481816"
                        ],
                        "name": "D. Mumford",
                        "slug": "D.-Mumford",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mumford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mumford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Therefore, classifying materials by their textural appearance in single images photographed under unknown viewing and illumination conditions is still quite an outstanding problem, though significant progress has been made recently [2,3,9,12,13,17,20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2171181,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a5f775f7490f410692bb224ff021da97d5e0ddc",
            "isKey": false,
            "numCitedBy": 583,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents a statistical theory for texture modeling. This theory combines filtering theory and Markov random field modeling through the maximum entropy principle, and interprets and clarifies many previous concepts and methods for texture analysis and synthesis from a unified point of view. Our theory characterizes the ensemble of images I with the same texture appearance by a probability distribution f(I) on a random field, and the objective of texture modeling is to make inference about f(I), given a set of observed texture examples.In our theory, texture modeling consists of two steps. (1) A set of filters is selected from a general filter bank to capture features of the texture, these filters are applied to observed texture images, and the histograms of the filtered images are extracted. These histograms are estimates of the marginal distributions of f( I). This step is called feature extraction. (2) The maximum entropy principle is employed to derive a distribution p(I), which is restricted to have the same marginal distributions as those in (1). This p(I) is considered as an estimate of f( I). This step is called feature fusion. A stepwise algorithm is proposed to choose filters from a general filter bank. The resulting model, called FRAME (Filters, Random fields And Maximum Entropy), is a Markov random field (MRF) model, but with a much enriched vocabulary and hence much stronger descriptive ability than the previous MRF models used for texture modeling. Gibbs sampler is adopted to synthesize texture images by drawing typical samples from p(I), thus the model is verified by seeing whether the synthesized texture images have similar visual appearances to the texture images being modeled. Experiments on a variety of 1D and 2D textures are described to illustrate our theory and to show the performance of our algorithms. These experiments demonstrate that many textures which are previously considered as from different categories can be modeled and synthesized in a common framework."
            },
            "slug": "Filters,-Random-Fields-and-Maximum-Entropy-(FRAME):-Zhu-Wu",
            "title": {
                "fragments": [],
                "text": "Filters, Random Fields and Maximum Entropy (FRAME): Towards a Unified Theory for Texture Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The resulting model, called FRAME (Filters, Random fields And Maximum Entropy), is a Markov random field (MRF) model, but with a much enriched vocabulary and hence much stronger descriptive ability than the previous MRF models used for texture modeling."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2841773"
                        ],
                        "name": "S. Konishi",
                        "slug": "S.-Konishi",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Konishi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Konishi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We would like to use the entire filter response of a texture for classification (in the manner of [ 12 ]),rather than its vector quantization into textons."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Their method stores the joint PDF of filter responses (in suitably sized bins),and is completely infeasible if the dimension of the filter space is large \u2013 [ 12 ] use a six dimensional space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Therefore,classifying materials by their textural appearance in single images photographed under unknown viewing and illumination conditions is still quite an outstanding problem,though significant progress has been made recently [2,3,9, 12 ,13,17,20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In a similar spirit to [11, 12 ] we have chosen the scale of our filters as that which gives the strongest response to the textures in the database,and only use a single scale for each filter (we return to the issue of scale below)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "By developing the MR set,we are now in a position where we will be able to compare the \u201ctexton clustering and distribution\u201d method of discriminating textures with the Bayesian approach proposed by Konishi and Yuille [ 12 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14777835,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "9083e6709b52cba61b8a6c9d2628c489b5e2c1db",
            "isKey": true,
            "numCitedBy": 97,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates the use of colour and texture cues for segmentation of images within two specified domains. The first is the Sowerby dataset, which contains one hundred colour photographs of country roads in England that have been interactively segmented and classified into six classes-edge, vegetation, air, road, building, and other. The second domain is a set of thirty five-images, taken in San Francisco, which have been interactively segmented into similar classes. In each domain we learn the joint probability distributions of filter responses, based on colour and texture, for each class. These distributions are then used for classification. We restrict ourselves to a limited number of filters in order to ensure that the learnt filter responses do not overfit the training data (our region classes are chosen so as to ensure that there is enough data to avoid over fitting). We do performance analysis on the two datasets by evaluating the false positive and false negative error rates for the classification. This shows that the learnt models achieve high accuracy in classifying individual pixels into those classes for which the filter responses are approximately spatially homogeneous (i.e. road, vegetation, and air but not edge and building). A more sensitive performance measure, the Chernoff information, is calculated in order to quantify how well the cues for edge and building are doing. This demonstrates that statistical knowledge of the domain is a powerful tool for segmentation."
            },
            "slug": "Statistical-cues-for-domain-specific-image-with-Konishi-Yuille",
            "title": {
                "fragments": [],
                "text": "Statistical cues for domain specific image segmentation with performance analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Performance analysis on the Sowerby dataset shows that the learnt models achieve high accuracy in classifying individual pixels into those classes for which the filter responses are approximately spatially homogeneous."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3205375"
                        ],
                        "name": "T. Lindeberg",
                        "slug": "T.-Lindeberg",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Lindeberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lindeberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3260234"
                        ],
                        "name": "J. G\u00e5rding",
                        "slug": "J.-G\u00e5rding",
                        "structuredName": {
                            "firstName": "Jonas",
                            "lastName": "G\u00e5rding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. G\u00e5rding"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In [17] it was demonstrated that,provided a texture has sufficient directional variation,it can be pose normalized by maximizing weak isotropy of the second moment gradient matrix (a method originally suggested in [ 14 ])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7210200,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "aacb988081214bfd87c9ae06310453d01119e326",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Rotationally symmetric operations in the image domain may give rise to shape distortions. This article describes a way of reducing this effect for a general class of methods for deriving 3-D shape cues from 2-D image data, which are based on the estimation of locally linearized distortion of brightness patterns. By extending the linear scale-space concept into an affine scale-space representation and performing affine shape adaption of the smoothing kernels, the accuracy of surface orientation estimates derived from texture and disparity cues can be improved by typically one order of magnitude. The reason for this is that the image descriptors, on which the methods are based, will be relative invariant under affine transformations, and the error will thus be confined to the higher-order terms in the locally linearized perspective mapping."
            },
            "slug": "Shape-Adapted-Smoothing-in-Estimation-of-3-D-Depth-Lindeberg-G\u00e5rding",
            "title": {
                "fragments": [],
                "text": "Shape-Adapted Smoothing in Estimation of 3-D Depth Cues from Affine Distortions of Local 2-D Brightness Structure"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "By extending the linear scale-space concept into an affine scale- space representation and performing affine shape adaption of the smoothing kernels, the accuracy of surface orientation estimates derived from texture and disparity cues can be improved by typically one order of magnitude."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3266057"
                        ],
                        "name": "A. Zalesny",
                        "slug": "A.-Zalesny",
                        "structuredName": {
                            "firstName": "Alexey",
                            "lastName": "Zalesny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zalesny"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The importance of such effects for classification [1,4,5] and synthesis [15,19] has also been noted by other researchers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2423738,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2fbd5b1a43d87ef142b59ce3a2028e4a19ed33e",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "A texture synthesis method is presented that generates similar texture from an example image. It is based on the emulation of simple but rather carefully chosen image intensity statistics. The resulting texture models are compact and no longer require the example image from which they were derived. They make explicit some structural aspects of the textures and the modeling allows knitting together different textures with convincingly looking transition zones. As textures are seldom flat, it is important to also model 3D effects when textures change under changing viewpoint. The simulation of such changes is supported by the model, assuming examples for the different viewpoints are given."
            },
            "slug": "A-Compact-Model-for-Viewpoint-Dependent-Texture-Zalesny-Gool",
            "title": {
                "fragments": [],
                "text": "A Compact Model for Viewpoint Dependent Texture Synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "A texture synthesis method is presented that generates similar texture from an example image based on the emulation of simple but rather carefully chosen image intensity statistics."
            },
            "venue": {
                "fragments": [],
                "text": "SMILE"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31983980"
                        ],
                        "name": "G. M. Haley",
                        "slug": "G.-M.-Haley",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Haley",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. M. Haley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50591689"
                        ],
                        "name": "B. S. Manjunath",
                        "slug": "B.-S.-Manjunath",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Manjunath",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. S. Manjunath"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Therefore,classifying materials by their textural appearance in single images photographed under unknown viewing and illumination conditions is still quite an outstanding problem,though significant progress has been made recently [2,3, 9 ,12,13,17,20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9784645,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0a41a866ddd70555b629bede2317edb840d481c",
            "isKey": false,
            "numCitedBy": 252,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "A method of rotation-invariant texture classification based on a complete space-frequency model is introduced. A polar, analytic form of a two-dimensional (2-D) Gabor wavelet is developed, and a multiresolution family of these wavelets is used to compute information-conserving microfeatures. From these microfeatures a micromodel, which characterizes spatially localized amplitude, frequency, and directional behavior of the texture, is formed. The essential characteristics of a texture sample, its macrofeatures, are derived from the estimated selected parameters of the micromodel. Classification of texture samples is based on the macromodel derived from a rotation invariant subset of macrofeatures. In experiments, comparatively high correct classification rates were obtained using large sample sets."
            },
            "slug": "Rotation-invariant-texture-classification-using-a-Haley-Manjunath",
            "title": {
                "fragments": [],
                "text": "Rotation-invariant texture classification using a complete space-frequency model"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A method of rotation-invariant texture classification based on a complete space-frequency model is introduced, and the essential characteristics of a texture sample, its macrofeatures, are derived from the estimated selected parameters of the micromodel."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2841773"
                        ],
                        "name": "S. Konishi",
                        "slug": "S.-Konishi",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Konishi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Konishi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8436115"
                        ],
                        "name": "J. Coughlan",
                        "slug": "J.-Coughlan",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Coughlan",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Coughlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In a similar spirit to [11,12] we have chosen the scale of our filters as that which gives the strongest response to the textures in the database, and only use a single scale for each filter (we return to the issue of scale below)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5565516,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5f2ca6cbca6ff1cbcec8a1a7db70663a0d72514",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We treat the problem of edge detection as one of statistical inference. Local edge cues, implemented by filters, provide information about the likely positions of edges which can be used as input to higher-level models. Different edge cues can be evaluated by the statistical effectiveness of their corresponding filters evaluated on a dataset of 100 presegmented images. We use information theoretic measures to determine the effectiveness of a variety of different edge detectors working at multiple scales on black and white and color images. Our results give quantitative measures for the advantages of multi-level processing, for the use of chromaticity in addition to greyscale, and for the relative effectiveness of different detectors."
            },
            "slug": "Fundamental-bounds-on-edge-detection:-an-theoretic-Konishi-Yuille",
            "title": {
                "fragments": [],
                "text": "Fundamental bounds on edge detection: an information theoretic evaluation of different edge cues"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work uses information theoretic measures to determine the effectiveness of a variety of different edge detectors working at multiple scales on black and white and color images and gives quantitative measures for the advantages of multi-level processing, for the use of chromaticity in addition to greyscale, and for the relative effectiveness of different detectors."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16926,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256718"
                        ],
                        "name": "W. Press",
                        "slug": "W.-Press",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Press",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Press"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48590121"
                        ],
                        "name": "S. Teukolsky",
                        "slug": "S.-Teukolsky",
                        "structuredName": {
                            "firstName": "Saul",
                            "lastName": "Teukolsky",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Teukolsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2608024"
                        ],
                        "name": "W. Vetterling",
                        "slug": "W.-Vetterling",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Vetterling",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Vetterling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35046585"
                        ],
                        "name": "B. Flannery",
                        "slug": "B.-Flannery",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Flannery",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Flannery"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The distance function used to define closest is the \u03c72 significance test [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61769312,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "ca2832d2c30287a9ee5b8584cc498d2b1cb14753",
            "isKey": false,
            "numCitedBy": 16689,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Note: Includes bibliographical references, 3 appendixes and 2 indexes.- Diskette v 2.06, 3.5''[1.44M] for IBM PC, PS/2 and compatibles [DOS] Reference Record created on 2004-09-07, modified on 2016-08-08"
            },
            "slug": "Numerical-recipes-in-C-Press-Teukolsky",
            "title": {
                "fragments": [],
                "text": "Numerical recipes in C"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Diskette v 2.06, 3.5''[1.44M] for IBM PC, PS/2 and compatibles [DOS] Reference Record created on 2004-09-07, modified on 2016-08-08."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50718271"
                        ],
                        "name": "L. Kaufman",
                        "slug": "L.-Kaufman",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Kaufman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaufman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2218687"
                        ],
                        "name": "P. Rousseeuw",
                        "slug": "P.-Rousseeuw",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Rousseeuw",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Rousseeuw"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "K-Medoid is a standard clustering algorithm [10] where the update rule always moves the cluster centre to the nearest data point in the cluster, but does not merge the points as in K-means."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60820748,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "965ea5946708f0b669403a377c48e78e9326be61",
            "isKey": false,
            "numCitedBy": 8262,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "An electrical signal transmission system, applicable to the transmission of signals from trackside hot box detector equipment for railroad locomotives and rolling stock, wherein a basic pulse train is transmitted whereof the pulses are of a selected first amplitude and represent a train axle count, and a spike pulse of greater selected amplitude is transmitted, occurring immediately after the axle count pulse to which it relates, whenever an overheated axle box is detected. To enable the signal receiving equipment to determine on which side of a train the overheated box is located, the spike pulses are of two different amplitudes corresponding, respectively, to opposite sides of the train."
            },
            "slug": "Finding-Groups-in-Data:-An-Introduction-to-Cluster-Kaufman-Rousseeuw",
            "title": {
                "fragments": [],
                "text": "Finding Groups in Data: An Introduction to Cluster Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An electrical signal transmission system, applicable to the transmission of signals from trackside hot box detector equipment for railroad locomotives and rolling stock, wherein a basic pulse train is transmitted whereof the pulses are of a selected first amplitude and represent a train axle count."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 12,
            "methodology": 9,
            "result": 5
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 20,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Classifying-Images-of-Materials:-Achieving-and-Varma-Zisserman/7417d7dcf6152736612e3f04ccc72731dc8d9505?sort=total-citations"
}