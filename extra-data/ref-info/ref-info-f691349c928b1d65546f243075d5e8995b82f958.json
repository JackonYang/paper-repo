{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35238678"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 3
                            }
                        ],
                        "text": "In [13, 14] Lowe presents a novel method based on local scale-invariant features detected at interest points."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "differential invariants in [20], and local scale-invariant features in [13])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Alternatively, in [13], after detecting interest points, the image is locally characterized by a set of Scale Invariant Feature Transform (SIFT) features that represents a vector of local image measurements."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "For example, in [13], interest points are represented by local extrema, with respect to both image location and scale, in the responses of difference of filters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5258236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9f836d28f52ad260213d32224a6d227f8e8849a",
            "isKey": true,
            "numCitedBy": 16256,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds."
            },
            "slug": "Object-recognition-from-local-scale-invariant-Lowe",
            "title": {
                "fragments": [],
                "text": "Object recognition from local scale-invariant features"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35238678"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 3
                            }
                        ],
                        "text": "In [13, 14] Lowe presents a novel method based on local scale-invariant features detected at interest points."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 326732,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d5d623643939358bb057e7b48aa7887e358f97e",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "There is considerable evidence that object recognition in primates is based on the detection of local image features of intermediate complexity that are largely invariant to imaging transformations. A computer vision system has been developed that performs object recognition using features with similar properties. Invariance to image translation, scale and rotation is achieved by first selecting stable key points in scale space and performing feature detection only at these locations. The features measure local image gradients in a manner modeled on the response of complex cells in primary visual cortex, and thereby obtain partial invariance to illumination, affine change, and other local distortions. The features are used as input to a nearest-neighbor indexing method and Hough transform that identify candidate object matches. Final verification of each match is achieved by finding a best-fit solution for the unknown model parameters and integrating the features consistent with these parameter values. This verification procedure provides a model for the serial process of attention in human vision that integrates features belonging to a single object. Experimental results show that this approach can achieve rapid and robust object recognition in cluttered partially-occluded images."
            },
            "slug": "Towards-a-Computational-Model-for-Object-in-IT-Lowe",
            "title": {
                "fragments": [],
                "text": "Towards a Computational Model for Object Recognition in IT Cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This verification procedure provides a model for the serial process of attention in human vision that integrates features belonging to a single object that can achieve rapid and robust object recognition in cluttered partially-occluded images."
            },
            "venue": {
                "fragments": [],
                "text": "Biologically Motivated Computer Vision"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793739"
                        ],
                        "name": "David J. Fleet",
                        "slug": "David-J.-Fleet",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fleet",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Fleet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723930"
                        ],
                        "name": "A. Jepson",
                        "slug": "A.-Jepson",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Jepson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Jepson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144968076"
                        ],
                        "name": "M. Jenkin",
                        "slug": "M.-Jenkin",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jenkin",
                            "middleNames": [
                                "R.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jenkin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10758766,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "609034cfbf7c94d69acdff42c4124cfd3d858575",
            "isKey": false,
            "numCitedBy": 342,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Phase-based-disparity-measurement-Fleet-Jepson",
            "title": {
                "fragments": [],
                "text": "Phase-based disparity measurement"
            },
            "venue": {
                "fragments": [],
                "text": "CVGIP Image Underst."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723930"
                        ],
                        "name": "A. Jepson",
                        "slug": "A.-Jepson",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Jepson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Jepson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 211
                            }
                        ],
                        "text": "However they suffer from difficulties such as: 1) illumination changes are hard to be dealt with; 2) pose and position dependence; and 3) partial occlusion and clutter can damage the system performance (but see [1, 11])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 572947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9158048426a7f673bcc513e074e76b0b7e435b7",
            "isKey": false,
            "numCitedBy": 1297,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an approach for tracking rigid and articulated objects using a view-based representation. The approach builds on and extends work on eigenspace representations, robust estimation techniques, and parameterized optical flow estimation. First, we note that the least-squares image reconstruction of standard eigenspace techniques has a number of problems and we reformulate the reconstruction problem as one of robust estimation. Second we define a \u201csubspace constancy assumption\u201d that allows us to exploit techniques for parameterized optical flow estimation to simultaneously solve for the view of an object and the affine transformation between the eigenspace and the image. To account for large affine transformations between the eigenspace and the image we define a multi-scale eigenspace representation and a coarse-to-fine matching strategy. Finally, we use these techniques to track objects over long image sequences in which the objects simultaneously undergo both affine image motions and changes of view. In particular we use this \u201cEigenTracking\u201d technique to track and recognize the gestures of a moving hand."
            },
            "slug": "EigenTracking:-Robust-Matching-and-Tracking-of-a-Black-Jepson",
            "title": {
                "fragments": [],
                "text": "EigenTracking: Robust Matching and Tracking of Articulated Objects Using a View-Based Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A \u201csubspace constancy assumption\u201d is defined that allows techniques for parameterized optical flow estimation to simultaneously solve for the view of an object and the affine transformation between the eigenspace and the image."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793739"
                        ],
                        "name": "David J. Fleet",
                        "slug": "David-J.-Fleet",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fleet",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Fleet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723930"
                        ],
                        "name": "A. Jepson",
                        "slug": "A.-Jepson",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Jepson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Jepson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2247062,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "8488ac7843c819db6f2ab9f2b77b8e35d86a53ec",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper concerns the robustness of phase information for measuring image velocity and binocular disparity, its stability with respect to geometric deformations, and its linearity as a function of spatial position. These properties are shown to depend on the form of the filters used and their frequency bandwidths. The authors also discuss situations in which phase is unstable, many of which can be detected using the model of phase singularities (see Image Vis. Comput. (UK) vol.9, no.5, p.333-7 (Oct. 1991)).<<ETX>>"
            },
            "slug": "Stability-of-phase-information-Fleet-Jepson",
            "title": {
                "fragments": [],
                "text": "Stability of Phase Information"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The robustness of local phase information for measuring image velocity and binocular disparity is addressed, particularly in the stability of phase with respect to geometric deformations, and its linearity as a function of spatial position."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145687022"
                        ],
                        "name": "J. Crowley",
                        "slug": "J.-Crowley",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Crowley",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Crowley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 729473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b634b2001e3fb6159ab15d5375eb4f78213d1eee",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a probabilistic object recognition technique which does not require correspondence matching of images. This technique is an extension of our earlier work (1996) on object recognition using matching of multi-dimensional receptive field histograms. In the earlier paper we have shown that multi-dimensional receptive field histograms can be matched to provide object recognition which is robust in the face of changes in viewing position and independent of image plane rotation and scale. In this paper we extend this method to compute the probability of the presence of an object in an image. The paper begins with a review of the method and previously presented experimental results. We then extend the method for histogram matching to obtain a genuine probability of the presence of an object. We present experimental results on a database of 100 objects showing that the approach is capable recognizing all objects correctly by using only a small portion of the image. Our results show that receptive field histograms provide a technique for object recognition which is robust, has low computational cost and a computational complexity which is linear with the number of pixels."
            },
            "slug": "Probabilistic-object-recognition-using-receptive-Schiele-Crowley",
            "title": {
                "fragments": [],
                "text": "Probabilistic object recognition using multidimensional receptive field histograms"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The method for histogram matching is extended to compute the probability of the presence of an object in an image and shows that receptive field histograms provide a technique for object recognition which is robust, has low computational cost and a computational complexity which is linear with the number of pixels."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 13th International Conference on Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793739"
                        ],
                        "name": "David J. Fleet",
                        "slug": "David-J.-Fleet",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fleet",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Fleet"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "This builds on previous work [3] in which it was shown that the phase information provided by such filters is often locally stable with respect to scale changes, noise, and common brightness changes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 119063696,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "bcee1eb7fed03bc928460e2c4963bf0e011b25c9",
            "isKey": false,
            "numCitedBy": 224,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Foreword A.D. Jepson. Preface. I. Background. 1. Introduction. 2. Time-Varying Image Formation. 3. Image Velocity and Frequency Analysis. 4. Velocity-Specific Representation. 5. Review of Existing Techniques. II: Phase-Based Velocity Measurement. 6. Image Velocity as Local Phase Behavior. 7. Experimental Results. 8. Computing 2-D Velocity. III: On Phase Properties of Band-Pass Signals. 9. Scale-Space Phase Stability. 10. Scale-Space Phase Singularities. 11. Application to Natural Images. IV: Conclusions. 12. Summary and Discussion. Appendices: A. Reflectance Model. B. Proof of and n-D Uncertainty Relation. C. Numerical Interpolation and Differentiation of R (x,t). D. Additional Experiments. E. Approximations to E[Deltaphi] and E[|Deltaphi-E[Deltaphi]|] F. Derivations of z1. G. Density Functions for phix(x) and rhox(x)/rho(x). Bibliography. Index."
            },
            "slug": "Measurement-of-Image-Velocity-Fleet",
            "title": {
                "fragments": [],
                "text": "Measurement of Image Velocity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 180
                            }
                        ],
                        "text": "At each spatial sample point the filters are steered to equally spaced orientations, namely % # * ) 1 1 1 1 (13) Here is the main orientation of the pixel computed as described in [5], except we use the sign of the imaginary response of the filter steered to this orientation to resolve a particular direction (i."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 77
                            }
                        ],
                        "text": "More specifically, we use the steerable quadrature filter pairs described in [5] as follows: Let (10)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29187618,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "993b1083455b5c4d631eaf44f230b061994e75c3",
            "isKey": false,
            "numCitedBy": 3379,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present an efficient architecture to synthesize filters of arbitrary orientations from linear combinations of basis filters, allowing one to adaptively steer a filter to any orientation, and to determine analytically the filter output as a function of orientation. Steerable filters may be designed in quadrature pairs to allow adaptive control over phase as well as orientation. The authors show how to design and steer the filters and present examples of their use in the analysis of orientation and phase, angularly adaptive filtering, edge detection, and shape from shading. One can also build a self-similar steerable pyramid representation. The same concepts can be generalized to the design of 3-D steerable filters. >"
            },
            "slug": "The-Design-and-Use-of-Steerable-Filters-Freeman-Adelson",
            "title": {
                "fragments": [],
                "text": "The Design and Use of Steerable Filters"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present an efficient architecture to synthesize filters of arbitrary orientations from linear combinations of basis filters, allowing one to adaptively steer a filter to any orientation, and to determine analytically the filter output as a function of orientation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732672"
                        ],
                        "name": "A. Leonardis",
                        "slug": "A.-Leonardis",
                        "structuredName": {
                            "firstName": "Ale\u0161",
                            "lastName": "Leonardis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Leonardis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144746444"
                        ],
                        "name": "H. Bischof",
                        "slug": "H.-Bischof",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bischof",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bischof"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 24
                            }
                        ],
                        "text": "View-based methods (see [11, 15, 22]) have avoided this problem since they are capable of learning the object appearance without a user-input model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 211
                            }
                        ],
                        "text": "However they suffer from difficulties such as: 1) illumination changes are hard to be dealt with; 2) pose and position dependence; and 3) partial occlusion and clutter can damage the system performance (but see [1, 11])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11703645,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84603b1fdadbc06dc289f8b28d226d8557104454",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The basic limitations of the current appearance-based matching methods using eigenimages are non-robust estimation of coefficients and inability to cope with problems related to occlusions and segmentation. In this paper we present a new approach which successfully solves these problems. The major novelty of our approach lies in the way how the coefficients of the eigenimages are determined. Instead of computing the coefficients by a projection of the data onto the eigenimages, we extract them by a hypothesize-and-test paradigm using subsets of image points. Competing hypotheses are then subject to a selection procedure based on the Minimum Description Length principle. The approach enables us not only to reject outliers and to deal with occlusions but also to simultaneously use multiple classes of eigenimages."
            },
            "slug": "Dealing-with-occlusions-in-the-eigenspace-approach-Leonardis-Bischof",
            "title": {
                "fragments": [],
                "text": "Dealing with occlusions in the eigenspace approach"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "A new approach to appearance-based matching methods using eigenimages which enables us not only to reject outliers and to deal with occlusions but also to simultaneously use multiple classes of eigen images."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097660"
                        ],
                        "name": "M. Turk",
                        "slug": "M.-Turk",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Turk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Turk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 250
                            }
                        ],
                        "text": "3, we can see common type of image deformations, the true positive rate and the reliability rate for the interest point detector (note: from left to right, image number 2 is extracted from the COIL database [18], and image number 4 is extracted from [21])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 26127529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6f1dfcc44277d4cfd8507284d994c9283dc3a2f",
            "isKey": false,
            "numCitedBy": 14954,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed a near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals. The computational approach taken in this system is motivated by both physiology and information theory, as well as by the practical requirements of near-real-time performance and accuracy. Our approach treats the face recognition problem as an intrinsically two-dimensional (2-D) recognition problem rather than requiring recovery of three-dimensional geometry, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. The system functions by projecting face images onto a feature space that spans the significant variations among known face images. The significant features are known as \"eigenfaces,\" because they are the eigenvectors (principal components) of the set of faces; they do not necessarily correspond to features such as eyes, ears, and noses. The projection operation characterizes an individual face by a weighted sum of the eigenface features, and so to recognize a particular face it is necessary only to compare these weights to those of known individuals. Some particular advantages of our approach are that it provides for the ability to learn and later recognize new faces in an unsupervised manner, and that it is easy to implement using a neural network architecture."
            },
            "slug": "Eigenfaces-for-Recognition-Turk-Pentland",
            "title": {
                "fragments": [],
                "text": "Eigenfaces for Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals, and that is easy to implement using a neural network architecture."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Cognitive Neuroscience"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35238678"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 76
                            }
                        ],
                        "text": "The first type of system, namely those that utilize geometric features (see [2, 6, 9, 12]), are successful in some restricted areas, but the need of user-input models makes the representation of some objects, such as paintings or jackets, extremely hard."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 678619,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8735690a9e8f8884bf27717877ddf7f9071472e5",
            "isKey": false,
            "numCitedBy": 1457,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Three-Dimensional-Object-Recognition-from-Single-Lowe",
            "title": {
                "fragments": [],
                "text": "Three-Dimensional Object Recognition from Single Two-Dimensional Images"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143615848"
                        ],
                        "name": "Rajesh P. N. Rao",
                        "slug": "Rajesh-P.-N.-Rao",
                        "structuredName": {
                            "firstName": "Rajesh",
                            "lastName": "Rao",
                            "middleNames": [
                                "P.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rajesh P. N. Rao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691804"
                        ],
                        "name": "D. Ballard",
                        "slug": "D.-Ballard",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ballard",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ballard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "Rao and Ballard [17] explore the use of local features for recognizing human faces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2572973,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94b1397583cbda2f078f8ee2e8696e20e57ef1f0",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work regarding the statistics of natural images has revealed that the dominant eigenvectors of arbitrary natural images closely approximate various oriented derivative-of-Gaussian functions; these functions have also been shown to provide the best fit to the receptive field profiles of cells in the primate striate cortex. We propose a scheme for expression-invariant face recognition that employs a fixed set of these \"natural\" basis functions to generate multiscale iconic representations of human faces. Using a fixed set of basis functions obviates the need for recomputing eigenvectors (a step that was necessary in some previous approaches employing principal component analysis (PCA) for recognition) while at the same time retaining the redundancy-reducing properties of PCA. A face is represented by a set of iconic representations automatically extracted from an input image. The description thus obtained is stored in a topographically-organized sparse distributed memory that is based on a model of human long-term memory first proposed by Kanerva. We describe experimental results for an implementation of the method on a pipeline image processor that is capable of achieving near real-time recognition by exploiting the processor's frame-rate convolution capability for indexing purposes. 1 Introduction The problem of object recognition has been a central subject in the field of computer vision. An especially interesting albeit difficult subproblem is that of recognizing human faces. In addition to the difficulties posed by changing viewing conditions, computational methods for face recognition have had to confront the fact that faces are complex non-rigid stimuli that defy easy geometric characterizations and form a dense cluster in the multidimensional space of input images. One of the most important issues in face recognition has therefore been the representation of faces. Early schemes for face recognition utilized geometrical representations; prominent features such as eyes, nose, mouth, and chin were detected and geometrical models of faces given by feature vectors whose dimensions, for instance, denoted the relative positions of the facial features were used for the purposes of recognition [Bledsoe, 1966; Kanade, 1973]. Recently, researchers have reported successful results using photometric representations i.e. representations that are computed directly from the intensity values of the input image. Some prominent examples include face representations based on biologically-motivated Gabor filter \"jets\" [Buhmann et al., 1990], randomly placed zeroth-order Gaussian kernels [Edelman et a/. This paper explores the use of an iconic representation of human faces that exploits the dimensionality-reducing properties of PCA. However, unlike previous approaches employing \u2026"
            },
            "slug": "Natural-Basis-Functions-and-Topographic-Memory-for-Rao-Ballard",
            "title": {
                "fragments": [],
                "text": "Natural Basis Functions and Topographic Memory for Face Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes a scheme for expression-invariant face recognition that employs a fixed set of these \"natural\" basis functions to generate multiscale iconic representations of human faces that exploits the dimensionality-reducing properties of PCA."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3014629"
                        ],
                        "name": "Deeptiman Jugessur",
                        "slug": "Deeptiman-Jugessur",
                        "structuredName": {
                            "firstName": "Deeptiman",
                            "lastName": "Jugessur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deeptiman Jugessur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798721"
                        ],
                        "name": "G. Dudek",
                        "slug": "G.-Dudek",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Dudek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Dudek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "A symmetry based operator is utilized in [10] to detect local interest points for the problem of scene and landmark recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18183963,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3be0590b0112d3b6633dbda410bf7a31fffc063f",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an approach to appearance-based object recognition using single camera images. Our approach is based on using an attention mechanism to obtain visual features that are generic, robust and informative. The features themselves are recognized using principal components an the frequency domain. In this paper we show how the visual characteristics of only a small number of such features can be used for appearance-based object recognition that is not confounded by planar rotations or background clutter."
            },
            "slug": "Local-appearance-for-robust-object-recognition-Jugessur-Dudek",
            "title": {
                "fragments": [],
                "text": "Local appearance for robust object recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown how the visual characteristics of only a small number of such features can be used for appearance-based object recognition that is not confounded by planar rotations or background clutter."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145687022"
                        ],
                        "name": "J. Crowley",
                        "slug": "J.-Crowley",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Crowley",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Crowley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 114
                            }
                        ],
                        "text": "We are currently investigating the use of brightness renormalization for the local differential invariants, as in [19], in order to reduce the brightness sensitivity of the differential invariant approach and provide a fairer comparison."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18689886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a30589d274c2867425ead17780a0d22c69fc672",
            "isKey": false,
            "numCitedBy": 304,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a technique to determine the identity of objects in a scene using histograms of the responses of a vector of local linear neighborhood operators (receptive fields). This technique can be used to determine the most probable objects in a scene, independent of the object's position, image-plane orientation and scale. In this paper we describe the mathematical foundations of the technique and present the results of experiments which compare robustness and recognition rates for different local neighborhood operators and histogram similarity measurements."
            },
            "slug": "Object-Recognition-Using-Multidimensional-Receptive-Schiele-Crowley",
            "title": {
                "fragments": [],
                "text": "Object Recognition Using Multidimensional Receptive Field Histograms"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The mathematical foundations of the technique are described and the results of experiments which compare robustness and recognition rates for different local neighborhood operators and histogram similarity measurements are presented."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31614700"
                        ],
                        "name": "R. Nelson",
                        "slug": "R.-Nelson",
                        "structuredName": {
                            "firstName": "Randal",
                            "lastName": "Nelson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nelson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [16], a contour detection is run on the image, and points of high curvature around the shape are selected as interest points."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [16], Nelson presented a technique to automatically extract a geometric description of an object by detecting semi-invariants at localized points."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16310754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5bd2826a22cc1fde0e7ff371fc8171423c294277",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Memory-based object recognition methods work by comparing an object against many representations stored in a memory, and nding the closest match. However matches are generally made to representations of complete objects, hence such methods tend to be sensitive to clutter and occlusion and require good global segmentation for success. We describe a method that combines an associative memory with a Hough-like evidence combination technique, allowing local segmentation to be used. This resolves the clutter and occlusion sensitivity of traditional memory-based methods, without encountering the space problems that plague voting methods for high DOF problems. The method is based on the two stage use of a general purpose associative memory and semi-invariant local objects called keys. Experiments using keys based on a curve segmentation process are reported , using both polyhedral and curved objects."
            },
            "slug": "Memory-Based-Recognition-for-3-D-Objects-Nelson",
            "title": {
                "fragments": [],
                "text": "Memory-Based Recognition for 3-D Objects"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A method is described that combines an associative memory with a Hough-like evidence combination technique, allowing local segmentation to be used, resolving the clutter and occlusion sensitivity of traditional memory-based methods, without encountering the space problems that plague voting methods for high DOF problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40309692"
                        ],
                        "name": "C. G. Harris",
                        "slug": "C.-G.-Harris",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Harris",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. G. Harris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40365651"
                        ],
                        "name": "M. Stephens",
                        "slug": "M.-Stephens",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Stephens",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stephens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 49
                            }
                        ],
                        "text": "Here we consider the Harris corner detector (see [7]) used in [20], where a matrix that averages the first derivatives of the signal in a window is built as follows: / 0 2143 5\"6 3 87 3:9 ; 1 1 6 1 6 6=< (3)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 56
                            }
                        ],
                        "text": "Given the fact that the threshold function described in [7] does not produce a value between 0 and 1, we have found the following function to provide a more convenient threshold criterion: \" / % 8 $ / % % / (4)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1694378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6818668fb895d95861a2eb9673ddc3a41e27b3b3",
            "isKey": false,
            "numCitedBy": 14111,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem we are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work. For example, we desire to obtain an understanding of natural scenes, containing roads, buildings, trees, bushes, etc., as typified by the two frames from a sequence illustrated in Figure 1. The solution to this problem that we are pursuing is to use a computer vision system based upon motion analysis of a monocular image sequence from a mobile camera. By extraction and tracking of image features, representations of the 3D analogues of these features can be constructed."
            },
            "slug": "A-Combined-Corner-and-Edge-Detector-Harris-Stephens",
            "title": {
                "fragments": [],
                "text": "A Combined Corner and Edge Detector"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The problem the authors are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work."
            },
            "venue": {
                "fragments": [],
                "text": "Alvey Vision Conference"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145299933"
                        ],
                        "name": "R. Mohr",
                        "slug": "R.-Mohr",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Mohr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mohr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "A new concept was presented by Schmid and Mohr [20], where, instead of using geometric features, the authors use a set of differential invariants extracted from interest points."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "Here we consider the Harris corner detector (see [7]) used in [20], where a matrix that averages the first derivatives of the signal in a window is built as follows: / 0 2143 5\"6 3 87 3:9 ; 1 1 6 1 6 6=< (3)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "A recent proposal for local features described in [20] uses a set of derivatives, coined the \u201cLocal-Jet\u201d, that is invariant to rotation and is defined as follows: 5 5 5 5 5 $ 5 $ 5 * 5 $ 5 $ - $) 5 $ 5 5 5 5 $ 5 5 5 5 2 $ 5 $ 5 5 0 5 $ 5 5 $ 5 - $ 5 $ 5 5 5 5 $ 5 5 $ 5 (9)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 152
                            }
                        ],
                        "text": "Alternatively, a detector that uses the auto-correlation function in order to determine locations where the signal changes in two directions is used in [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 226
                            }
                        ],
                        "text": "The threshold for both similarity functions is varied as follows: for the phase correlation, that has values in 2 ' , the variation step is 1 ; the differential invariant feature uses the Mahalanobis distance, as described in [20], which can have practically any value above 0, so the variation step is 1 until !&\" 1 \" \" ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "differential invariants in [20], and local scale-invariant features in [13])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 325871,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49fcd806450d947e56c82ef2b438ad9c484069dc",
            "isKey": true,
            "numCitedBy": 1792,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of retrieving images from large image databases. The method is based on local grayvalue invariants which are computed at automatically detected interest points. A voting algorithm and semilocal constraints make retrieval possible. Indexing allows for efficient retrieval from a database of more than 1,000 images. Experimental results show correct retrieval in the case of partial visibility, similarity transformations, extraneous features, and small perspective deformations."
            },
            "slug": "Local-Grayvalue-Invariants-for-Image-Retrieval-Schmid-Mohr",
            "title": {
                "fragments": [],
                "text": "Local Grayvalue Invariants for Image Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "This paper addresses the problem of retrieving images from large image databases with a method based on local grayvalue invariants which are computed at automatically detected interest points and allows for efficient retrieval from a database of more than 1,000 images."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719838"
                        ],
                        "name": "W. Grimson",
                        "slug": "W.-Grimson",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Grimson",
                            "middleNames": [
                                "Eric",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Grimson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388700951"
                        ],
                        "name": "Tomas Lozano-Perez",
                        "slug": "Tomas-Lozano-Perez",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Lozano-Perez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Lozano-Perez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 76
                            }
                        ],
                        "text": "The first type of system, namely those that utilize geometric features (see [2, 6, 9, 12]), are successful in some restricted areas, but the need of user-input models makes the representation of some objects, such as paintings or jackets, extremely hard."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14484943,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1230072567fed362e7a39bb8bc83c867c0c1a9f5",
            "isKey": false,
            "numCitedBy": 550,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses how local measurements of positions and surface normals may be used to identify and locate overlapping objects. The objects are modeled as polyhedra (or polygons) having up to six degrees of positional freedom relative to the sensors. The approach operates by examining all hypotheses about pairings between sensed data and object surfaces and efficiently discarding inconsistent ones by using local constraints on: distances between faces, angles between face normals, and angles (relative to the surface normals) of vectors between sensed points. The method described here is an extension of a method for recognition and localization of nonoverlapping parts previously described in [18] and [15]."
            },
            "slug": "Localizing-Overlapping-Parts-by-Searching-the-Tree-Grimson-Lozano-Perez",
            "title": {
                "fragments": [],
                "text": "Localizing Overlapping Parts by Searching the Interpretation Tree"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The approach operates by examining all hypotheses about pairings between sensed data and object surfaces and efficiently discarding inconsistent ones by using local constraints on distances between faces, angles between face normals, and angles of vectors between sensed points."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72419159"
                        ],
                        "name": "R. Brooks",
                        "slug": "R.-Brooks",
                        "structuredName": {
                            "firstName": "Rodney",
                            "lastName": "Brooks",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Brooks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5711553,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "ed34b294b10e8fe49d8584578216d87da2f2de34",
            "isKey": false,
            "numCitedBy": 385,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "ACRONYM is a comprehensive domain independent model-based system for vision and manipulation related tasks. Many of its submodules and representations have been described elsewhere. Here the derivation and use of invariants for image feature prediction is described. Predictions of image features and their relations are made from three-dimensional geometric models. Instructions are generated which teli the interpretation algorithms how to make use of image feature measurements to derive three-dimensional size, structural, and spatial constraints on the original three-dimensional models. Some preliminary examples of ACRONYM's interpretations of aerial images are shown."
            },
            "slug": "Model-Based-Three-Dimensional-Interpretations-of-Brooks",
            "title": {
                "fragments": [],
                "text": "Model-Based Three-Dimensional Interpretations of Two-Dimensional Images"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The derivation and use of invariants for image feature prediction is described and predictions of image features and their relations are made from three-dimensional geometric models."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116158963"
                        ],
                        "name": "S. Nayar",
                        "slug": "S.-Nayar",
                        "structuredName": {
                            "firstName": "Sheila",
                            "lastName": "Nayar",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nayar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 207
                            }
                        ],
                        "text": "3, we can see common type of image deformations, the true positive rate and the reliability rate for the interest point detector (note: from left to right, image number 2 is extracted from the COIL database [18], and image number 4 is extracted from [21])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58758670,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "77afac8f4d7f47c8b34371d8f8355cefbea1d4f6",
            "isKey": false,
            "numCitedBy": 2004,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Columbia Object Image Library COIL is a database of color images of objects The objects were placed on a motorized turntable against a black background The turntable was rotated through degrees to vary object pose with respect to a xed color camera Images of the objects were taken at pose intervals of degrees This corresponds to poses per object The images were size normalized COIL is available online via ftp"
            },
            "slug": "Columbia-Object-Image-Library-(COIL100)-Nayar",
            "title": {
                "fragments": [],
                "text": "Columbia Object Image Library (COIL100)"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "Columbia Object Image Library COIL is a database of color images of objects that were placed on a motorized turntable against a black background and rotated through degrees to vary object pose with respect to a xed color camera."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 76
                            }
                        ],
                        "text": "The first type of system, namely those that utilize geometric features (see [2, 6, 9, 12]), are successful in some restricted areas, but the need of user-input models makes the representation of some objects, such as paintings or jackets, extremely hard."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Object recognition using alignment"
            },
            "venue": {
                "fragments": [],
                "text": "In International Conference on Computer Vision,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 76
                            }
                        ],
                        "text": "The first type of system, namely those that utilize geometric features (see [2, 6, 9, 12]), are successful in some restricted areas, but the need of user-input models makes the representation of some objects, such as paintings or jackets, extremely hard."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Model-based 3-d interpretations of 2-d images"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence,"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dudek . Local appearancefor robust object recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Computer Vision and Pattern Recognition"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 106
                            }
                        ],
                        "text": "2 Saturating the Amplitude Information The amplitude saturation is similar to contrast normalization (see [8]) and to the constraint on a minimum absolute amplitude (see [4])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computational model of cat striate physiology"
            },
            "venue": {
                "fragments": [],
                "text": "Technical report, Massachusetts Institute of Technology,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dudek . Local appearancefor robust object recognition"
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Computer Vision , pages"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 170
                            }
                        ],
                        "text": "2 Saturating the Amplitude Information The amplitude saturation is similar to contrast normalization (see [8]) and to the constraint on a minimum absolute amplitude (see [4])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phase-based disparity measure"
            },
            "venue": {
                "fragments": [],
                "text": "In CVGIP: Image Understanding,"
            },
            "year": 1991
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 14,
            "methodology": 11
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 26,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Phase-Based-Local-Features-Carneiro-Jepson/f691349c928b1d65546f243075d5e8995b82f958?sort=total-citations"
}