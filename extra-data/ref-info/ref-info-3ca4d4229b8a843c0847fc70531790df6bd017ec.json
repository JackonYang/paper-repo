{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895207"
                        ],
                        "name": "C. Gentile",
                        "slug": "C.-Gentile",
                        "structuredName": {
                            "firstName": "Claudio",
                            "lastName": "Gentile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gentile"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2946659,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a9e441188b4a41da300c2de923a674dd5d179c5",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a unifying method for proving relative loss bounds for online linear threshold classification algorithms, such as the Perceptron and the Winnow algorithms. For classification problems the discrete loss is used, i.e., the total number of prediction mistakes. We introduce a continuous loss function, called the \"linear hinge loss\", that can be employed to derive the updates of the algorithms. We first prove bounds w.r.t. the linear hinge loss and then convert them to the discrete loss. We introduce a notion of \"average margin\" of a set of examples. We show how relative loss bounds based on the linear hinge loss can be converted to relative loss bounds i.t.o. the discrete loss using the average margin."
            },
            "slug": "Linear-Hinge-Loss-and-Average-Margin-Gentile-Warmuth",
            "title": {
                "fragments": [],
                "text": "Linear Hinge Loss and Average Margin"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A unifying method for proving relative loss bounds for online linear threshold classification algorithms, such as the Perceptron and the Winnow algorithms, is described and a notion of \"average margin\" of a set of examples is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389955537"
                        ],
                        "name": "S. Shalev-Shwartz",
                        "slug": "S.-Shalev-Shwartz",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Shalev-Shwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shalev-Shwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 155
                            }
                        ],
                        "text": "We have used this observation in devising online algorithms for the task of music to score alignment which we cast as learning with structured output (see Shalev-Shwartz et al. (2004a))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 154
                            }
                        ],
                        "text": "\u2026conducted further research on applications of the PA algorithmic framework f r learning margin-based suffix trees (Dekel et al., 2004b), pseudo-metrics (Shalev-Shwartz et al., 2004b), hierarchical classification (Dekel et al., 2004a), and segmentation of sequences (Shalev-Shwartz et al., 2004a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6488984,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af74e4ceebfd8a9bf2e3e52ab1d262fdb2653bc4",
            "isKey": false,
            "numCitedBy": 341,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe and analyze an online algorithm for supervised learning of pseudo-metrics. The algorithm receives pairs of instances and predicts their similarity according to a pseudo-metric. The pseudo-metrics we use are quadratic forms parameterized by positive semi-definite matrices. The core of the algorithm is an update rule that is based on successive projections onto the positive semi-definite cone and onto half-space constraints imposed by the examples. We describe an efficient procedure for performing these projections, derive a worst case mistake bound on the similarity predictions, and discuss a dual version of the algorithm in which it is simple to incorporate kernel operators. The online algorithm also serves as a building block for deriving a large-margin batch algorithm. We demonstrate the merits of the proposed approach by conducting experiments on MNIST dataset and on document filtering."
            },
            "slug": "Online-and-batch-learning-of-pseudo-metrics-Shalev-Shwartz-Singer",
            "title": {
                "fragments": [],
                "text": "Online and batch learning of pseudo-metrics"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "An online algorithm for supervised learning of pseudo-metrics that is based on successive projections onto the positive semi-definite cone and onto half-space constraints imposed by the examples is described and analyzed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716876"
                        ],
                        "name": "O. Dekel",
                        "slug": "O.-Dekel",
                        "structuredName": {
                            "firstName": "Ofer",
                            "lastName": "Dekel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Dekel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771345"
                        ],
                        "name": "Joseph Keshet",
                        "slug": "Joseph-Keshet",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Keshet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Keshet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 70
                            }
                        ],
                        "text": "The prediction-based and max-loss updates were previouslydiscussed in Dekel et al. (2004a), in the context of hierarchical classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 126
                            }
                        ],
                        "text": "We already conducted further research on applications of the PA algorithmic framework f r learning margin-based suffix trees (Dekel et al., 2004b), pseudo-metrics (Shalev-Shwartz et al., 2004b), hierarchical classification (Dekel et al., 2004a), and segmentation of sequences (Shalev-Shwartz et al.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 214
                            }
                        ],
                        "text": "\u2026conducted further research on applications of the PA algorithmic framework f r learning margin-based suffix trees (Dekel et al., 2004b), pseudo-metrics (Shalev-Shwartz et al., 2004b), hierarchical classification (Dekel et al., 2004a), and segmentation of sequences (Shalev-Shwartz et al., 2004a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Dekel et al. (2004a) evaluated both techniques empiricallyand found them to be highly effective on speech recognition and text classification tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The prediction-based and max-loss updates were previously discussed in Dekel et al. (2004a), in the context of hierarchical classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16308336,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "094d9601e6f6c45579647e20b5f7b0eeb4e2819f",
            "isKey": false,
            "numCitedBy": 224,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithmic framework for supervised classification learning where the set of labels is organized in a predefined hierarchical structure. This structure is encoded by a rooted tree which induces a metric over the label set. Our approach combines ideas from large margin kernel methods and Bayesian analysis. Following the large margin principle, we associate a prototype with each label in the tree and formulate the learning task as an optimization problem with varying margin constraints. In the spirit of Bayesian methods, we impose similarity requirements between the prototypes corresponding to adjacent labels in the hierarchy. We describe new online and batch algorithms for solving the constrained optimization problem. We derive a worst case loss-bound for the online algorithm and provide generalization analysis for its batch counterpart. We demonstrate the merits of our approach with a series of experiments on synthetic, text and speech data."
            },
            "slug": "Large-margin-hierarchical-classification-Dekel-Keshet",
            "title": {
                "fragments": [],
                "text": "Large margin hierarchical classification"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "This work presents an algorithmic framework for supervised classification learning where the set of labels is organized in a predefined hierarchical structure encoded by a rooted tree which induces a metric over the label set."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1839746"
                        ],
                        "name": "M. Herbster",
                        "slug": "M.-Herbster",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Herbster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Herbster"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 58
                            }
                        ],
                        "text": "Therefore, the algorithm\u2019s goal is to achieve a margin of at least1 as often as possible."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 59
                            }
                        ],
                        "text": "Of all the work on online learning algorithms, the work by Herbster (2001) is probably the closest to the work presented here."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7031107,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "471c91c090daec9c2a08b7f99e1187d1e657ab8d",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop three new techniques to build on the recent advances in online learning with kernels. First,w e show that an exponential speed-up in prediction time per trial is possible for such algorithms as the Kernel-Adatron,the Kernel-Perceptron,and ROMMA for specific additive models. Second,w e show that the techniques of the recent algorithms developed for online linear prediction when the best predictor changes over time may be implemented for kernel-based learners at no additional asymptotic cost. Finally,w e introduce a new online kernel-based learning algorithm for which we give worst-case loss bounds for the \u0190-insensitive square loss."
            },
            "slug": "Learning-Additive-Models-Online-with-Fast-Kernels-Herbster",
            "title": {
                "fragments": [],
                "text": "Learning Additive Models Online with Fast Evaluating Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Three new techniques are developed to build on the recent advances in online learning with kernels and show that an exponential speed-up in prediction time per trial is possible for such algorithms as the Kernel-Adatron, Kernel-Perceptron, and ROMMA for specific additive models."
            },
            "venue": {
                "fragments": [],
                "text": "COLT/EuroCOLT"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We demonstrate some of the merits of the proposed algorithms in a series of experiments with synthetic and real datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 186
                            }
                        ],
                        "text": "More modern examples include the ROMMA algorithm of Li and Long (2002), Gentile\u2019s ALMA algorithm (Gentile, 2001), the MIRA algorithm (Crammer and Singer, 2003b), and the NORMA algorithm (Kivinen et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 171
                            }
                        ],
                        "text": "Rather than projecting thecurrent hypothesis onto the set of con-\nstraints induced by the most recent example, NORMA\u2019s updaterule is based on a stochastic gradient approach (Kivinen et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2959806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4",
            "isKey": false,
            "numCitedBy": 1044,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel-based algorithms such as support vector machines have achieved considerable success in various problems in batch setting, where all of the training data is available in advance. Support vector machines combine the so-called kernel trick with the large margin idea. There has been little use of these methods in an online setting suitable for real-time applications. In this paper, we consider online learning in a reproducing kernel Hilbert space. By considering classical stochastic gradient descent within a feature space and the use of some straightforward tricks, we develop simple and computationally efficient algorithms for a wide range of problems such as classification, regression, and novelty detection. In addition to allowing the exploitation of the kernel trick in an online setting, we examine the value of large margins for classification in the online setting with a drifting target. We derive worst-case loss bounds, and moreover, we show the convergence of the hypothesis to the minimizer of the regularized risk functional. We present some experimental results that support the theory as well as illustrating the power of the new algorithms for online novelty detection."
            },
            "slug": "Online-learning-with-kernels-Kivinen-Smola",
            "title": {
                "fragments": [],
                "text": "Online learning with kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper considers online learning in a reproducing kernel Hilbert space, and allows the exploitation of the kernel trick in an online setting, and examines the value of large margins for classification in the online setting with a drifting target."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Signal Processing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693407"
                        ],
                        "name": "K. Crammer",
                        "slug": "K.-Crammer",
                        "structuredName": {
                            "firstName": "Koby",
                            "lastName": "Crammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Crammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 107
                            }
                        ],
                        "text": "We compared the multiclassver ions of PA, PA-I and PA-II to the online multiclass algorithms described in (Crammer and Singer, 2003a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 15
                            }
                        ],
                        "text": "01, following (Crammer and Singer, 2003a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 114
                            }
                        ],
                        "text": "10.3, we compare our multiclass versions of the PA algorithms to other online algorithms for multiclass problems (Crammer and Singer, 2003a) on natural datasets."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We demonstrate some of the merits of the proposed algorithms in a series of experiments with synthetic and real datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 133
                            }
                        ],
                        "text": "More modern examples include the ROMMA algorithm of Li and Long (2002), Gentile\u2019s ALMA algorithm (Gentile, 2001), the MIRA algorithm (Crammer and Singer, 2003b), and the NORMA algorithm (Kivinen et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 120
                            }
                        ],
                        "text": "The MIRA algorithm for binary classification is identical to our basic PA algorithm, however the loss bounds derived in Crammer and Singer (2003b) are inferior to the bounds derived in this paper and are also substantially less general."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 249
                            }
                        ],
                        "text": "This definition generalizes the definition of margin for binary classification and was employed by both single-label and multilabel learning algorithms for support vector machines (Vapnik, 1998, Weston and Watkins, 1999, Elisseeff and Weston, 2001, Crammer and Singer, 2003a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 14
                            }
                        ],
                        "text": "Specifically, Crammer and Singer (2003a) present three multiclass versions of thePerceptron algorithm and a new margin based online multiclass algorithm named MIRA."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 75
                            }
                        ],
                        "text": "MR (Schapire and Singer, 1998) and adaptations of support vector machines (Crammer and Singer, 2003a) have been devised for the task of label ranking."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8729730,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28b9bacde6499f8cc6f7e70feee4232107211e39",
            "isKey": false,
            "numCitedBy": 433,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we study online classification algorithms for multiclass problems in the mistake bound model. The hypotheses we use maintain one prototype vector per class. Given an input instance, a multiclass hypothesis computes a similarity-score between each prototype and the input instance and then sets the predicted label to be the index of the prototype achieving the highest similarity. To design and analyze the learning algorithms in this paper we introduce the notion of ultracon-servativeness. Ultraconservative algorithms are algorithms that update only the prototypes attaining similarity-scores which are higher than the score of the correct label's prototype. We start by describing a family of additive ultraconservative algorithms where each algorithm in the family updates its prototypes by finding a feasible solution for a set of linear constraints that depend on the instantaneous similarity-scores. We then discuss a specific online algorithm that seeks a set of prototypes which have a small norm. The resulting algorithm, which we term MIRA (for Margin Infused Relaxed Algorithm) is ultraconservative as well. We derive mistake bounds for all the algorithms and provide further analysis of MIRA using a generalized notion of the margin for multiclass problems."
            },
            "slug": "Ultraconservative-Online-Algorithms-for-Multiclass-Crammer-Singer",
            "title": {
                "fragments": [],
                "text": "Ultraconservative Online Algorithms for Multiclass Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This paper studies online classification algorithms for multiclass problems in the mistake bound model and introduces the notion of ultracon-servativeness, a family of additive ultraconservative algorithms where each algorithm in the family updates its prototypes by finding a feasible solution for a set of linear constraints that depend on the instantaneous similarity-scores."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2961287"
                        ],
                        "name": "N. Littlestone",
                        "slug": "N.-Littlestone",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Littlestone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Littlestone"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 197
                            }
                        ],
                        "text": "The mere idea of deriving an update as a result of a constrained optimization problem compr ising of two opposing terms, has been largely advocated by Littlestone, Warmuth, Kivinen and colleagues (Littlestone, 1989, Kivinen and Warmuth, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The mere idea of deriving an update as a result of a constrained optimization problem compromising of two opposing terms, has been largely advocated by Littlestone, Warmuth, Kivinen and colleagues (Littlestone, 1989; Kivinen and Warmuth, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122204757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbda209682dd2b1b1d0006a41cf84ca1b2487c71",
            "isKey": false,
            "numCitedBy": 185,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider a problem of learning from examples. In each of a sequence of trials the learner observes an instance to be classified and must respond with a prediction of its correct classification. Following the prediction the learner is told the correct response. We consider the two category case, and work for the most part with instances composed of a number of Boolean attributes or, more generally, chosen from (0, 1) $\\sp{n}$. Initially, we assume that the correct response in each trial is a function of the corresponding instance, this target function being chosen from some target class of $\\{0,1\\}$-valued functions. We relax this assumption somewhat later. \nWe focus on evaluation of on-line predictive performance, counting the number of mistakes made by the learner during the learning process. For certain target classes we have found algorithms for which we can prove excellent mistake bounds, using no probabilistic assumptions. In the first part of the dissertation we study the properties of such worst-case mistake bounds. \nIn the central part of this dissertation, we present a group of linear-threshold algorithms that are particularly well suited to circumstances in which most of the attributes of the instances are irrelevant to determining the correct predictions. For target classes that can be handled by these algorithms, we show that the worst-case mistake bounds grow only logarithmically with the number of irrelevant attributes. We demonstrate that these algorithms have some measure of robustness in the face of anomalies in the training data caused, for example, by noise. \nWe also consider the implications of on-line, worst-case mistake bounds for learning in a batch setting with probabilistic assumptions, making use of the probabilistic PAC-learning model introduced by Valiant (Va184). We present an analysis that shows that a straightforward transformation applied to mistake bounded algorithms, consisting of adding a hypothesis testing phase, produces algorithms that have asymptotically optimal PAC-learning bounds for certain target classes."
            },
            "slug": "Mistake-bounds-and-logarithmic-linear-threshold-Littlestone",
            "title": {
                "fragments": [],
                "text": "Mistake bounds and logarithmic linear-threshold learning algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "An analysis that shows that a straightforward transformation applied to mistake bounded algorithms, consisting of adding a hypothesis testing phase, produces algorithms that have asymptotically optimal PAC-learning bounds for certain target classes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730156"
                        ],
                        "name": "Carlos Guestrin",
                        "slug": "Carlos-Guestrin",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Guestrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Guestrin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 201720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c450531e1121cfb657be5195e310217a4675397",
            "isKey": false,
            "numCitedBy": 1477,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches."
            },
            "slug": "Max-Margin-Markov-Networks-Taskar-Guestrin",
            "title": {
                "fragments": [],
                "text": "Max-Margin Markov Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data, and a new theoretical bound for generalization in structured domains is provided."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2550989"
                        ],
                        "name": "Norbert Klasner",
                        "slug": "Norbert-Klasner",
                        "structuredName": {
                            "firstName": "Norbert",
                            "lastName": "Klasner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Norbert Klasner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723095"
                        ],
                        "name": "H. Simon",
                        "slug": "H.-Simon",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Simon",
                            "middleNames": [
                                "Ulrich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Simon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[4,  13 , 14]) and is omitted due to the lack of space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5464604,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8e18455c51bb6fcb9e00ca88b652e2561bdbf74",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple method is presented which, loosely speaking, virtually removes noise or misfit from data, and thereby converts a \u201cnoise-free\u201d algorithm A, which on-line learns linear functions from data without noise or misfit, into a \u201cnoise-tolerant\u201d algorithm Ant which learns linear functions from data containing noise or misfit. Given some technical conditions, this conversion preserves optimality. For instance, the optimal noise-free algorithm B of Bernstein from [3] is converted into an optimal noisetolerant algorithm 13nt. The conversion also works properly for all function classes which are closed under addition and contain linear functions as a subclass. In the second part of the paper, we show that Bernstein\u2019s on-line learning algorithm B can ~e converted into a batch learning algorithm B which consumes an (almost) minimal number of random training examples. This is true for a whole class of \u201cpat-style\u201d batch learning models (including learning with an (c, ~)good model and learning with an e-bounded expected r-loss). The upper bound on the sample size is shown by applying a method of Littlestone from [9] which converts an online learning algorithm A into a batch learning algorithm A and relates the sample size of A to the total loss of A. For this purpose, Littlestone\u2019s method is extended from *The author gratefully acknowledges the support of Bundesministerium fur Forschung und Technologies grant olINlo2c/2. 1 Let O, l-valued functions to real-valued functions. The lower bound on the sample size is shown by generalizing and applying a method of Simon from [11]."
            },
            "slug": "From-noise-free-to-noise-tolerant-and-from-on-line-Klasner-Simon",
            "title": {
                "fragments": [],
                "text": "From noise-free to noise-tolerant and from on-line to batch learning"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A simple method is presented which virtually removes noise or misfit from data, and thereby converts a \u201cnoise-free\u201d algorithm A, which on-line learns linear functions from data without noise orMisfit, into a \u2018noises-tolerant\u2019 algorithm Ant which learns linearfunction from data containing noise ormisfit."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693407"
                        ],
                        "name": "K. Crammer",
                        "slug": "K.-Crammer",
                        "structuredName": {
                            "firstName": "Koby",
                            "lastName": "Crammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Crammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10151608,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "cfc6d0c8260594ebc5dd20ee558d29b1014ed41a",
            "isKey": false,
            "numCitedBy": 2190,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe the algorithmic implementation of multiclass kernel-based vector machines. Our starting point is a generalized notion of the margin to multiclass problems. Using this notion we cast multiclass categorization problems as a constrained optimization problem with a quadratic objective function. Unlike most of previous approaches which typically decompose a multiclass problem into multiple independent binary classification tasks, our notion of margin yields a direct method for training multiclass predictors. By using the dual of the optimization problem we are able to incorporate kernels with a compact set of constraints and decompose the dual problem into multiple optimization problems of reduced size. We describe an efficient fixed-point algorithm for solving the reduced optimization problems and prove its convergence. We then discuss technical details that yield significant running time improvements for large datasets. Finally, we describe various experiments with our approach comparing it to previously studied kernel-based methods. Our experiments indicate that for multiclass problems we attain state-of-the-art accuracy."
            },
            "slug": "On-the-Algorithmic-Implementation-of-Multiclass-Crammer-Singer",
            "title": {
                "fragments": [],
                "text": "On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper describes the algorithmic implementation of multiclass kernel-based vector machines using a generalized notion of the margin to multiclass problems, and describes an efficient fixed-point algorithm for solving the reduced optimization problems and proves its convergence."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 4
                            }
                        ],
                        "text": "MR (Schapire and Singer, 1998) and adaptations of support vector machines (Crammer and Singer, 2003a) have been devised for the task of label ranking."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "Plugging this back into Eq."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2329907,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a9309e056272ff2076f447df8dbc536f46fc466",
            "isKey": false,
            "numCitedBy": 1920,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe several improvements to Freund and Schapire's AdaBoost boosting algorithm, particularly in a setting in which hypotheses may assign confidences to each of their predictions. We give a simplified analysis of AdaBoost in this setting, and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses. We give a specific method for assigning confidences to the predictions of decision trees, a method closely related to one used by Quinlan. This method also suggests a technique for growing decision trees which turns out to be identical to one proposed by Kearns and Mansour. We focus next on how to apply the new boosting algorithms to multiclass classification problems, particularly to the multi-label case in which each example may belong to more than one class. We give two boosting methods for this problem, plus a third method based on output coding. One of these leads to a new method for handling the single-label case which is simpler but as effective as techniques suggested by Freund and Schapire. Finally, we give some experimental results comparing a few of the algorithms discussed in this paper."
            },
            "slug": "Improved-Boosting-Algorithms-Using-Confidence-rated-Schapire-Singer",
            "title": {
                "fragments": [],
                "text": "Improved Boosting Algorithms Using Confidence-rated Predictions"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Several improvements to Freund and Schapire's AdaBoost boosting algorithm are described, particularly in a setting in which hypotheses may assign confidences to each of their predictions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693407"
                        ],
                        "name": "K. Crammer",
                        "slug": "K.-Crammer",
                        "structuredName": {
                            "firstName": "Koby",
                            "lastName": "Crammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Crammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 107
                            }
                        ],
                        "text": "We compared the multiclassver ions of PA, PA-I and PA-II to the online multiclass algorithms described in (Crammer and Singer, 2003a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 15
                            }
                        ],
                        "text": "01, following (Crammer and Singer, 2003a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 114
                            }
                        ],
                        "text": "10.3, we compare our multiclass versions of the PA algorithms to other online algorithms for multiclass problems (Crammer and Singer, 2003a) on natural datasets."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 133
                            }
                        ],
                        "text": "More modern examples include the ROMMA algorithm of Li and Long (2002), Gentile\u2019s ALMA algorithm (Gentile, 2001), the MIRA algorithm (Crammer and Singer, 2003b), and the NORMA algorithm (Kivinen et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 120
                            }
                        ],
                        "text": "The MIRA algorithm for binary classification is identical to our basic PA algorithm, however the loss bounds derived in Crammer and Singer (2003b) are inferior to the bounds derived in this paper and are also substantially less general."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 249
                            }
                        ],
                        "text": "This definition generalizes the definition of margin for binary classification and was employed by both single-label and multilabel learning algorithms for support vector machines (Vapnik, 1998, Weston and Watkins, 1999, Elisseeff and Weston, 2001, Crammer and Singer, 2003a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 14
                            }
                        ],
                        "text": "Specifically, Crammer and Singer (2003a) present three multiclass versions of thePerceptron algorithm and a new margin based online multiclass algorithm named MIRA."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 75
                            }
                        ],
                        "text": "MR (Schapire and Singer, 1998) and adaptations of support vector machines (Crammer and Singer, 2003a) have been devised for the task of label ranking."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3162247,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bfdcd1e6cb8b93cd571b98514990bb91fe7a4ea",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new family of topic-ranking algorithms for multi-labeled documents. The motivation for the algorithms stem from recent advances in online learning algorithms. The algorithms are simple to implement and are also time and memory efficient. We provide a unified analysis of the family of algorithms in the mistake bound model. We then discuss experiments with the proposed family of topic-ranking algorithms on the Reuters-21578 corpus and the new corpus released by Reuters in 2000. On both corpora, the algorithms we present achieve state-of-the-art results and outperforms topic-ranking adaptations of Rocchio's algorithm and of the Perceptron algorithm."
            },
            "slug": "A-Family-of-Additive-Online-Algorithms-for-Category-Crammer-Singer",
            "title": {
                "fragments": [],
                "text": "A Family of Additive Online Algorithms for Category Ranking"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A new family of topic- ranking algorithms for multi-labeled documents that achieve state-of-the-art results and outperforms topic-ranking adaptations of Rocchio's algorithm and of the Perceptron algorithm are described."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153682885"
                        ],
                        "name": "Yi Li",
                        "slug": "Yi-Li",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144007105"
                        ],
                        "name": "Philip M. Long",
                        "slug": "Philip-M.-Long",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Long",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip M. Long"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "We begin with binary classificationwhich serves as the main building block for the remainder of the paper."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 52
                            }
                        ],
                        "text": "More modern examples include the ROMMA algorithm of Li and Long (2002), Gentile\u2019s ALMA algorithm (Gentile, 2001), the MIRA algorithm (Crammer and Singer, 2003b), and the NORMA algorithm (Kivinen et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 939841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f6d77af70db1b19bd515e98ffeb72f5ed1feecf",
            "isKey": false,
            "numCitedBy": 241,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new incremental algorithm for training linear threshold functions: the Relaxed Online Maximum Margin Algorithm, or ROMMA. ROMMA can be viewed as an approximation to the algorithm that repeatedly chooses the hyperplane that classifies previously seen examples correctly with the maximum margin. It is known that such a maximum-margin hypothesis can be computed by minimizing the length of the weight vector subject to a number of linear constraints. ROMMA works by maintaining a relatively simple relaxation of these constraints that can be efficiently updated. We prove a mistake bound for ROMMA that is the same as that proved for the perceptron algorithm. Our analysis implies that the maximum-margin algorithm also satisfies this mistake bound; this is the first worst-case performance guarantee for this algorithm. We describe some experiments using ROMMA and a variant that updates its hypothesis more aggressively as batch algorithms to recognize handwritten digits. The computational complexity and simplicity of these algorithms is similar to that of perceptron algorithm, but their generalization is much better. We show that a batch algorithm based on aggressive ROMMA converges to the fixed threshold SVM hypothesis."
            },
            "slug": "The-Relaxed-Online-Maximum-Margin-Algorithm-Li-Long",
            "title": {
                "fragments": [],
                "text": "The Relaxed Online Maximum Margin Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This work describes a new incremental algorithm for training linear threshold functions: the Relaxed Online Maximum Margin Algorithm, or ROMMA, and proves a mistake bound for ROMMA that is the same as that proved for the perceptron algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1943813"
                        ],
                        "name": "Erin Allwein",
                        "slug": "Erin-Allwein",
                        "structuredName": {
                            "firstName": "Erin",
                            "lastName": "Allwein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erin Allwein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9790719,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd74cc5129c45268d4e766d3619e7cb0ead5c8c8",
            "isKey": false,
            "numCitedBy": 1991,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a unifying framework for studying the solution of multiclass categorization problems by reducing them to multiple binary problems that are then solved using a margin-based binary learning algorithm. The proposed framework unifies some of the most popular approaches in which each class is compared against all others, or in which all pairs of classes are compared to each other, or in which output codes with error-correcting properties are used. We propose a general method for combining the classifiers generated on the binary problems, and we prove a general empirical multiclass loss bound given the empirical loss of the individual binary learning algorithms. The scheme and the corresponding bounds apply to many popular classification learning algorithms including support-vector machines, AdaBoost, regression, logistic regression and decision-tree algorithms. We also give a multiclass generalization error analysis for general output codes with AdaBoost as the binary learner. Experimental results with SVM and AdaBoost show that our scheme provides a viable alternative to the most commonly used multiclass algorithms."
            },
            "slug": "Reducing-Multiclass-to-Binary:-A-Unifying-Approach-Allwein-Schapire",
            "title": {
                "fragments": [],
                "text": "Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A general method for combining the classifiers generated on the binary problems is proposed, and a general empirical multiclass loss bound is proved given the empirical loss of the individual binary learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977260"
                        ],
                        "name": "E. Xing",
                        "slug": "E.-Xing",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Xing",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Xing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2643381,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1a2d203733208deda7427c8e20318334193d9d7",
            "isKey": false,
            "numCitedBy": 3026,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many \"plausible\" ways, and if a clustering algorithm such as K-means initially fails to find one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufficiently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they consider \"similar.\" For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, if desired, dissimilar) pairs of points in \u211dn, learns a distance metric over \u211dn that respects these relationships. Our method is based on posing metric learning as a convex optimization problem, which allows us to give efficient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to significantly improve clustering performance."
            },
            "slug": "Distance-Metric-Learning-with-Application-to-with-Xing-Ng",
            "title": {
                "fragments": [],
                "text": "Distance Metric Learning with Application to Clustering with Side-Information"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents an algorithm that, given examples of similar (and, if desired, dissimilar) pairs of points in \ufffd\u201dn, learns a distance metric over \u211dn that respects these relationships."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 216
                            }
                        ],
                        "text": "The mere idea of deriving an update as a result of a constrained optimization problem compr ising of two opposing terms, has been largely advocated by Littlestone, Warmuth, Kivinen and colleagues (Littlestone, 1989, Kivinen and Warmuth, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We demonstrate some of the merits of the proposed algorithms in a series of experiments with synthetic and real datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 121
                            }
                        ],
                        "text": "The above formalism is motivated by the work of Warmuth and colleagues for deriving online algorithms (see for instance (Kivinen and Warmuth, 1997) and the references therein)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 138
                            }
                        ],
                        "text": "Online algorithms are typically simple to implement and their analysis often provides tight bounds on their performance (see for instance Kivinen and Warmuth (1997))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6130401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98eed3f082351c4821d1edb315846207a8fefbe9",
            "isKey": true,
            "numCitedBy": 911,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider two algorithm for on-line prediction based on a linear model. The algorithms are the well-known Gradient Descent (GD) algorithm and a new algorithm, which we call EG(+/-). They both maintain a weight vector using simple updates. For the GD algorithm, the update is based on subtracting the gradient of the squared error made on a prediction. The EG(+/-) algorithm uses the components of the gradient in the exponents of factors that are used in updating the weight vector multiplicatively. We present worst-case loss bounds for EG(+/-) and compare them to previously known bounds for the GD algorithm. The bounds suggest that the losses of the algorithms are in general incomparable, but EG(+/-) has a much smaller loss if only a few components of the input are relevant for the predictions. We have performed experiments, which show that our worst-case upper bounds are quite tight already on simple artificial data."
            },
            "slug": "Exponentiated-Gradient-Versus-Gradient-Descent-for-Kivinen-Warmuth",
            "title": {
                "fragments": [],
                "text": "Exponentiated Gradient Versus Gradient Descent for Linear Predictors"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The bounds suggest that the losses of the algorithms are in general incomparable, but EG(+/-) has a much smaller loss if only a few components of the input are relevant for the predictions, which is quite tight already on simple artificial data."
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Comput."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693407"
                        ],
                        "name": "K. Crammer",
                        "slug": "K.-Crammer",
                        "structuredName": {
                            "firstName": "Koby",
                            "lastName": "Crammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Crammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 488,
                                "start": 41
                            }
                        ],
                        "text": "Furthermo re, the loss bounds derived in Crammer and Singer (2003b) are inferior and less general th an the bounds derived in this paper. The NORMA algorithm also shares a similar view of classification proble ms. Rather than projecting the current hypothesis onto the set of constraints induced by th e most recent example, NORMA\u2019s update rule is based on a stochastic gradient approach (Kivin en et al., 2002). Of all the work on online learning algorithms, the work by Herbster (2001) is probab ly the closest to the work"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 194
                            }
                        ],
                        "text": "The common construction in such settings is to assume that each instance is a vec tor in Rn and to associate a different weight vector (often referred to as prototype) w ith each of thek labels (Vapnik, 1998; Weston and Watkins, 1999; Crammer and Singer, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 231
                            }
                        ],
                        "text": "The common construction in such settings is to assume that each instance is a vector in n and to\nassociate a different weight vector (often referred to as prototype) with each of thek label (Vapnik, 1998, Weston and Watkins, 1999, Crammer and Singer, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 36
                            }
                        ],
                        "text": "multiclass algorithms described in (Crammer and Singer, 2003a). Specifically, Crammer and Singer (2003a) present three multiclass versions of the Perceptron algorithm an d a new margin based online multiclass algorithm named MIRA."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 41
                            }
                        ],
                        "text": "Furthermo re, the loss bounds derived in Crammer and Singer (2003b) are inferior and less general th an the bounds derived in this paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10135658,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59571f7460fe6df6b2741dc9598fe6f1ec1d4493",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new family of topic-ranking algorithms for multi-labeled documents. The motivation for the algorithms stems from recent advances in online learning algorithms. The algorithms we present are simple to implement and are time and memory efficient. We evaluate the algorithms on the Reuters-21578 corpus and the new corpus released by Reuters in 2000. On both corpora the algorithms we present outperform adaptations to topic-ranking of Rocchio's algorithm and the Perceptron algorithm. We also outline the formal analysis of the algorithm in the mistake bound model. To our knowledge, this work is the first to report performance results with the entire new Reuters corpus."
            },
            "slug": "A-new-family-of-online-algorithms-for-category-Crammer-Singer",
            "title": {
                "fragments": [],
                "text": "A new family of online algorithms for category ranking"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This work describes a new family of topic-ranking algorithms for multi-labeled documents and outlines the formal analysis of the algorithm in the mistake bound model, which is the first to report performance results with the entire new Reuters corpus."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '02"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772099"
                        ],
                        "name": "D. Helmbold",
                        "slug": "D.-Helmbold",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Helmbold",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Helmbold"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 205937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd43a1091eacc41a8c686cd11185419927138532",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the problem of estimating the proportion vector which maximizes the likelihood of a given sample for a mixture of given densities. We adapt a framework developed for supervised learning and give simple derivations for many of the standard iterative algorithms like gradient projection and EM. In this framework, the distance between the new and old proportion vectors is used as a penalty term. The square distance leads to the gradient projection update, and the relative entropy to a new update which we call the exponentiated gradient update (EG\u03ae). Curiously, when a second order Taylor expansion of the relative entropy is used, we arrive at an update EM\u03ae which, for \u03ae=1, gives the usual EM update. Experimentally, both the EM\u03ae-update and the EG\u03ae-update for \u03ae > 1 outperform the EM algorithm and its variants. We also prove a polynomial bound on the rate of convergence of the EG\u03ae algorithm."
            },
            "slug": "A-Comparison-of-New-and-Old-Algorithms-for-a-Helmbold-Schapire",
            "title": {
                "fragments": [],
                "text": "A Comparison of New and Old Algorithms for a Mixture Estimation Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A framework developed for supervised learning is adapted and simple derivations for many of the standard iterative algorithms like gradient projection and EM are given, and a polynomial bound on the rate of convergence of the EG\u03ae algorithm is proved."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765700"
                        ],
                        "name": "Ioannis Tsochantaridis",
                        "slug": "Ioannis-Tsochantaridis",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Tsochantaridis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ioannis Tsochantaridis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783941"
                        ],
                        "name": "Y. Altun",
                        "slug": "Y.-Altun",
                        "structuredName": {
                            "firstName": "Yasemin",
                            "lastName": "Altun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Altun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 564746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93aa298b40bb3ec23c25239089284fdf61ded917",
            "isKey": false,
            "numCitedBy": 1455,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning general functional dependencies is one of the main goals in machine learning. Recent progress in kernel-based methods has focused on designing flexible and powerful input representations. This paper addresses the complementary issue of problems involving complex outputs such as multiple dependent output variables and structured output spaces. We propose to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs. The resulting optimization problem is solved efficiently by a cutting plane algorithm that exploits the sparseness and structural decomposition of the problem. We demonstrate the versatility and effectiveness of our method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment."
            },
            "slug": "Support-vector-machine-learning-for-interdependent-Tsochantaridis-Hofmann",
            "title": {
                "fragments": [],
                "text": "Support vector machine learning for interdependent and structured output spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs, and demonstrates the versatility and effectiveness of the method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772099"
                        ],
                        "name": "D. Helmbold",
                        "slug": "D.-Helmbold",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Helmbold",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Helmbold"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14556109,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff3dcbb55d485ac07a0b91ff9a2d19ecc4f6d10c",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze and compare the well-known gradient descent algorithm and the more recent exponentiated gradient algorithm for training a single neuron with an arbitrary transfer function. Both algorithms are easily generalized to larger neural networks, and the generalization of gradient descent is the standard backpropagation algorithm. In this paper we prove worst-case loss bounds for both algorithms in the single neuron case. Since local minima make it difficult to prove worst-case bounds for gradient-based algorithms, we must use a loss function that prevents the formation of spurious local minima. We define such a matching loss function for any strictly increasing differentiable transfer function and prove worst-case loss bounds for any such transfer function and its corresponding matching loss. For example, the matching loss for the identity function is the square loss and the matching loss for the logistic transfer function is the entropic loss. The different forms of the two algorithms' bounds indicates that exponentiated gradient outperforms gradient descent when the inputs contain a large number of irrelevant components. Simulations on synthetic data confirm these analytical results."
            },
            "slug": "Relative-loss-bounds-for-single-neurons-Helmbold-Kivinen",
            "title": {
                "fragments": [],
                "text": "Relative loss bounds for single neurons"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This paper analyzes and compares the well-known gradient descent algorithm and the more recent exponentiated gradient algorithm for training a single neuron with an arbitrary transfer function and proves worst-case loss bounds for both algorithms in the single neuron case."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In this case it might be possible to derive general loss bounds, see for example [ 9 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 216
                            }
                        ],
                        "text": "The mere idea of deriving an update as a result of a constrained optimization problem compr ising of two opposing terms, has been largely advocated by Littlestone, Warmuth, Kivinen and colleagues (Littlestone, 1989, Kivinen and Warmuth, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 121
                            }
                        ],
                        "text": "The above formalism is motivated by the work of Warmuth and colleagues for deriving online algorithms (see for instance (Kivinen and Warmuth, 1997) and the references therein)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 138
                            }
                        ],
                        "text": "Online algorithms are typically simple to implement and their analysis often provides tight bounds on their performance (see for instance Kivinen and Warmuth (1997))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 470902,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1554931d9ce2f18e5515b24601c3f18ebe76c80d",
            "isKey": true,
            "numCitedBy": 138,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We study on-line generalized linear regression with multidimensional outputs, i.e., neural networks with multiple output nodes but no hidden nodes. We allow at the final layer transfer functions such as the softmax function that need to consider the linear activations to all the output neurons. The weight vectors used to produce the linear activations are represented indirectly by maintaining separate parameter vectors. We get the weight vector by applying a particular parameterization function to the parameter vector. Updating the parameter vectors upon seeing new examples is done additively, as in the usual gradient descent update. However, by using a nonlinear parameterization function between the parameter vectors and the weight vectors, we can make the resulting update of the weight vector quite different from a true gradient descent update. To analyse such updates, we define a notion of a matching loss function and apply it both to the transfer function and to the parameterization function. The loss function that matches the transfer function is used to measure the goodness of the predictions of the algorithm. The loss function that matches the parameterization function can be used both as a measure of divergence between models in motivating the update rule of the algorithm and as a measure of progress in analyzing its relative performance compared to an arbitrary fixed model. As a result, we have a unified treatment that generalizes earlier results for the gradient descent and exponentiated gradient algorithms to multidimensional outputs, including multiclass logistic regression."
            },
            "slug": "Relative-Loss-Bounds-for-Multidimensional-Problems-Kivinen-Warmuth",
            "title": {
                "fragments": [],
                "text": "Relative Loss Bounds for Multidimensional Regression Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A unified treatment of on-line generalized linear regression with multidimensional outputs, i.e., neural networks with multiple output nodes but no hidden nodes, is studied, which generalizes earlier results for the gradient descent and exponentiated gradient algorithms to multiddimensional outputs, including multiclass logistic regression."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766703"
                        ],
                        "name": "A. Elisseeff",
                        "slug": "A.-Elisseeff",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Elisseeff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Elisseeff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Since we assumed that`t > 0, we know that\u0300 t = \u2016wt \u2212 yt\u2016 \u2212 and\u03c4 is thus equal to `t1+1/(2C) ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 221
                            }
                        ],
                        "text": "This definition generalizes the definition of margin for binary classification and was employed by both single-label and multilabel learning algorithms for support vector machines (Vapnik, 1998, Weston and Watkins, 1999, Elisseeff and Weston, 2001, Crammer and Singer, 2003a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1976599,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30b2a3422332a76663110beae4bfc4d74763f4a0",
            "isKey": false,
            "numCitedBy": 1246,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents a Support Vector Machine (SVM) like learning system to handle multi-label problems. Such problems are usually decomposed into many two-class problems but the expressive power of such a system can be weak [5, 7]. We explore a new direct approach. It is based on a large margin ranking system that shares a lot of common properties with SVMs. We tested it on a Yeast gene functional classification problem with positive results."
            },
            "slug": "A-kernel-method-for-multi-labelled-classification-Elisseeff-Weston",
            "title": {
                "fragments": [],
                "text": "A kernel method for multi-labelled classification"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This article presents a Support Vector Machine like learning system to handle multi-label problems, based on a large margin ranking system that shares a lot of common properties with SVMs."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716876"
                        ],
                        "name": "O. Dekel",
                        "slug": "O.-Dekel",
                        "structuredName": {
                            "firstName": "Ofer",
                            "lastName": "Dekel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Dekel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389955537"
                        ],
                        "name": "S. Shalev-Shwartz",
                        "slug": "S.-Shalev-Shwartz",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Shalev-Shwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shalev-Shwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 70
                            }
                        ],
                        "text": "The prediction-based and max-loss updates were previouslydiscussed in Dekel et al. (2004a), in the context of hierarchical classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 126
                            }
                        ],
                        "text": "We already conducted further research on applications of the PA algorithmic framework f r learning margin-based suffix trees (Dekel et al., 2004b), pseudo-metrics (Shalev-Shwartz et al., 2004b), hierarchical classification (Dekel et al., 2004a), and segmentation of sequences (Shalev-Shwartz et al.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Dekel et al. (2004a) evaluated both techniques empiricallyand found them to be highly effective on speech recognition and text classification tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 214
                            }
                        ],
                        "text": "\u2026conducted further research on applications of the PA algorithmic framework f r learning margin-based suffix trees (Dekel et al., 2004b), pseudo-metrics (Shalev-Shwartz et al., 2004b), hierarchical classification (Dekel et al., 2004a), and segmentation of sequences (Shalev-Shwartz et al., 2004a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9670064,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f50e6feecd60406d9ca3d473dde4c6fd27254d56",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Prediction suffix trees (PST) provide a popular and effective tool for tasks such as compression, classification, and language modeling. In this paper we take a decision theoretic view of PSTs for the task of sequence prediction. Generalizing the notion of margin to PSTs, we present an online PST learning algorithm and derive a loss bound for it. The depth of the PST generated by this algorithm scales linearly with the length of the input. We then describe a self-bounded enhancement of our learning algorithm which automatically grows a bounded-depth PST. We also prove an analogous mistake-bound for the self-bounded algorithm. The result is an efficient algorithm that neither relies on a-priori assumptions on the shape or maximal depth of the target PST nor does it require any parameters. To our knowledge, this is the first provably-correct PST learning algorithm which generates a bounded-depth PST while being competitive with any fixed PST determined in hindsight."
            },
            "slug": "The-Power-of-Selective-Memory:-Self-Bounded-of-Dekel-Shalev-Shwartz",
            "title": {
                "fragments": [],
                "text": "The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper takes a decision theoretic view of PSTs for the task of sequence prediction and presents an online PST learning algorithm that generates a bounded-depth PST while being competitive with any fixed PST determined in hindsight."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895207"
                        ],
                        "name": "C. Gentile",
                        "slug": "C.-Gentile",
                        "structuredName": {
                            "firstName": "Claudio",
                            "lastName": "Gentile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gentile"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We demonstrate some of the merits of the proposed algorithms in a series of experiments with synthetic and real datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 97
                            }
                        ],
                        "text": "More modern examples include the ROMMA algorithm of Li and Long (2002), Gentile\u2019s ALMA algorithm (Gentile, 2001), the MIRA algorithm (Crammer and Singer, 2003b), and the NORMA algorithm (Kivinen et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8756196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f5f56c26ad2f3c07049b88ff74ed4c5fa7b2c12",
            "isKey": false,
            "numCitedBy": 255,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "A new incremental learning algorithm is described which approximates the maximal margin hyperplane w.r.t. norm p \u2265 2 for a set of linearly separable data. Our algorithm, called ALMA_p (Approximate Large Margin algorithm w.r.t. norm p), takes O( (p-1) / (\u03b12 \u03b32 ) ) corrections to separate the data with p-norm margin larger than (1-\u03b1)\u03b3, where g is the (normalized) p-norm margin of the data. ALMA_p avoids quadratic (or higher-order) programming methods. It is very easy to implement and is as fast as on-line algorithms, such as Rosenblatt's Perceptron algorithm. We performed extensive experiments on both real-world and artificial datasets. We compared ALMA_2 (i.e., ALMA_p with p = 2) to standard Support vector Machines (SVM) and to two incremental algorithms: the Perceptron algorithm and Li and Long's ROMMA. The accuracy levels achieved by ALMA_2 are superior to those achieved by the Perceptron algorithm and ROMMA, but slightly inferior to SVM's. On the other hand, ALMA_2 is quite faster and easier to implement than standard SVM training algorithms. When learning sparse target vectors, ALMA_p with p > 2 largely outperforms Perceptron-like algorithms, such as ALMA_2."
            },
            "slug": "A-New-Approximate-Maximal-Margin-Classification-Gentile",
            "title": {
                "fragments": [],
                "text": "A New Approximate Maximal Margin Classification Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A new incremental learning algorithm is described which approximates the maximal margin hyperplane w.r.t. norm p \u2265 2 for a set of linearly separable data and is as fast as on-line algorithms, such as Rosenblatt's Perceptron algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 106
                            }
                        ],
                        "text": "Furthermore, an analogous optimization problem arises in support vectormachines (SVM) for classification (Vapnik, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The updates we derive are based on an optimization problem directly related to the one employed by Support Vector Machines [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 73
                            }
                        ],
                        "text": "We adopt the technique previously used to derive soft-margin classifier (Vapnik, 1998) and introduce a nonnegative slack variable\u03be into the optimization problem defined in Eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 130
                            }
                        ],
                        "text": "For concreteness, our presentation and analysis are confined to the linear case which is often referred to as the primal version (Vapnik, 1998, Cristianini and Shawe-Taylor, 2000, Scho\u0308lkopf and Smola, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 181
                            }
                        ],
                        "text": "This definition generalizes the definition of margin for binary classification and was employed by both single-label and multilabel learning algorithms for support vector machines (Vapnik, 1998, Weston and Watkins, 1999, Elisseeff and Weston, 2001, Crammer and Singer, 2003a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 191
                            }
                        ],
                        "text": "The common construction in such settings is to assume that each instance is a vector in n and to\nassociate a different weight vector (often referred to as prototype) with each of thek label (Vapnik, 1998, Weston and Watkins, 1999, Crammer and Singer, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This representation enables us to employ kernels [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 62
                            }
                        ],
                        "text": "While this class may seem restrictive, the pioneeri g work of Vapnik (1998) and colleagues demonstrates that by using Mercer kernels one can employ highly non-linear predictors and still entertain all the formal properties and simplicity oflinear predictors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[15]) that the loss of wT on unseen data is small as well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": true,
            "numCitedBy": 26322,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4562073"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Watkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This definition generalizes the definition of margin for binary classification and was employed by both single-label and multilabel learning algorithms for support vector machines (Vapnik, 1998; Weston and Watkins, 1999; Elisseeff and Weston, 2001; Crammer and Singer, 2003a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 205
                            }
                        ],
                        "text": "The common construction in such settings is to assume that each instance is a vector in n and to\nassociate a different weight vector (often referred to as prototype) with each of thek label (Vapnik, 1998, Weston and Watkins, 1999, Crammer and Singer, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 195
                            }
                        ],
                        "text": "This definition generalizes the definition of margin for binary classification and was employed by both single-label and multilabel learning algorithms for support vector machines (Vapnik, 1998, Weston and Watkins, 1999, Elisseeff and Weston, 2001, Crammer and Singer, 2003a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The common construction in such settings is to assume that each instance is a vector in Rn and to associate a different weight vector (often referred to as prototype) with each of the k labels (Vapnik, 1998; Weston and Watkins, 1999; Crammer and Singer, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15059183,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b74aa66dd2e4ac0f9be884f810eab6c3cbd57ba",
            "isKey": true,
            "numCitedBy": 847,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The solution of binary classi cation problems using support vector machines (SVMs) is well developed, but multi-class problems with more than two classes have typically been solved by combining independently produced binary classi ers. We propose a formulation of the SVM that enables a multi-class pattern recognition problem to be solved in a single optimisation. We also propose a similar generalization of linear programming machines. We report experiments using bench-mark datasets in which these two methods achieve a reduction in the number of support vectors and kernel calculations needed. 1. k-Class Pattern Recognition The k-class pattern recognition problem is to construct a decision function given ` iid (independent and identically distributed) samples (points) of an unknown function, typically with noise: (x1; y1); : : : ; (x`; y`) (1) where xi; i = 1; : : : ; ` is a vector of length d and yi 2 f1; : : : ; kg represents the class of the sample. A natural loss function is the number of mistakes made. 2. Solving k-Class Problems with Binary SVMs For the binary pattern recognition problem (case k = 2), the support vector approach has been well developed [3, 5]. The classical approach to solving k-class pattern recognition problems is to consider the problem as a collection of binary classi cation problems. In the one-versus-rest method one constructs k classi ers, one for each class. The n classi er constructs a hyperplane between class n and the k 1 other classes. A particular point is assigned to the class for which the distance from the margin, in the positive direction (i.e. in the direction in which class \\one\" lies rather than class \\rest\"), is maximal. This method has been used widely in ESANN'1999 proceedings European Symposium on Artificial Neural Networks Bruges (Belgium), 21-23 April 1999, D-Facto public., ISBN 2-600049-9-X, pp. 219-224"
            },
            "slug": "Support-vector-machines-for-multi-class-pattern-Weston-Watkins",
            "title": {
                "fragments": [],
                "text": "Support vector machines for multi-class pattern recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A formulation of the SVM is proposed that enables a multi-class pattern recognition problem to be solved in a single optimisation and a similar generalization of linear programming machines is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ESANN"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713257"
                        ],
                        "name": "Heinz H. Bauschke",
                        "slug": "Heinz-H.-Bauschke",
                        "structuredName": {
                            "firstName": "Heinz",
                            "lastName": "Bauschke",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heinz H. Bauschke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705817"
                        ],
                        "name": "J. Borwein",
                        "slug": "J.-Borwein",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Borwein",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Borwein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[1, 2])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17608448,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3fee9ae75afa07b619eadcf1965e9e916e066dc3",
            "isKey": false,
            "numCitedBy": 1528,
            "numCiting": 149,
            "paperAbstract": {
                "fragments": [],
                "text": "Due to their extraordinary utility and broad applicability in many areas of classical mathematics and modern physical sciences (most notably, computerized tomography), algorithms for solving convex feasibility problems continue to receive great attention. To unify, generalize, and review some of these algorithms, a very broad and flexible framework is investigated. Several crucial new concepts which allow a systematic discussion of questions on behaviour in general Hilbert spaces and on the quality of convergence are brought out. Numerous examples are given."
            },
            "slug": "On-Projection-Algorithms-for-Solving-Convex-Bauschke-Borwein",
            "title": {
                "fragments": [],
                "text": "On Projection Algorithms for Solving Convex Feasibility Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A very broad and flexible framework is investigated which allows a systematic discussion of questions on behaviour in general Hilbert spaces and on the quality of convergence in convex feasibility problems."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Rev."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686506"
                        ],
                        "name": "A. Atiya",
                        "slug": "A.-Atiya",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Atiya",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Atiya"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 180
                            }
                        ],
                        "text": "For concreteness, our presentation and analysis are confined to the linear case which is often referred to as the primal version (Vapnik, 1998, Cristianini and Shawe-Taylor, 2000, Scho\u0308lkopf and Smola, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For concreteness, our presentation and analysis are confined to the linear case which is often referred to as the primal version (Vapnik, 1998; Cristianini and Shawe-Taylor, 2000; Sch\u00f6lkopf and Smola, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7406938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ff61b8e097ccdb784a35b466ba9e130c2502513",
            "isKey": false,
            "numCitedBy": 5534,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Chapters 2\u20137 make up Part II of the book: artificial neural networks. After introducing the basic concepts of neurons and artificial neuron learning rules in Chapter 2, Chapter 3 describes a particular formalism, based on signal-plus-noise, for the learning problem in general. After presenting the basic neural network types this chapter reviews the principal algorithms for error function minimization/optimization and shows how these learning issues are addressed in various supervised models. Chapter 4 deals with issues in unsupervised learning networks, such as the Hebbian learning rule, principal component learning, and learning vector quantization. Various techniques and learning paradigms are covered in Chapters 3\u20136, and especially the properties and relative merits of the multilayer perceptron networks, radial basis function networks, self-organizing feature maps and reinforcement learning are discussed in the respective four chapters. Chapter 7 presents an in-depth examination of performance issues in supervised learning, such as accuracy, complexity, convergence, weight initialization, architecture selection, and active learning. Par III (Chapters 8\u201315) offers an extensive presentation of techniques and issues in evolutionary computing. Besides the introduction to the basic concepts in evolutionary computing, it elaborates on the more important and most frequently used techniques on evolutionary computing paradigm, such as genetic algorithms, genetic programming, evolutionary programming, evolutionary strategies, differential evolution, cultural evolution, and co-evolution, including design aspects, representation, operators and performance issues of each paradigm. The differences between evolutionary computing and classical optimization are also explained. Part IV (Chapters 16 and 17) introduces swarm intelligence. It provides a representative selection of recent literature on swarm intelligence in a coherent and readable form. It illustrates the similarities and differences between swarm optimization and evolutionary computing. Both particle swarm optimization and ant colonies optimization are discussed in the two chapters, which serve as a guide to bringing together existing work to enlighten the readers, and to lay a foundation for any further studies. Part V (Chapters 18\u201321) presents fuzzy systems, with topics ranging from fuzzy sets, fuzzy inference systems, fuzzy controllers, to rough sets. The basic terminology, underlying motivation and key mathematical models used in the field are covered to illustrate how these mathematical tools can be used to handle vagueness and uncertainty. This book is clearly written and it brings together the latest concepts in computational intelligence in a friendly and complete format for undergraduate/postgraduate students as well as professionals new to the field. With about 250 pages covering such a wide variety of topics, it would be impossible to handle everything at a great length. Nonetheless, this book is an excellent choice for readers who wish to familiarize themselves with computational intelligence techniques or for an overview/introductory course in the field of computational intelligence. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond\u2014Bernhard Sch\u00f6lkopf and Alexander Smola, (MIT Press, Cambridge, MA, 2002, ISBN 0-262-19475-9). Reviewed by Amir F. Atiya."
            },
            "slug": "Learning-with-Kernels:-Support-Vector-Machines,-and-Atiya",
            "title": {
                "fragments": [],
                "text": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This book is an excellent choice for readers who wish to familiarize themselves with computational intelligence techniques or for an overview/introductory course in the field of computational intelligence."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50263663"
                        ],
                        "name": "D. Boswell",
                        "slug": "D.-Boswell",
                        "structuredName": {
                            "firstName": "Dustin",
                            "lastName": "Boswell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Boswell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 144
                            }
                        ],
                        "text": "For concreteness, our presentation and analysis are confined to the linear case which is often referred to as the primal version (Vapnik, 1998, Cristianini and Shawe-Taylor, 2000, Scho\u0308lkopf and Smola, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 18
                            }
                        ],
                        "text": "(see for insta ce Cristianini and Shawe-Taylor (2000))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 17
                            }
                        ],
                        "text": "As before, we initializew1 = (0, . . . , 0)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18986102,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea2ea7c6e280c1cfb67ee38ea63a327b1ba3ca36",
            "isKey": false,
            "numCitedBy": 2113,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Support Vector Machines (SVM\u2019s) are a relatively new learning method used for binary classification. The basic idea is to find a hyperplane which separates the d-dimensional data perfectly into its two classes. However, since example data is often not linearly separable, SVM\u2019s introduce the notion of a \u201ckernel induced feature space\u201d which casts the data into a higher dimensional space where the data is separable. Typically, casting into such a space would cause problems computationally, and with overfitting. The key insight used in SVM\u2019s is that the higher-dimensional space doesn\u2019t need to be dealt with directly (as it turns out, only the formula for the dot-product in that space is needed), which eliminates the above concerns. Furthermore, the VC-dimension (a measure of a system\u2019s likelihood to perform well on unseen data) of SVM\u2019s can be explicitly calculated, unlike other learning methods like neural networks, for which there is no measure. Overall, SVM\u2019s are intuitive, theoretically wellfounded, and have shown to be practically successful. SVM\u2019s have also been extended to solve regression tasks (where the system is trained to output a numerical value, rather than \u201cyes/no\u201d classification)."
            },
            "slug": "Introduction-to-Support-Vector-Machines-Boswell",
            "title": {
                "fragments": [],
                "text": "Introduction to Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "Support Vector Machines (SVM\u2019s) are intuitive, theoretically wellfounded, and have shown to be practically successful."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31577094"
                        ],
                        "name": "T. Motzkin",
                        "slug": "T.-Motzkin",
                        "structuredName": {
                            "firstName": "Theodore",
                            "lastName": "Motzkin",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Motzkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50456127"
                        ],
                        "name": "I. J. Schoenberg",
                        "slug": "I.-J.-Schoenberg",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Schoenberg",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. J. Schoenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123653448,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3bd7fe1a5c67bb2f1d445e95816922c11db78ae8",
            "isKey": false,
            "numCitedBy": 460,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Let A be a closed set of points in the n-dimensional euclidean space En. If p and p 1 are points of En such that 1.1 then p 1 is said to be point-wise closer than p to the set A. If p is such that there is no point p1 which is point-wise closer than p to A, then p is called a closest point to the set A."
            },
            "slug": "The-Relaxation-Method-for-Linear-Inequalities-Motzkin-Schoenberg",
            "title": {
                "fragments": [],
                "text": "The Relaxation Method for Linear Inequalities"
            },
            "venue": {
                "fragments": [],
                "text": "Canadian Journal of Mathematics"
            },
            "year": 1954
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2185716,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7a07c3aaef303850e5a1fcc81bb44f6d2db6696",
            "isKey": false,
            "numCitedBy": 2272,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "This work focuses on algorithms which learn from examples to perform multiclass text and speech categorization tasks. Our approach is based on a new and improved family of boosting algorithms. We describe in detail an implementation, called BoosTexter, of the new boosting algorithms for text categorization tasks. We present results comparing the performance of BoosTexter and a number of other text-categorization algorithms on a variety of tasks. We conclude by describing the application of our system to automatic call-type identification from unconstrained spoken customer responses."
            },
            "slug": "BoosTexter:-A-Boosting-based-System-for-Text-Schapire-Singer",
            "title": {
                "fragments": [],
                "text": "BoosTexter: A Boosting-based System for Text Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work describes in detail an implementation, called BoosTexter, of the new boosting algorithms for text categorization tasks, and presents results comparing the performance of Boos Texter and a number of other text-categorization algorithms on a variety of tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060101052"
                        ],
                        "name": "Terry Koo",
                        "slug": "Terry-Koo",
                        "structuredName": {
                            "firstName": "Terry",
                            "lastName": "Koo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Terry Koo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 405878,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "844db702be4bc149b06b822b47247e15f5894cc3",
            "isKey": false,
            "numCitedBy": 776,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 F-measure, a 13 relative decrease in F-measure error over the baseline model's score of 88.2. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative-in terms of both simplicity and efficiency-to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation."
            },
            "slug": "Discriminative-Reranking-for-Natural-Language-Collins-Koo",
            "title": {
                "fragments": [],
                "text": "Discriminative Reranking for Natural Language Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The boosting approach to ranking problems described in Freund et al. (1998) is applied to parsing the Wall Street Journal treebank, and it is argued that the method is an appealing alternative-in terms of both simplicity and efficiency-to work on feature selection methods within log-linear (maximum-entropy) models."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8195063"
                        ],
                        "name": "Martin A. Zinkevich",
                        "slug": "Martin-A.-Zinkevich",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Zinkevich",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin A. Zinkevich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Last, we would like to note we are currently exploring generalizations of our framework to other decision tasks such as distance-learning [12] and online convex programming [ 13 ]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 553962,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1f153c6df86d1ca8ecb9561daddfe7a54f901e7",
            "isKey": false,
            "numCitedBy": 1943,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Convex programming involves a convex set F \u2286 Rn and a convex cost function c : F \u2192 R. The goal of convex programming is to find a point in F which minimizes c. In online convex programming, the convex set is known in advance, but in each step of some repeated optimization problem, one must select a point in F before seeing the cost function for that step. This can be used to model factory production, farm production, and many other industrial optimization problems where one is unaware of the value of the items produced until they have already been constructed. We introduce an algorithm for this domain. We also apply this algorithm to repeated games, and show that it is really a generalization of infinitesimal gradient ascent, and the results here imply that generalized infinitesimal gradient ascent (GIGA) is universally consistent."
            },
            "slug": "Online-Convex-Programming-and-Generalized-Gradient-Zinkevich",
            "title": {
                "fragments": [],
                "text": "Online Convex Programming and Generalized Infinitesimal Gradient Ascent"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An algorithm for convex programming is introduced, and it is shown that it is really a generalization of infinitesimal gradient ascent, and the results here imply that generalized inf initesimalgradient ascent (GIGA) is universally consistent."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767325"
                        ],
                        "name": "Y. Censor",
                        "slug": "Y.-Censor",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Censor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Censor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732625"
                        ],
                        "name": "S. Zenios",
                        "slug": "S.-Zenios",
                        "structuredName": {
                            "firstName": "Stavros",
                            "lastName": "Zenios",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Zenios"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "The algorithms discussed in this paper make predictions using a classification function which theymaintain in their internal memory and update from round to round."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": "Additionally, we can obtain the PA-I and PA-II variants for online regression by introducing a slack variable into the optimization problem in Eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 115480745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d044b2ece3579ad1d6452f725524694a078b4cc",
            "isKey": false,
            "numCitedBy": 393,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Foreword Preface Glossary of Symbols 1. Introduction Part I Theory 2. Generalized Distances and Generalized Projections 3. Proximal Minimization with D-Functions Part II Algorithms 4. Penalty Methods, Barrier Methods and Augmented Lagrangians 5. Iterative Methods for Convex Feasibility Problems 6. Iterative Algorithms for Linearly Constrained Optimization Problems 7. Model Decomposition Algorithms 8. Decompositions in Interior Point Algorithms Part III Applications 9. Matrix Estimation Problems 10. Image Reconsturction from Projections 11. The Inverse Problem in Radiation Therapy Treatment Planning 12. Multicommodity Network Flow Problems 13. Planning Under Uncertainty 14. Decompositions for Parallel Computing 15. Numerical Investigations"
            },
            "slug": "Parallel-Optimization:-Theory,-Algorithms,-and-Censor-Zenios",
            "title": {
                "fragments": [],
                "text": "Parallel Optimization: Theory, Algorithms, and Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783941"
                        ],
                        "name": "Y. Altun",
                        "slug": "Y.-Altun",
                        "structuredName": {
                            "firstName": "Yasemin",
                            "lastName": "Altun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Altun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765700"
                        ],
                        "name": "Ioannis Tsochantaridis",
                        "slug": "Ioannis-Tsochantaridis",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Tsochantaridis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ioannis Tsochantaridis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9699301,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5fe5ed2a3b50becdbbcd17e7733653d5ef6ac398",
            "isKey": false,
            "numCitedBy": 556,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel discriminative learning technique for label sequences based on a combination of the two most successful learning algorithms, Support Vector Machines and Hidden Markov Models which we call Hidden Markov Support Vector Machine. The proposed architecture handles dependencies between neighboring labels using Viterbi decoding. In contrast to standard HMM training, the learning procedure is discriminative and is based on a maximum/soft margin criterion. Compared to previous methods like Conditional Random Fields, Maximum Entropy Markov Models and label sequence boosting, HM-SVMs have a number of advantages. Most notably, it is possible to learn non-linear discriminant functions via kernel functions. At the same time, HM-SVMs share the key advantages with other discriminative methods, in particular the capability to deal with overlapping features. We report experimental evaluations on two tasks, named entity recognition and part-of-speech tagging, that demonstrate the competitiveness of the proposed approach."
            },
            "slug": "Hidden-Markov-Support-Vector-Machines-Altun-Tsochantaridis",
            "title": {
                "fragments": [],
                "text": "Hidden Markov Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "This paper presents a novel discriminative learning technique for label sequences based on a combination of the two most successful learning algorithms, Support Vector Machines and Hidden Markov Models which it is called HM-SVMs and handles dependencies between neighboring labels using Viterbi decoding."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 144
                            }
                        ],
                        "text": "For concreteness, our presentation and analysis are confined to the linear case which is often referred to as the primal version (Vapnik, 1998, Cristianini and Shawe-Taylor, 2000, Scho\u0308lkopf and Smola, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 18
                            }
                        ],
                        "text": "(see for insta ce Cristianini and Shawe-Taylor (2000))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14727192,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c04f8002e24a8c09bfbfedca3c6c346fe1e5d53",
            "isKey": false,
            "numCitedBy": 13352,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "From the publisher: This is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory. SVMs deliver state-of-the-art performance in real-world applications such as text categorisation, hand-written character recognition, image classification, biosequences analysis, etc., and are now established as one of the standard tools for machine learning and data mining. Students will find the book both stimulating and accessible, while practitioners will be guided smoothly through the material required for a good grasp of the theory and its applications. The concepts are introduced gradually in accessible and self-contained stages, while the presentation is rigorous and thorough. Pointers to relevant literature and web sites containing software ensure that it forms an ideal starting point for further study. Equally, the book and its associated web site will guide practitioners to updated literature, new applications, and on-line software."
            },
            "slug": "An-Introduction-to-Support-Vector-Machines-and-Cristianini-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "This is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory, and will guide practitioners to updated literature, new applications, and on-line software."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Since we assumed that`t > 0, we know that\u0300 t = \u2016wt \u2212 yt\u2016 \u2212 and\u03c4 is thus equal to `t1+1/(2C) ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 124
                            }
                        ],
                        "text": "We describe a variant of theTerm Frequency - Inverse Document Frequency(TF-IDF) representation of documents (Rocchio, 1971, Salton and Buckley, 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7725217,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e50a316f97c9a405aa000d883a633bd5707f1a34",
            "isKey": false,
            "numCitedBy": 9463,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Term-Weighting-Approaches-in-Automatic-Text-Salton-Buckley",
            "title": {
                "fragments": [],
                "text": "Term-Weighting Approaches in Automatic Text Retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Manag."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389955537"
                        ],
                        "name": "S. Shalev-Shwartz",
                        "slug": "S.-Shalev-Shwartz",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Shalev-Shwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shalev-Shwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771345"
                        ],
                        "name": "Joseph Keshet",
                        "slug": "Joseph-Keshet",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Keshet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Keshet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 155
                            }
                        ],
                        "text": "We have used this observation in devising online algorithms for the task of music to score alignment which we cast as learning with structured output (see Shalev-Shwartz et al. (2004a))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 154
                            }
                        ],
                        "text": "\u2026conducted further research on applications of the PA algorithmic framework f r learning margin-based suffix trees (Dekel et al., 2004b), pseudo-metrics (Shalev-Shwartz et al., 2004b), hierarchical classification (Dekel et al., 2004a), and segmentation of sequences (Shalev-Shwartz et al., 2004a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12956854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d03435382eaa31b4b558788906f975d209580ed0",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an efficient learning algorithm for aligning a symbolic representation of a musical piece with its acoustic counterpart. Our method employs a supervised learning approach by using a training set of aligned symbolic and acoustic representations. The alignment function we devise is based on mapping the input acousticsymbolic representation along with the target alignment into an abstract vector-space. Building on techniques used for learning support vector machines (SVM), our alignment function distills to a classifier in the abstract vectorspace which separates correct alignments from incorrect ones. We describe a simple iterative algorithm for learning the alignment function and discuss its formal properties. We use our method for aligning MIDI and MP3 representations of polyphonic recordings of piano music. We also compare our discriminative approach to a generative method based on a generalization of hidden Markov models. In all of our experiments, the discriminative method outperforms the HMM-based method."
            },
            "slug": "Learning-to-Align-Polyphonic-Music-Shalev-Shwartz-Keshet",
            "title": {
                "fragments": [],
                "text": "Learning to Align Polyphonic Music"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This work describes an efficient learning algorithm for aligning a symbolic representation of a musical piece with its acoustic counterpart and compares its discriminative approach to a generative method based on a generalization of hidden Markov models."
            },
            "venue": {
                "fragments": [],
                "text": "ISMIR"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51083130"
                        ],
                        "name": "F. Rosenblatt",
                        "slug": "F.-Rosenblatt",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Rosenblatt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rosenblatt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The roots of many of the papers date back to the Perceptron algorithm (Agmon, 1954; Rosenblatt, 1958; Novikoff, 1962)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 84
                            }
                        ],
                        "text": "The roots of many of the papers date back to the Perceptron algorithm (Agmon, 1954, Rosenblatt, 1958, Novikoff, 1962)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12781225,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "5d11aad09f65431b5d3cb1d85328743c9e53ba96",
            "isKey": false,
            "numCitedBy": 9075,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The first of these questions is in the province of sensory physiology, and is the only one for which appreciable understanding has been achieved. This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory. With regard to the second question, two alternative positions have been maintained. The first suggests that storage of sensory information is in the form of coded representations or images, with some sort of one-to-one mapping between the sensory stimulus"
            },
            "slug": "The-perceptron:-a-probabilistic-model-for-storage-Rosenblatt",
            "title": {
                "fragments": [],
                "text": "The perceptron: a probabilistic model for information storage and organization in the brain."
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1958
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "134211067"
                        ],
                        "name": "J. Rocchio",
                        "slug": "J.-Rocchio",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Rocchio",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rocchio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61859400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4083ad1066cfa2ff0d65866ef4b011399d6873d1",
            "isKey": false,
            "numCitedBy": 3242,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Relevance-feedback-in-information-retrieval-Rocchio",
            "title": {
                "fragments": [],
                "text": "Relevance feedback in information retrieval"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[15]) thatthelossof wT onunseendatais smallaswell."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "This representationenablesusto employ kernels[15]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 106
                            }
                        ],
                        "text": "Furthermore, an analogous optimization problem arises in support vectormachines (SVM) for classification (Vapnik, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "Theupdateswederivearebasedonanoptimizationproblemdirectly relatedto theoneemployed by SupportVectorMachines[15]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 73
                            }
                        ],
                        "text": "We adopt the technique previously used to derive soft-margin classifier (Vapnik, 1998) and introduce a nonnegative slack variable\u03be into the optimization problem defined in Eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 130
                            }
                        ],
                        "text": "For concreteness, our presentation and analysis are confined to the linear case which is often referred to as the primal version (Vapnik, 1998, Cristianini and Shawe-Taylor, 2000, Scho\u0308lkopf and Smola, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 181
                            }
                        ],
                        "text": "This definition generalizes the definition of margin for binary classification and was employed by both single-label and multilabel learning algorithms for support vector machines (Vapnik, 1998, Weston and Watkins, 1999, Elisseeff and Weston, 2001, Crammer and Singer, 2003a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 191
                            }
                        ],
                        "text": "The common construction in such settings is to assume that each instance is a vector in n and to\nassociate a different weight vector (often referred to as prototype) with each of thek label (Vapnik, 1998, Weston and Watkins, 1999, Crammer and Singer, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 62
                            }
                        ],
                        "text": "While this class may seem restrictive, the pioneeri g work of Vapnik (1998) and colleagues demonstrates that by using Mercer kernels one can employ highly non-linear predictors and still entertain all the formal properties and simplicity oflinear predictors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "StatisticalLearningTheory"
            },
            "venue": {
                "fragments": [],
                "text": "Wiley,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 119
                            }
                        ],
                        "text": "We compared the multiclassver ions of PA, PA-I and PA-II to the online multiclass algorithms described in (Crammer and Singer, 2003a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 27
                            }
                        ],
                        "text": "01, following (Crammer and Singer, 2003a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 126
                            }
                        ],
                        "text": "10.3, we compare our multiclass versions of the PA algorithms to other online algorithms for multiclass problems (Crammer and Singer, 2003a) on natural datasets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 145
                            }
                        ],
                        "text": "More modern examples include the ROMMA algorithm of Li and Long (2002), Gentile\u2019s ALMA algorithm (Gentile, 2001), the MIRA algorithm (Crammer and Singer, 2003b), and the NORMA algorithm (Kivinen et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 132
                            }
                        ],
                        "text": "The MIRA algorithm for binary classification is identical to our basic PA algorithm, however the loss bounds derived in Crammer and Singer (2003b) are inferior to the bounds derived in this paper and are also substantially less general."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "Ourwork alsogeneralizesandgreatlyimprovesonlinelossbounds for classificationgiven in [3]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 261
                            }
                        ],
                        "text": "This definition generalizes the definition of margin for binary classification and was employed by both single-label and multilabel learning algorithms for support vector machines (Vapnik, 1998, Weston and Watkins, 1999, Elisseeff and Weston, 2001, Crammer and Singer, 2003a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 26
                            }
                        ],
                        "text": "Specifically, Crammer and Singer (2003a) present three multiclass versions of thePerceptron algorithm and a new margin based online multiclass algorithm named MIRA."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 87
                            }
                        ],
                        "text": "MR (Schapire and Singer, 1998) and adaptations of support vector machines (Crammer and Singer, 2003a) have been devised for the task of label ranking."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ultraconservative onlinealgorithmsfor multiclassproblems.Jornal of MachineLearningResear  ch, 3:951\u2013991,2003"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Li andLong[14] wereamongthefirst to suggestheidea of converting a batchoptimizationprobleminto an online task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "[4, 13, 14]) and is omitteddueto thelackof space."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "This boundis commonto onlinealgorithmsfor classificationsuch asROMMA [14]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 52
                            }
                        ],
                        "text": "More modern examples include the ROMMA algorithm of Li and Long (2002), Gentile\u2019s ALMA algorithm (Gentile, 2001), the MIRA algorithm (Crammer and Singer, 2003b), and the NORMA algorithm (Kivinen et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The relaxed online maximummargin algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning, 46(1\u20133):361\u2013387, "
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 70
                            }
                        ],
                        "text": "The prediction-based and max-loss updates were previouslydiscussed in Dekel et al. (2004a), in the context of hierarchical classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 422,
                                "start": 71
                            }
                        ],
                        "text": "The prediction-based and max-loss updates were previously discussed in Dekel et al. (2004a), in the context of hierarchical classification. In that paper, a predefined hierarchy over the label set was used to induce the cost function \u03c1. The basic online algorithm presented there used the predic tionbased update, whereas the max-loss update was mentioned in t he context of a batch learning setting. Dekel et al. (2004a) evaluated both techniques empirically and found them to be highly effective on speech recognition and text classification tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 126
                            }
                        ],
                        "text": "We already conducted further research on applications of the PA algorithmic framework f r learning margin-based suffix trees (Dekel et al., 2004b), pseudo-metrics (Shalev-Shwartz et al., 2004b), hierarchical classification (Dekel et al., 2004a), and segmentation of sequences (Shalev-Shwartz et al.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 214
                            }
                        ],
                        "text": "\u2026conducted further research on applications of the PA algorithmic framework f r learning margin-based suffix trees (Dekel et al., 2004b), pseudo-metrics (Shalev-Shwartz et al., 2004b), hierarchical classification (Dekel et al., 2004a), and segmentation of sequences (Shalev-Shwartz et al., 2004a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Dekel et al. (2004a) evaluated both techniques empiricallyand found them to be highly effective on speech recognition and text classification tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 71
                            }
                        ],
                        "text": "The prediction-based and max-loss updates were previously discussed in Dekel et al. (2004a), in the context of hierarchical classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The power of sele"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767325"
                        ],
                        "name": "Y. Censor",
                        "slug": "Y.-Censor",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Censor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Censor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 115
                            }
                        ],
                        "text": "Finally, we would like to note that similar algorithms have been devised in the convex optimization community (see [1])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 204676156,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2c0e1de245c9f98870cf1654068d638929d40c6",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Parallel-Optimization:theory-Censor",
            "title": {
                "fragments": [],
                "text": "Parallel Optimization:theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102790204"
                        ],
                        "name": "A. Novikoff",
                        "slug": "A.-Novikoff",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Novikoff",
                            "middleNames": [
                                "B.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Novikoff"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We demonstrate some of the merits of the proposed algorithms in a series of experiments with synthetic and real datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 102
                            }
                        ],
                        "text": "The roots of many of the papers date back to the Perceptron algorithm (Agmon, 1954, Rosenblatt, 1958, Novikoff, 1962)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122810543,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "eba6b55f7d3302471579eedd5c1558b38f4ea8da",
            "isKey": false,
            "numCitedBy": 439,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "ON-CONVERGENCE-PROOFS-FOR-PERCEPTRONS-Novikoff",
            "title": {
                "fragments": [],
                "text": "ON CONVERGENCE PROOFS FOR PERCEPTRONS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Since we assumed that`t > 0, we know that\u0300 t = \u2016wt \u2212 yt\u2016 \u2212 and\u03c4 is thus equal to `t1+1/(2C) ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 109
                            }
                        ],
                        "text": "We describe a variant of theTerm Frequency - Inverse Document Frequency(TF-IDF) representation of documents (Rocchio, 1971, Salton and Buckley, 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61113802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "939d409a615d4b88c0a84e1f9b99ed67a9053208",
            "isKey": false,
            "numCitedBy": 3147,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-SMART-Retrieval-System\u2014Experiments-in-Automatic-Salton",
            "title": {
                "fragments": [],
                "text": "The SMART Retrieval System\u2014Experiments in Automatic Document Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Max-margin markov net"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Warmuth.Relative lossboundsfor multidimensionalregression problems.Journalof MachineLearning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Term weighting approaches in autom"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Zenios.Parallel Optimization. Oxford UniversityPress,1997"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Appendix A. Derivation of the PA-I and PA-II Updates As in Sec. 3, whenever\u0300t = 0 no update"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Distance metric learning"
            },
            "venue": {
                "fragments": [],
                "text": "with application to clustering with side-information. In Advances in Neural Information Processing Systems 15"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hidden markov support"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1954
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On projectionalgorithmsfor solving convex feasibilityproblems.SIAMReview"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Perceptron Learning\". Neural Networks - A Systematic Introduction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "We restrict our discussion to classification functions based on a vector of weightsw \u2208 n , which take the formsign(w \u00b7 x)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parallel Optimization"
            },
            "venue": {
                "fragments": [],
                "text": "Parallel Optimization"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Warmuth.Relativelossboundsfor singleneurons"
            },
            "venue": {
                "fragments": [],
                "text": "IEEETransactionsonNeural Networks,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 97
                            }
                        ],
                        "text": "More modern examples include the ROMMA algorithm of Li and Long (2002), Gentile\u2019s ALMA algorithm (Gentile, 2001), the MIRA algorithm (Crammer and Singer, 2003b), and the NORMA algorithm (Kivinen et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new approximatemaximalmargin classificationalgorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of MachineLearningResear  ch,"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Perceptron Learning Neural Networks -A Systematic Introduction <http://page.mi.fu-berlin.de/rojas/neural/chapterThe Kernel Trick"
            },
            "venue": {
                "fragments": [],
                "text": "October"
            },
            "year": 1996
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 29
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 60,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/Online-Passive-Aggressive-Algorithms-Crammer-Dekel/3ca4d4229b8a843c0847fc70531790df6bd017ec?sort=total-citations"
}