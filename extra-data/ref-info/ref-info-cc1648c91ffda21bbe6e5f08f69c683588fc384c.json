{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073182644"
                        ],
                        "name": "K. Scheffler",
                        "slug": "K.-Scheffler",
                        "structuredName": {
                            "firstName": "Konrad",
                            "lastName": "Scheffler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Scheffler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145259603"
                        ],
                        "name": "S. Young",
                        "slug": "S.-Young",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Young",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Young"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 136
                            }
                        ],
                        "text": "Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15730778,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c471188c103ddb08c2423b5c3d9c0de27d2e9dd8",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a method for automatic design of human-computer dialogue strategies by means of reinforcement learning, using a dialogue simulation tool to model the user behaviour and system recognition performance. To the authors' knowledge this is the first application of a detailed simulation tool to this problem. The simulation tool is trained on a corpus of real user data. Compared to direct state transition modelling, it has the major advantage that different state space representations can be studied without collecting more training data. \n \nWe applied Q-learning with eligibility traces to obtain policies for a telephone-based cinema information system, comparing the effect of different state space representations and evaluation functions. The policies outperformed handcrafted policies that operated in the same restricted state space, and gave performance similar to the original design that had been through several iterations of manual refinement."
            },
            "slug": "Automatic-learning-of-dialogue-strategy-using-and-Scheffler-Young",
            "title": {
                "fragments": [],
                "text": "Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Q-learning with eligibility traces was applied to obtain policies for a telephone-based cinema information system, and the policies outperformed handcrafted policies that operated in the same restricted state space, and gave performance similar to the original design that had been through several iterations of manual refinement."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144830983"
                        ],
                        "name": "Y. Mansour",
                        "slug": "Y.-Mansour",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Mansour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Mansour"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 71
                            }
                        ],
                        "text": "Although there is no closed form solution, policy gradient algorithms (Sutton et al., 2000) estimate the parameters \u03b8 by performing stochastic gradient ascent."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1211821,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a20f0ce0616def7cc9a87446c228906cd5da093b",
            "isKey": false,
            "numCitedBy": 4405,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy."
            },
            "slug": "Policy-Gradient-Methods-for-Reinforcement-Learning-Sutton-McAllester",
            "title": {
                "fragments": [],
                "text": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proves for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "81338045"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737616"
                        ],
                        "name": "D. Litman",
                        "slug": "D.-Litman",
                        "structuredName": {
                            "firstName": "Diane",
                            "lastName": "Litman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Litman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760530"
                        ],
                        "name": "M. Walker",
                        "slug": "M.-Walker",
                        "structuredName": {
                            "firstName": "Marilyn",
                            "lastName": "Walker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Walker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 203
                            }
                        ],
                        "text": "Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6543198,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "839f1b61803e65f20f067b361d0ebf6db337172c",
            "isKey": false,
            "numCitedBy": 255,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, a number of authors have proposed treating dialogue systems as Markov decision processes (MDPs). However, the practical application of MDP algorithms to dialogue systems faces a number of severe technical challenges. We have built a general software tool (RLDS, for Reinforcement Learning for Dialogue Systems) based on the MDP framework, and have applied it to dialogue corpora gathered from two dialogue systems built at AT&T Labs. Our experiments demonstrate that RLDS holds promise as a tool for \"browsing\" and understanding correlations in complex, temporally dependent dialogue corpora."
            },
            "slug": "Reinforcement-Learning-for-Spoken-Dialogue-Systems-Singh-Kearns",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning for Spoken Dialogue Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A general software tool (RLDS, for Reinforcement Learning for Dialogue Systems) based on the MDP framework is built and applied to dialogue corpora gathered from two dialogue systems built at AT&T Labs, demonstrating that RLDS holds promise as a tool for \"browsing\" and understanding correlations in complex, temporally dependent dialogue Corpora."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 103
                            }
                        ],
                        "text": "Reinforcement learning is a natural framework for building models using validation from an environment (Sutton and Barto, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 120
                            }
                        ],
                        "text": "As is commonly done in reinforcement learning, we use a softmax temperature parameter to smooth the policy distribution (Sutton and Barto, 1998), set to 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9166388,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97efafdb4a3942ab3efba53ded7413199f79c054",
            "isKey": false,
            "numCitedBy": 32850,
            "numCiting": 636,
            "paperAbstract": {
                "fragments": [],
                "text": "Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning."
            },
            "slug": "Reinforcement-Learning:-An-Introduction-Sutton-Barto",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning: An Introduction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This book provides a clear and simple account of the key ideas and algorithms of reinforcement learning, which ranges from the history of the field's intellectual foundations to the most recent developments and applications."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144102030"
                        ],
                        "name": "M. Fleischman",
                        "slug": "M.-Fleischman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Fleischman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fleischman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145364504"
                        ],
                        "name": "D. Roy",
                        "slug": "D.-Roy",
                        "structuredName": {
                            "firstName": "Deb",
                            "lastName": "Roy",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 161
                            }
                        ],
                        "text": "Grounded Language Acquisition Our work fits into a broader class of approaches that aim to learn language from a situated context (Mooney, 2008a; Mooney, 2008b; Fleischman and Roy, 2005; Yu and Ballard, 2004; Siskind, 2001; Oates, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5881871,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20aea590de572a824b2afee830a7f3017bfea87d",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural language interfaces designed for situationally embedded domains (e.g. cars, videogames) must incorporate knowledge about the users' context to address the many ambiguities of situated language use. We introduce a model of situated language acquisition that operates in two phases. First, intentional context is represented and inferred from user actions using probabilistic context free grammars. Then, utterances are mapped onto this representation in a noisy channel framework. The acquisition model is trained on unconstrained speech collected from subjects playing an interactive game, and tested on an understanding task."
            },
            "slug": "Intentional-Context-in-Situated-Natural-Language-Fleischman-Roy",
            "title": {
                "fragments": [],
                "text": "Intentional Context in Situated Natural Language Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A model of situated language acquisition that operates in two phases, where intentional context is represented and inferred from user actions using probabilistic context free grammars and utterances are mapped onto this representation in a noisy channel framework."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 130
                            }
                        ],
                        "text": "Grounded Language Acquisition Our work fits into a broader class of approaches that aim to learn language from a situated context (Mooney, 2008a; Mooney, 2008b; Fleischman and Roy, 2005; Yu and Ballard, 2004; Siskind, 2001; Oates, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 38309112,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8dbc7fa065c6f01bf8f4c83bded06c0adba4d54",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Current systems that learn to process natural language require laboriously constructed human-annotated training data. Ideally, a computer would be able to acquire language like a child by being exposed to linguistic input in the context of a relevant but ambiguous perceptual environment. As a step in this direction, we present a system that learns to sportscast simulated robot soccer games by example. The training data consists of textual human commentaries on Robocup simulation games. A set of possible alternative meanings for each comment is automatically constructed from game event traces. Our previously developed systems for learning to parse and generate natural language (KRISP and WASP) were augmented to learn from this data and then commentate novel games. Using this approach, the system has learned to sportscast in both English and Korean. The system has been evaluated based on its ability to properly match sentences to the events being described, parse sentences into correct meanings, and generate accurate linguistic descriptions of events. Human evaluation was also conducted on the overall quality of the generated sportscasts and compared to human-generated commentaries, demonstrating that its sportscasts are on par with those generated by humans."
            },
            "slug": "Learning-Language-from-Its-Perceptual-Context-Mooney",
            "title": {
                "fragments": [],
                "text": "Learning Language from Its Perceptual Context"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A system that learns to sportscast simulated robot soccer games by example using textual human commentaries on Robocup simulation games and has been evaluated based on its ability to properly match sentences to the events being described, parse sentences into correct meanings, and generate accurate linguistic descriptions of events."
            },
            "venue": {
                "fragments": [],
                "text": "PADL"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153642390"
                        ],
                        "name": "David L. Chen",
                        "slug": "David-L.-Chen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David L. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 204
                            }
                        ],
                        "text": "Instances of such approaches include work on inferring the meaning of words from video data (Roy and Pentland, 2002; Barnard and Forsyth, 2001), and interpreting the commentary of a simulated soccer game (Chen and Mooney, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2488088,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0008076a1968d9fb590c9013ab27b824849a4e80",
            "isKey": false,
            "numCitedBy": 233,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel commentator system that learns language from sportscasts of simulated soccer games. The system learns to parse and generate commentaries without any engineered knowledge about the English language. Training is done using only ambiguous supervision in the form of textual human commentaries and simulation states of the soccer games. The system simultaneously tries to establish correspondences between the commentaries and the simulation states as well as build a translation model. We also present a novel algorithm, Iterative Generation Strategy Learning (IGSL), for deciding which events to comment on. Human evaluations of the generated commentaries indicate they are of reasonable quality compared to human commentaries."
            },
            "slug": "Learning-to-sportscast:-a-test-of-grounded-language-Chen-Mooney",
            "title": {
                "fragments": [],
                "text": "Learning to sportscast: a test of grounded language acquisition"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A novel commentator system that learns language from sportscasts of simulated soccer games and uses a novel algorithm, Iterative Generation Strategy Learning (IGSL), for deciding which events to comment on."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143724999"
                        ],
                        "name": "N. Roy",
                        "slug": "N.-Roy",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Roy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Roy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145134886"
                        ],
                        "name": "Joelle Pineau",
                        "slug": "Joelle-Pineau",
                        "structuredName": {
                            "firstName": "Joelle",
                            "lastName": "Pineau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joelle Pineau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 164
                            }
                        ],
                        "text": "Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10903652,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b472a261d183d31c40e1ad4bfb4f63abc2c25d6",
            "isKey": false,
            "numCitedBy": 324,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Spoken dialogue managers have benefited from using stochastic planners such as Markov Decision Processes (MDPs). However, so far, MDPs do not handle well noisy and ambiguous speech utterances. We use a Partially Observable Markov Decision Process (POMDP)-style approach to generate dialogue strategies by inverting the notion of dialogue state; the state represents the user's intentions, rather than the system state. We demonstrate that under the same noisy conditions, a POMDP dialogue manager makes fewer mistakes than an MDP dialogue manager. Furthermore, as the quality of speech recognition degrades, the POMDP dialogue manager automatically adjusts the policy."
            },
            "slug": "Spoken-Dialogue-Management-Using-Probabilistic-Roy-Pineau",
            "title": {
                "fragments": [],
                "text": "Spoken Dialogue Management Using Probabilistic Reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work uses a Partially Observable Markov Decision Process (POMDP)-style approach to generate dialogue strategies by inverting the notion of dialogue state; the state represents the user's intentions, rather than the system state."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 104
                            }
                        ],
                        "text": "Reinforcement learning is a natural framework for building models using validation from an environment (Sutton and Barto, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 121
                            }
                        ],
                        "text": "As is commonly done in reinforcement learning, we use a softmax temperature parameter to smooth the policy distribution (Sutton and Barto, 1998), set to 0.1 in our experiments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61032464,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b1362879e77efef96ab552f5cb1198c2a67204d6",
            "isKey": true,
            "numCitedBy": 5347,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nIn Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability."
            },
            "slug": "Introduction-to-Reinforcement-Learning-Sutton-Barto",
            "title": {
                "fragments": [],
                "text": "Introduction to Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737616"
                        ],
                        "name": "D. Litman",
                        "slug": "D.-Litman",
                        "structuredName": {
                            "firstName": "Diane",
                            "lastName": "Litman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Litman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "81338045"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760530"
                        ],
                        "name": "M. Walker",
                        "slug": "M.-Walker",
                        "structuredName": {
                            "firstName": "Marilyn",
                            "lastName": "Walker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Walker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 182
                            }
                        ],
                        "text": "Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7686656,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42c028978b2260be2cf8cea6c73ee5c5192e8913",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Designing the dialogue strategy of a spoken dialogue system involves many nontrivial choices. This paper presents a reinforcement learning approach for automatically optimizing a dialogue strategy that addresses the technical challenges in applying reinforcement learning to a working dialogue system with human users. We then show that our approach measurably improves performance in an experimental system."
            },
            "slug": "Automatic-Optimization-of-Dialogue-Management-Litman-Kearns",
            "title": {
                "fragments": [],
                "text": "Automatic Optimization of Dialogue Management"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This paper presents a reinforcement learning approach for automatically optimizing a dialogue strategy that addresses the technical challenges in applying reinforcement learning to a working dialogue system with human users and shows that this approach measurably improves performance in an experimental system."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 181
                            }
                        ],
                        "text": "\u2026of d not in W ), and the possible commands and parameters in environment state E .4 We model the policy distribution p(a|s; \u03b8) over this action space in a log-linear fashion (Della Pietra et al., 1997; Lafferty et al., 2001), giving us the flexibility to incorporate a diverse range of features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b951b9f78b98a186ba259027996a48e4189d37e5",
            "isKey": false,
            "numCitedBy": 1305,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing."
            },
            "slug": "Inducing-Features-of-Random-Fields-Pietra-Pietra",
            "title": {
                "fragments": [],
                "text": "Inducing Features of Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 130
                            }
                        ],
                        "text": "Grounded Language Acquisition Our work fits into a broader class of approaches that aim to learn language from a situated context (Mooney, 2008a; Mooney, 2008b; Fleischman and Roy, 2005; Yu and Ballard, 2004; Siskind, 2001; Oates, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8219525,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1899a99686afd05f00dadf75148f8e0b20f546ea",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "To truly understand language, an intelligent system must be able to connect words, phrases, and sentences to its perception of objects and events in the world. Current natural language processing and computer vision systems make extensive use of machine learning to acquire the probabilistic knowledge needed to comprehend linguistic and visual input. However, to date, there has been relatively little work on learning the relationships between the two modalities. In this talk, I will review some of the existing work on learning to connect language and perception, discuss important directions for future research in this area, and argue that the time is now ripe to make a concerted effort to address this important, integrative AI problem."
            },
            "slug": "Learning-to-Connect-Language-and-Perception-Mooney",
            "title": {
                "fragments": [],
                "text": "Learning to Connect Language and Perception"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is argued that the time is now ripe to make a concerted effort to address this important, integrative AI problem of connecting language and perception."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699245"
                        ],
                        "name": "T. Winograd",
                        "slug": "T.-Winograd",
                        "structuredName": {
                            "firstName": "Terry",
                            "lastName": "Winograd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Winograd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 141
                            }
                        ],
                        "text": "The problem of interpreting instructions written in natural language has been widely studied since the early days of artificial intelligence (Winograd, 1972; Di Eugenio, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56798209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb20f121c979b535bbeade5ac06676d627d4ad7d",
            "isKey": false,
            "numCitedBy": 2455,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This paper describes a computer system for understanding English. The system answers questions, executes commands, and accepts information in an interactive English dialog. It is based on the belief that in modeling language understanding, we must deal in an integrated way with all of the aspects of language\u2014syntax, semantics, and inference. The system contains a parser, a recognition grammar of English, programs for semantic analysis, and a general problem solving system. We assume that a computer cannot deal reasonably with language unless it can understand the subject it is discussing. Therefore, the program is given a detailed model of a particular domain. In addition, the system has a simple model of its own mentality. It can remember and discuss its plans and actions as well as carrying them out. It enters into a dialog with a person, responding to English sentences with actions and English replies, asking for clarification when its heuristic programs cannot understand a sentence through the use of syntactic, semantic, contextual, and physical knowledge. Knowledge in the system is represented in the form of procedures, rather than tables of rules or lists of patterns. By developing special procedural representations for syntax, semantics, and inference, we gain flexibility and power. Since each piece of knowledge can be a procedure, it can call directly on any other piece of knowledge in the system."
            },
            "slug": "Understanding-natural-language-Winograd",
            "title": {
                "fragments": [],
                "text": "Understanding natural language"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A computer system for understanding English that contains a parser, a recognition grammar of English, programs for semantic analysis, and a general problem solving system based on the belief that in modeling language understanding, it must deal in an integrated way with all of the aspects of language\u2014syntax, semantics, and inference."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745798"
                        ],
                        "name": "Barbara Maria Di Eugenio",
                        "slug": "Barbara-Maria-Di-Eugenio",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Di Eugenio",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barbara Maria Di Eugenio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18534704,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "70688e0267cc5bc8ad74bf74fcb39bb985aa6dad",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an analysis of purpose clauses in the context of instruction understanding. Such analysis shows that goals affect the interpretation and / or execution of actions, lends support to the proposal of using generation and enablement to model relations between actions, and sheds light on some inference processes necessary to interpret purpose clauses."
            },
            "slug": "Understanding-Natural-Language-Instructions:-The-of-Eugenio",
            "title": {
                "fragments": [],
                "text": "Understanding Natural Language Instructions: The Case of Purpose Clauses"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "Analysis of purpose clauses in the context of instruction understanding shows that goals affect the interpretation and / or execution of actions, lends support to the proposal of using generation and enablement to model relations between actions, and sheds light on some inference processes necessary to interpret purpose clauses."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49717577"
                        ],
                        "name": "H. Kim",
                        "slug": "H.-Kim",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Kim",
                            "middleNames": [
                                "Jin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144797536"
                        ],
                        "name": "S. Sastry",
                        "slug": "S.-Sastry",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Sastry",
                            "middleNames": [
                                "Shankar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sastry"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 156
                            }
                        ],
                        "text": "The algorithm we have presented belongs to a family of policy gradient algorithms that have been successfully used for complex tasks such as robot control (Ng et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7142905,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13e2d5c4d39bc58b42f9004e5b03905f847dfa0f",
            "isKey": false,
            "numCitedBy": 338,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Autonomous helicopter flight represents a challenging control problem, with complex, noisy, dynamics. In this paper, we describe a successful application of reinforcement learning to autonomous helicopter flight. We first fit a stochastic, nonlinear model of the helicopter dynamics. We then use the model to learn to hover in place, and to fly a number of maneuvers taken from an RC helicopter competition."
            },
            "slug": "Autonomous-Helicopter-Flight-via-Reinforcement-Ng-Kim",
            "title": {
                "fragments": [],
                "text": "Autonomous Helicopter Flight via Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper first fit a stochastic, nonlinear model of the helicopter dynamics, then uses the model to learn to hover in place, and to fly a number of maneuvers taken from an RC helicopter competition."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113414328"
                        ],
                        "name": "Fernando Pereira",
                        "slug": "Fernando-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 92
                            }
                        ],
                        "text": "4 We model the policy distribution p(a|s; \u03b8) over this action space in a log-linear fashion (Della Pietra et al., 1997; Lafferty et al., 2001), giving us the flexibility to incorporate a diverse range of features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 202
                            }
                        ],
                        "text": "\u2026of d not in W ), and the possible commands and parameters in environment state E .4 We model the policy distribution p(a|s; \u03b8) over this action space in a log-linear fashion (Della Pietra et al., 1997; Lafferty et al., 2001), giving us the flexibility to incorporate a diverse range of features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 219683473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4ba954b0412773d047dc41231c733de0c1f4926",
            "isKey": false,
            "numCitedBy": 13413,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "slug": "Conditional-Random-Fields:-Probabilistic-Models-for-Lafferty-McCallum",
            "title": {
                "fragments": [],
                "text": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents iterative parameter estimation algorithms for conditional random fields and compares the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116578563"
                        ],
                        "name": "Chen Yu",
                        "slug": "Chen-Yu",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691804"
                        ],
                        "name": "D. Ballard",
                        "slug": "D.-Ballard",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ballard",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ballard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 130
                            }
                        ],
                        "text": "Grounded Language Acquisition Our work fits into a broader class of approaches that aim to learn language from a situated context (Mooney, 2008a; Mooney, 2008b; Fleischman and Roy, 2005; Yu and Ballard, 2004; Siskind, 2001; Oates, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11444648,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "85a0613d2c8d908aa9c3c42a2f3295bbb6ef6a90",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a multimodal learning system that can ground spoken names of objects in their physical referents and learn to recognize those objects simultaneously from naturally co-occurring multisensory input. There are two technical problems involved: (1) the correspondence problem in symbol grounding - how to associate words (symbols) with their perceptually grounded meanings from multiple cooccurrences between words and objects in the physical environment. (2) object learning - how to recognize and categorize visual objects. We argue that those two problems can be fundamentally simplified by considering them in a general system and incorporating the spatio-temporal and cross-modal constraints of multimodal data. The system collects egocentric data including image sequences as well as speech while users perform natural tasks. It is able to automatically infer the meanings of object names from vision, and categorize objects based on teaching signals potentially encoded in speech. The experimental results reported in this paper reveal the effectiveness of using multimodal data and integrating heterogeneous techniques in machine learning, natural language processing and computer vision."
            },
            "slug": "On-the-Integration-of-Grounding-Language-and-Yu-Ballard",
            "title": {
                "fragments": [],
                "text": "On the Integration of Grounding Language and Learning Objects"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A multimodal learning system that can ground spoken names of objects in their physical referents and learn to recognize those objects simultaneously from naturally co-occurring multisensory input and incorporating the spatio-temporal and cross-modal constraints of multimodals."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737754"
                        ],
                        "name": "J. Siskind",
                        "slug": "J.-Siskind",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Siskind",
                            "middleNames": [
                                "Mark"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Siskind"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 130
                            }
                        ],
                        "text": "Grounded Language Acquisition Our work fits into a broader class of approaches that aim to learn language from a situated context (Mooney, 2008a; Mooney, 2008b; Fleischman and Roy, 2005; Yu and Ballard, 2004; Siskind, 2001; Oates, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15738746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d607a7f816436b6a55a11badd260a0cc58b8f8c4",
            "isKey": false,
            "numCitedBy": 298,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an implemented system for recognizing the occurrence of events described by simple spatial-motion verbs in short image sequences. The semantics of these verbs is specified with event-logic expressions that describe changes in the state of force-dynamic relations between the participants of the event. An efficient finite representation is introduced for the infinite sets of intervals that occur when describing liquid and semi-liquid events. Additionally, an efficient procedure using this representation is presented for inferring occurrences of compound events, described with event-logic expressions, from occurrences of primitive events. Using force dynamics and event logic to specify the lexical semantics of events allows the system to be more robust than prior systems based on motion profile."
            },
            "slug": "Grounding-the-Lexical-Semantics-of-Verbs-in-Visual-Siskind",
            "title": {
                "fragments": [],
                "text": "Grounding the Lexical Semantics of Verbs in Visual Perception using Force Dynamics and Event Logic"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "An implemented system for recognizing the occurrence of events described by simple spatial-motion verbs in short image sequences using force dynamics and event logic to specify the lexical semantics of events to be more robust than prior systems based on motion profile."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145364504"
                        ],
                        "name": "D. Roy",
                        "slug": "D.-Roy",
                        "structuredName": {
                            "firstName": "Deb",
                            "lastName": "Roy",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 92
                            }
                        ],
                        "text": "Instances of such approaches include work on inferring the meaning of words from video data (Roy and Pentland, 2002; Barnard and Forsyth, 2001), and interpreting the commentary of a simulated soccer game (Chen and Mooney, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 113992,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98d6a531a741964ea547f5c8533a3b65e04603d9",
            "isKey": false,
            "numCitedBy": 629,
            "numCiting": 170,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-words-from-sights-and-sounds:-a-model-Roy-Pentland",
            "title": {
                "fragments": [],
                "text": "Learning words from sights and sounds: a computational model"
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145602732"
                        ],
                        "name": "Kobus Barnard",
                        "slug": "Kobus-Barnard",
                        "structuredName": {
                            "firstName": "Kobus",
                            "lastName": "Barnard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kobus Barnard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 92
                            }
                        ],
                        "text": "Instances of such approaches include work on inferring the meaning of words from video data (Roy and Pentland, 2002; Barnard and Forsyth, 2001), and interpreting the commentary of a simulated soccer game (Chen and Mooney, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13121800,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e36d141e2964817c3d926c380793e404a3a3367",
            "isKey": false,
            "numCitedBy": 615,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a statistical model for organizing image collections which integrates semantic information provided by associate text and visual information provided by image features. The model is very promising for information retrieval tasks such as database browsing and searching for images based on text and/or image features. Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition."
            },
            "slug": "Learning-the-semantics-of-words-and-pictures-Barnard-Forsyth",
            "title": {
                "fragments": [],
                "text": "Learning the semantics of words and pictures"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The model is very promising for information retrieval tasks such as database browsing and searching for images based on text and/or image features, and can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643953204"
                        ],
                        "name": "S. Mark",
                        "slug": "S.-Mark",
                        "structuredName": {
                            "firstName": "SiskindJeffrey",
                            "lastName": "Mark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mark"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 221596863,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "7e218bd2b45ca3d279a172995aa09709f3955a9d",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an implemented system for recognizing the occurrence of events described by simple spatial-motion verbs in short image sequences. The semantics of these verbs is specified with ..."
            },
            "slug": "Grounding-the-lexical-semantics-of-verbs-in-visual-Mark",
            "title": {
                "fragments": [],
                "text": "Grounding the lexical semantics of verbs in visual perception using force dynamics and event logic"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "This paper presents an implemented system for recognizing the occurrence of events described by simple spatial-motion verbs in short image sequences by specifying the semantics of these verbs with respect to short-term image sequences."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "67288754"
                        ],
                        "name": "James T. Oates",
                        "slug": "James-T.-Oates",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Oates",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James T. Oates"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580830"
                        ],
                        "name": "P. Cohen",
                        "slug": "P.-Cohen",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Cohen",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cohen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 130
                            }
                        ],
                        "text": "Grounded Language Acquisition Our work fits into a broader class of approaches that aim to learn language from a situated context (Mooney, 2008a; Mooney, 2008b; Fleischman and Roy, 2005; Yu and Ballard, 2004; Siskind, 2001; Oates, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62363578,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "08217b5619f7e99340f4e8e91f01bfd343bbce3e",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Grounding-knowledge-in-sensors:-unsupervised-for-Oates-Cohen",
            "title": {
                "fragments": [],
                "text": "Grounding knowledge in sensors: unsupervised learning for language and planning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fundamentally different problems Assume parallel corpus of text and semantic representations"
            },
            "venue": {
                "fragments": [],
                "text": "Fundamentally different problems Assume parallel corpus of text and semantic representations"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 130
                            }
                        ],
                        "text": "Grounded Language Acquisition Our work fits into a broader class of approaches that aim to learn language from a situated context (Mooney, 2008a; Mooney, 2008b; Fleischman and Roy, 2005; Yu and Ballard, 2004; Siskind, 2001; Oates, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Intentional context in situated language learning"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of CoNLL."
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 181
                            }
                        ],
                        "text": "\u2026of d not in W ), and the possible commands and parameters in environment state E .4 We model the policy distribution p(a|s; \u03b8) over this action space in a log-linear fashion (Della Pietra et al., 1997; Lafferty et al., 2001), giving us the flexibility to incorporate a diverse range of features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Inducing features of random fields.IEEE"
            },
            "venue": {
                "fragments": [],
                "text": "Trans. Pattern Anal. Mach. Intell.,"
            },
            "year": 1997
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 16,
            "methodology": 8
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 25,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Reinforcement-Learning-for-Mapping-Instructions-to-Branavan-Chen/cc1648c91ffda21bbe6e5f08f69c683588fc384c?sort=total-citations"
}