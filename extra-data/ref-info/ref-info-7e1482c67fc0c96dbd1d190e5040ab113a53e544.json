{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281877"
                        ],
                        "name": "J. Atick",
                        "slug": "J.-Atick",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Atick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Atick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144513847"
                        ],
                        "name": "A. Redlich",
                        "slug": "A.-Redlich",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Redlich",
                            "middleNames": [
                                "Norman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Redlich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8843455,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02ee9ba6631a5f654696533fdb0856fad0789b89",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Barlow's redundancy reduction hypothesis is applied using techniques developed from signal processing theory to derive the one-dimensional ganglion and simple cell kernels. The resulting closed-form expression for the ganglion cell kernel reduces redundancy over the entire range of signal-to-noise ratios, and resembles the phenomenological kernel. Significantly, it exhibits observed nontrivial dependence of the cell's parameters on background luminosity. The one-dimensional simple cell kernel is deduced by requiring that it maintain the redundancy reduction achieved by the ganglion cells despite the presence of intrinsic noise introduced through transmission along the optic nerve Neural update algorithms which converge to these cell profiles, starting from any initial set of synaptic strengths, are also derived. These learning algorithms turn out to be anti-Hebbian."
            },
            "slug": "Predicting-Ganglion-and-Simple-Cell-Receptive-Field-Atick-Redlich",
            "title": {
                "fragments": [],
                "text": "Predicting Ganglion and Simple Cell Receptive Field Organizations"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "Barlow's redundancy reduction hypothesis is applied using techniques developed from signal processing theory to derive the one-dimensional ganglion and simple cell kernels, and the resulting closed-form expression reduces redundancy over the entire range of signal-to-noise ratios."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Neural Syst."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7708166,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f8fba8a2a6f1209f4e09cede01c8b2c64a656aea",
            "isKey": false,
            "numCitedBy": 377,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The functional architecture of mammalian visual cortex has been elucidated in impressive detail by experimental work of the past 20-25 years. The origin of many of the salient features of this architecture, however, has remained unexplained. This paper is the first of three (the others will appear in subsequent issues of these Proceedings) that address the origin and organization of feature-analyzing (spatial-opponent and orientation-selective) cells in simple systems governed by biologically plausible development rules. I analyze the progressive maturation of a system composed of a few layers of cells, with connections that develop according to a simple set of rules (including Hebb-type modification). To understand the prenatal origin of orientation-selective cells in certain primates, I consider the case in which there is no external input, with the first layer exhibiting random spontaneous electrical activity. No orientation preference is specified to the system at any stage, and none of the basic developmental rules is specific to visual processing. Here I introduce the theory of \"modular self-adaptive networks,\" of which this system is an example, and explicitly demonstrate the emergence of a layer of spatial-opponent cells. This sets the stage for the emergence, in succeeding layers, of an orientation-selective cell population."
            },
            "slug": "From-basic-network-principles-to-neural-emergence-Linsker",
            "title": {
                "fragments": [],
                "text": "From basic network principles to neural architecture: emergence of spatial-opponent cells."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper is the first of three that address the origin and organization of feature-analyzing cells in simple systems governed by biologically plausible development rules, and introduces the theory of \"modular self-adaptive networks,\" of which this system is an example, and explicitly demonstrates the emergence of a layer of spatial-opponent cells."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807117"
                        ],
                        "name": "T. Sanger",
                        "slug": "T.-Sanger",
                        "structuredName": {
                            "firstName": "Terence",
                            "lastName": "Sanger",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sanger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10138295,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "709b4bfc5198336ba5d70da987889a157f695c1e",
            "isKey": false,
            "numCitedBy": 1524,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Optimal-unsupervised-learning-in-a-single-layer-Sanger",
            "title": {
                "fragments": [],
                "text": "Optimal unsupervised learning in a single-layer linear feedforward neural network"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1527671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16d70e8af45ca0ae2c1bb73f3be6628518d40b8f",
            "isKey": false,
            "numCitedBy": 1417,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The emergence of a feature-analyzing function from the development rules of simple, multilayered networks is explored. It is shown that even a single developing cell of a layered network exhibits a remarkable set of optimization properties that are closely related to issues in statistics, theoretical physics, adaptive signal processing, the formation of knowledge representation in artificial intelligence, and information theory. The network studied is based on the visual system. These results are used to infer an information-theoretic principle that can be applied to the network as a whole, rather than a single cell. The organizing principle proposed is that the network connections develop in such a way as to maximize the amount of information that is preserved when signals are transformed at each processing stage, subject to certain constraints. The operation of this principle is illustrated for some simple cases.<<ETX>>"
            },
            "slug": "Self-organization-in-a-perceptual-network-Linsker",
            "title": {
                "fragments": [],
                "text": "Self-organization in a perceptual network"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "It is shown that even a single developing cell of a layered network exhibits a remarkable set of optimization properties that are closely related to issues in statistics, theoretical physics, adaptive signal processing, the formation of knowledge representation in artificial intelligence, and information theory."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281877"
                        ],
                        "name": "J. Atick",
                        "slug": "J.-Atick",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Atick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Atick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144513847"
                        ],
                        "name": "A. Redlich",
                        "slug": "A.-Redlich",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Redlich",
                            "middleNames": [
                                "Norman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Redlich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 25
                            }
                        ],
                        "text": "We have shown previously (Atick and Redlich 1992) that the retinal ganglion cell kernel W-' acts to decorrelate the environmental signal-at least at low frequencies where the signal to noise is high-which in frequency space means W-'(f) N If1 so that I W(f)I2 = R(f)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 187
                            }
                        ],
                        "text": "The idea that the nervous system could be engaged in trying to build such a minimum entropy representation of the environment has been tested in the limited context of retinal processing (Atick and Redlich 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 96
                            }
                        ],
                        "text": "which when convoluted with a noise filter was shown to reproduce ganglion cell receptive fields (Atick and Redlich 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17515861,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "841cd4a6cac86fa0cfb0e8542eac5ed164f23f50",
            "isKey": false,
            "numCitedBy": 711,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "By examining the experimental data on the statistical properties of natural scenes together with (retinal) contrast sensitivity data, we arrive at a first principle, theoretical hypothesis for the purpose of retinal processing and its relationship to an animal's environment. We argue that the retinal goal is to transform the visual input as much as possible into a statistically independent basis as the first step in creating a redundancy reduced representation in the cortex, as suggested by Barlow. The extent of this whitening of the input is limited, however, by the need to suppress input noise. Our explicit theoretical solutions for the retinal filters also show a simple dependence on mean stimulus luminance: they predict an approximate Weber law at low spatial frequencies and a De Vries-Rose law at high frequencies. Assuming that the dominant source of noise is quantum, we generate a family of contrast sensitivity curves as a function of mean luminance. This family is compared to psychophysical data."
            },
            "slug": "What-Does-the-Retina-Know-about-Natural-Scenes-Atick-Redlich",
            "title": {
                "fragments": [],
                "text": "What Does the Retina Know about Natural Scenes?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is argued that the retinal goal is to transform the visual input as much as possible into a statistically independent basis as the first step in creating a redundancy reduced representation in the cortex, as suggested by Barlow."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1600874,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aeb37769d72999bcbfb0582b73607fd8d23f4545",
            "isKey": false,
            "numCitedBy": 3274,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "The relative efficiency of any particular image-coding scheme should be defined only in relation to the class of images that the code is likely to encounter. To understand the representation of images by the mammalian visual system, it might therefore be useful to consider the statistics of images from the natural environment (i.e., images with trees, rocks, bushes, etc). In this study, various coding schemes are compared in relation to how they represent the information in such natural images. The coefficients of such codes are represented by arrays of mechanisms that respond to local regions of space, spatial frequency, and orientation (Gabor-like transforms). For many classes of image, such codes will not be an efficient means of representing information. However, the results obtained with six natural images suggest that the orientation and the spatial-frequency tuning of mammalian simple cells are well suited for coding the information in such images if the goal of the code is to convert higher-order redundancy (e.g., correlation between the intensities of neighboring pixels) into first-order redundancy (i.e., the response distribution of the coefficients). Such coding produces a relatively high signal-to-noise ratio and permits information to be transmitted with only a subset of the total number of cells. These results support Barlow's theory that the goal of natural vision is to represent the information in the natural environment with minimal redundancy."
            },
            "slug": "Relations-between-the-statistics-of-natural-images-Field",
            "title": {
                "fragments": [],
                "text": "Relations between the statistics of natural images and the response properties of cortical cells."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results obtained with six natural images suggest that the orientation and the spatial-frequency tuning of mammalian simple cells are well suited for coding the information in such images if the goal of the code is to convert higher-order redundancy into first- order redundancy."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics and image science"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150259524"
                        ],
                        "name": "P. Foldiak",
                        "slug": "P.-Foldiak",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Foldiak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Foldiak"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8917254,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "076e5a741d0c7485b1cf43b5c3ec4cde08e73ec0",
            "isKey": false,
            "numCitedBy": 296,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A network of highly interconnected linear neuron-like processing units and a simple, local, unsupervised rule for the modification of connection strengths between these units are proposed. After training the network on a high (m) dimensional distribution of input vectors, the lower (n) dimensional output will be a projection into the subspace of the n largest principal components (the subspace spanned by the n eigenvectors of the largest eigenvalues of the input covariance matrix) and maximize the mutual information between the input and the output in the same way as principal component analysis does. The purely local nature of the synaptic modification rule (simple Hebbian and anti-Hebbian) makes the implementation of the network easier, faster, and biologically more plausible than rules depending on error propagation.<<ETX>>"
            },
            "slug": "Adaptive-network-for-optimal-linear-feature-Foldiak",
            "title": {
                "fragments": [],
                "text": "Adaptive network for optimal linear feature extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A network of highly interconnected linear neuron-like processing units and a simple, local, unsupervised rule for the modification of connection strengths between these units are proposed, making the implementation of the network easier, faster, and biologically more plausible than rules depending on error propagation."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281877"
                        ],
                        "name": "J. Atick",
                        "slug": "J.-Atick",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Atick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Atick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144513847"
                        ],
                        "name": "A. Redlich",
                        "slug": "A.-Redlich",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Redlich",
                            "middleNames": [
                                "Norman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Redlich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 187
                            }
                        ],
                        "text": "There are also information theoretic formalisms in which both noise filtering and decorrelation can be achieved through minimizing (or maximizing) a single information theoretic quantity (Atick and Redlich 1990; Linsker 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1999,
                                "start": 140
                            }
                        ],
                        "text": "That is because it is a purely decorreluting kernel, whereas the true ganglion cell kernel both decorrelates and filters noise, as shown in Atick and Redlich (1992). In that paper we demonstrated that the total retinal kernel can be obtained by first low pass filtering the noisy input signal and then decorrelating the result. Here we have demonstrated that 3.4 can perform the decorrelation step. Therefore if one first low pass filters in exactly the same way as in Atick and Redlich (1992) (where we were careful to include the appropriate transmission noise following the lowpass stage) and then applies 3.4, one must arrive at the same realistic ganglion cell kernels found in that paper, since this effectively reproduces the steps outlined there. We also expect that applying 3.4 to natural scenes, since they have the same spectrum as our simulation scenes, would likewise produce results close to those shown here. In a two-stage process in which noise is filtered and then the signal is decorrelated, it is also possible to derive the noise filter separately using a different learning algorithm. For example in Atick and Redlich (1991) we gave a convergent developmental rule for learning least mean square noise smoothing. (Such noise filtering algorithms differ from decorrelation algorithms such as 3.4 in that they require sufficient supervision to distinguish signal from noise.) Therefore, both noise filtering and decorrelation can be achieved developmentally in a two-step process in which first noise filtering and then decorrelation are learned. There are also information theoretic formalisms in which both noise filtering and decorrelation can be achieved through minimizing (or maximizing) a single information theoretic quantity (Atick and Redlich 1990; Linsker 1988). If a developmental algorithm could be found to perform this minimization (or maximization) then both goals, noise filtering and decorrelation, could be achieved through one learning stage. Linsker (1991) has made some progress in this direction, though his learning algorithm still requires a two-phase process."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1148,
                                "start": 140
                            }
                        ],
                        "text": "That is because it is a purely decorreluting kernel, whereas the true ganglion cell kernel both decorrelates and filters noise, as shown in Atick and Redlich (1992). In that paper we demonstrated that the total retinal kernel can be obtained by first low pass filtering the noisy input signal and then decorrelating the result. Here we have demonstrated that 3.4 can perform the decorrelation step. Therefore if one first low pass filters in exactly the same way as in Atick and Redlich (1992) (where we were careful to include the appropriate transmission noise following the lowpass stage) and then applies 3.4, one must arrive at the same realistic ganglion cell kernels found in that paper, since this effectively reproduces the steps outlined there. We also expect that applying 3.4 to natural scenes, since they have the same spectrum as our simulation scenes, would likewise produce results close to those shown here. In a two-stage process in which noise is filtered and then the signal is decorrelated, it is also possible to derive the noise filter separately using a different learning algorithm. For example in Atick and Redlich (1991) we gave a convergent developmental rule for learning least mean square noise smoothing."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1658,
                                "start": 6
                            }
                        ],
                        "text": "1989; Atick and Redlich 1990,1992). This minimum is achieved when there is the least possible statistical dependence between elements. The idea that the nervous system could be engaged in trying to build such a minimum entropy representation of the environment has been tested in the limited context of retinal processing (Atick and Redlich 1992). There it was assumed that the retina, being the first stage in the visual pathway, could reduce pixel-entropy by eliminating no higher than two-point correlations (pairwise correlations). The linear transform on the photoreceptor activities needed to achieve pixel-pixel decorrelations was shown to agree with observed retinal filters-after being careful to take noise into account. In general, the problem of finding entropy reducing maps is very difficult. It is also very unlikely that one will be able to analytically solve for the explicit form of these maps as was done for the pairwise decorrelating map in the retina. An alternate approach is to use neural networks to try to compute these maps. What one needs are developmental algorithms that iteratively reduce statistical dependence among the elements of the representation as a network is trained over more sensory inputs, and preferably algorithms that are guaranteed to converge to the optimal maps. In this paper we derive a simple developmental algorithm, for the linear class of maps, which we prove lowers pixel-entropy at each learning stage. For this class of maps lowering pixel-entropy is equivalent to decreasing pairwise correlations at each step. The algorithm turns out to be identical to one originally introduced by Goodall (1960) who was interested in decorrelation in a different context."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 140
                            }
                        ],
                        "text": "That is because it is a purely decorreluting kernel, whereas the true ganglion cell kernel both decorrelates and filters noise, as shown in Atick and Redlich (1992). In that paper we demonstrated that the total retinal kernel can be obtained by first low pass filtering the noisy input signal and then decorrelating the result."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 494,
                                "start": 140
                            }
                        ],
                        "text": "That is because it is a purely decorreluting kernel, whereas the true ganglion cell kernel both decorrelates and filters noise, as shown in Atick and Redlich (1992). In that paper we demonstrated that the total retinal kernel can be obtained by first low pass filtering the noisy input signal and then decorrelating the result. Here we have demonstrated that 3.4 can perform the decorrelation step. Therefore if one first low pass filters in exactly the same way as in Atick and Redlich (1992) (where we were careful to include the appropriate transmission noise following the lowpass stage) and then applies 3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 28154878,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03128ecdd3c3dfb9752861d8555b97535e1cfc14",
            "isKey": false,
            "numCitedBy": 531,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a theory of the early processing in the mammalian visual pathway. The theory is formulated in the language of information theory and hypothesizes that the goal of this processing is to recode in order to reduce a generalized redundancy subject to a constraint that specifies the amount of average information preserved. In the limit of no noise, this theory becomes equivalent to Barlow's redundancy reduction hypothesis, but it leads to very different computational strategies when noise is present. A tractable approach for finding the optimal encoding is to solve the problem in successive stages where at each stage the optimization is performed within a restricted class of transfer functions. We explicitly find the solution for the class of encodings to which the parvocellular retinal processing belongs, namely linear and nondivergent transformations. The solution shows agreement with the experimentally observed transfer functions at all levels of signal to noise."
            },
            "slug": "Towards-a-Theory-of-Early-Visual-Processing-Atick-Redlich",
            "title": {
                "fragments": [],
                "text": "Towards a Theory of Early Visual Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A theory of the early processing in the mammalian visual pathway is proposed and the solution for the class of encodings to which the parvocellular retinal processing belongs, namely linear and nondivergent transformations is found."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144513847"
                        ],
                        "name": "A. Redlich",
                        "slug": "A.-Redlich",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Redlich",
                            "middleNames": [
                                "Norman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Redlich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27820793,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6c9aa374f3f5e1338a5cd9bbbd0bddea212788e",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Factorial learning, finding a statistically independent representation of a sensory imagea factorial codeis applied here to solve multilayer supervised learning problems that have traditionally required backpropagation. This lends support to Barlow's argument for factorial sensory processing, by demonstrating how it can solve actual pattern recognition problems. Two techniques for supervised factorial learning are explored, one of which gives a novel distributed solution requiring only positive examples. Also, a new nonlinear technique for factorial learning is introduced that uses neural networks based on almost reversible cellular automata. Due to the special functional connectivity of these networkswhich resemble some biological microcircuitslearning requires only simple local algorithms. Also, supervised factorial learning is shown to be a viable alternative to backpropagation. One significant advantage is the existence of a measure for the performance of intermediate learning stages."
            },
            "slug": "Supervised-Factorial-Learning-Redlich",
            "title": {
                "fragments": [],
                "text": "Supervised Factorial Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work lends support to Barlow's argument for factorial sensory processing, by demonstrating how it can solve actual pattern recognition problems, and two techniques for supervised factorial learning are explored, one of which gives a novel distributed solution requiring only positive examples."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804703"
                        ],
                        "name": "Mark D. Plumbley",
                        "slug": "Mark-D.-Plumbley",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Plumbley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark D. Plumbley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1644387,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "9dbd339097e8e4971fc5785364dbbec1107ac147",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficient-information-transfer-and-anti-Hebbian-Plumbley",
            "title": {
                "fragments": [],
                "text": "Efficient information transfer and anti-Hebbian neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 21243149,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3f19a131970681eebbb003ac6e9b50c7a4db352b",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Animals that are primarily dependent on olfaction must obtain a description of the spatial location and the individual odor quality of environmental odor sources through olfaction alone. The variable nature of turbulent air flow makes such a remote sensing problem solvable if the animal can make use of the information conveyed by the fluctuation with time of the mixture of odor sources. Behavioral evidence suggests that such analysis takes place. An adaptive network can solve the essential problem, isolating the quality and intensity of the components within a mixture of several individual unknown odor sources. The network structure is an idealization of olfactory bulb circuitry. The dynamics of synapse change is essential to the computation. The synaptic variables themselves contain information needed by higher processing centers. The use of the same axons to convey intensity information and quality information requires time-coding of information. Covariation defines an individual odor source (object), and this may have a parallel in vision."
            },
            "slug": "Olfactory-computation-and-object-perception.-Hopfield",
            "title": {
                "fragments": [],
                "text": "Olfactory computation and object perception."
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An adaptive network can solve the essential problem, isolating the quality and intensity of the components within a mixture of several individual unknown odor sources."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281877"
                        ],
                        "name": "J. Atick",
                        "slug": "J.-Atick",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Atick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Atick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5062473"
                        ],
                        "name": "Zhaoping Li",
                        "slug": "Zhaoping-Li",
                        "structuredName": {
                            "firstName": "Zhaoping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhaoping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144513847"
                        ],
                        "name": "A. Redlich",
                        "slug": "A.-Redlich",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Redlich",
                            "middleNames": [
                                "Norman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Redlich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 43473047,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b4311a172097266fb29f04437085d622ced514e",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What-does-post-adaptation-color-appearance-reveal-Atick-Li",
            "title": {
                "fragments": [],
                "text": "What does post-adaptation color appearance reveal about cortical color representation?"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144246983"
                        ],
                        "name": "H. Barlow",
                        "slug": "H.-Barlow",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Barlow",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Barlow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2315855"
                        ],
                        "name": "T. P. Kaushal",
                        "slug": "T.-P.-Kaushal",
                        "structuredName": {
                            "firstName": "Tej",
                            "lastName": "Kaushal",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. P. Kaushal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145666307"
                        ],
                        "name": "G. Mitchison",
                        "slug": "G.-Mitchison",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Mitchison",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mitchison"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28107770,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5074f2219ddac70995f0a5e81ee2e892cb884b56",
            "isKey": false,
            "numCitedBy": 203,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "To determine whether a particular sensory event is a reliable predictor of reward or punishment it is necessary to know the prior probability of that event. If the variables of a sensory representation normally occur independently of each other, then it is possible to derive the prior probability of any logical function of the variables from the prior probabilities of the individual variables, without any additional knowledge; hence such a representation enormously enlarges the scope of definable events that can be searched for reliable predictors. Finding a Minimum Entropy Code is a possible method of forming such a representation, and methods for doing this are explored in this paper. The main results are (1) to show how to find such a code when the probabilities of the input states form a geometric progression, as is shown to be nearly true for keyboard characters in normal text; (2) to show how a Minimum Entropy Code can be approximated by repeatedly recoding pairs, triples, etc. of an original 7-bit code for keyboard characters; (3) to prove that in some cases enlarging the capacity of the output channel can lower the entropy."
            },
            "slug": "Finding-Minimum-Entropy-Codes-Barlow-Kaushal",
            "title": {
                "fragments": [],
                "text": "Finding Minimum Entropy Codes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Find a Minimum Entropy Code is a possible method of forming such a representation, and methods for doing this are explored, and the main results are to show how to find such a code when the probabilities of the input states form a geometric progression."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1391863648"
                        ],
                        "name": "M. Goodall",
                        "slug": "M.-Goodall",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Goodall",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Goodall"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 4266472,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2a77eeb1b608548dbb92690f36358ee07fec2af5",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "STOCHASTIC nets have been proposed by several authors1\u20133 as models of cognitive activity. So far, these have been dominated by heuristic constraints designed to show some analogy with the nervous system or merely for ease of computation. An adequate mathematical theory has, as was recognized by Uttley4, to show how such a device can learn relations."
            },
            "slug": "Performance-of-a-Stochastic-Net-Goodall",
            "title": {
                "fragments": [],
                "text": "Performance of a Stochastic Net"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An adequate mathematical theory has been recognized to show how aSTOCHASTIC nets can learn relations, as was recognized by Uttley4."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1960
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 1
                            }
                        ],
                        "text": "Neural Computation 5,45-60 (1993) @ 1993 Massachusetts Institute of Technology"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16577977,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3e00dd12caea7c4dab1633a35d1da3cb2e76b420",
            "isKey": false,
            "numCitedBy": 2357,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple linear neuron model with constrained Hebbian-type synaptic modification is analyzed and a new class of unconstrained learning rules is derived. It is shown that the model neuron tends to extract the principal component from a stationary input vector sequence."
            },
            "slug": "Simplified-neuron-model-as-a-principal-component-Oja",
            "title": {
                "fragments": [],
                "text": "Simplified neuron model as a principal component analyzer"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A simple linear neuron model with constrained Hebbian-type synaptic modification is analyzed and a new class of unconstrained learning rules is derived."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of mathematical biology"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144920356"
                        ],
                        "name": "H. Guy",
                        "slug": "H.-Guy",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Guy",
                            "middleNames": [
                                "Robert"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Guy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2546783"
                        ],
                        "name": "P. Seetharamulu",
                        "slug": "P.-Seetharamulu",
                        "structuredName": {
                            "firstName": "Peddaiahgari",
                            "lastName": "Seetharamulu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Seetharamulu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 20271947,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "532f1e1df0a9901f2567cd0237f0ef2f5d59a90b",
            "isKey": false,
            "numCitedBy": 498,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Secondary and tertiary structural models of sodium channel transmembrane segments were developed from its recently determined primary sequence in Electrophorus electricus. The model has four homologous domains, and each domain has eight homologous transmembrane segments, S1 through S8. Each domain contains three relatively apolar segments (S1, S2 and S3) and two very apolar segments (S5 and S8), all postulated to be transmembrane alpha-helices. S4 segments have positively charged residues, mainly arginines, at every third residue. The model channel lining is formed by four S4 transmembrane alpha-helices and four negatively charged S7 segments. S7 segments are postulated to be short, partially transmembrane amphipathic alpha-helices in three domains and a beta-strand in the last domain. S7 segments are preceded by short apolar segments (S6) postulated to be alpha-helices in three domains and a beta-strand in the last domain. Positively charged side chains of S4 form salt bridges with negatively charged side chains on S7 and near the ends of S1 and S3. Putative extracellular segments that contain 5 of the 10 potential N-glycosylation sites link S5 to S6. Channel activation may involve a 'helical screw' mechanism in which S4 helices rotate around their axes as they move toward the extracellular surface."
            },
            "slug": "Molecular-model-of-the-action-potential-sodium-Guy-Seetharamulu",
            "title": {
                "fragments": [],
                "text": "Molecular model of the action potential sodium channel."
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "Secondary and tertiary structural models of sodium channel transmembrane segments were developed from its recently determined primary sequence in Electrophorus electricus, and found that channel activation may involve a 'helical screw' mechanism in which S4 helices rotate around their axes as they move toward the extracellular surface."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187514"
                        ],
                        "name": "R. Durbin",
                        "slug": "R.-Durbin",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Durbin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Durbin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7912930"
                        ],
                        "name": "C. Miall",
                        "slug": "C.-Miall",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Miall",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Miall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145666307"
                        ],
                        "name": "G. Mitchison",
                        "slug": "G.-Mitchison",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Mitchison",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mitchison"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61649254,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6e32b4f82c2a1833ef7ae32967ce2f3685fa1c5",
            "isKey": false,
            "numCitedBy": 289,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-computing-neuron-Durbin-Miall",
            "title": {
                "fragments": [],
                "text": "The computing neuron"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60683349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1094e087102df1686de0a21d7e2cc233d1821386",
            "isKey": false,
            "numCitedBy": 208,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "From-basic-network-principles-to-neural-Linsker",
            "title": {
                "fragments": [],
                "text": "From basic network principles to neural architecture"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Predicting ganglion and simple cell"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 3,
            "methodology": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 19,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Convergent-Algorithm-for-Sensory-Receptive-Field-Atick-Redlich/7e1482c67fc0c96dbd1d190e5040ab113a53e544?sort=total-citations"
}