{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143791812"
                        ],
                        "name": "S. Gull",
                        "slug": "S.-Gull",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gull",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gull"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 67
                            }
                        ],
                        "text": "Bayesian interpolation through noise{free data has been studied by Skilling and Sibisi [17]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 92
                            }
                        ],
                        "text": "This radial basis function model is identical to the `intrinsic correlation' model of Gull, Skilling and Sibisi [6, 17]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 83
                            }
                        ],
                        "text": "The Bayesian framework I will describe for these tasks is due to Gull and Skilling [5, 6, 8, 17, 18], who have used Bayesian methods to achieve the state of the art in image reconstruction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 27
                            }
                        ],
                        "text": "What then was the cause of Skilling's result?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "Bayesian methods automatically and quantitatively embody Occam's razor [5], without the introduction of ad hoc penalty terms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 126
                            }
                        ],
                        "text": "Thus the evidence is found by taking the best t likelihood that the model can achieve and multiplying it by an `Occam factor' [5], which is a term with magnitude less than one that penalises H i for having the parameter w."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 25
                            }
                        ],
                        "text": "This framework is due to Gull and Skilling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 61
                            }
                        ],
                        "text": "For large dimensional problems where this task is demanding, Skilling has developed methods for estimating TraceA 1 statistically [21]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 182
                            }
                        ],
                        "text": "the ratio of the posterior accessible volume of H i 's parameter space to the prior accessible volume, or the factor by which H i 's hypothesis space collapses when the data arrives [5, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 13
                            }
                        ],
                        "text": "Furthermore, Skilling demonstrated that with some data sets a free form (maximum entropy) hypothesis can have greater evidence than the truth [20], but is it possible for this to happen in the typical case, as Skilling seems to claim?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 50
                            }
                        ],
                        "text": "I will show that the answer is no, the e ect that Skilling demonstrated cannot be systematic."
                    },
                    "intents": []
                }
            ],
            "corpusId": 117915279,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6272baf82e2e442edab4fb613ef2b7186bf5f1fb",
            "isKey": true,
            "numCitedBy": 288,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The principles of Bayesian reasoning are reviewed and applied to problems of inference from data sampled from Poisson, Gaussian and Cauchy distributions. Probability distributions (priors and likelihoods) are assigned in appropriate hypothesis spaces using the Maximum Entropy Principle, and then manipulated via Bayes\u2019 Theorem. Bayesian hypothesis testing requires careful consideration of the prior ranges of any parameters involved, and this leads to a quantitive statement of Occam\u2019s Razor. As an example of this general principle we offer a solution to an important problem in regression analysis; determining the optimal number of parameters to use when fitting graphical data with a set of basis functions."
            },
            "slug": "Bayesian-Inductive-Inference-and-Maximum-Entropy-Gull",
            "title": {
                "fragments": [],
                "text": "Bayesian Inductive Inference and Maximum Entropy"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 96
                            }
                        ],
                        "text": "Multiple maxima complicate the analysis, but Bayesian methods can still successfully be applied [13, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14439684,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d25984d8b27fcde46170f7d443c45b87fcc2acb2",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown that Bayesian inference from data modeled by a mixture distribution can feasibly be performed via Monte Carlo simulation. This method exhibits the true Bayesian predictive distribution, implicitly integrating over the entire underlying parameter space. An innnite number of mixture components can be accommodated without diiculty, using a prior distribution for mixing proportions that selects a reasonable subset of components to explain any nite training set. The need to decide on a \\correct\" number of components is thereby avoided. The feasibility of the method is shown empirically for a simple classiication task."
            },
            "slug": "Bayesian-Mixture-Modeling-by-Monte-Carlo-Simulation-Neal",
            "title": {
                "fragments": [],
                "text": "Bayesian Mixture Modeling by Monte Carlo Simulation"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "It is shown that Bayesian inference from data modeled by a mixture distribution can feasibly be performed via Monte Carlo simulation, and the true Bayesian predictive distribution is exhibited, implicitly integrating over the entire underlying parameter space."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4371968"
                        ],
                        "name": "G. L. Bretthorst",
                        "slug": "G.-L.-Bretthorst",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Bretthorst",
                            "middleNames": [
                                "Larry"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. L. Bretthorst"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 64
                            }
                        ],
                        "text": "8 There are analytic methods for performing such integrals over [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "Bayesian model comparison is also discussed by Bretthorst [2], who has used Bayesian methods to push back the limits of NMR signal detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6828423,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9f54600834ee9d85e58428e6edaa3547bf9b77bd",
            "isKey": false,
            "numCitedBy": 147,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bayesian-analysis.-I.-Parameter-estimation-using-Bretthorst",
            "title": {
                "fragments": [],
                "text": "Bayesian analysis. I. Parameter estimation using quadrature NMR models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32583901"
                        ],
                        "name": "R. Kashyap",
                        "slug": "R.-Kashyap",
                        "structuredName": {
                            "firstName": "Rangasami",
                            "lastName": "Kashyap",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kashyap"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120707616,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "edf68ac1fee7d5042c2bcd8200957921f61bf287",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper deals with the Bayesian methods of comparing different types of dynamical structures for representing the given set of observations. Specifically, given that a given process y(\\cdot) obeys one of r distinct stochastic or deterministic difference equations each involving a vector of unknown parameters, we compute the posterior probability that a set of observations {y(1),...,y(N)} obeys the i th equation, after making suitable assumptions about the prior probability distribution of the parameters in each equation. The difference equations can be nonlinear in the variable y but should be linear in the parameter vector in it. Once the posterior probability is known, we can find a decision rule to choose between the various structures so as to minimize the average value of a loss function. The optimum decision rule is asymptotically consistent and gives a quantitative explanation for the \"principle of parsimony\" often used in the construction of models from empirical data. The decision rule answers a wide variety of questions such as the advisability of a nonlinear transformation of data, the limitations of a model which yields a perfect fit to the data (i.e., zero residual variance), etc. The method can be used not only to compare different types of structures but also to determine a reliable estimate of spectral density of process. We compare the method in detail with the hypothesis testing method, and other methods and give a number of illustrative examples."
            },
            "slug": "A-Bayesian-comparison-of-different-classes-of-using-Kashyap",
            "title": {
                "fragments": [],
                "text": "A Bayesian comparison of different classes of dynamic models using empirical data"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The optimum decision rule is asymptotically consistent and gives a quantitative explanation for the \"principle of parsimony\" often used in the construction of models from empirical data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16543854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b959164d1efca4b73986ba5d21e664aadbbc0457",
            "isKey": false,
            "numCitedBy": 2590,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian \"evidence\" automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained."
            },
            "slug": "A-Practical-Bayesian-Framework-for-Backpropagation-Mackay",
            "title": {
                "fragments": [],
                "text": "A Practical Bayesian Framework for Backpropagation Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks that automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35206065"
                        ],
                        "name": "E. Jaynes",
                        "slug": "E.-Jaynes",
                        "structuredName": {
                            "firstName": "Edwin",
                            "lastName": "Jaynes",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Jaynes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8154444,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6ba8a74338ccb89d8b7242884289753653b86e7",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We note the main points of history, as a framework on which to hang many background remarks concerning the nature and motivation of Bayesian/Maximum Entropy methods. Experience has shown that these are needed in order to understand recent work and problems. A more complete account of the history, with many more details and references, is given in Jaynes (1978). The following discussion is essentially nontechnical; the aim is only to convey a little introductory \\feel\" for our outlook, purpose, and terminology, and to alert newcomers to common pitfalls of misunderstanding."
            },
            "slug": "Maximum-Entropy-and-Bayesian-Methods-in-Applied-Jaynes",
            "title": {
                "fragments": [],
                "text": "Maximum Entropy and Bayesian Methods in Applied Statistics: Bayesian Methods: General Background"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "The main points of history are noted, as a framework on which to hang many background remarks concerning the nature and motivation of Bayesian/Maximum Entropy methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 177
                            }
                        ],
                        "text": "The Bayesian framework I will describe for these tasks is due to Gull and Skilling [5, 6, 8, 17, 18], who have used Bayesian methods to achieve the state of the art in image reconstruction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This framework is due to Gull and Skilling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15883988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0f2433c088591d265891231f1c22424047f1bc1",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible: (1) objective comparisons between solutions using alternative network architectures; (2) objective stopping rules for deletion of weights; (3) objective choice of magnitude and type of weight decay terms or additive regularisers (for penalising large weights, etc.); (4) a measure of the e ective number of well{determined parameters in a model; (5) quanti ed estimates of the error bars on network parameters and on network output; (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian `evidence' automatically embodies `Occam's razor,' penalising over{ exible and over{complex architectures. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well{ matched to a problem, a good correlation between generalisation ability and the Bayesian evidence is obtained."
            },
            "slug": "A-Practical-Bayesian-Framework-for-Backprop-Mackay",
            "title": {
                "fragments": [],
                "text": "A Practical Bayesian Framework for Backprop Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks and a good correlation between generalisation ability and the Bayesian evidence is obtained."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6530745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7abda1941534d3bb558dd959025d67f1df526303",
            "isKey": false,
            "numCitedBy": 792,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Three Bayesian ideas are presented for supervised adaptive classifiers. First, it is argued that the output of a classifier should be obtained by marginalizing over the posterior distribution of the parameters; a simple approximation to this integral is proposed and demonstrated. This involves a \"moderation\" of the most probable classifier's outputs, and yields improved performance. Second, it is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems. This framework successfully chooses the magnitude of weight decay terms, and ranks solutions found using different numbers of hidden units. Third, an information-based data selection criterion is derived and demonstrated within this framework."
            },
            "slug": "The-Evidence-Framework-Applied-to-Classification-Mackay",
            "title": {
                "fragments": [],
                "text": "The Evidence Framework Applied to Classification Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems and an information-based data selection criterion is derived and demonstrated within this framework."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143791812"
                        ],
                        "name": "S. Gull",
                        "slug": "S.-Gull",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gull",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gull"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118754484,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "82fa37d5be8e747131a5857992cc33bb95469ce3",
            "isKey": false,
            "numCitedBy": 316,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian derivation of \u201cClassic\u201d MaxEnt image processing (Skilling 1989a) shows that exp(\u03b1S(f,m)), where S(f,m) is the entropy of image f relative to model m, is the only consistent prior probability distribution for positive, additive images. In this paper the derivation of \u201cClassic\u201d MaxEnt is completed, showing that it leads to a natural choice for the regularising parameter \u03b1, that supersedes the traditional practice of setting x2=N. The new condition is that the dimensionless measure of structure -2\u03b1S should be equal to the number of good singular values contained in the data. The performance of this new condition is discussed with reference to image deconvolution, but leads to a reconstruction that is visually disappointing. A deeper hypothesis space is proposed that overcomes these difficulties, by allowing for spatial correlations across the image."
            },
            "slug": "Developments-in-Maximum-Entropy-Data-Analysis-Gull",
            "title": {
                "fragments": [],
                "text": "Developments in Maximum Entropy Data Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16378859,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d167055b0947a880de6afee09559a84b0c2c407e",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Many of the processing tasks arising in early vision involve the solution of ill-posed inverse problems. Two techniques that are often used to solve these inverse problems are regularization and Bayesian modeling. Regularization is used to find a solution that both fits the data and is also sufficiently smooth. Bayesian modeling uses a statistical prior model of the field being estimated to determine an optimal solution. One convenient way of specifying the prior model is to associate an energy function with each possible solution, and to use a Boltzmann distribution to relate the solution energy to its probability. This paper shows that regularization is an example of Bayesian modeling, and that using the regularization energy function for the surface interpolation problem results in a prior model that is fractal (self-affine over a range of scales). We derive an algorithm for generating typical (fractal) estimates from the posterior distribution. We also show how this algorithm can be used to estimate the uncertainty associated with a regularized solution, and how this uncertainty can be used at later stages of processing."
            },
            "slug": "Regularization-Uses-Fractal-Priors-Szeliski",
            "title": {
                "fragments": [],
                "text": "Regularization Uses Fractal Priors"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper shows that regularization is an example of Bayesian modeling, and that using the regularization energy function for the surface interpolation problem results in a prior model that is fractal (self-affine over a range of scales)."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117692519,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a2f60515990f178c3268aed5bf0fc92497a9f1e",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible: (1) objective comparisons between solutions using alternative network architectures; (2) objective stopping rules for network pruning or growing procedures; (3) objective choice of magnitude and type of weight decay terms or additive regularisers (for penalising large weights, etc.); (4) a measure of the effective number of well-determined parameters in a model; (5) quantified estimates of the error bars on network parameters and on network output; (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian \u2018evidence\u2019 automatically embodies \u2018Occam\u2019s razor,\u2019 penalising over-flexible and over-complex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalisation ability and the Bayesian evidence is obtained."
            },
            "slug": "The-Evidence-for-Neural-Networks-Mackay",
            "title": {
                "fragments": [],
                "text": "The Evidence for Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks that helps detect poor underlying assumptions in learning models and achieves a good correlation between generalisation ability and the Bayesian evidence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "65793277"
                        ],
                        "name": "T. Loredo",
                        "slug": "T.-Loredo",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Loredo",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Loredo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 90
                            }
                        ],
                        "text": "For a general review of Bayesian philosophy see the excellent papers by Loredo and Jaynes [10, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 58
                            }
                        ],
                        "text": "The classic example is the `Sure Thing' hypothesis,\nc E.T Jaynes, which is the hypothesis that the data set will\nbeD, the precise data set that actually occured; the evidence for the Sure Thing hypothesis is huge."
                    },
                    "intents": []
                }
            ],
            "corpusId": 120668555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "065803f662547c47fc46bf7e649847f90e3e90e9",
            "isKey": false,
            "numCitedBy": 203,
            "numCiting": 227,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian approach to probability theory is presented as an alternative to the currently used long-run relative frequency approach, which does not offer clear, compelling criteria for the design of statistical methods. Bayesian probability theory offers unique and demonstrably optimal solutions to well-posed statistical problems, and is historically the original approach to statistics. The reasons for earlier rejection of Bayesian methods are discussed, and it is noted that the work of Cox, Jaynes, and others answers earlier objections, giving Bayesian inference a firm logical and mathematical foundation as the correct mathematical language for quantifying uncertainty. The Bayesian approaches to parameter estimation and model comparison are outlined and illustrated by application to a simple problem based on the gaussian distribution. As further illustrations of the Bayesian paradigm, Bayesian solutions to two interesting astrophysical problems are outlined: the measurement of weak signals in a strong background, and the analysis of the neutrinos detected from supernova SN 1987A. A brief bibliography of astrophysically interesting applications of Bayesian inference is provided."
            },
            "slug": "From-Laplace-to-Supernova-SN-1987A:-Bayesian-in-Loredo",
            "title": {
                "fragments": [],
                "text": "From Laplace to Supernova SN 1987A: Bayesian Inference in Astrophysics"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The Bayesian approach to probability theory is presented as an alternative to the currently used long-run relative frequency approach, and Bayesian solutions to two interesting astrophysical problems are outlined: the measurement of weak signals in a strong background and the analysis of the neutrinos detected from supernova SN 1987A."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33438590"
                        ],
                        "name": "D. Keren",
                        "slug": "D.-Keren",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Keren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Keren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27379268"
                        ],
                        "name": "M. Werman",
                        "slug": "M.-Werman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Werman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Werman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36242378,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c7e0bc09a52c20bfbf602a528262873e8db56a0a",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In order to use interpolated data wisely, it is important to have reliability and confidence measures associated with it. A method for computing the reliability at each point of any linear functional of a surface reconstructed using regularization is presented. The proposed method is to define a probability structure on the class of possible objects and compute the variance of the corresponding random variable. This variance is a natural measure for uncertainty, and experiments have shown it to correlate well with reality. The probability distribution used is based on the Boltzmann distribution. The theoretical part of the work utilizes tools from classical analysis, functional analysis, and measure theory on function spaces. The theory was tested and applied to real depth images. It was also applied to formalize a paradigm of optimal sampling, which was successfully tested on real depth images. >"
            },
            "slug": "Probabilistic-Analysis-of-Regularization-Keren-Werman",
            "title": {
                "fragments": [],
                "text": "Probabilistic Analysis of Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A method for computing the reliability at each point of any linear functional of a surface reconstructed using regularization is presented, to define a probability structure on the class of possible objects and compute the variance of the corresponding random variable."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8635127"
                        ],
                        "name": "C. S. Wallace",
                        "slug": "C.-S.-Wallace",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Wallace"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47640603"
                        ],
                        "name": "P. Freeman",
                        "slug": "P.-Freeman",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Freeman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Freeman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "Any implementation of MDL necessitates approximations in evaluating the length of the ideal shortest message."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "`Minimum description length' (MDL) methods are closely related to this Bayesian framework."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "I can see no advantage in MDL, and recommend that the evidence should be approximated directly."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 46
                            }
                        ],
                        "text": "Akaike's criteria are an approximation to MDL [16, 24, 25]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 118095811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04eb446825da7a4c2ab3fa6df7ebd377baa66ebe",
            "isKey": true,
            "numCitedBy": 599,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY The systematic variation within a set of data, as represented by a usual statistical model, may be used to encode the data in a more compact form than would be possible if they were considered to be purely random. The encoded form has two parts. The first states the inferred estimates of the unknown parameters in the model, the second states the data using an optimal code based on the data probability distribution implied by those parameter estimates. Choosing the model and the estimates that give the most compact coding leads to an interesting general inference procedure. In its strict form it has great generality and several nice properties but is computationally infeasible. An approximate form is developed and its relation to other methods is explored."
            },
            "slug": "Estimation-and-Inference-by-Compact-Coding-Wallace-Freeman",
            "title": {
                "fragments": [],
                "text": "Estimation and Inference by Compact Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "The systematic variation within a set of data, as represented by a usual statistical model, may be used to encode the data in a more compact form than would be possible if they were considered to be purely random."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067844876"
                        ],
                        "name": "Robin Hanson",
                        "slug": "Robin-Hanson",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Hanson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robin Hanson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87277825"
                        ],
                        "name": "J. Stutz",
                        "slug": "J.-Stutz",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Stutz",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stutz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40484982"
                        ],
                        "name": "P. Cheeseman",
                        "slug": "P.-Cheeseman",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Cheeseman",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cheeseman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 85
                            }
                        ],
                        "text": "The same Bayesian theory underlies the unsupervised classification system, Autoclass (Hanson et al. 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 130
                            }
                        ],
                        "text": "Multiple maxima that arise in more complex models complicate the analysis, but Bayesian methods can still successfully be applied (Hanson et al. 1991; MacKay 1992a; Neal 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14393184,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23e61f54b505c09c13e24101ccec44241be4725f",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The task of inferring a set of classes and class descriptions most likely to explain a given data set can be placed on a firm theoretical foundation using Bayesian statistics. Within this framework and using various mathematical and algorithmic approximations, the AutoClass system searches for the most probable classifications, automatically choosing the number of classes and complexity of class descriptions. A simpler version of AutoClass has been applied to many large real data sets, has discovered new independently-verified phenomena, and has been released as a robust software package. Recent extensions allow attributes to be selectively correlated within particular classes, and allow classes to inherit or share model parameters though a class hierarchy. We summarize the mathematical foundations of AutoClass."
            },
            "slug": "Bayesian-classification-theory-Hanson-Stutz",
            "title": {
                "fragments": [],
                "text": "Bayesian classification theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The mathematical foundations of AutoClass are summarized, which allow attributes to be selectively correlated within particular classes, and allow classes to inherit or share model parameters though a class hierarchy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 188
                            }
                        ],
                        "text": "If a poor regulariser is used, for example, one that is ill{matched to the statistics of the world, then the Bayesian choice of will often not be the best in terms of generalisation error [3, 6, 9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1330691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "305c0eccac29c37da6ca9a33b247de27531e6ef5",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we study a Bayesian or average-case model of concept learning with a twofold goal: to provide more precise characterizations of learning curve (sample complexity) behavior that depend on properties of both the prior distribution over concepts and the sequence of instances seen by the learner, and to smoothly unite in a common framework the popular statistical physics and VC dimension theories of learning curves. To achieve this, we undertake a systematic investigation and comparison of two fundamental quantities in learning and information theory: the probability of an incorrect prediction for an optimal learning algorithm, and the Shannon information gain. This study leads to a new understanding of the sample complexity of learning in several existing models."
            },
            "slug": "Bounds-on-the-sample-complexity-of-Bayesian-using-Haussler-Kearns",
            "title": {
                "fragments": [],
                "text": "Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A systematic investigation and comparison of two fundamental quantities in learning and information theory: the probability of an incorrect prediction for an optimal learning algorithm, and the Shannon information gain is undertaken."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings 1992] IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8992604"
                        ],
                        "name": "E. Levin",
                        "slug": "E.-Levin",
                        "structuredName": {
                            "firstName": "Esther",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Levin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15012839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b10440620da8a43a1b97e3da4b1ff13746306475",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of learning a general input-output relation using a layered neural network is discussed in a statistical framework. By imposing the consistency condition that the error minimization be equivalent to a likelihood maximization for training the network, the authors arrive at a Gibbs distribution on a canonical ensemble of networks with the same architecture. This statistical description enables them to evaluate the probability of a correct prediction of an independent example, after training the network on a given training set. The prediction probability is highly correlated with the generalization ability of the network, as measured outside the training set. This suggests a general and practical criterion for training layered networks by minimizing prediction errors. The authors demonstrate the utility of this criterion for selecting the optimal architecture in the continuity problem. As a theoretical application of the statistical formalism, they discuss the question of learning curves and estimate the sufficient training size needed for correct generalization, in a simple example.<<ETX>>"
            },
            "slug": "Consistent-inference-of-probabilities-in-layered-Tishby-Levin",
            "title": {
                "fragments": [],
                "text": "Consistent inference of probabilities in layered networks: predictions and generalizations"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The problem of learning a general input-output relation using a layered neural network is discussed in a statistical framework and the authors arrive at a Gibbs distribution on a canonical ensemble of networks with the same architecture."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143791812"
                        ],
                        "name": "S. Gull",
                        "slug": "S.-Gull",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gull",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gull"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118370612,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c6e7b1f4c8ca77899e8ed2fa222ead7fab6bfdb5",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A Bayesian solution is presented to the problem of straight-line fitting when both variables x and y are subject to error. The solution, which is fully symmetric with respect to x and y, contains a very surprising feature: it requires a informative prior for the distribution of sample positions. An uninformative prior leads to a bias in the estimated slope."
            },
            "slug": "Bayesian-Data-Analysis:-Straight-line-fitting-Gull",
            "title": {
                "fragments": [],
                "text": "Bayesian Data Analysis: Straight-line fitting"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A Bayesian solution to the problem of straight-line fitting when both variables x and y are subject to error contains a very surprising feature: it requires a informative prior for the distribution of sample positions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103143973"
                        ],
                        "name": "D. F. Utreras",
                        "slug": "D.-F.-Utreras",
                        "structuredName": {
                            "firstName": "D",
                            "lastName": "Utreras",
                            "middleNames": [
                                "Florencio"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. F. Utreras"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This framework is due to Gull and Skilling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121075111,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "01c80313ad8a81f2e72ee16df597ceaa15dc3434",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of approximating a function f supposed to be \u201csmooth\u201d, given its values known with error at n different points of a real interval $[a,b]$. To approximate f we use the natural smoothing spline of order q and parameter $\\tau $. For choosing $\\tau $, the method of generalized cross validation, proposed by Wahba and others, has very interesting theoretical properties, but requires expensive calculations.The asymptotic behavior of the eigenvalues associated with the spline functions provides a practical method for calculating the GCV function which reduces the computation time by a factor of n."
            },
            "slug": "Optimal-Smoothing-of-Noisy-Data-Using-Spline-Utreras",
            "title": {
                "fragments": [],
                "text": "Optimal Smoothing of Noisy Data Using Spline Functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97924060"
                        ],
                        "name": "G. Erickson",
                        "slug": "G.-Erickson",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Erickson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Erickson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153551190"
                        ],
                        "name": "C. R. Smith",
                        "slug": "C.-R.-Smith",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Smith",
                            "middleNames": [
                                "Ray"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. R. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 108573483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ba14634897df862cc97df6eda25fddb4dfaf9e5",
            "isKey": false,
            "numCitedBy": 312,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This volume has its origin in the Fifth, Sixth and Seventh Workshops on \"Maximum-Entropy and Bayesian Methods in Applied Statistics,\" held at the University of Wyoming, August 5-8, 1985, and at Seattle University, August 5-8, 1986, and August 4-7, 1987. It was anticipated that the proceedings of these workshops would be combined, so most of the papers were not collected until after the seventh workshop. Because most of the papers in this volume are in the nature of advancing theory or solving specific problems, as opposed to status reports, it is believed that the contents of this volume will be of lasting interest to the Bayesian community. The workshop was organized to bring together researchers from different fields to critically examine maximum-entropy and Bayesian methods in science and engineering as well as other disciplines. Some of the papers were chosen specifically to kindle interest in new areas that may offer new tools or insight to the reader or to stimulate work on pressing problems that appear to be ideally suited to the maximum-entropy or Bayesian method. These workshops and their proceedings could not have been brought to their final form without the support or help of a number of people."
            },
            "slug": "Maximum-entropy-and-Bayesian-methods-in-science-and-Erickson-Smith",
            "title": {
                "fragments": [],
                "text": "Maximum-entropy and Bayesian methods in science and engineering"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This volume has its origin in the Fifth, Sixth and Seventh Workshops on \"Maximum-Entropy and Bayesian Methods in Applied Statistics,\" held at the University of Wyoming and at Seattle University, August 5-8, 1986, and August 4-7, 1987."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46933679"
                        ],
                        "name": "M. Bertero",
                        "slug": "M.-Bertero",
                        "structuredName": {
                            "firstName": "Mario",
                            "lastName": "Bertero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bertero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145730991"
                        ],
                        "name": "V. Torre",
                        "slug": "V.-Torre",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Torre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Torre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 22
                            }
                        ],
                        "text": "In this paper, the Bayesian approach to regularisation and model{comparison is demonstrated by studying the inference problem of interpolating noisy data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14285485,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "31b5a06273e75f159d5d9e42bc5bdfd7fd4b625e",
            "isKey": false,
            "numCitedBy": 855,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": "Mathematical results on ill-posed and ill-conditioned problems are reviewed and the formal aspects of regularization theory in the linear case are introduced. Specific topics in early vision and their regularization are then analyzed rigorously, characterizing existence, uniqueness, and stability of solutions. A fundamental difficulty that arises in almost every vision problem is scale, that is, the resolution at which to operate. Methods that have been proposed to deal with the problem include scale-space techniques that consider the behavior of the result across a continuum of scales. From the point of view of regulation theory, the concept of scale is related quite directly to the regularization parameter lambda . It suggested that methods used to obtained the optimal value of lambda may provide, either directly or after suitable modification, the optimal scale associated with the specific instance of certain problems. >"
            },
            "slug": "Ill-posed-problems-in-early-vision-Bertero-Poggio",
            "title": {
                "fragments": [],
                "text": "Ill-posed problems in early vision"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145948621"
                        ],
                        "name": "G. Box",
                        "slug": "G.-Box",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Box",
                            "middleNames": [
                                "E.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Box"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36184409"
                        ],
                        "name": "G. C. Tiao",
                        "slug": "G.-C.-Tiao",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Tiao",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. C. Tiao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122028907,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a205103d4f25ae39f417bac7bd5142302d7f448c",
            "isKey": false,
            "numCitedBy": 4326,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Nature of Bayesian Inference Standard Normal Theory Inference Problems Bayesian Assessment of Assumptions: Effect of Non-Normality on Inferences About a Population Mean with Generalizations Bayesian Assessment of Assumptions: Comparison of Variances Random Effect Models Analysis of Cross Classification Designs Inference About Means with Information from More than One Source: One-Way Classification and Block Designs Some Aspects of Multivariate Analysis Estimation of Common Regression Coefficients Transformation of Data Tables References Indexes."
            },
            "slug": "Bayesian-inference-in-statistical-analysis-Box-Tiao",
            "title": {
                "fragments": [],
                "text": "Bayesian inference in statistical analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This chapter discusses Bayesian Assessment of Assumptions, which investigates the effect of non-Normality on Inferences about a Population Mean with Generalizations in the context of a Bayesian inference model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103265471"
                        ],
                        "name": "A. M. Walker",
                        "slug": "A.-M.-Walker",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Walker",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. M. Walker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 138
                            }
                        ],
                        "text": "The emphasis is that degrees of preference for alternative hypotheses are represented by probabilities, and relative preferences for hypotheses are assigned by evaluating those probabilities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 8
                            }
                        ],
                        "text": "The Bayesian framework I will describe for these tasks is due to Gull and Skilling [5, 6, 8, 17, 18], who have used Bayesian methods to achieve the state of the art in image reconstruction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 116092200,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7b03fd5101079d47029bedd702fd2aff7cc00ebe",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY Let a random sample of size n be taken from a distribution having a density depending on a real parameter 0, and let 0 have an absolutely continuous prior distribution with density ir(G). We give a rigorous proof that, under suitable regularity conditions, the posterior distribution of 0 will, when n tends to infinity, be asymptotically normal with mean equal to the maximumlikelihood estimator and variance equal to the reciprocal of the second derivative of the logarithm of the likelihood function evaluated at the maximum-likelihood estimator, independently of the form of 7r(G)."
            },
            "slug": "On-the-Asymptotic-Behaviour-of-Posterior-Walker",
            "title": {
                "fragments": [],
                "text": "On the Asymptotic Behaviour of Posterior Distributions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145730991"
                        ],
                        "name": "V. Torre",
                        "slug": "V.-Torre",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Torre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Torre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624227"
                        ],
                        "name": "C. Koch",
                        "slug": "C.-Koch",
                        "structuredName": {
                            "firstName": "Christof",
                            "lastName": "Koch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Koch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 55
                            }
                        ],
                        "text": "This is the well known Bayesian view of regularization (Poggio et al. 1985; Titterington 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4346156,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4c6bd3b8d35c4fc14360160efc9c66727abac9df",
            "isKey": false,
            "numCitedBy": 1264,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Descriptions of physical properties of visible surfaces, such as their distance and the presence of edges, must be recovered from the primary image data. Computational vision aims to understand how such descriptions can be obtained from inherently ambiguous and noisy data. A recent development in this field sees early vision as a set of ill-posed problems, which can be solved by the use of regularization methods. These lead to algorithms and parallel analog circuits that can solve \u2018ill-posed problems\u2019 and which are suggestive of neural equivalents in the brain."
            },
            "slug": "Computational-vision-and-regularization-theory-Poggio-Torre",
            "title": {
                "fragments": [],
                "text": "Computational vision and regularization theory"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "Descriptions of physical properties of visible surfaces, such as their distance and the presence of edges, must be recovered from the primary image data and algorithms and parallel analog circuits that can solve \u2018ill-posed problems\u2019 and which are suggestive of neural equivalents in the brain are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150910777"
                        ],
                        "name": "A. R. Davies",
                        "slug": "A.-R.-Davies",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Davies",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. R. Davies"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143744122"
                        ],
                        "name": "R. Anderssen",
                        "slug": "R.-Anderssen",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Anderssen",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Anderssen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123565120,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "691edda4156c41f13c7f75411e941fa38ab0633e",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We survey the role played by optimization in the choice of parameters for Tikhonov regularization of first-kind integral equations. Asymptotic analyses are presented for a selection of practical optimizing methods applied to a model deconvolution problem. These methods include the discrepancy principle, cross-validation and maximum likelihood. The relationship between optimality and regularity is emphasized. New bounds on the constants appearing in asymptotic estimates are presented."
            },
            "slug": "Optimisation-in-the-regularisation-ill-posed-Davies-Anderssen",
            "title": {
                "fragments": [],
                "text": "Optimisation in the regularisation ill-posed problems"
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of the Australian Mathematical Society. Series B. Applied Mathematics"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152591573"
                        ],
                        "name": "D. Titterington",
                        "slug": "D.-Titterington",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Titterington",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Titterington"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 55
                            }
                        ],
                        "text": "This is the well known Bayesian view of regularisation [15, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122781195,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "055e2eda310a7cfe30808877df53144892620244",
            "isKey": false,
            "numCitedBy": 217,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "Summary A wide variety of statistical techniques involve the choice of a prescription and smoothing parameter for modifying an unsmoothed estimator towards some ultrasmooth solution. The basic common structure is illustrated by multinomial smoothing, nonparametric density estimation, linear and nonparametric regression, the solution of integral equations and image processing. Possible cross-fertilization among the different areas of application is suggested."
            },
            "slug": "Common-structure-of-smoothing-techniques-in-Titterington",
            "title": {
                "fragments": [],
                "text": "Common structure of smoothing techniques in statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This framework is due to Gull and Skilling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15819455,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2046412fecff64e095cc5190b69172055afd2094",
            "isKey": false,
            "numCitedBy": 1202,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning can be made more efficient if we can actively select particularly salient data points. Within a Bayesian learning framework, objective functions are discussed that measure the expected informativeness of candidate measurements. Three alternative specifications of what we want to gain information about lead to three different criteria for data selection. All these criteria depend on the assumption that the hypothesis space is correct, which may prove to be their main weakness."
            },
            "slug": "Information-Based-Objective-Functions-for-Active-Mackay",
            "title": {
                "fragments": [],
                "text": "Information-Based Objective Functions for Active Data Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "Within a Bayesian learning framework, objective functions are discussed that measure the expected informativeness of candidate measurements that depend on the assumption that the hypothesis space is correct."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2010050"
                        ],
                        "name": "J. Skilling",
                        "slug": "J.-Skilling",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Skilling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Skilling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 92614342,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3db579e6fd702e3da02720e89b3fd4b6db39a51b",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This tutorial paper discusses the theoretical basis of quantified maximum entropy, as a technique for obtaining probabilistic estimates of images and other positive additive distributions from noisy and incomplete data. The analysis is fully Bayesian, with estimates always being obtained as probability distributions from which appropriate error bars can be found. This supersedes earlier techniques, even those using maximum entropy, which aimed to produce a single optimal distribution."
            },
            "slug": "Quantified-Maximum-Entropy-Skilling",
            "title": {
                "fragments": [],
                "text": "Quantified Maximum Entropy"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This tutorial paper discusses the theoretical basis of quantified maximum entropy, as a technique for obtaining probabilistic estimates of images and other positive additive distributions from noisy and incomplete data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2010050"
                        ],
                        "name": "J. Skilling",
                        "slug": "J.-Skilling",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Skilling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Skilling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115543752,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c89000d89799052565e63b63e6b69d771c6f74aa",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "We give a Bayesian comparison between parameter estimation and free-form reconstruction by quantified MaxEnt. The evidence favours the latter prior for the example analysed, and we suggest that this may hold more generally."
            },
            "slug": "On-Parameter-Estimation-and-Quantified-Maxent-Skilling",
            "title": {
                "fragments": [],
                "text": "On Parameter Estimation and Quantified Maxent"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47865366"
                        ],
                        "name": "M. Charter",
                        "slug": "M.-Charter",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Charter",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Charter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118219397,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "88ef99fbee594216d24b2bdcedc838a824e60850",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "When a drug is given orally, the proportion of the administered dose which reaches the systemic circulation varies widely, depending on the drug and the conditions of its use. Measurement of this proportion, and the rate f(t) at which the drug reaches the circulation, is an important part of drug development. The reconstruction of f(t) is an inference problem with sparse and noisy data. Estimates of f(t), the total amount of drug reaching the bloodstream, and other quantities of interest may be obtained, together with their uncertainties, using a Bayesian analysis in which entropy appears naturally in the prior probability."
            },
            "slug": "Quantifying-Drug-Absorption-Charter",
            "title": {
                "fragments": [],
                "text": "Quantifying Drug Absorption"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Estimates of f(t), the total amount of drug reaching the bloodstream, and other quantities of interest may be obtained, together with their uncertainties, using a Bayesian analysis in which entropy appears naturally in the prior probability."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70359527"
                        ],
                        "name": "D. Osteyee",
                        "slug": "D.-Osteyee",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Osteyee",
                            "middleNames": [
                                "Bridston"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Osteyee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145179124"
                        ],
                        "name": "I. Good",
                        "slug": "I.-Good",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Good",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Good"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 172
                            }
                        ],
                        "text": "To be precise, the expectation over possible data sets of the log evidence for the true model is greater than the expectation of the log evidence for any other fixed model (Osteyee and Good 1974)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117969358,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40c8be5900b8d1e97c56b4a437ccf84c885955cc",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "List of symbols.- Information in events and weight of evidence.- Entropy.- Singularity between two probability measures.- Expected mutual information.- Expected weight of evidence.- Divergence.- Expected mutual information, expected weight of evidence, and divergence for random processes.- Relationships between certain random processes and the singularity between probability measures.- Other expressions for expected mutual information.- Expressing expected weight of evidence for gaussian generalized processes in terms of integral operators.- Comparison between I(RT:ST), WT(HN/HS+N), and JT(N, S+N) for gaussian signals and noise.- Expected mutual information rate.- Rate of expected weight of evidence.- Gaussian processes with equal covariance functions including nonrandom signals in gaussian noise.- Summary of the major results in this survey for gaussian processes, including gaussian signals and noise.- Conclusions and areas for additional research."
            },
            "slug": "Information,-weight-of-evidence,-the-singularity-Osteyee-Good",
            "title": {
                "fragments": [],
                "text": "Information, weight of evidence, the singularity between probability measures and signal detection"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The major results in this survey are Gaussian processes with equal covariance functions including nonrandom signals in gaussian noise and rate of expected weight of evidence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2010050"
                        ],
                        "name": "J. Skilling",
                        "slug": "J.-Skilling",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Skilling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Skilling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117844915,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3e9229dd827dda0d462dffbdec7fdf50b724d587",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Often, we need to know some integral property of the eigenvalues {x} of a large N \u00d7 N symmetric matrix A. For example, determinants det (A) = exp(\u2211 log (x)) play a role in the classic maximum entropy algorithm [Gull, 1988] . Likewise in physics, the specific heat of a system is a temperature- -dependent sum over the eigenvalues of the Hamiltonian matrix. However, the matrix may be so large that direct O (N 3 calculation of all N eigenvalues is prohibited. Indeed, if A is coded as a \u201cfast\u201d procedure, then O (N 2 operations may also be prohibited."
            },
            "slug": "The-Eigenvalues-of-Mega-dimensional-Matrices-Skilling",
            "title": {
                "fragments": [],
                "text": "The Eigenvalues of Mega-dimensional Matrices"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8635127"
                        ],
                        "name": "C. S. Wallace",
                        "slug": "C.-S.-Wallace",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Wallace"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4110643"
                        ],
                        "name": "D. Boulton",
                        "slug": "D.-Boulton",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Boulton",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Boulton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61324799,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1d76ee9b6fb4d441e31abcd4dadc4e44c576017",
            "isKey": false,
            "numCitedBy": 1100,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "1. The class to which each thing belongs. 2. The average properties of each class. 3. The deviations of each thing from the average properties of its parent class. If the things are found to be concentrated in a small area of the region of each class in the measurement space then the deviations will be small, and with reference to the average class properties most of the information about a thing is given by naming the class to which it belongs. In this case the information may be recorded much more briefly than if a classification had not been used. We suggest that the best classification is that which results in the briefest recording of all the attribute information. In this context, we will regard the measurements of each thing as being a message about that thing. Shannon (1948) showed that where messages may be regarded as each nominating the occurrence of a particular event among a universe of possible events, the information needed to record a series of such messages is minimised if the messages are encoded so that the length of each message is proportional to minus the logarithm of the relative frequency of occurrence of the event which it nominates. The information required is greatest when all frequencies are equal. The messages here nominate the positions in measurement space of the 5 1 points representing the attributes of the things. If the expected density of points in the measurement space is everywhere uniform, the positions of the points cannot be encoded more briefly than by a simple list of the measured values. However, if the expected density is markedly non-uniform, application"
            },
            "slug": "An-Information-Measure-for-Classification-Wallace-Boulton",
            "title": {
                "fragments": [],
                "text": "An Information Measure for Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is suggested that the best classification is that which results in the briefest recording of all the attribute information, and the measurements of each thing are regarded as being a message about that thing."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3277582"
                        ],
                        "name": "S. Stigler",
                        "slug": "S.-Stigler",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Stigler",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Stigler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 61
                            }
                        ],
                        "text": "\u201cIt is remarkable that Laplace almost got this right in 1774 (Stigler 1986); when inferring the mean of a Laplacian distribution, he both inferred the posterior probability of a nuisance parameter like in equation 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120281169,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e4404756d3732f0f48bd8ed14cbb67e01385235e",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Laplace's first major article on mathematical statistics was published in 1774. It is arguably the most influential article in this field to appear before 1800, being the first widely read presentation of inverse probability and its application to both binomial and location parameter estimation. After a brief introduction, an English translation of this epochal memoir is given."
            },
            "slug": "Laplace's-1774-Memoir-on-Inverse-Probability-Stigler",
            "title": {
                "fragments": [],
                "text": "Laplace's 1774 Memoir on Inverse Probability"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39201543"
                        ],
                        "name": "J. Berger",
                        "slug": "J.-Berger",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Berger",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Berger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 113
                            }
                        ],
                        "text": "Since Jeffreys the emphasis of most Bayesian probability theory has been `to formally utilize prior information' [1], i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120366929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9dd05b69d6906fff6ea6c4ba3609a6d97c9b8a3",
            "isKey": false,
            "numCitedBy": 7325,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory. The text assumes a knowledge of basic probability theory and some advanced calculus is also required."
            },
            "slug": "Statistical-Decision-Theory-and-Bayesian-Analysis-Berger",
            "title": {
                "fragments": [],
                "text": "Statistical Decision Theory and Bayesian Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 50
                            }
                        ],
                        "text": "The\nposterior distribution (solid line) has a single peak at w\nMP\nwith\ncharacteristic width w."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30140639,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d382b9c11e5c6a8e173fbeb442545e3be8d3e3a5",
            "isKey": false,
            "numCitedBy": 6261,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modeling-By-Shortest-Data-Description*-Rissanen",
            "title": {
                "fragments": [],
                "text": "Modeling By Shortest Data Description*"
            },
            "venue": {
                "fragments": [],
                "text": "Autom."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102846212"
                        ],
                        "name": "D. Heggie",
                        "slug": "D.-Heggie",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Heggie",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heggie"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 161537523,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "4981919584cff4701638a84cad57c76143dc70da",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface List of contributors Part I. Invited Papers: 1. Megalithic astronomy: highlights and problems D. C. Heggie 2. Archaeology and astronomy: an archaeological view J. N. G. Ritchie 3. The statistical approach P. R. Freeman 4. Statistical and philosophical arguments for the astronomical significance of standing stones with a section on the solar calendar A. and A. S. Thom 5. Megalithic astronomical sightlines: current reassessment and future directions C. L. N. Ruggles 6. Aspects of the archaeoastronomy of Stonehenge R. J. C. Atkinson 7. Implications for archaeology E. W. MacKie 8. Pi in the sky H. A. W. Burl Part II. Contributed Papers: 9. A survey of the Barbrook stone circles and their claimed astronomical alignments R. P. Norris, P. N. Appleton and R. W. Few 10. Observations at Kintraw T. McCreery, A. J. Hastie and T. Moulds 11. Decoding the Callandish complex - a progress report M. R. and G. H. Ponting 12. Astronomy and stone alignments in S. W. Ireland A. Lynch 13. Stone rings of Northern Poland R. M. Sadowski, M. S. Ziokowski and K. Piasecki 14. Astronomical orientation of Neolithic sites in Central Europe W. Schlosser and J. Cierny 15. Stone circle geometries: an information theory approach J. D. Patrick and C. S. Wallace Part III. Invited Paper: 16. The present position of archaeoastronomy C. Pedersen Index."
            },
            "slug": "Archaeoastronomy-in-the-Old-World-Heggie",
            "title": {
                "fragments": [],
                "text": "Archaeoastronomy in the Old World"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2547829"
                        ],
                        "name": "B. Yandell",
                        "slug": "B.-Yandell",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Yandell",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yandell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122765020,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "56c810594ae2ec5d72f5f2cfd4799f75ff6f8fe2",
            "isKey": false,
            "numCitedBy": 456,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "A wheeled toy vehicle including a drive assembly which comprises a monofilament line having one extremity connected to a manually operable control means and the opposite end connected to the running gear of the vehicle. The dimensions and configuration of the monofilament line is such as to transmit rotation of the line about its own longitudinal axis, caused by activation of the control means, directly to the running gear which may comprise a drive axle and/or one or more drive wheels. Connecting means may attach the one extremity of the line to a predetermined outer portion of an axle or wheel by means of forming a socket therein correspondingly shaped to at least partially enclose a finger attached to the extremity of the line means cooperating therewith. Alternately, a finger can be connected to the extremity of the drive axle and be disposed so as to be enclosed within a socket formed within a sleeve which is connected to the extremity of the line and comprises another embodiment of the connecting means."
            },
            "slug": "Spline-smoothing-and-nonparametric-regression-Yandell",
            "title": {
                "fragments": [],
                "text": "Spline smoothing and nonparametric regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737945"
                        ],
                        "name": "H. Akaike",
                        "slug": "H.-Akaike",
                        "structuredName": {
                            "firstName": "Hirotugu",
                            "lastName": "Akaike",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Akaike"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120510150,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "22cef227af3c3700693a053655ca82f01244167b",
            "isKey": false,
            "numCitedBy": 1170,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "In a recent paper by the present author [1] a simple practical procedure of predictor identification has been proposed. It is the purpose of this paper to provide a theoretical and empirical basis of the procedure."
            },
            "slug": "Statistical-predictor-identification-Akaike",
            "title": {
                "fragments": [],
                "text": "Statistical predictor identification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052516042"
                        ],
                        "name": "G. Schwarz",
                        "slug": "G.-Schwarz",
                        "structuredName": {
                            "firstName": "Gideon",
                            "lastName": "Schwarz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Schwarz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "Any implementation of MDL necessitates approximations in evaluating the length of the ideal shortest message."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "`Minimum description length' (MDL) methods are closely related to this Bayesian framework."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "I can see no advantage in MDL, and recommend that the evidence should be approximated directly."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 46
                            }
                        ],
                        "text": "Akaike's criteria are an approximation to MDL [16, 24, 25]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 123722079,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "37e44d1de8003d8394d158ec6afd1ff0e87e595b",
            "isKey": true,
            "numCitedBy": 39570,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimating-the-Dimension-of-a-Model-Schwarz",
            "title": {
                "fragments": [],
                "text": "Estimating the Dimension of a Model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 67
                            }
                        ],
                        "text": "Bayesian interpolation through noise{free data has been studied by Skilling and Sibisi [17]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 92
                            }
                        ],
                        "text": "This radial basis function model is identical to the `intrinsic correlation' model of Gull, Skilling and Sibisi [6, 17]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 83
                            }
                        ],
                        "text": "The Bayesian framework I will describe for these tasks is due to Gull and Skilling [5, 6, 8, 17, 18], who have used Bayesian methods to achieve the state of the art in image reconstruction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 27
                            }
                        ],
                        "text": "What then was the cause of Skilling's result?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 25
                            }
                        ],
                        "text": "This framework is due to Gull and Skilling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 61
                            }
                        ],
                        "text": "For large dimensional problems where this task is demanding, Skilling has developed methods for estimating TraceA 1 statistically [21]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 13
                            }
                        ],
                        "text": "Furthermore, Skilling demonstrated that with some data sets a free form (maximum entropy) hypothesis can have greater evidence than the truth [20], but is it possible for this to happen in the typical case, as Skilling seems to claim?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 50
                            }
                        ],
                        "text": "I will show that the answer is no, the e ect that Skilling demonstrated cannot be systematic."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Quanti ed Maximum Entropy. MemSys5 User's manual, M.E.D.C., 33 North End, Royston, SG8 6NR, England"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "Any implementation of MDL necessitates approximations in evaluating the length of the ideal shortest message."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "`Minimum description length' (MDL) methods are closely related to this Bayesian framework."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "I can see no advantage in MDL, and recommend that the evidence should be approximated directly."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 46
                            }
                        ],
                        "text": "Akaike's criteria are an approximation to MDL [16, 24, 25]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An information measure for classi cation, Comput"
            },
            "venue": {
                "fragments": [],
                "text": "J. 11"
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144850958"
                        ],
                        "name": "J. Patrick",
                        "slug": "J.-Patrick",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Patrick",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Patrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8635127"
                        ],
                        "name": "C. S. Wallace",
                        "slug": "C.-S.-Wallace",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Wallace"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Thus the evidence is found by taking the best t likelihood that the model can achieve and multiplying it by an `Occam factor' [5], which is a term with magnitude less than one that penalises H\ni\nfor having the parameter w."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 12
                            }
                        ],
                        "text": "As the quantities of data collected throughout science and engineering continue to increase, and the computational power and techniques available to model that data also multiply, I believe Bayesian methods will prove an ever more important tool for re ning our modelling abilities."
                    },
                    "intents": []
                }
            ],
            "corpusId": 190222497,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "0dced67b882a610063ce9de08af102978d1288ec",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Archaeoastronomy-in-the-Old-World:-STONE-CIRCLE-AN-Patrick-Wallace",
            "title": {
                "fragments": [],
                "text": "Archaeoastronomy in the Old World: STONE CIRCLE GEOMETRIES: AN INFORMATION THEORY APPROACH"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9835269"
                        ],
                        "name": "T. Fomby",
                        "slug": "T.-Fomby",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Fomby",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Fomby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3623267"
                        ],
                        "name": "A. Zellner",
                        "slug": "A.-Zellner",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Zellner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zellner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 209
                            }
                        ],
                        "text": "Thus the evidence is found by taking the best t likelihood that the model can achieve and multiplying it by an `Occam factor' [5], which is a term with magnitude less than one that penalises H\ni\nfor having the parameter w."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This framework is due to Gull and Skilling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 124119213,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "a6b5ceda6e1d6607527cfa086e095bca0482ecb7",
            "isKey": false,
            "numCitedBy": 188,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Basic-Issues-in-Econometrics.-Fomby-Zellner",
            "title": {
                "fragments": [],
                "text": "Basic Issues in Econometrics."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48133796"
                        ],
                        "name": "R. T. Cox",
                        "slug": "R.-T.-Cox",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Cox",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. T. Cox"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120613997,
            "fieldsOfStudy": [
                "Physics",
                "Mathematics"
            ],
            "id": "636b6040f970c0a1857039de2b3ea49cd4ac2101",
            "isKey": false,
            "numCitedBy": 1272,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probability,-frequency-and-reasonable-expectation-Cox",
            "title": {
                "fragments": [],
                "text": "Probability, frequency and reasonable expectation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145409498"
                        ],
                        "name": "J. Justice",
                        "slug": "J.-Justice",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Justice",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Justice"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115305104,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8150272d9b5aa033273161611664f6a611c355b7",
            "isKey": false,
            "numCitedBy": 166,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Maximum-entropy-and-bayesian-methods-in-applied-Justice",
            "title": {
                "fragments": [],
                "text": "Maximum entropy and bayesian methods in applied statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400920289"
                        ],
                        "name": "N. Weir",
                        "slug": "N.-Weir",
                        "structuredName": {
                            "firstName": "Nic",
                            "lastName": "Weir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Weir"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117181385,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "56917e0825718186fe42569d6820fafb6a075c58",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Applications-of-Maximum-Entropy-Techniques-to-HST-Weir",
            "title": {
                "fragments": [],
                "text": "Applications of Maximum Entropy Techniques to HST Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2848403"
                        ],
                        "name": "M. Buhmann",
                        "slug": "M.-Buhmann",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Buhmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117680070,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4f9b2110f1f1639406d69953f554f907b001d0f3",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Radial-basis-function-interpolation-on-an-infinite-Buhmann-Powell",
            "title": {
                "fragments": [],
                "text": "Radial basis function interpolation on an infinite regular grid"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123729051"
                        ],
                        "name": "L. M. M.-T.",
                        "slug": "L.-M.-M.-T.",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "M.-T.",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. M. M.-T."
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 187
                            }
                        ],
                        "text": "that is, the ratio of the posterior accessible volume of \u2018His parameter space to the prior accessible volume, or the factor by which \u2018Hi\u2019s hypothesis space collapses when the data arrive (Gull 1988; Jeffreys 1939)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4036480,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f1f4386524be3ed96caaf05f661aacb94db1e566",
            "isKey": false,
            "numCitedBy": 5289,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-of-Probability-M.-T.",
            "title": {
                "fragments": [],
                "text": "Theory of Probability"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1929
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 220119561,
            "fieldsOfStudy": [],
            "id": "0a55b22bc98bc997bc31af0244038643e2bae74a",
            "isKey": false,
            "numCitedBy": 6373,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Received",
            "title": {
                "fragments": [],
                "text": "Received"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum Entropy and Bayesian Methods, Laramie 1990, Kluwer"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 92
                            }
                        ],
                        "text": "0 \u201dMinimum description length\u201d (MDL) methods are closely related to this Bayesian framework (Rissanen 1978; Wallace and Boulton 1968; Wallace and Freeman 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An information measure for classifica"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An information measure for classication"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J"
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A bayesian framework for regularization. 1994. submitted to IEEE Transaction on Pattern Analysis and Machine Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "A bayesian framework for regularization. 1994. submitted to IEEE Transaction on Pattern Analysis and Machine Intelligence"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Quantified Maximum Entropy. MemSys5 User's Manual"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probability, frequency, and reasonable expectation', AmOptimization in the regularization of ill{posed problems"
            },
            "venue": {
                "fragments": [],
                "text": "J. Physics J. Austral. Mat. Soc. Ser. B"
            },
            "year": 1964
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian methods: General background"
            },
            "venue": {
                "fragments": [],
                "text": "In Maximum Entropy and Bayesian Methods in Applied Statistics"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 55
                            }
                        ],
                        "text": "This is the well known Bayesian view of regularisation [15, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computational vision and regularization theory, Nature"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 90
                            }
                        ],
                        "text": "For a general review of Bayesian philosophy see the excellent papers by Loredo and Jaynes [10, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 58
                            }
                        ],
                        "text": "The classic example is the `Sure Thing' hypothesis,\nc E.T Jaynes, which is the hypothesis that the data set will\nbeD, the precise data set that actually occured; the evidence for the Sure Thing hypothesis is huge."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian methods: general background, in Maximum Entropy and Bayesian Methods in Applied Statistics, ed"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1992a. A practical Bayesian framework for backpropagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 188
                            }
                        ],
                        "text": "If a poor regulariser is used, for example, one that is ill{matched to the statistics of the world, then the Bayesian choice of will often not be the best in terms of generalisation error [3, 6, 9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimization in the regularization of ill{posed problems"
            },
            "venue": {
                "fragments": [],
                "text": "J. Austral. Mat. Soc. Ser. B"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Regularization of visual problems involving discontinuities"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. on Pattern Analysis and Machine Intelligence"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On parameter estimation and quantied MaxEnt"
            },
            "venue": {
                "fragments": [],
                "text": "On parameter estimation and quantied MaxEnt"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic relaxation, gibbs distribution, and the bayesian restorat ion of images Determining optical ow"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. on Pattern Analysis and Machine Intelligence Artiicial Intelligence"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 201
                            }
                        ],
                        "text": "It is possible to visualize the joint error bars on the interpolant by making typical samples from the posterior distribution, performing a random walk around the posterior \"bubble\" in parameter space (Sibisi 1991; Skilling et al. 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian interpolation"
            },
            "venue": {
                "fragments": [],
                "text": "Maximum Entropy and Bayesian Methods, Laramie, 1990, W. T. Grandy, Jr., and L. H. Schick, eds., pp. 349-355. Kluwer,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 155
                            }
                        ],
                        "text": "The Bayesian framework I will describe for these tasks is due to Gull and Skilling [5, 6, 8, 17, 18], who have used Bayesian methods to achieve the state of the art in image reconstruction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This framework is due to Gull and Skilling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian classication theory"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian classication theory"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This framework is due to Gull and Skilling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic displays"
            },
            "venue": {
                "fragments": [],
                "text": "Probabilistic displays"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 142
                            }
                        ],
                        "text": "Furthermore, Skilling demonstrated that with some data sets a free form (maximum entropy) hypothesis can have greater evidence than the truth [20], but is it possible for this to happen in the typical case, as Skilling seems to claim? I will show that the answer is no, the e ect that Skilling demonstrated cannot be systematic."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 76
                            }
                        ],
                        "text": "(If you are interested to see problems where subjective priors do arise see [7, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On parameter estimation and quanti ed MaxEnt"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Zellner, A"
            },
            "venue": {
                "fragments": [],
                "text": "1984."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian methods: General background"
            },
            "venue": {
                "fragments": [],
                "text": "Maximum Entropy and Bayesian Methods in Applied Statistics, J. H. Justice, ed., pp. 1-25. Cambridge University Press, Cambridge."
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Solution of Ill-Posed Problems"
            },
            "venue": {
                "fragments": [],
                "text": "Solution of Ill-Posed Problems"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation and inference by compact"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This framework is due to Gull and Skilling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Quantied Maximum Entropy . MemSys5 User's manual"
            },
            "venue": {
                "fragments": [],
                "text": "Quantied Maximum Entropy . MemSys5 User's manual"
            },
            "year": 1991
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 17,
            "methodology": 17
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 73,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/Bayesian-Interpolation-Mackay/8e68c54f39e87daf3a8bdc0ee005aece3c652d11?sort=total-citations"
}