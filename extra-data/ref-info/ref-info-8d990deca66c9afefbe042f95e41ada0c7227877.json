{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48491978"
                        ],
                        "name": "W. K. Hastings",
                        "slug": "W.-K.-Hastings",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Hastings",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. K. Hastings"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 21204149,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "143d2e02ab91ae6259576ac50b664b8647af8988",
            "isKey": false,
            "numCitedBy": 13621,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed. For numerical problems in a large number of dimensions, Monte Carlo methods are often more efficient than conventional numerical methods. However, implementation of the Monte Carlo methods requires sampling from high dimensional probability distributions and this may be very difficult and expensive in analysis and computer time. General methods for sampling from, or estimating expectations with respect to, such distributions are as follows. (i) If possible, factorize the distribution into the product of one-dimensional conditional distributions from which samples may be obtained. (ii) Use importance sampling, which may also be used for variance reduction. That is, in order to evaluate the integral J = X) p(x)dx = Ev(f), where p(x) is a probability density function, instead of obtaining independent samples XI, ..., Xv from p(x) and using the estimate J, = Zf(xi)/N, we instead obtain the sample from a distribution with density q(x) and use the estimate J2 = Y{f(xj)p(x1)}/{q(xj)N}. This may be advantageous if it is easier to sample from q(x) thanp(x), but it is a difficult method to use in a large number of dimensions, since the values of the weights w(xi) = p(x1)/q(xj) for reasonable values of N may all be extremely small, or a few may be extremely large. In estimating the probability of an event A, however, these difficulties may not be as serious since the only values of w(x) which are important are those for which x -A. Since the methods proposed by Trotter & Tukey (1956) for the estimation of conditional expectations require the use of importance sampling, the same difficulties may be encountered in their use. (iii) Use a simulation technique; that is, if it is difficult to sample directly from p(x) or if p(x) is unknown, sample from some distribution q(y) and obtain the sample x values as some function of the corresponding y values. If we want samples from the conditional dis"
            },
            "slug": "Monte-Carlo-Sampling-Methods-Using-Markov-Chains-Hastings",
            "title": {
                "fragments": [],
                "text": "Monte Carlo Sampling Methods Using Markov Chains and Their Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3371112"
                        ],
                        "name": "L. Tierney",
                        "slug": "L.-Tierney",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Tierney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Tierney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741375972"
                        ],
                        "name": "J. Kadane",
                        "slug": "J.-Kadane",
                        "structuredName": {
                            "firstName": "Joseph (Jay) B.",
                            "lastName": "Kadane",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kadane"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 29
                            }
                        ],
                        "text": "niques for such calculations (e.g., see Geweke 1988; Naylor and Smith 1982, 1988; Shaw 1988; Smith et al. 1987; Smith, Skene, Shaw, Naylor, and Dransfield 1985; Tierney and Kadane 1986), but implementation of these approaches typically requires sophisticated numerical analytic expertise, and possibly specialist software."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16603443,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "aa7138c899fd48d3c8df2ccbb65dce06ca4d12c2",
            "isKey": false,
            "numCitedBy": 1982,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This article describes approximations to the posterior means and variances of positive functions of a real or vector-valued parameter, and to the marginal posterior densities of arbitrary (i.e., not necessarily positive) parameters. These approximations can also be used to compute approximate predictive densities. To apply the proposed method, one only needs to be able to maximize slightly modified likelihood functions and to evaluate the observed information at the maxima. Nevertheless, the resulting approximations are generally as accurate and in some cases more accurate than approximations based on third-order expansions of the likelihood and requiring the evaluation of third derivatives. The approximate marginal posterior densities behave very much like saddle-point approximations for sampling distributions. The principal regularity condition required is that the likelihood times prior be unimodal."
            },
            "slug": "Accurate-Approximations-for-Posterior-Moments-and-Tierney-Kadane",
            "title": {
                "fragments": [],
                "text": "Accurate Approximations for Posterior Moments and Marginal Densities"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "These approximations to the posterior means and variances of positive functions of a real or vector-valued parameter, and to the marginal posterior densities of arbitrary parameters can also be used to compute approximate predictive densities."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15974963"
                        ],
                        "name": "A. F. Smith",
                        "slug": "A.-F.-Smith",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Smith",
                            "middleNames": [
                                "F.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. F. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909853"
                        ],
                        "name": "A. Skene",
                        "slug": "A.-Skene",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Skene",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Skene"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144988486"
                        ],
                        "name": "J. E. H. Shaw",
                        "slug": "J.-E.-H.-Shaw",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Shaw",
                            "middleNames": [
                                "E.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. E. H. Shaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50471303"
                        ],
                        "name": "J. Naylor",
                        "slug": "J.-Naylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Naylor",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Naylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071939372"
                        ],
                        "name": "M. Dransfield",
                        "slug": "M.-Dransfield",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Dransfield",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dransfield"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122931538,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a692a8448f53c6569a3ae246ff368041f34d441",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Routine implementation of the Bayesian paradigm requires an efficient approach to the calculation and display of posterior or predictive distributions for given likelihood and prior specifi- cations. In this paper we shall review some of the analytic and numerical approaches currently available, describing in detail a numerical integration strategy based on Gaussian quadrature, and an associated strategy for the reconstruction and display of distributions based on spline techniques."
            },
            "slug": "The-implementation-of-the-bayesian-paradigm-Smith-Skene",
            "title": {
                "fragments": [],
                "text": "The implementation of the bayesian paradigm"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A numerical integration strategy based on Gaussian quadrature, and an associated strategy for the reconstruction and display of distributions based on spline techniques are described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31727329"
                        ],
                        "name": "C. Morris",
                        "slug": "C.-Morris",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Morris",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Morris"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53601432,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2446ff36818d62c8851e39bef95da9c02235388a",
            "isKey": false,
            "numCitedBy": 1376,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This article reviews the state of multiparameter shrinkage estimators with emphasis on the empirical Bayes viewpoint, particularly in the case of parametric prior distributions. Some successful applications of major importance are considered. Recent results concerning estimates of error and confidence intervals are described and illustrated with data."
            },
            "slug": "Parametric-Empirical-Bayes-Inference:-Theory-and-Morris",
            "title": {
                "fragments": [],
                "text": "Parametric Empirical Bayes Inference: Theory and Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145161220"
                        ],
                        "name": "J. Geweke",
                        "slug": "J.-Geweke",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Geweke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Geweke"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120558029,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "a155da36c401bdd7fee08fae2b0bf27e9c559466",
            "isKey": false,
            "numCitedBy": 219,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Antithetic-acceleration-of-Monte-Carlo-integration-Geweke",
            "title": {
                "fragments": [],
                "text": "Antithetic acceleration of Monte Carlo integration in Bayesian inference"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50471303"
                        ],
                        "name": "J. Naylor",
                        "slug": "J.-Naylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Naylor",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Naylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109412403"
                        ],
                        "name": "A.F.M. Smith",
                        "slug": "A.F.M.-Smith",
                        "structuredName": {
                            "firstName": "A.F.M.",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A.F.M. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123146264,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e16e12ff05e9481d00daeb2e2c00c8f0b221e4d6",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Econometric-illustrations-of-novel-numerical-for-Naylor-Smith",
            "title": {
                "fragments": [],
                "text": "Econometric illustrations of novel numerical integration strategies for Bayesian inference"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1410343996"
                        ],
                        "name": "G. M. El-Sayyad",
                        "slug": "G.-M.-El-Sayyad",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "El-Sayyad",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. M. El-Sayyad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102452458"
                        ],
                        "name": "M. Samuddin",
                        "slug": "M.-Samuddin",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Samuddin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Samuddin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1476083416"
                        ],
                        "name": "A. A. Al-Harbey",
                        "slug": "A.-A.-Al-Harbey",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Al-Harbey",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. A. Al-Harbey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122570714,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "0c004ff1faff2e498895e46df1d9b7ab0ad1ab58",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY The paper makes a critical assessment of Aitchison's criterion of density estimation. It seems not to satisfy certain intuitive requirements. A new criterion based on decisions theoretic considerations is introduced. It is shown through examples that the judgement about the choice of density estimates is seriously dependent on the criterion chosen. Some other density estimates are introduced which look similar to Bayesian predictive density estimates without using a prior distribution."
            },
            "slug": "On-parametric-density-estimation-El-Sayyad-Samuddin",
            "title": {
                "fragments": [],
                "text": "On parametric density estimation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15974963"
                        ],
                        "name": "A. F. Smith",
                        "slug": "A.-F.-Smith",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Smith",
                            "middleNames": [
                                "F.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. F. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909853"
                        ],
                        "name": "A. Skene",
                        "slug": "A.-Skene",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Skene",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Skene"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144988486"
                        ],
                        "name": "J. E. H. Shaw",
                        "slug": "J.-E.-H.-Shaw",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Shaw",
                            "middleNames": [
                                "E.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. E. H. Shaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50471303"
                        ],
                        "name": "J. Naylor",
                        "slug": "J.-Naylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Naylor",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Naylor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61481503,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b36b35d12c3cde467d409fbb9a4b9cfd4339fc1",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the main obstacles to the routine implementation of Bayesian methods has been the absence of efficient algorithms for carrying out the computational tasks implicit in the Bayesian approach. In this paper, recent progress towards overcoming this problem is reviewed. In particular, novel numerical integration and interpolation methods, which exploit the opportunities offered by modern interactive computing and graphics facilities, are outlined and illustrated."
            },
            "slug": "Progress-with-numerical-and-graphical-methods-for-Smith-Skene",
            "title": {
                "fragments": [],
                "text": "Progress with numerical and graphical methods for practical Bayesian statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Novel numerical integration and interpolation methods, which exploit the opportunities offered by modern interactive computing and graphics facilities, are outlined and illustrated."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31781461"
                        ],
                        "name": "M. Tanner",
                        "slug": "M.-Tanner",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Tanner",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tanner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143725639"
                        ],
                        "name": "W. Wong",
                        "slug": "W.-Wong",
                        "structuredName": {
                            "firstName": "Wing",
                            "lastName": "Wong",
                            "middleNames": [
                                "Hung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Wong"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122088924,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a44241bf4d932fc09bc683f211360c43f02fd106",
            "isKey": false,
            "numCitedBy": 4042,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The idea of data augmentation arises naturally in missing value problems, as exemplified by the standard ways of filling in missing cells in balanced two-way tables. Thus data augmentation refers to a scheme of augmenting the observed data so as to make it more easy to analyze. This device is used to great advantage by the EM algorithm (Dempster, Laird, and Rubin 1977) in solving maximum likelihood problems. In situations when the likelihood cannot be approximated closely by the normal likelihood, maximum likelihood estimates and the associated standard errors cannot be relied upon to make valid inferential statements. From the Bayesian point of view, one must now calculate the posterior distribution of parameters of interest. If data augmentation can be used in the calculation of the maximum likelihood estimate, then in the same cases one ought to be able to use it in the computation of the posterior distribution. It is the purpose of this article to explain how this can be done. The basic idea ..."
            },
            "slug": "The-calculation-of-posterior-distributions-by-data-Tanner-Wong",
            "title": {
                "fragments": [],
                "text": "The calculation of posterior distributions by data augmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "If data augmentation can be used in the calculation of the maximum likelihood estimate, then in the same cases one ought to be able to use it in the computation of the posterior distribution of parameters of interest."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47970684"
                        ],
                        "name": "D. Gaver",
                        "slug": "D.-Gaver",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Gaver",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gaver"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152498011"
                        ],
                        "name": "I. O'Muircheartaigh",
                        "slug": "I.-O'Muircheartaigh",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "O'Muircheartaigh",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. O'Muircheartaigh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 120645772,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bf307dd427539f64d87b22e01af0e5e2da80524d",
            "isKey": false,
            "numCitedBy": 147,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "A collection of I similar items generates point event histories; for example, machines experience failures or operators make mistakes. Suppose the intervals between events are modeled as iid exponential (\u03bb i , or the counts as Poisson (\u03bb i t i ,) for the ith item. Furthermore, so as to represent between-item variability, each individual rate parameter, \u03bb i , is presumed drawn from a fixed (super) population with density g \u03bb (\u00b7; \u03b8), \u03b8 being a vector parameter: a parametric empirical Bayes (PEB) setup. For g \u03bb, specified alternatively as log-Student t(n) or gamma, we exhibit the results of numerical procedures for estimating superpopulation parameters ll and for describing pooled estimates of the individual rates, \u03bb i , obtained via Bayes's formula. Three data sets are analyzed, and convenient explicit approximate formulas are furnished for \u03bb i estimates. In the Student-t case, the individual estimates are seen to have a robust quality."
            },
            "slug": "Robust-empirical-bayes-analyses-of-event-rates-Gaver-O'Muircheartaigh",
            "title": {
                "fragments": [],
                "text": "Robust empirical bayes analyses of event rates"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145948621"
                        ],
                        "name": "G. Box",
                        "slug": "G.-Box",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Box",
                            "middleNames": [
                                "E.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Box"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36184409"
                        ],
                        "name": "G. C. Tiao",
                        "slug": "G.-C.-Tiao",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Tiao",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. C. Tiao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122028907,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a205103d4f25ae39f417bac7bd5142302d7f448c",
            "isKey": false,
            "numCitedBy": 4326,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Nature of Bayesian Inference Standard Normal Theory Inference Problems Bayesian Assessment of Assumptions: Effect of Non-Normality on Inferences About a Population Mean with Generalizations Bayesian Assessment of Assumptions: Comparison of Variances Random Effect Models Analysis of Cross Classification Designs Inference About Means with Information from More than One Source: One-Way Classification and Block Designs Some Aspects of Multivariate Analysis Estimation of Common Regression Coefficients Transformation of Data Tables References Indexes."
            },
            "slug": "Bayesian-inference-in-statistical-analysis-Box-Tiao",
            "title": {
                "fragments": [],
                "text": "Bayesian inference in statistical analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This chapter discusses Bayesian Assessment of Assumptions, which investigates the effect of non-Normality on Inferences about a Population Mean with Generalizations in the context of a Bayesian inference model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3039324"
                        ],
                        "name": "R. Okafor",
                        "slug": "R.-Okafor",
                        "structuredName": {
                            "firstName": "Ray",
                            "lastName": "Okafor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Okafor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120058670,
            "fieldsOfStudy": [
                "Business",
                "Mathematics"
            ],
            "id": "d15b9bf22837470154cbcc976fd409b7287ba4cf",
            "isKey": false,
            "numCitedBy": 352,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY Y is a linear regression on a variable X; X is fixed and all its sample values are observed. Y, on the other hand, has some sample values missing. This work outlines a maximum likelihood (ml) procedure that tries to adjust for bias due to non-random missingness; here non-randomness is specified by a logistic distribution. The ml procedure is implemented via two iterative technologies, namely the EM algorithm (of Dempster, Laird & Rubin, 1977) and the Newton-Raphson method. Data from a dialysis study are used to illustrate our estimation procedure, and results show that the ml procedure is quite effective in adjusting for bias."
            },
            "slug": "Maximum-likelihood-estimation-from-incomplete-data-Okafor",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood estimation from incomplete data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The answer is taken up in great detail by Geman and Geman (1984), involving concepts such as graphs, neighborhood systems, cliques, Markov random fields, and Gibbs distributions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "An algorithm for extracting the marginal distributions from these full conditional distributions was formally introduced by Geman and Geman (1984) and is known as the Gibbs sampler."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "introduced by Geman and Geman (1984), and the form of importance-sampling algorithm proposed by Rubin (1987, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5837272,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "459b30a9a960080f3b313e41886b1aa0e51e882c",
            "isKey": true,
            "numCitedBy": 18711,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios."
            },
            "slug": "Stochastic-Relaxation,-Gibbs-Distributions,-and-the-Geman-Geman",
            "title": {
                "fragments": [],
                "text": "Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The analogy between images and statistical mechanics systems is made and the analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations, creating a highly parallel ``relaxation'' algorithm for MAP estimation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477489"
                        ],
                        "name": "L. Devroye",
                        "slug": "L.-Devroye",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Devroye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Devroye"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6898695,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "6733e22b52d8a86988a36be7404cd38fd5b8a66f",
            "isKey": false,
            "numCitedBy": 3705,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "This is a survey of the main methods in non-uniform random variate generation, and highlights recent research on the subject. Classical paradigms such as inversion, rejection, guide tables, and transformations are reviewed. We provide information on the expected time complexity of various algorithms, before addressing modern topics such as indirectly specified distributions, random processes, and Markov chain methods. Authors\u2019 address: School of Computer Science, McGill University, 3480 University Street, Montreal, Canada H3A 2K6. The authors\u2019 research was sponsored by NSERC Grant A3456 and FCAR Grant 90-ER-0291. 1. The main paradigms The purpose of this chapter is to review the main methods for generating random variables, vectors and processes. Classical workhorses such as the inversion method, the rejection method and table methods are reviewed in section 1. In section 2, we discuss the expected time complexity of various algorithms, and give a few examples of the design of generators that are uniformly fast over entire families of distributions. In section 3, we develop a few universal generators, such as generators for all log concave distributions on the real line. Section 4 deals with random variate generation when distributions are indirectly specified, e.g, via Fourier coefficients, characteristic functions, the moments, the moment generating function, distributional identities, infinite series or Kolmogorov measures. Random processes are briefly touched upon in section 5. Finally, the latest developments in Markov chain methods are discussed in section 6. Some of this work grew from Devroye (1986a), and we are carefully documenting work that was done since 1986. More recent references can be found in the book by H\u00f6rmann, Leydold and Derflinger (2004). Non-uniform random variate generation is concerned with the generation of random variables with certain distributions. Such random variables are often discrete, taking values in a countable set, or absolutely continuous, and thus described by a density. The methods used for generating them depend upon the computational model one is working with, and upon the demands on the part of the output. For example, in a ram (random access memory) model, one accepts that real numbers can be stored and operated upon (compared, added, multiplied, and so forth) in one time unit. Furthermore, this model assumes that a source capable of producing an i.i.d. (independent identically distributed) sequence of uniform [0, 1] random variables is available. This model is of course unrealistic, but designing random variate generators based on it has several advantages: first of all, it allows one to disconnect the theory of non-uniform random variate generation from that of uniform random variate generation, and secondly, it permits one to plan for the future, as more powerful computers will be developed that permit ever better approximations of the model. Algorithms designed under finite approximation limitations will have to be redesigned when the next generation of computers arrives. For the generation of discrete or integer-valued random variables, which includes the vast area of the generation of random combinatorial structures, one can adhere to a clean model, the pure bit model, in which each bit operation takes one time unit, and storage can be reported in terms of bits. Typically, one now assumes that an i.i.d. sequence of independent perfect bits is available. In this model, an elegant information-theoretic theory can be derived. For example, Knuth and Yao (1976) showed that to generate a random integer X described by the probability distribution {X = n} = pn, n \u2265 1, any method must use an expected number of bits greater than the binary entropy of the distribution, \u2211"
            },
            "slug": "Non-Uniform-Random-Variate-Generation-Devroye",
            "title": {
                "fragments": [],
                "text": "Non-Uniform Random Variate Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This chapter reviews the main methods for generating random variables, vectors and processes in non-uniform random variate generation, and provides information on the expected time complexity of various algorithms before addressing modern topics such as indirectly specified distributions, random processes, and Markov chain methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144988486"
                        ],
                        "name": "J. E. H. Shaw",
                        "slug": "J.-E.-H.-Shaw",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Shaw",
                            "middleNames": [
                                "E.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. E. H. Shaw"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120981215,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "30b976ac992dcf9a53e6e9bf161e8a0a3a8ad635",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "On passe en revue la theorie existante et on introduit de nouveaux criteres d'identification de suites quasi aleatoires. On suggere des extensions des regles d'integration de base"
            },
            "slug": "A-Quasirandom-Approach-to-Integration-in-Bayesian-Shaw",
            "title": {
                "fragments": [],
                "text": "A Quasirandom Approach to Integration in Bayesian Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122942"
                        ],
                        "name": "B. Ripley",
                        "slug": "B.-Ripley",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Ripley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ripley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6151205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4180abb06c3e6931474c4a3e0ff079d2d5a0382",
            "isKey": false,
            "numCitedBy": 2209,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "One fifth (4 of 20) of the research articles published in the Journal of Educational Statistics in 1988 include simulation studies that justify or illustrate the authors' conclusions. A similar fraction (6 of 33) of the articles in the 1988 volume of Psychometrika include simulations; comparable proportions could be expected in other journals at the boundary of theoretical statistics and social/psychological applications. Due in part to the complexity of the problems tackled today and in part to the availability of cheap, powerful computing\u2014by no means independent influences\u2014simulation and Monte Carlo methods have become both necessary and practical tools for statisticians and applied workers in quantitative areas of education and psychology. Simulation has become popular\u2014not only in the quantitative social sciences, but in all of the mathematical sciences from physics to operations research to number theory\u2014because it is almost always easy to do. This ease of use makes the simulation experimenter vulnerable to two common pitfalls. Selection of the basic source of \"random numbers\" is often passive: Whatever is available in the computer's standard subroutine library is used. However, the fact that a pseudo-random number generator appears in a popular software package or operating system is hardly reason to trust it, as is shown by the infamous RANDU generator, once popular on IBM mainframes and PDP mini-computers, and by the generators burned into RAM on today's PCs. Simulation design and reporting also deserve special care. Some attempt must be made to assess the accuracy of the simulation estimates: One should accurately estimate and report SE (6) as well as 6. In addition, enough detail should be reported that the interested reader can replicate the study and check the results, just as with other experiments. Yet these considerations are also easy to overlook. Brian D. Ripley's Stochastic Simulation is a short, yet ambitious, survey of modern simulation techniques. Three themes run throughout the book. First, one shoud not take basic simulation subroutines for granted, especially on minior microcomputers where they tend to be poor implementations, implementations of poor algorithms, or both. Second, design of experiments, or variance reduction as it is known in this field, deserves greater consideration. Third, modern methods make it possible to simulate and analyze processes that are dependent over time, and using such processes opens the door to new simulation techniques, such as simulated annealing in optimization. Ripley intends this book to be a \"comprehensive guide,\" and it is indeed most accurately described as a researcher's handbook with examples and"
            },
            "slug": "Stochastic-simulation-Ripley",
            "title": {
                "fragments": [],
                "text": "Stochastic simulation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Brian D. Ripley's Stochastic Simulation is a short, yet ambitious, survey of modern simulation techniques, and three themes run throughout the book."
            },
            "venue": {
                "fragments": [],
                "text": "Wiley series in probability and mathematical statistics : applied probability and statistics"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 65
                            }
                        ],
                        "text": "We note that this version of the substitution sampling algorithm differs slightly from the ImputatonPosterior (IP) algoiithm in Tanner and Wong (1987). They propose, at each iteration 1, I = 1,2,."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 21
                            }
                        ],
                        "text": "implicit in Tanner and Wong has also been noted by Clayton (1988), and systematic selection of the yJr was also proposed by Morris (1987b) in his discussion of the Tanner and Wong paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1353,
                                "start": 17
                            }
                        ],
                        "text": "In Section 2, we discuss and extend three alternative approaches put forward in the literature for calculating marginal densities via sampling algorithms. These are (variants of) the stochastic substitution algorithm described by Tanner and Wong (1987), the Gibbs sampler algorithm introduced by Geman and Geman (1984) and the form of importance sampling algorithm proposed by Rubin (1987, 1988). We note that the Gibbs sampler has been widely taken up in the image processing literature, and in other large-scale models such as neural networks and expert systems, but that its general potential for more conventional statistical problems seems to have been overlooked. As we shall show (and as has also been observed by Clayton, 1988), there is a close relationship between the Gibbs sampler and the substitution algorithm proposed by Tanner and Wong (1987). We shall generalize the latter and show that it is as least as efficient as the Gibbs sampler, and potentially more efficient, given the availability of distinct conditional distributions in addition to those in (i) above. We note that, as a consequence of the relationship between the two algorithms, the convergence results established by Geman and Geman (1984) are applicable to the generalised substitution algorithm. The stronger convergence results established by Tanner and Wong (1987) require the availability of a particular set of conditional distributions, including those in (i)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1224,
                                "start": 17
                            }
                        ],
                        "text": "In Section 2, we discuss and extend three alternative approaches put forward in the literature for calculating marginal densities via sampling algorithms. These are (variants of) the stochastic substitution algorithm described by Tanner and Wong (1987), the Gibbs sampler algorithm introduced by Geman and Geman (1984) and the form of importance sampling algorithm proposed by Rubin (1987, 1988). We note that the Gibbs sampler has been widely taken up in the image processing literature, and in other large-scale models such as neural networks and expert systems, but that its general potential for more conventional statistical problems seems to have been overlooked. As we shall show (and as has also been observed by Clayton, 1988), there is a close relationship between the Gibbs sampler and the substitution algorithm proposed by Tanner and Wong (1987). We shall generalize the latter and show that it is as least as efficient as the Gibbs sampler, and potentially more efficient, given the availability of distinct conditional distributions in addition to those in (i) above. We note that, as a consequence of the relationship between the two algorithms, the convergence results established by Geman and Geman (1984) are applicable to the generalised substitution algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 59
                            }
                        ],
                        "text": "1 Substitution algorithm The substitution algorithm for finding fixed point solutions to certain classes of integral equations is a standard mathematical tool, which has received considerable attention in the literature: see, for example, Rail (1969). Its potential utility in statistical problems of the kind we are concerned with in this paper was recently observed by Tanner and Wong (1987) and associated discussion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 17
                            }
                        ],
                        "text": "In Section 2, we discuss and extend three alternative approaches put forward in the literature for calculating marginal densities via sampling algorithms. These are (variants of) the stochastic substitution algorithm described by Tanner and Wong (1987), the Gibbs sampler algorithm introduced by Geman and Geman (1984) and the form of importance sampling algorithm proposed by Rubin (1987, 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 859,
                                "start": 17
                            }
                        ],
                        "text": "In Section 2, we discuss and extend three alternative approaches put forward in the literature for calculating marginal densities via sampling algorithms. These are (variants of) the stochastic substitution algorithm described by Tanner and Wong (1987), the Gibbs sampler algorithm introduced by Geman and Geman (1984) and the form of importance sampling algorithm proposed by Rubin (1987, 1988). We note that the Gibbs sampler has been widely taken up in the image processing literature, and in other large-scale models such as neural networks and expert systems, but that its general potential for more conventional statistical problems seems to have been overlooked. As we shall show (and as has also been observed by Clayton, 1988), there is a close relationship between the Gibbs sampler and the substitution algorithm proposed by Tanner and Wong (1987). We shall generalize the latter and show that it is as least as efficient as the Gibbs sampler, and potentially more efficient, given the availability of distinct conditional distributions in addition to those in (i) above."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 91
                            }
                        ],
                        "text": "= 4lX) i, in a notation making explicit the fact that /A is the integral operator associated with h. Exploiting standard theory of such integral operators, Tanner and Wong (1987) show that, under mild regularity conditions, this iterative process has the following properties (with obviously analogous results for [Y])."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Simulation in Hierarchical Models"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report, University of Leicester. Dempster"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31727329"
                        ],
                        "name": "C. Morris",
                        "slug": "C.-Morris",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Morris",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Morris"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123313553,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "aae16d0c98b66e53bfdbfd0965e77d6ec85d1f34",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Comment:-Simulation-in-Hierarchical-Models-Morris",
            "title": {
                "fragments": [],
                "text": "Comment: Simulation in Hierarchical Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7224133"
                        ],
                        "name": "H. Schwetlick",
                        "slug": "H.-Schwetlick",
                        "structuredName": {
                            "firstName": "Hartmut",
                            "lastName": "Schwetlick",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schwetlick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123426697,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9da932b9e463088cfdbff9945c9ffe5bef283eae",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "L.-B.-Rall,-Computational-Solution-of-Nonlinear-S.-Schwetlick",
            "title": {
                "fragments": [],
                "text": "L. B. Rall, Computational Solution of Nonlinear Operator Equations. VIII+224 S. New York/London/Sydney/Toronto 1969. John Wiley and Sons, Inc. Preis geb. 7.00 \u00a3"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31727329"
                        ],
                        "name": "C. Morris",
                        "slug": "C.-Morris",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Morris",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Morris"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120190707,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b33cfb1fa27ef9dff9e1eedfe75f9f4d995275ab",
            "isKey": false,
            "numCitedBy": 252,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Natural-Exponential-Families-with-Quadratic-Theory-Morris",
            "title": {
                "fragments": [],
                "text": "Natural Exponential Families with Quadratic Variance Functions: Statistical Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46822826"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Db",
                            "lastName": "Rubin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46822826"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Db",
                            "lastName": "Rubin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115305396,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f462abb6de423bda264ff8516762dfa8026b6040",
            "isKey": false,
            "numCitedBy": 675,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Using-the-SIR-algorithm-to-simulate-posterior-Rubin-Rubin",
            "title": {
                "fragments": [],
                "text": "Using the SIR algorithm to simulate posterior distributions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50471303"
                        ],
                        "name": "J. Naylor",
                        "slug": "J.-Naylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Naylor",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Naylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15974963"
                        ],
                        "name": "A. F. Smith",
                        "slug": "A.-F.-Smith",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Smith",
                            "middleNames": [
                                "F.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. F. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64820356,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d6c20c655098619c1845076045601c6e9fab1c29",
            "isKey": false,
            "numCitedBy": 397,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Applications-of-a-Method-for-the-Efficient-of-Naylor-Smith",
            "title": {
                "fragments": [],
                "text": "Applications of a Method for the Efficient Computation of Posterior Distributions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62590854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "168a289b4027f8c77e6aeaf69d91ca56f68b119f",
            "isKey": false,
            "numCitedBy": 171,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Comment-:-A-noniterative-sampling/importance-to-the-Rubin",
            "title": {
                "fragments": [],
                "text": "Comment : A noniterative sampling/importance resampling alternative to the data augmentation algorithm for creating a few imputations when fractions of missing information are modest : The SIR Algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3072213"
                        ],
                        "name": "J. Besag",
                        "slug": "J.-Besag",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Besag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Besag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 42087677,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8bf730243ed967afd5349bef053641a6043517a0",
            "isKey": false,
            "numCitedBy": 6166,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Spatial-Interaction-and-the-Statistical-Analysis-of-Besag",
            "title": {
                "fragments": [],
                "text": "Spatial Interaction and the Statistical Analysis of Lattice Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Implementation fthe Bayesian Paradigm"
            },
            "venue": {
                "fragments": [],
                "text": "Communications i Statistics - Theory and Methods"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Econometric Illustrations ofNovel Numerical Integration Strategies for Bayesian Inferences"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Econometrics"
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The calculation of posterior distributions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comment on \" The Calculation of Posterior Distributions by Data Augmentation , \" byM"
            },
            "venue": {
                "fragments": [],
                "text": "Stochastic Simulation"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data ( with discussion )"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Royal Statistical Society B"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computational Solution of Non-linear Operator Equations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 18
                            }
                        ],
                        "text": ", as described by Devroye 1986 and Ripley 1987). In Section 2, we discuss and extend three alternative approaches put forward in the literature for calculating marginal densities via sampling algorithms. These are (variants of) the data-augmentation algorithm described by Tanner and Wong (1987), the Gibbs sampler algorithm"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Non-uniform Random Variate Generation, New York: Springer-Verlag"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 104
                            }
                        ],
                        "text": "The first term on the right can be made arbitrarily small as m -$ X, since [x]~ 4 [XIi for almost all X (Glick 1974)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Consistency Conditions for Probability Estimators and Integrals of Density Estimators"
            },
            "venue": {
                "fragments": [],
                "text": "Utilitas Mathematica"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum Likelihood From Incomplete Data Via the EM Algorithm \" ( with discussion )"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Royal Statistical Society , Ser . B"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Numerical integration and display methods for Bayesian inference"
            },
            "venue": {
                "fragments": [],
                "text": "PhD Thesis, Department of Mathematics, University of Nottingham."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Progress with numerical and graphical methods for Bayesian statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Statistician,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 29
                            }
                        ],
                        "text": "niques for such calculations (e.g., see Geweke 1988; Naylor and Smith 1982, 1988; Shaw 1988; Smith et al. 1987; Smith, Skene, Shaw, Naylor, and Dransfield 1985; Tierney and Kadane 1986), but implementation of these approaches typically requires sophisticated numerical analytic expertise, and possibly specialist software."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Progress With Numerical and Graphical Methods for Bayesian Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "The Statistician"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Non-parametric Density Estimation: The L"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comment on \" The Calculation of Posterior Distributions by Data Augmentation , \" by M . A . Tanner and"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 5,
            "methodology": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 39,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Sampling-Based-Approaches-to-Calculating-Marginal-Gelfand-Smith/8d990deca66c9afefbe042f95e41ada0c7227877?sort=total-citations"
}