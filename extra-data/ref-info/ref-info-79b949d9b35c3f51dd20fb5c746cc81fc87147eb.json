{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47237027"
                        ],
                        "name": "Andreas Geiger",
                        "slug": "Andreas-Geiger",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37108776"
                        ],
                        "name": "Philip Lenz",
                        "slug": "Philip-Lenz",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Lenz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip Lenz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 79
                            }
                        ],
                        "text": "For details about the benchmarks and evaluation metrics we refer the reader to Geiger et al. (2012a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 240
                            }
                        ],
                        "text": "Next, we optimized an error criterion based on the Euclidean distance of 50 manually selected correspondences and a robust measure on the disparity error with respect to the three top performing stereo methods in the KITTI stereo benchmark Geiger et al. (2012a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 30
                            }
                        ],
                        "text": "While our introductory paper (Geiger et al., 2012a) mainly focuses on the benchmarks, their creation and use for evaluating state-of-the-art computer vision methods, here we complement this information by providing technical details on the raw data itself."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 164
                            }
                        ],
                        "text": "We have registered the Velodyne laser scanner with respect to the reference camera coordinate system (camera 0) by initializing the rigid body transformation using Geiger et al. (2012b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 53
                            }
                        ],
                        "text": "For a review on related work, we refer the reader to Geiger et al. (2012a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 93
                            }
                        ],
                        "text": "For calibrating the cameras intrinsically and extrinsically, we use the approach proposed in Geiger et al. (2012b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6724907,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de5b0fd02ea4f4d67fe3ae0d74603b9822df4e42",
            "isKey": false,
            "numCitedBy": 7186,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti."
            },
            "slug": "Are-we-ready-for-autonomous-driving-The-KITTI-suite-Geiger-Lenz",
            "title": {
                "fragments": [],
                "text": "Are we ready for autonomous driving? The KITTI vision benchmark suite"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The autonomous driving platform is used to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection, revealing that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47237027"
                        ],
                        "name": "Andreas Geiger",
                        "slug": "Andreas-Geiger",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3128253"
                        ],
                        "name": "F. Moosmann",
                        "slug": "F.-Moosmann",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Moosmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Moosmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2738114"
                        ],
                        "name": "Omer Car",
                        "slug": "Omer-Car",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Car",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Car"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057784291"
                        ],
                        "name": "Bernhard Schuster",
                        "slug": "Bernhard-Schuster",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bernhard Schuster"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 79
                            }
                        ],
                        "text": "For details about the benchmarks and evaluation metrics we refer the reader to Geiger et al. (2012a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 240
                            }
                        ],
                        "text": "Next, we optimized an error criterion based on the Euclidean distance of 50 manually selected correspondences and a robust measure on the disparity error with respect to the three top performing stereo methods in the KITTI stereo benchmark Geiger et al. (2012a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 30
                            }
                        ],
                        "text": "While our introductory paper (Geiger et al., 2012a) mainly focuses on the benchmarks, their creation and use for evaluating state-of-the-art computer vision methods, here we complement this information by providing technical details on the raw data itself."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 164
                            }
                        ],
                        "text": "We have registered the Velodyne laser scanner with respect to the reference camera coordinate system (camera 0) by initializing the rigid body transformation using Geiger et al. (2012b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 53
                            }
                        ],
                        "text": "For a review on related work, we refer the reader to Geiger et al. (2012a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 93
                            }
                        ],
                        "text": "For calibrating the cameras intrinsically and extrinsically, we use the approach proposed in Geiger et al. (2012b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12339854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "140a6bdfb0564eb18a1f51a39dff36f20272a461",
            "isKey": false,
            "numCitedBy": 392,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "As a core robotic and vision problem, camera and range sensor calibration have been researched intensely over the last decades. However, robotic research efforts still often get heavily delayed by the requirement of setting up a calibrated system consisting of multiple cameras and range measurement units. With regard to removing this burden, we present a toolbox with web interface for fully automatic camera-to-camera and camera-to-range calibration. Our system is easy to setup and recovers intrinsic and extrinsic camera parameters as well as the transformation between cameras and range sensors within one minute. In contrast to existing calibration approaches, which often require user intervention, the proposed method is robust to varying imaging conditions, fully automatic, and easy to use since a single image and range scan proves sufficient for most calibration scenarios. Experimentally, we demonstrate that the proposed checkerboard corner detector significantly outperforms current state-of-the-art. Furthermore, the proposed camera-to-range registration method is able to discover multiple solutions in the case of ambiguities. Experiments using a variety of sensors such as grayscale and color cameras, the Kinect 3D sensor and the Velodyne HDL-64 laser scanner show the robustness of our method in different indoor and outdoor settings and under various lighting conditions."
            },
            "slug": "Automatic-camera-and-range-sensor-calibration-using-Geiger-Moosmann",
            "title": {
                "fragments": [],
                "text": "Automatic camera and range sensor calibration using a single shot"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is demonstrated that the proposed checkerboard corner detector significantly outperforms current state-of-the-art and the proposed camera-to-range registration method is able to discover multiple solutions in the case of ambiguities."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE International Conference on Robotics and Automation"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47237027"
                        ],
                        "name": "Andreas Geiger",
                        "slug": "Andreas-Geiger",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500880"
                        ],
                        "name": "Martin Lauer",
                        "slug": "Martin-Lauer",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Lauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin Lauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 192
                            }
                        ],
                        "text": "The main purpose of this dataset is to push forward the development of computer vision and robotic algorithms targeted at autonomous driving (Paul and Newman, 2010; Pfeiffer and Franke, 2010; Geiger et al., 2011a,b; Wojek et al., 2012; Singh and Kosecka, 2012; Brubaker et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1640943,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14f82a4e399947ac549008ff5f1da72f05aab0f9",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "3D scene understanding is key for the success of applications such as autonomous driving and robot navigation. However, existing approaches either produce a mild level of understanding, e.g., segmentation, object detection, or are not accurate enough for these applications, e.g., 3D pop-ups. In this paper we propose a principled generative model of 3D urban scenes that takes into account dependencies between static and dynamic features. We derive a reversible jump MCMC scheme that is able to infer the geometric (e.g., street orientation) and topological (e.g., number of intersecting streets) properties of the scene layout, as well as the semantic activities occurring in the scene, e.g., traffic situations at an intersection. Furthermore, we show that this global level of understanding provides the context necessary to disambiguate current state-of-the-art detectors. We demonstrate the effectiveness of our approach on a dataset composed of short stereo video sequences of 113 different scenes captured by a car driving around a mid-size city."
            },
            "slug": "A-generative-model-for-3D-urban-scene-understanding-Geiger-Lauer",
            "title": {
                "fragments": [],
                "text": "A generative model for 3D urban scene understanding from movable platforms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A principled generative model of 3D urban scenes that takes into account dependencies between static and dynamic features and derives a reversible jump MCMC scheme that is able to infer the geometric and topological properties of the scene layout and the semantic activities occurring in the scene."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2575536"
                        ],
                        "name": "Marcus A. Brubaker",
                        "slug": "Marcus-A.-Brubaker",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Brubaker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus A. Brubaker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47237027"
                        ],
                        "name": "Andreas Geiger",
                        "slug": "Andreas-Geiger",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 261
                            }
                        ],
                        "text": "The main purpose of this dataset is to push forward the development of computer vision and robotic algorithms targeted at autonomous driving (Paul and Newman, 2010; Pfeiffer and Franke, 2010; Geiger et al., 2011a,b; Wojek et al., 2012; Singh and Kosecka, 2012; Brubaker et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9082669,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c38962ce01d4e4a940cb421e534e1c9dfd8d5d9",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose an affordable solution to self-localization, which utilizes visual odometry and road maps as the only inputs. To this end, we present a probabilistic model as well as an efficient approximate inference algorithm, which is able to utilize distributed computation to meet the real-time requirements of autonomous systems. Because of the probabilistic nature of the model we are able to cope with uncertainty due to noisy visual odometry and inherent ambiguities in the map (e.g., in a Manhattan world). By exploiting freely available, community developed maps and visual odometry measurements, we are able to localize a vehicle up to 3m after only a few seconds of driving on maps which contain more than 2,150km of drivable roads."
            },
            "slug": "Lost!-Leveraging-the-Crowd-for-Probabilistic-Visual-Brubaker-Geiger",
            "title": {
                "fragments": [],
                "text": "Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "An affordable solution to self-localization, which utilizes visual odometry and road maps as the only inputs and is able to localize a vehicle up to 3m after only a few seconds of driving on maps which contain more than 2,150km of drivable roads."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40521794"
                        ],
                        "name": "D. Pfeiffer",
                        "slug": "D.-Pfeiffer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pfeiffer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pfeiffer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145582788"
                        ],
                        "name": "Uwe Franke",
                        "slug": "Uwe-Franke",
                        "structuredName": {
                            "firstName": "Uwe",
                            "lastName": "Franke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Uwe Franke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 165
                            }
                        ],
                        "text": "The main purpose of this dataset is to push forward the development of computer vision and robotic algorithms targeted at autonomous driving (Paul and Newman, 2010; Pfeiffer and Franke, 2010; Geiger et al., 2011a,b; Wojek et al., 2012; Singh and Kosecka, 2012; Brubaker et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9739315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73df048c20ca94dfc0fbd460e11071f4d5b12ee1",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Correlation based stereo vision has proven its power in commercially available driver assistance systems. Recently, real-time dense stereo vision has become available on inexpensive FPGA hardware. In order to manage the huge amount of data, a medium-level representation named \u201cStixel World\u201d has been proposed for further analysis. In this representation the free space in front of the vehicle is limited by adjacent rectangular sticks of a certain width. Distance and height of each so called stixel are determined by those parts of the obstacle it represents. This Stixel World is a compact but flexible representation of the three-dimensional traffic situation. The underlying model assumption is that objects stand on the ground and have approximately vertical pose with a flat surface. So far, this representation is static since it is computed for each frame independently. Driver assistance, however, is most interested in pose and motion of moving obstacles. For this reason, we introduce tracking of stixels in this paper. Using the 6D-Vision Kalman filter framework, lateral as well as longitudinal motion is estimated for each stixel. That way, the grouping of stixels based on similar motion as well as the detection of moving obstacles turns out to be significantly simplified. The new dynamic Stixel World has proven to be well suited as a common basis for the scene understanding tasks of driver assistance and autonomous systems."
            },
            "slug": "Efficient-representation-of-traffic-scenes-by-means-Pfeiffer-Franke",
            "title": {
                "fragments": [],
                "text": "Efficient representation of traffic scenes by means of dynamic stixels"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The new dynamic Stixel World has proven to be well suited as a common basis for the scene understanding tasks of driver assistance and autonomous systems."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Intelligent Vehicles Symposium"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47237027"
                        ],
                        "name": "Andreas Geiger",
                        "slug": "Andreas-Geiger",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2340109"
                        ],
                        "name": "C. Wojek",
                        "slug": "C.-Wojek",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wojek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Wojek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 192
                            }
                        ],
                        "text": "The main purpose of this dataset is to push forward the development of computer vision and robotic algorithms targeted at autonomous driving (Paul and Newman, 2010; Pfeiffer and Franke, 2010; Geiger et al., 2011a,b; Wojek et al., 2012; Singh and Kosecka, 2012; Brubaker et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15249982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb65531e8f557c1d923402d211b8d42acc05ddf8",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel generative model that is able to reason jointly about the 3D scene layout as well as the 3D location and orientation of objects in the scene. In particular, we infer the scene topology, geometry as well as traffic activities from a short video sequence acquired with a single camera mounted on a moving car. Our generative model takes advantage of dynamic information in the form of vehicle tracklets as well as static information coming from semantic labels and geometry (i.e., vanishing points). Experiments show that our approach outperforms a discriminative baseline based on multiple kernel learning (MKL) which has access to the same image information. Furthermore, as we reason about objects in 3D, we are able to significantly increase the performance of state-of-the-art object detectors in their ability to estimate object orientation."
            },
            "slug": "Joint-3D-Estimation-of-Objects-and-Scene-Layout-Geiger-Wojek",
            "title": {
                "fragments": [],
                "text": "Joint 3D Estimation of Objects and Scene Layout"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A novel generative model is proposed that is able to reason jointly about the 3D scene layout as well as the3D location and orientation of objects in the scene and significantly increase the performance of state-of-the-art object detectors in their ability to estimate object orientation."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2340109"
                        ],
                        "name": "C. Wojek",
                        "slug": "C.-Wojek",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wojek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Wojek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24760164"
                        ],
                        "name": "S. Walk",
                        "slug": "S.-Walk",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Walk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Walk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144810819"
                        ],
                        "name": "K. Schindler",
                        "slug": "K.-Schindler",
                        "structuredName": {
                            "firstName": "Konrad",
                            "lastName": "Schindler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schindler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 216
                            }
                        ],
                        "text": "The main purpose of this dataset is to push forward the development of computer vision and robotic algorithms targeted at autonomous driving (Paul and Newman, 2010; Pfeiffer and Franke, 2010; Geiger et al., 2011a,b; Wojek et al., 2012; Singh and Kosecka, 2012; Brubaker et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6462900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef6bb6b4a5949d0aba98f06f1c8698d8f5aa907f",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "Following recent advances in detection, context modeling, and tracking, scene understanding has been the focus of renewed interest in computer vision research. This paper presents a novel probabilistic 3D scene model that integrates state-of-the-art multiclass object detection, object tracking and scene labeling together with geometric 3D reasoning. Our model is able to represent complex object interactions such as inter-object occlusion, physical exclusion between objects, and geometric context. Inference in this model allows us to jointly recover the 3D scene context and perform 3D multi-object tracking from a mobile observer, for objects of multiple categories, using only monocular video as input. Contrary to many other approaches, our system performs explicit occlusion reasoning and is therefore capable of tracking objects that are partially occluded for extended periods of time, or objects that have never been observed to their full extent. In addition, we show that a joint scene tracklet model for the evidence collected over multiple frames substantially improves performance. The approach is evaluated for different types of challenging onboard sequences. We first show a substantial improvement to the state of the art in 3D multipeople tracking. Moreover, a similar performance gain is achieved for multiclass 3D tracking of cars and trucks on a challenging dataset."
            },
            "slug": "Monocular-Visual-Scene-Understanding:-Understanding-Wojek-Walk",
            "title": {
                "fragments": [],
                "text": "Monocular Visual Scene Understanding: Understanding Multi-Object Traffic Scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A novel probabilistic 3D scene model that integrates state-of-the-art multiclass object detection, object tracking and scene labeling together with geometric 3D reasoning is presented, able to represent complex object interactions such as inter-object occlusion, physical exclusion between objects, and geometric context."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34674427"
                        ],
                        "name": "Gautam Singh",
                        "slug": "Gautam-Singh",
                        "structuredName": {
                            "firstName": "Gautam",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gautam Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743020"
                        ],
                        "name": "J. Kosecka",
                        "slug": "J.-Kosecka",
                        "structuredName": {
                            "firstName": "Jana",
                            "lastName": "Kosecka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kosecka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 236
                            }
                        ],
                        "text": "The main purpose of this dataset is to push forward the development of computer vision and robotic algorithms targeted at autonomous driving (Paul and Newman, 2010; Pfeiffer and Franke, 2010; Geiger et al., 2011a,b; Wojek et al., 2012; Singh and Kosecka, 2012; Brubaker et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1061956,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a6086731f7647df94d7fa7edebd6ae272258c84",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Methods for acquisition and maintenance of an environment model are central to a broad class of mobility and navigation problems. Towards this end, various metric, topological or hybrid models have been proposed. Due to recent advances in sensing and recognition, acquisition of semantic models of the environments have gained increased interest in the community. In this work, we will demonstrate a capability of using weak semantic models of the environment to induce different topological models, capturing the spatial semantics of the environment at different levels. In the first stage of the model acquisition, we propose to compute semantic layout of the street scenes imagery by recognizing and segmenting buildings, roads, sky, cars and trees. Given such semantic layout, we propose an informative feature characterizing the layout and train a classifier to recognize street intersections in challenging urban inner city scenes. We also show how the evidence of different semantic concepts can induce useful topological representation of the environment, which can aid navigation and localization tasks. To demonstrate the approach, we carry out experiments on a challenging dataset of omnidirectional inner city street views and report the performance of both semantic segmentation and intersection classification."
            },
            "slug": "Acquiring-semantics-induced-topology-in-urban-Singh-Kosecka",
            "title": {
                "fragments": [],
                "text": "Acquiring semantics induced topology in urban environments"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work demonstrates a capability of using weak semantic models of the environment to induce different topological models, capturing the spatial semantics of the environments at different levels, and shows how this can aid navigation and localization tasks."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE International Conference on Robotics and Automation"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3619154"
                        ],
                        "name": "Rohan Paul",
                        "slug": "Rohan-Paul",
                        "structuredName": {
                            "firstName": "Rohan",
                            "lastName": "Paul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rohan Paul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144214578"
                        ],
                        "name": "P. Newman",
                        "slug": "P.-Newman",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Newman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Newman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 142
                            }
                        ],
                        "text": "The main purpose of this dataset is to push forward the development of computer vision and robotic algorithms targeted at autonomous driving (Paul and Newman, 2010; Pfeiffer and Franke, 2010; Geiger et al., 2011a,b; Wojek et al., 2012; Singh and Kosecka, 2012; Brubaker et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9478056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01b2c2186b85dc50f3c59aaa27e04ee69931ba3b",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a probabilistic framework for appearance based navigation and mapping using spatial and visual appearance data. Like much recent work on appearance based navigation we adopt a bag-of-words approach in which positive or negative observations of visual words in a scene are used to discriminate between already visited and new places. In this paper we add an important extra dimension to the approach. We explicitly model the spatial distribution of visual words as a random graph in which nodes are visual words and edges are distributions over distances. Care is taken to ensure that the spatial model is able to capture the multi-modal distributions of inter-word spacing and account for sensor errors both in word detection and distances. Crucially, these inter-word distances are viewpoint invariant and collectively constitute strong place signatures and hence the impact of using both spatial and visual appearance is marked. We provide results illustrating a tremendous increase in precision-recall area compared to a state-of-the-art visual appearance only systems."
            },
            "slug": "FAB-MAP-3D:-Topological-mapping-with-spatial-and-Paul-Newman",
            "title": {
                "fragments": [],
                "text": "FAB-MAP 3D: Topological mapping with spatial and visual appearance"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This paper describes a probabilistic framework for appearance based navigation and mapping using spatial and visual appearance data and explicitly model the spatial distribution of visual words as a random graph in which nodes are visual words and edges are distributions over distances."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE International Conference on Robotics and Automation"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066785677"
                        ],
                        "name": "Sandrine Remy",
                        "slug": "Sandrine-Remy",
                        "structuredName": {
                            "firstName": "Sandrine",
                            "lastName": "Remy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sandrine Remy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756828"
                        ],
                        "name": "M. Dhome",
                        "slug": "M.-Dhome",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Dhome",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dhome"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3163701"
                        ],
                        "name": "J. Lavest",
                        "slug": "J.-Lavest",
                        "structuredName": {
                            "firstName": "Jean-Marc",
                            "lastName": "Lavest",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lavest"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2170298"
                        ],
                        "name": "N. Daucher",
                        "slug": "N.-Daucher",
                        "structuredName": {
                            "firstName": "Nadine",
                            "lastName": "Daucher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Daucher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2348246,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "963ad83dd58ee253153e99e5b80ff663989dd860",
            "isKey": false,
            "numCitedBy": 213,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Deals with the hand-eye calibration problem. The question is to find the relative position and orientation between a camera rigidly mounted on the robot's last joint and the gripper. Hand-eye calibration is useful in many cases as for example, grasping objects or reconstructing 3D scenes. Almost all existing solutions lead to solving for homogeneous transformation equations of the form AX=XB. We propose a new formulation. Our method determines simultaneously the hand-eye transformation and the location of a calibration object with respect to the robot world coordinate system. The main advantage is that the number of unknowns remains constant and the solution is constrained to be consistent with the whole set of calibration data. Results of simulation experiments and comparisons with classical techniques are reported and analysed. Real experiments with a Cartesian robot are described and the results accuracy discussed."
            },
            "slug": "Hand-eye-calibration-Remy-Dhome",
            "title": {
                "fragments": [],
                "text": "Hand-eye calibration"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work determines simultaneously the hand-eye transformation and the location of a calibration object with respect to the robot world coordinate system and proposes a new formulation that is constrained to be consistent with the whole set of calibration data."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794229"
                        ],
                        "name": "R. Horaud",
                        "slug": "R.-Horaud",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Horaud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Horaud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803584"
                        ],
                        "name": "F. Dornaika",
                        "slug": "F.-Dornaika",
                        "structuredName": {
                            "firstName": "Fadi",
                            "lastName": "Dornaika",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Dornaika"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 137
                            }
                        ],
                        "text": "Given two trajectories this problem corresponds to the well-known hand-eye calibration problem which can be solved using standard tools (Horaud and Dornaika, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 39024775,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5b0ba98568f624da1c1949bb74b67c88a787a39f",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Whenever a sensor is mounted on a robot hand, it is important to know the relationship between the sensor and the hand. The problem of determining this relationship is referred to as the hand-eye calibration problem. Hand-eye calibration is impor tant in at least two types of tasks: (1) map sensor centered measurements into the robot workspace frame and (2) tasks allowing the robot to precisely move the sensor. In the past some solutions were proposed, particularly in the case of the sensor being a television camera. With almost no exception, all existing solutions attempt to solve a homogeneous matrix equation of the form AX = X B. This article has the following main contributions. First we show that there are two possible formulations of the hand-eye calibration problem. One formu lation is the classic one just mentioned. A second formulation takes the form of the following homogeneous matrix equation: MY = M'YB. The advantage of the latter formulation is that the extrinsic and intrinsic parameters of the camera need not be made explicit. Indeed, this formulation directly uses the 3 x4 perspective matrices ( M and M' ) associated with two positions of the camera with respect to the calibration frame. Moreover, this formulation together with the classic one covers a wider range of camera-based sensors to be calibrated with respect to the robot hand: single scan-line cameras, stereo heads, range finders, etc. Second, we develop a common mathematical framework to solve for the hand-eye calibration problem using either of the two formulations. We represent rotation by a unit quaternion and present two methods: (1) a closed-form solution for solving for rotation using unit quaternions and then solving for translation and (2) a nonlinear technique for simultane ously solving for rotation and translation. Third, we perform a stability analysis both for our two methods and for the lin ear method developed by Tsai and Lenz (1989). This analysis allows the comparison of the three methods. In light of this comparison, the nonlinear optimization method, which solves for rotation and translation simultaneously, seems to be the most robust one with respect to noise and measurement errors."
            },
            "slug": "Hand-Eye-Calibration-Horaud-Dornaika",
            "title": {
                "fragments": [],
                "text": "Hand-Eye Calibration"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A common mathematical framework is developed to solve for the hand-eye calibration problem using either of the two formulations and the nonlinear optimization method, which solves for rotation and translation simultaneously, seems to be the most robust one with respect to noise and measurement errors."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Robotics Res."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7749011"
                        ],
                        "name": "M. Goebl",
                        "slug": "M.-Goebl",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Goebl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Goebl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98762326"
                        ],
                        "name": "G. Farber",
                        "slug": "G.-Farber",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Farber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Farber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 66
                            }
                        ],
                        "text": "Our computer runs Ubuntu Linux (64 bit) and a real-time database (Goebl and Faerber, 2007) to store the incoming data streams."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17875422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef8559be95cbd55503ef48074118b624917df340",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Cognitive automobiles consist of a set of algorithms that cover a wide range of processing levels: from low-level image acquisition and feature extraction up to situation assessment and decision making. The modules implementing them are naturally characterized by decreasing data rates at higher levels, because raw data is discarded after evaluation, and increasing processing intervals, as knowledge based levels require longer computation times. The architecture presented in this papers offers a method to interchange information with different temporal resolutions liberally among modules with distinct cycle times and realtime demands. It allows effortless buffering of raw data for subsequent data fusion and verification, facilitating innovative processing structures. The paper is completed by measurements demonstrating the achieved real-time capabilities on our selected hardware architecture."
            },
            "slug": "A-Real-Time-capable-Hard-and-Software-Architecture-Goebl-Farber",
            "title": {
                "fragments": [],
                "text": "A Real-Time-capable Hard-and Software Architecture for Joint Image and Knowledge Processing in Cognitive Automobiles"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The architecture presented in this papers offers a method to interchange information with different temporal resolutions liberally among modules with distinct cycle times and realtime demands, allowing effortless buffering of raw data for subsequent data fusion and verification, facilitating innovative processing structures."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Intelligent Vehicles Symposium"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 193
                            }
                        ],
                        "text": "Velodyne and IMU Calibration We have registered the Velodyne laser scanner with respect to the reference camera coordinate system (camera 0) by initializing the rigid body transformation using [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 79
                            }
                        ],
                        "text": "For details about the benchmarks and evaluation metrics we refer the reader to Geiger et al. (2012a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "Camera Calibration For calibrating the cameras intrinsically and extrinsically, we use the approach proposed in [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 240
                            }
                        ],
                        "text": "Next, we optimized an error criterion based on the Euclidean distance of 50 manually selected correspondences and a robust measure on the disparity error with respect to the three top performing stereo methods in the KITTI stereo benchmark Geiger et al. (2012a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 30
                            }
                        ],
                        "text": "While our introductory paper (Geiger et al., 2012a) mainly focuses on the benchmarks, their creation and use for evaluating state-of-the-art computer vision methods, here we complement this information by providing technical details on the raw data itself."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 164
                            }
                        ],
                        "text": "We have registered the Velodyne laser scanner with respect to the reference camera coordinate system (camera 0) by initializing the rigid body transformation using Geiger et al. (2012b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 53
                            }
                        ],
                        "text": "For a review on related work, we refer the reader to Geiger et al. (2012a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 93
                            }
                        ],
                        "text": "For calibrating the cameras intrinsically and extrinsically, we use the approach proposed in Geiger et al. (2012b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A toolbox for automatic calibration of range and camera sensors using a single shot"
            },
            "venue": {
                "fragments": [],
                "text": "ICRA, 2012."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144209348"
                        ],
                        "name": "P. Osborne",
                        "slug": "P.-Osborne",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Osborne",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Osborne"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58087894,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ec81fbe561b4690d6f1fa3585d0344176f3b99e",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Mercator-Projections-Osborne",
            "title": {
                "fragments": [],
                "text": "The Mercator Projections"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A toolbox for automatic calibration of range and camera sensors using a single shot"
            },
            "venue": {
                "fragments": [],
                "text": "ICRA"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 60
                            }
                        ],
                        "text": "For this conversion we make use of the Mercator projection (Osborne, 2008)\nx = s \u00d7 r \u00d7 \u03c0 lon 180\n(1)\ny = s \u00d7 r \u00d7 log ( tan ( \u03c0 ( 90 + lat)\n360\n)) (2)\nwith earth radius r \u2248 6, 378, 137 meters, scale s = cos ( lat0\u00d7\u03c0\n180\n) , and ( lat, lon) the geographic coordinates. lat0\ndenotes the latitude of the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 59
                            }
                        ],
                        "text": "For this conversion we make use of the Mercator projection (Osborne, 2008)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The mercator projections. Available at: http://mercator.myzen.co.uk/mercator.pdf"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 11,
            "methodology": 5
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 16,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Vision-meets-robotics:-The-KITTI-dataset-Geiger-Lenz/79b949d9b35c3f51dd20fb5c746cc81fc87147eb?sort=total-citations"
}