{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 70
                            }
                        ],
                        "text": "in the 1-slack formulation, OP4 results in the algorithm presented in (Joachims, 2006) and implemented in the SVM-perf software3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 53
                            }
                        ],
                        "text": "Even further, following the original publication in (Joachims, 2006), Teo et al (2007) have already shown that the algorithm can also be extended to Conditional Random Field training."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 28
                            }
                        ],
                        "text": "Generalizing the proof from (Joachims, 2006), we will show that both optimization problems have the same objective value and an equivalent set of constraints."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 140
                            }
                        ],
                        "text": "The 1-slack algorithm scales linearly in both n and the sparsity s of the feature vectors, even if the\ntotal number N of features is large (Joachims, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 81
                            }
                        ],
                        "text": "In this paper, we explore an extension of the cutting-plane method presented in (Joachims, 2006) for training linear structural SVMs, both in the margin-rescaling and in the slack-rescaling formulation (Tsochantaridis et al, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 37
                            }
                        ],
                        "text": "The SVMlight results are quoted from (Joachims, 2006), the 1-slack results are re-run with the latest version of SVM-struct using the same experiment setup as in (Joachims, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 49
                            }
                        ],
                        "text": "Note that the linear-time algorithm proposed in (Joachims, 2006) for training binary classification SVMs is a special case of the 1-slack methods developed here."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 113
                            }
                        ],
                        "text": "However, the O( 1\u03b5 ) bound on the maximum number of iterations derived here is tighter than the O( 1\u03b52 ) bound in (Joachims, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 86
                            }
                        ],
                        "text": "Using a similar argument, it can also be shown the ordinal regression method from the (Joachims, 2006) is a special case of the 1-slack algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 19
                            }
                        ],
                        "text": "The discussion in (Joachims, 2006) concludes that runtime is comparable to the 1-slack algorithm implemented in SVM-perf."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 101
                            }
                        ],
                        "text": "To find this solution, we propose Algorithms 3 and 4, which are generalizations of the algorithm in (Joachims, 2006) to structural SVMs.\nSimilar to the cutting-plane algorithms for the n-slack formulations, Algorithms 3 and 4 iteratively construct a working set W of constraints."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 181
                            }
                        ],
                        "text": "Empirically, the new algorithm is substantially faster than existing methods, in particular decomposition methods like SMO and SVM light , and it includes the training algorithm of Joachims (2006) for linear binary classification SVMs as a special case."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 150
                            }
                        ],
                        "text": "Using\n\u03a8(x,y) = 1 2\nyx and \u0394(y, y\u0304) = 100[y = y\u0304] = {\n0 if y = y\u0304 100 otherwise\n(29)\nin the 1-slack formulation, OP4 results in the algorithm presented in (Joachims, 2006) and implemented in the SVM-perf software 3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 86
                            }
                        ],
                        "text": "Using a similar argument, it can also be shown that the ordinal regression method in (Joachims, 2006) is a special case of the 1-slack algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 27
                            }
                        ],
                        "text": "Generalizing the proof in (Joachims, 2006), we will show that for every w the smallest feasible \u03be and \u2211ni \u03bei are equal."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 30
                            }
                        ],
                        "text": "The proof extends the one in (Joachims, 2006) to general structural SVMs, and is based on the technique introduced in (Joachims, 2003) and generalized in (Tsochantaridis et al, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 100
                            }
                        ],
                        "text": "To find this solution, we propose Algorithms 3 and 4, which are generalizations of the algorithm in (Joachims, 2006) to structural SVMs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 73
                            }
                        ],
                        "text": "The 1-slack formulations and algorithms are then equivalent to those in (Joachims, 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 116
                            }
                        ],
                        "text": "However, the O( 1 \u03b5 ) bound on the maximum number of iterations derived here is tighter than the O( 1 \u03b52 ) bound in (Joachims, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 74
                            }
                        ],
                        "text": "The algorithm is a generalization of the cutting-planemethod presented in (Joachims, 2006) to general structural SVMs, both in the margin-rescaling and in the slack-rescaling formulation (Tsochantaridis et al, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5155714,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "427b168f490b56716f22b129ac93aba5425ea08f",
            "isKey": false,
            "numCitedBy": 2113,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Linear Support Vector Machines (SVMs) have become one of the most prominent machine learning techniques for high-dimensional sparse data commonly encountered in applications like text classification, word-sense disambiguation, and drug design. These applications involve a large number of examples n as well as a large number of features N, while each example has only s << N non-zero features. This paper presents a Cutting Plane Algorithm for training linear SVMs that provably has training time 0(s,n) for classification problems and o(sn log (n))for ordinal regression problems. The algorithm is based on an alternative, but equivalent formulation of the SVM optimization problem. Empirically, the Cutting-Plane Algorithm is several orders of magnitude faster than decomposition methods like svm light for large datasets."
            },
            "slug": "Training-linear-SVMs-in-linear-time-Joachims",
            "title": {
                "fragments": [],
                "text": "Training linear SVMs in linear time"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A Cutting Plane Algorithm for training linear SVMs that provably has training time 0(s,n) for classification problems and o(sn log (n)) for ordinal regression problems and several orders of magnitude faster than decomposition methods like svm light for large datasets."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '06"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3166569"
                        ],
                        "name": "C. Yu",
                        "slug": "C.-Yu",
                        "structuredName": {
                            "firstName": "Chun-Nam",
                            "lastName": "Yu",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3346672"
                        ],
                        "name": "R. Elber",
                        "slug": "R.-Elber",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Elber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Elber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786404"
                        ],
                        "name": "J. Pillardy",
                        "slug": "J.-Pillardy",
                        "structuredName": {
                            "firstName": "Jaroslaw",
                            "lastName": "Pillardy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pillardy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1866901,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "a6a03fab612d2b490c8460d07f97c30178acd5a2",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Sequence to structure alignment is an important step in homology modeling of protein structures. Incorporation of features such as secondary structure, solvent accessibility, or evolutionary information improve sequence to structure alignment accuracy, but conventional generative estimation techniques for alignment models impose independence assumptions that make these features difficult to include in a principled way. In this paper, we overcome this problem using a Support Vector Machine (SVM) method that provides a well-founded way of estimating complex alignment models with hundred of thousands of parameters. Furthermore, we show that the method can be trained using a variety of loss functions. In a rigorous empirical evaluation, the SVM algorithm outperforms the generative alignment method SSALN, a highly accurate generative alignment model that incorporates structural information. The alignment model learned by the SVM aligns 50% of the residues correctly and aligns over 70% of the residues within a shift of four positions."
            },
            "slug": "Support-Vector-Training-of-Protein-Alignment-Models-Yu-Joachims",
            "title": {
                "fragments": [],
                "text": "Support Vector Training of Protein Alignment Models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A Support Vector Machine (SVM) method is used that provides a well-founded way of estimating complex alignment models with hundred of thousands of parameters and outperforms the Generative alignment method SSALN, a highly accurate generative alignment model that incorporates structural information."
            },
            "venue": {
                "fragments": [],
                "text": "RECOMB"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144848317"
                        ],
                        "name": "Glenn Fung",
                        "slug": "Glenn-Fung",
                        "structuredName": {
                            "firstName": "Glenn",
                            "lastName": "Fung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Glenn Fung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747026"
                        ],
                        "name": "O. Mangasarian",
                        "slug": "O.-Mangasarian",
                        "structuredName": {
                            "firstName": "Olvi",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mangasarian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 145
                            }
                        ],
                        "text": "\u2026algorithms for linear SVMs that scale linearly with n (e.g., Lagrangian SVM (Mangasarian and Musicant, 2001) (using the \u2211\u03be 2i loss), Proximal SVM (Fung and Mangasarian, 2001) (using an L 2 regression loss), and Interior Point Methods (Ferris and Munson, 2003)), they use the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5593513,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f96a214a253aaab297a138405812c856945ed335",
            "isKey": false,
            "numCitedBy": 853,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Instead of a standard support vector machine (SVM) that classifies points by assigning them to one of two disjoint half-spaces, points are classified by assigning them to the closest of two parallel planes (in input or feature space) that are pushed apart as far as possible. This formulation, which can also be interpreted as regularized least squares and considered in the much more general context of regularized networks [8, 9], leads to an extremely fast and simple algorithm for generating a linear or nonlinear classifier that merely requires the solution of a single system of linear equations. In contrast, standard SVMs solve a quadratic or a linear program that require considerably longer computational time. Computational results on publicly available datasets indicate that the proposed proximal SVM classifier has comparable test set correctness to that of standard SVM classifiers, but with considerably faster computational time that can be an order of magnitude faster. The linear proximal SVM can easily handle large datasets as indicated by the classification of a 2 million point 10-attribute set in 20.8 seconds. All computational results are based on 6 lines of MATLAB code."
            },
            "slug": "Proximal-support-vector-machine-classifiers-Fung-Mangasarian",
            "title": {
                "fragments": [],
                "text": "Proximal support vector machine classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Computational results on publicly available datasets indicate that the proposed proximal SVM classifier has comparable test set correctness to that of standard S VM classifiers, but with considerably faster computational time that can be an order of magnitude faster."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765700"
                        ],
                        "name": "Ioannis Tsochantaridis",
                        "slug": "Ioannis-Tsochantaridis",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Tsochantaridis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ioannis Tsochantaridis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783941"
                        ],
                        "name": "Y. Altun",
                        "slug": "Y.-Altun",
                        "structuredName": {
                            "firstName": "Yasemin",
                            "lastName": "Altun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Altun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 66
                            }
                        ],
                        "text": "2005), which is possible for the special case of margin-rescaling (Tsochantaridis et al. 2005) with linearly decomposable loss."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 200
                            }
                        ],
                        "text": "In this paper, we explore an extension of the cutting-plane method presented in Joachims (2006) for training linear structural SVMs, both in the margin-rescaling and in the slackrescaling formulation (Tsochantaridis et al. 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 54
                            }
                        ],
                        "text": "In contrast to the cutting-plane method presented in (Tsochantaridis et al, 2005), we show that the size of the cutting-plane models and the number of iterations are independent of the number of training examples n."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 233
                            }
                        ],
                        "text": "\u2026and slack-rescaling, leading to a cutting-plane training algorithm that has not only provably linear runtime in the number of training examples, but is also several orders of magnitude faster than conventional cuttingplane methods (Tsochantaridis et al, 2005) on large-scale problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 136
                            }
                        ],
                        "text": "A more general algorithm that applies to both margin-rescaling and slackrescaling under a large variety of loss functions was given in (Tsochantaridis et al, 2004, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 102
                            }
                        ],
                        "text": "The argmax in Line 5 has an efficient solution for a wide variety of choices for\u03a8 , Y , and \u0394 (see e.g., Tsochantaridis et al,\n2005; Joachims, 2005; Yu et al, 2007; Yue et al, 2007), and often the algorithm for making predictions (see Eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 109
                            }
                        ],
                        "text": "In particular, since the size of the cutting-plane model typically grows linearly with the dataset size (see Tsochantaridis et al, 2005, and Section 5.5), QPs of increasing size need to be solved to compute the optimal steps, which leads to the super-linear runtime."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 156
                            }
                        ],
                        "text": "It is easy to verify that for both margin-rescaling and for slack-rescaling, \u2211\u03bei is an upper bound on the empirical risk R\u0394S (h) on the training sample S (see Tsochantaridis et al, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 193
                            }
                        ],
                        "text": "For training the weights w of the linear discriminant function, the standard SVM optimization problem can be generalized in several ways (Altun et al, 2003; Joachims, 2003; Taskar et al, 2003; Tsochantaridis et al, 2004, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 194
                            }
                        ],
                        "text": "The algorithm relies on the theoretical result that for any desired precision \u03b5 , a greedily constructed cutting-plane model of OP2 and OP3 requires only O( n\u03b52 ) many constraints (Joachims, 2003; Tsochantaridis et al, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 209
                            }
                        ],
                        "text": "The first group of algorithms relies on an elegant polynomial-size reformulation of the training problem (Taskar et al, 2003; Anguelov et al, 2005), which is possible for the special case of margin-rescaling (Tsochantaridis et al, 2005) with linearly decomposable loss."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 203
                            }
                        ],
                        "text": "In this paper, we explore an extension of the cutting-plane method presented in (Joachims, 2006) for training linear structural SVMs, both in the margin-rescaling and in the slack-rescaling formulation (Tsochantaridis et al, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 151
                            }
                        ],
                        "text": "This is feasible, since a polynomially-sized subset of the constraints from the original QP is already sufficient for a solution of arbitrary accuracy (Joachims 2003; Tsochantaridis et al. 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 168
                            }
                        ],
                        "text": "This is feasible, since a polynomially-sized subset of the constraints from the original QP is already sufficient for a solution of arbitrary accuracy (Joachims, 2003; Tsochantaridis et al, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 180
                            }
                        ],
                        "text": "The algorithm relies on the theoretical result that for any desired precision \u03b5, a greedily constructed cutting-plane model of OP2 and OP3 requires only O( n \u03b52 ) many constraints (Joachims 2003; Tsochantaridis et al. 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Tsochantaridis et al (2005) identify two different ways of using a hinge loss to convex upper bound the loss, namely \u201cmargin-rescaling\u201d and \u201cslack-rescaling\u201d."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 50
                            }
                        ],
                        "text": "In practice, however, the cutting-plane method of Tsochantaridis et al (2005) is known to scale super-linearly with the number of training examples."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 43
                            }
                        ],
                        "text": "This paper uses the formulations given in (Tsochantaridis et al, 2005), which subsume all other approaches."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 147
                            }
                        ],
                        "text": "provably linear runtime in the number of training examples, but is also several orders of magnitude faster than conventional cutting-plane methods (Tsochantaridis et al. 2005) on large-scale problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 78
                            }
                        ],
                        "text": "A key conceptual difference of the new algorithm compared to the algorithm of Tsochantaridis et al (2005) and most other SVM training methods is that not only individual data points are considered as potential Support Vectors (SVs), but also linear combinations of those."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 155
                            }
                        ],
                        "text": "The proof extends the one in (Joachims, 2006) to general structural SVMs, and is based on the technique introduced in (Joachims, 2003) and generalized in (Tsochantaridis et al, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 18
                            }
                        ],
                        "text": "The algorithm in (Tsochantaridis et al, 2005) shows how such a cutting-plane can be constructed efficiently."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17671150,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc97e7dbb821a4edfb5151bff4352655eedca9ee",
            "isKey": false,
            "numCitedBy": 2247,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing flexible and powerful input representations, this paper addresses the complementary issue of designing classification algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classification problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach."
            },
            "slug": "Large-Margin-Methods-for-Structured-and-Output-Tsochantaridis-Joachims",
            "title": {
                "fragments": [],
                "text": "Large Margin Methods for Structured and Interdependent Output Variables"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper proposes to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation and presents a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730156"
                        ],
                        "name": "Carlos Guestrin",
                        "slug": "Carlos-Guestrin",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Guestrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Guestrin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 207
                            }
                        ],
                        "text": "\u2026this provides greater modeling flexibility through avoidance of independence assumptions, and it was shown to provide substantially improved prediction accuracy in many domains (e.g., Lafferty et al, 2001; Taskar et al, 2003; Tsochantaridis et al, 2004; Taskar et al, 2004; Yu et al, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 173
                            }
                        ],
                        "text": "For training the weights w of the linear discriminant function, the standard SVM optimization problem can be generalized in several ways (Altun et al, 2003; Joachims, 2003; Taskar et al, 2003; Tsochantaridis et al, 2004, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 157
                            }
                        ],
                        "text": "These smaller QPs can then be solved, for example, with general-purpose optimization methods (Anguelov et al, 2005) or decomposition methods similar to SMO (Taskar et al, 2003; Platt, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 106
                            }
                        ],
                        "text": "The first group of algorithms relies on an elegant polynomial-size reformulation of the training problem (Taskar et al, 2003; Anguelov et al, 2005), which is possible for the special case of margin-rescaling (Tsochantaridis et al, 2005) with linearly decomposable loss."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 250
                            }
                        ],
                        "text": "\u2026provided intriguing advances in extending methods like Logistic Regression, Perceptrons, and Support Vector Machines (SVMs) to global training of such structured prediction models (e.g., Lafferty et al, 2001; Collins, 2004; Collins and Duffy, 2002; Taskar et al, 2003; Tsochantaridis et al, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 148
                            }
                        ],
                        "text": "\u20261 2 wT w + C n\nn\n\u2211 i=1 \u03bei\ns.t. \u2200i \u2208 {1, ...,n} : yi(wT xi)\u2265 1\u2212 \u03bei\nIt was shown that SVM training can be generalized to structured outputs (Altun et al, 2003; Taskar et al, 2003; Tsochantaridis et al, 2004), leading to an optimiza-\n1 Note, however, that all formal results in this paper also hold\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "Empirically, the new algorithm is substantially faster than existing methods, in particular decomposition methods like SMO and SVM light , and it includes the training algorithm of Joachims (2006) for linear binary classification SVMs as a special case."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 115
                            }
                        ],
                        "text": "The most widely used algorithms for training binary SVMs are decomposition methods like SVMlight (Joachims, 1999), SMO (Platt, 1999), and others (Chang and Lin, 2001; Collobert and Bengio, 2001)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 100
                            }
                        ],
                        "text": "For the special case of margin-rescaling with linearly decomposable loss functions \u0394 , Taskar et al. (Taskar et al, 2003) have shown that the problem can be reformulated as a quadratic program with only a polynomial number of constraints and variables."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "In the case of binary classification, their SMO algorithm reduces to a variant of the traditional SMO algorithm, which can be seen as a special case of the SVMlight algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Taskar et al (2003) extended the SMO algorithm to structured prediction problems based on their polynomial-size reformulation of the n-slack optimization problem OP2 for the special case of decomposable models and decomposable loss functions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 201720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c450531e1121cfb657be5195e310217a4675397",
            "isKey": true,
            "numCitedBy": 1477,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches."
            },
            "slug": "Max-Margin-Markov-Networks-Taskar-Guestrin",
            "title": {
                "fragments": [],
                "text": "Max-Margin Markov Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data, and a new theoretical bound for generalization in structured domains is provided."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 100
                            }
                        ],
                        "text": "Unfortunately, decomposition methods are known to scale super-linearly with the number of examples (Platt, 1999; Joachims, 1999), and so do general-purpose optimizers, since they do not exploit the special structure of this optimization problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 99
                            }
                        ],
                        "text": "Unfortunately, decomposition methods are known to scale super-linearly with the number of examples (Platt 1999; Joachims 1999), and so do general-purpose optimizers, since they do not exploit the special structure of this optimization problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 177
                            }
                        ],
                        "text": "These smaller QPs can then be solved, for example, with general-purpose optimization methods (Anguelov et al, 2005) or decomposition methods similar to SMO (Taskar et al, 2003; Platt, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 119
                            }
                        ],
                        "text": "The most widely used algorithms for training binary SVMs are decomposition methods like SVM-light (Joachims 1999), SMO (Platt 1999), and others (Chang and Lin 2001;"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "Empirically, the new algorithm is substantially faster than existing methods, in particular decomposition methods like SMO and SVM light , and it includes the training algorithm of Joachims (2006) for linear binary classification SVMs as a special case."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 46
                            }
                        ],
                        "text": "2005) or decomposition methods similar to SMO (Taskar et al. 2003; Platt 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 120
                            }
                        ],
                        "text": "The most widely used algorithms for training binary SVMs are decomposition methods like SVMlight (Joachims, 1999), SMO (Platt, 1999), and others (Chang and Lin, 2001; Collobert and Bengio, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "In the case of binary classification, their SMO algorithm reduces to a variant of the traditional SMO algorithm, which can be seen as a special case of the SVMlight algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "Taskar et al (2003) extended the SMO algorithm to structured prediction problems based on their polynomial-size reformulation of the n-slack optimization problem OP2 for the special case of decomposable models and decomposable loss functions."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1099857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4de39c94e340a108fff01a90a67b0c17c86fb981",
            "isKey": true,
            "numCitedBy": 5910,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter describes a new algorithm for training Support Vector Machines: Sequential Minimal Optimization, or SMO. Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because large matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while a standard projected conjugate gradient (PCG) chunking algorithm scales somewhere between linear and cubic in the training set size. SMO's computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. For the MNIST database, SMO is as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be more than 1000 times faster than the PCG chunking algorithm."
            },
            "slug": "Fast-training-of-support-vector-machines-using-in-Platt",
            "title": {
                "fragments": [],
                "text": "Fast training of support vector machines using sequential minimal optimization, advances in kernel methods"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "SMO breaks this large quadratic programming problem into a series of smallest possible QP problems, which avoids using a time-consuming numerical QP optimization as an inner loop and hence SMO is fastest for linear SVMs and sparse data sets."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61116019,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7550a05bf00f7b24aed9c1ac3ef000575388d21c",
            "isKey": false,
            "numCitedBy": 5454,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Training a support vector machine SVM leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples on the shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SVM light is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains."
            },
            "slug": "Making-large-scale-SVM-learning-practical-Joachims",
            "title": {
                "fragments": [],
                "text": "Making large scale SVM learning practical"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical and give guidelines for the application of SVMs to large domains."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3166569"
                        ],
                        "name": "C. Yu",
                        "slug": "C.-Yu",
                        "structuredName": {
                            "firstName": "Chun-Nam",
                            "lastName": "Yu",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17962853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "48490bec879a1a4e868f3a71f176d4f344f465e1",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Protein threading is the problem of inferring the structure of a protein from its sequence by matching the sequence against a set of known structures. Unlike conventional sequence to sequence alignment tasks, alignment models for threading can exploit a rich set of features derived from the geometry of the known structure. To make use of these complex and interdependent features, we explore the use of discriminative training with structural Support Vector Machines. We present empirical results for the CASP5 dataset and compare against conventional generative training."
            },
            "slug": "Training-Protein-Threading-Models-using-Structural-Yu-Joachims",
            "title": {
                "fragments": [],
                "text": "Training Protein Threading Models using Structural SVMs"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work explores the use of discriminative training with structural Support Vector Machines for protein threading and presents empirical results for the CASP5 dataset and compares against conventional generative training."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6659485"
                        ],
                        "name": "Tamara Galor",
                        "slug": "Tamara-Galor",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Galor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara Galor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3346672"
                        ],
                        "name": "R. Elber",
                        "slug": "R.-Elber",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Elber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Elber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 157
                            }
                        ],
                        "text": "For training the weights w of the linear discriminant function, the standard SVM optimization problem can be generalized in several ways (Altun et al, 2003; Joachims, 2003; Taskar et al, 2003; Tsochantaridis et al, 2004, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 178
                            }
                        ],
                        "text": "The algorithm relies on the theoretical result that for any desired precision \u03b5 , a greedily constructed cutting-plane model of OP2 and OP3 requires only O( n\u03b52 ) many constraints (Joachims, 2003; Tsochantaridis et al, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 152
                            }
                        ],
                        "text": "This is feasible, since a polynomially-sized subset of the constraints from the original QP is already sufficient for a solution of arbitrary accuracy (Joachims, 2003; Tsochantaridis et al, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 182
                            }
                        ],
                        "text": "The algorithm relies on the theoretical results that for any desired precision \u03b5 , a greedily constructed cutting-plane model of OP2 and OP3 requires only O( n \u03b52 ) many constraints (Joachims, 2003; Tsochantaridis et al, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 119
                            }
                        ],
                        "text": "The proof extends the one in (Joachims, 2006) to general structural SVMs, and is based on the technique introduced in (Joachims, 2003) and generalized in (Tsochantaridis et al, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14311110,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "011b9ca48aa1e0ad9ee9481668ef43f6dc4d2383",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a discriminative method for learning the parameters of linear sequence alignment models from training examples. Compared to conventional generative approaches, the discriminative method is straightforward to use when operations (e.g. substitutions, deletions, insertions) and sequence elements are described by vectors of attributes. This admits learning flexible and more complex alignment models. While the resulting training problem leads to an optimization problem with an exponential number of constraints, we present a simple algorithm that finds an arbitrarily close approximation after considering only a subset of the constraints that is linear in the number of training examples and polynomial in the length of the sequences. We also evaluate empirically that the method effectively learns good parameter values while being computationally feasible."
            },
            "slug": "Learning-to-Align-Sequences:-A-Maximum-Margin-Joachims-Galor",
            "title": {
                "fragments": [],
                "text": "Learning to Align Sequences: A Maximum-Margin Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A discriminative method for learning the parameters of linear sequence alignment models from training examples that finds an arbitrarily close approximation after considering only a subset of the constraints that is linear in the number of training examples and polynomial in the length of the sequences."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783941"
                        ],
                        "name": "Y. Altun",
                        "slug": "Y.-Altun",
                        "structuredName": {
                            "firstName": "Yasemin",
                            "lastName": "Altun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Altun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765700"
                        ],
                        "name": "Ioannis Tsochantaridis",
                        "slug": "Ioannis-Tsochantaridis",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Tsochantaridis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ioannis Tsochantaridis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 137
                            }
                        ],
                        "text": "For training the weights w of the linear discriminant function, the standard SVM optimization problem can be generalized in several ways (Altun et al. 2003; Joachims 2003; Taskar et al. 2003; Tsochantaridis et al. 2004, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 150
                            }
                        ],
                        "text": "\u2026(PRIMAL))\nmin w,\u03bei\u22650 1 2 wT w + C n\nn\n\u2211 i=1 \u03bei\ns.t. \u2200i \u2208 {1, ...,n} : yi(wT xi)\u2265 1\u2212 \u03bei\nIt was shown that SVM training can be generalized to structured outputs (Altun et al, 2003; Taskar et al, 2003; Tsochantaridis et al, 2004), leading to an optimiza-\n1 Note, however, that all formal results in\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 72
                            }
                        ],
                        "text": "It was shown that SVM training can be generalized to structured outputs (Altun et al. 2003; Taskar et al. 2003; Tsochantaridis et al. 2004), leading to an optimization problem that is similar to multi-class SVMs (Crammer and Singer 2001) and extending the Perceptron approach described in Collins (2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 138
                            }
                        ],
                        "text": "For training the weights w of the linear discriminant function, the standard SVM optimization problem can be generalized in several ways (Altun et al, 2003; Joachims, 2003; Taskar et al, 2003; Tsochantaridis et al, 2004, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9699301,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5fe5ed2a3b50becdbbcd17e7733653d5ef6ac398",
            "isKey": true,
            "numCitedBy": 556,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel discriminative learning technique for label sequences based on a combination of the two most successful learning algorithms, Support Vector Machines and Hidden Markov Models which we call Hidden Markov Support Vector Machine. The proposed architecture handles dependencies between neighboring labels using Viterbi decoding. In contrast to standard HMM training, the learning procedure is discriminative and is based on a maximum/soft margin criterion. Compared to previous methods like Conditional Random Fields, Maximum Entropy Markov Models and label sequence boosting, HM-SVMs have a number of advantages. Most notably, it is possible to learn non-linear discriminant functions via kernel functions. At the same time, HM-SVMs share the key advantages with other discriminative methods, in particular the capability to deal with overlapping features. We report experimental evaluations on two tasks, named entity recognition and part-of-speech tagging, that demonstrate the competitiveness of the proposed approach."
            },
            "slug": "Hidden-Markov-Support-Vector-Machines-Altun-Tsochantaridis",
            "title": {
                "fragments": [],
                "text": "Hidden Markov Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "This paper presents a novel discriminative learning technique for label sequences based on a combination of the two most successful learning algorithms, Support Vector Machines and Hidden Markov Models which it is called HM-SVMs and handles dependencies between neighboring labels using Viterbi decoding."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765700"
                        ],
                        "name": "Ioannis Tsochantaridis",
                        "slug": "Ioannis-Tsochantaridis",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Tsochantaridis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ioannis Tsochantaridis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783941"
                        ],
                        "name": "Y. Altun",
                        "slug": "Y.-Altun",
                        "structuredName": {
                            "firstName": "Yasemin",
                            "lastName": "Altun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Altun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 187
                            }
                        ],
                        "text": "Such algorithms either take stochastic subgradi-\nent steps (Collins, 2002; Ratliff et al, 2007; Shalev-Shwartz et al, 2007), or build a cutting-plane model which is easy to solve directly (Tsochantaridis et al, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 72
                            }
                        ],
                        "text": "It was shown that SVM training can be generalized to structured outputs (Altun et al. 2003; Taskar et al. 2003; Tsochantaridis et al. 2004), leading to an optimization problem that is similar to multi-class SVMs (Crammer and Singer 2001) and extending the Perceptron approach described in Collins (2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 337,
                                "start": 239
                            }
                        ],
                        "text": "Akin to moving from Naive Bayes to an SVM for classification, this provides greater modeling flexibility through avoidance of independence assumptions, and it was shown to provide substantially improved prediction accuracy in many domains (e.g., Lafferty et al. 2001; Taskar et al. 2003, 2004; Tsochantaridis et al. 2004; Yu et al. 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 70
                            }
                        ],
                        "text": "2007), or build a cutting-plane model which is easy to solve directly (Tsochantaridis et al. 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 136
                            }
                        ],
                        "text": "A more general algorithm that applies to both margin-rescaling and slackrescaling under a large variety of loss functions was given in (Tsochantaridis et al, 2004, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 227
                            }
                        ],
                        "text": "\u2026this provides greater modeling flexibility through avoidance of independence assumptions, and it was shown to provide substantially improved prediction accuracy in many domains (e.g., Lafferty et al, 2001; Taskar et al, 2003; Tsochantaridis et al, 2004; Taskar et al, 2004; Yu et al, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 193
                            }
                        ],
                        "text": "For training the weights w of the linear discriminant function, the standard SVM optimization problem can be generalized in several ways (Altun et al, 2003; Joachims, 2003; Taskar et al, 2003; Tsochantaridis et al, 2004, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 312,
                                "start": 198
                            }
                        ],
                        "text": "Recent years have provided intriguing advances in extending methods like Logistic Regression, Perceptrons, and Support Vector Machines (SVMs) to global training of such structured prediction models (e.g., Lafferty et al. 2001; Collins 2004; Collins and Duffy 2002; Taskar et al. 2003; Tsochantaridis et al. 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 270
                            }
                        ],
                        "text": "\u2026provided intriguing advances in extending methods like Logistic Regression, Perceptrons, and Support Vector Machines (SVMs) to global training of such structured prediction models (e.g., Lafferty et al, 2001; Collins, 2004; Collins and Duffy, 2002; Taskar et al, 2003; Tsochantaridis et al, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 149
                            }
                        ],
                        "text": "\u2026i=1 \u03bei\ns.t. \u2200i \u2208 {1, ...,n} : yi(wT xi)\u2265 1\u2212 \u03bei\nIt was shown that SVM training can be generalized to structured outputs (Altun et al, 2003; Taskar et al, 2003; Tsochantaridis et al, 2004), leading to an optimiza-\n1 Note, however, that all formal results in this paper also hold for non i.i.d. data,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 564746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93aa298b40bb3ec23c25239089284fdf61ded917",
            "isKey": false,
            "numCitedBy": 1455,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning general functional dependencies is one of the main goals in machine learning. Recent progress in kernel-based methods has focused on designing flexible and powerful input representations. This paper addresses the complementary issue of problems involving complex outputs such as multiple dependent output variables and structured output spaces. We propose to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs. The resulting optimization problem is solved efficiently by a cutting plane algorithm that exploits the sparseness and structural decomposition of the problem. We demonstrate the versatility and effectiveness of our method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment."
            },
            "slug": "Support-vector-machine-learning-for-interdependent-Tsochantaridis-Hofmann",
            "title": {
                "fragments": [],
                "text": "Support vector machine learning for interdependent and structured output spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs, and demonstrates the versatility and effectiveness of the method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693407"
                        ],
                        "name": "K. Crammer",
                        "slug": "K.-Crammer",
                        "structuredName": {
                            "firstName": "Koby",
                            "lastName": "Crammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Crammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 78
                            }
                        ],
                        "text": "2004), leading to an optimization problem that is similar to multi-class SVMs (Crammer and Singer 2001) and extending the Perceptron approach described in Collins (2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 115
                            }
                        ],
                        "text": "By adding a constant feature, an offset can easily be simulated.\ntion problem that is similar to multi-class SVMs (Crammer and Singer, 2001) and extending the Perceptron approach described in (Collins, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 195
                            }
                        ],
                        "text": "Using \u0394(y, y\u0304) = 100[y = y\u0304] and\n\u03a8multi(x,y) = \u239b \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d 0 ... 0 x 0 ... 0 \u239e \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0\n(30)\nwhere the feature vector x is stacked into position y, the resulting n-slack problem becomes identical to the multi-class SVM of Crammer and Singer (2001)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10151608,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "cfc6d0c8260594ebc5dd20ee558d29b1014ed41a",
            "isKey": true,
            "numCitedBy": 2190,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe the algorithmic implementation of multiclass kernel-based vector machines. Our starting point is a generalized notion of the margin to multiclass problems. Using this notion we cast multiclass categorization problems as a constrained optimization problem with a quadratic objective function. Unlike most of previous approaches which typically decompose a multiclass problem into multiple independent binary classification tasks, our notion of margin yields a direct method for training multiclass predictors. By using the dual of the optimization problem we are able to incorporate kernels with a compact set of constraints and decompose the dual problem into multiple optimization problems of reduced size. We describe an efficient fixed-point algorithm for solving the reduced optimization problems and prove its convergence. We then discuss technical details that yield significant running time improvements for large datasets. Finally, we describe various experiments with our approach comparing it to previously studied kernel-based methods. Our experiments indicate that for multiclass problems we attain state-of-the-art accuracy."
            },
            "slug": "On-the-Algorithmic-Implementation-of-Multiclass-Crammer-Singer",
            "title": {
                "fragments": [],
                "text": "On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper describes the algorithmic implementation of multiclass kernel-based vector machines using a generalized notion of the margin to multiclass problems, and describes an efficient fixed-point algorithm for solving the reduced optimization problems and proves its convergence."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2612372"
                        ],
                        "name": "L. Backstrom",
                        "slug": "L.-Backstrom",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Backstrom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Backstrom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The benchmarks include two text classification problems from the Reuters RCV1 collection5 (Lewis et al, 2004), a problem of classifying Arxiv abstracts, a binary classifier for class 1 of the Covertype dataset6 of Blackard, Jock & Dean, and the KDD04 Physics task from the KDD-Cup 2004 ( Caruana et al, 2004 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 273
                            }
                        ],
                        "text": "\u2026include two text classification problems from the Reuters RCV1 collection5 (Lewis et al, 2004), a problem of classifying ArXiv abstracts, a binary classifier for class 1 of the Covertype dataset6 of Blackard, Jock & Dean, and the KDD04 Physics task from the KDD-Cup 2004 (Caruana et al, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14892986,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b80a01e7df6e5d7c560cb233e089c96fffa745ad",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper summarizes and analyzes the results of the 2004 KDD-Cup. The competition consisted of two tasks from the areas of particle physics and protein homology detection. It focused on the problem of optimizing supervised learning to different performance measures (accuracy, cross-entropy, ROC area, SLAC-Q, squared error, average precision, top 1, and rank of last). A total of 102 groups participated in the competition, 6 of which received awards or honorable mentions. Their approaches are described in other papers in this issue of SIGKDD Explorations. In this paper we do not analyze any particular approach, but give insight into the performance of the field of competitors as a whole. We study what fraction of the participants found good solutions, how well participants were able to optimize to different performance measures, how homogeneous their submitted predictions are, and if the best submissions represent the maximal performances that could reasonably be achieved. We are keeping the KDD-Cup 2004 WWW site open and have added an automatic scoring system for new submissions in order to encourage further research in this area."
            },
            "slug": "KDD-Cup-2004:-results-and-analysis-Caruana-Joachims",
            "title": {
                "fragments": [],
                "text": "KDD-Cup 2004: results and analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "What fraction of the participants found good solutions, how well participants were able to optimize to different performance measures, how homogeneous their submitted predictions are, and if the best submissions represent the maximal performances that could reasonably be achieved are studied."
            },
            "venue": {
                "fragments": [],
                "text": "SKDD"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740159"
                        ],
                        "name": "Yisong Yue",
                        "slug": "Yisong-Yue",
                        "structuredName": {
                            "firstName": "Yisong",
                            "lastName": "Yue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yisong Yue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50256971"
                        ],
                        "name": "Thomas Finley",
                        "slug": "Thomas-Finley",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Finley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Finley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803571"
                        ],
                        "name": "Filip Radlinski",
                        "slug": "Filip-Radlinski",
                        "structuredName": {
                            "firstName": "Filip",
                            "lastName": "Radlinski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Filip Radlinski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 162
                            }
                        ],
                        "text": "The argmax in Line 5 has an efficient solution for a wide variety of choices for\u03a8 , Y , and \u0394 (see e.g., Tsochantaridis et al,\n2005; Joachims, 2005; Yu et al, 2007; Yue et al, 2007), and often the algorithm for making predictions (see Eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 72
                            }
                        ],
                        "text": "2007), learning ranking functions that optimize IR performance measures (Yue et al. 2007), and segmenting images (Anguelov et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 235
                            }
                        ],
                        "text": "\u2026for problems as diverse as natural language parsing (Taskar et al, 2004), protein sequence alignment (Yu et al, 2007), supervised clustering (Finley and Joachims, 2005), learning ranking functions that optimize IR performance measures (Yue et al, 2007), and segmenting images (Anguelov et al, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7423459,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "094ce8379a16030a7a993b2f70af46916f7ea99b",
            "isKey": false,
            "numCitedBy": 698,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine learning is commonly used to improve ranked retrieval systems. Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems. Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive. In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP. We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea. In most cases we show our method to produce statistically significant improvements in MAP scores."
            },
            "slug": "A-support-vector-method-for-optimizing-average-Yue-Finley",
            "title": {
                "fragments": [],
                "text": "A support vector method for optimizing average precision"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work presents a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP, and shows its method to produce statistically significant improvements in MAP scores."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144106136"
                        ],
                        "name": "S. Keerthi",
                        "slug": "S.-Keerthi",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Keerthi",
                            "middleNames": [
                                "Sathiya"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Keerthi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703049"
                        ],
                        "name": "D. DeCoste",
                        "slug": "D.-DeCoste",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "DeCoste",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. DeCoste"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 67
                            }
                        ],
                        "text": "Algorithms for finding such approximations have been suggested in (Keerthi et al, 2006; Fukumizu et al, 2004; Smola and Scho\u0308lkopf, 2000) for\nclassifications SVMs, and at least some of them can be extended to structural SVMs as well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13375961,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e44e0167ef3ff820917425839be0c145179c5a5a",
            "isKey": false,
            "numCitedBy": 300,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Support vector machines (SVMs), though accurate, are not preferred in applications requiring great classification speed, due to the number of support vectors being large. To overcome this problem we devise a primal method with the following properties: (1) it decouples the idea of basis functions from the concept of support vectors; (2) it greedily finds a set of kernel basis functions of a specified maximum size (dmax) to approximate the SVM primal cost function well; (3) it is efficient and roughly scales as O(ndmax2) where n is the number of training examples; and, (4) the number of basis functions it requires to achieve an accuracy close to the SVM accuracy is usually far less than the number of SVM support vectors."
            },
            "slug": "Building-Support-Vector-Machines-with-Reduced-Keerthi-Chapelle",
            "title": {
                "fragments": [],
                "text": "Building Support Vector Machines with Reduced Classifier Complexity"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A primal method that decouples the idea of basis functions from the concept of support vectors and greedily finds a set of kernel basis functions of a specified maximum size to approximate the SVM primal cost function well."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693668"
                        ],
                        "name": "K. Fukumizu",
                        "slug": "K.-Fukumizu",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Fukumizu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukumizu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144570279"
                        ],
                        "name": "F. Bach",
                        "slug": "F.-Bach",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bach",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7642935,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e8f36ed137fb7b598650e8012ad6bb8727c9f00",
            "isKey": false,
            "numCitedBy": 597,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel method of dimensionality reduction for supervised learning problems. Given a regression or classification problem in which we wish to predict a response variable Y from an explanatory variable X, we treat the problem of dimensionality reduction as that of finding a low-dimensional \"effective subspace\" for X which retains the statistical relationship between X and Y. We show that this problem can be formulated in terms of conditional independence. To turn this formulation into an optimization problem we establish a general nonparametric characterization of conditional independence using covariance operators on reproducing kernel Hilbert spaces. This characterization allows us to derive a contrast function for estimation of the effective subspace. Unlike many conventional methods for dimensionality reduction in supervised learning, the proposed method requires neither assumptions on the marginal distribution of X, nor a parametric model of the conditional distribution of Y. We present experiments that compare the performance of the method with conventional methods."
            },
            "slug": "Dimensionality-Reduction-for-Supervised-Learning-Fukumizu-Bach",
            "title": {
                "fragments": [],
                "text": "Dimensionality Reduction for Supervised Learning with Reproducing Kernel Hilbert Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A novel method of dimensionality reduction for supervised learning problems that requires neither assumptions on the marginal distribution of X, nor a parametric model of the conditional distribution of Y, and establishes a general nonparametric characterization of conditional independence using covariance operators on reproducing kernel Hilbert spaces."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786843"
                        ],
                        "name": "A. Globerson",
                        "slug": "A.-Globerson",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Globerson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Globerson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060101052"
                        ],
                        "name": "Terry Koo",
                        "slug": "Terry-Koo",
                        "structuredName": {
                            "firstName": "Terry",
                            "lastName": "Koo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Terry Koo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701734"
                        ],
                        "name": "X. Carreras",
                        "slug": "X.-Carreras",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Carreras",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Carreras"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 187
                            }
                        ],
                        "text": "Exponentiated gradient methods, originally proposed for online learning of linear predictors (Kivinen and Warmuth, 1997), have also been applied to the training of structured predictors (Globerson et al, 2007; Bartlett et al, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 186
                            }
                        ],
                        "text": "\u2026et al, 2005), which applies only to problems where subgradients of the QP can be computed via a convex real relaxation, as well as exponentiated gradient methods (Bartlett et al, 2004; Globerson et al, 2007), which require the ability to compute \u201cmarginals\u201d (e.g. via the sum-product algorithm)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 61
                            }
                        ],
                        "text": "Unlike exponentiated gradient methods (Bartlett et al, 2004; Globerson et al, 2007), PEGASOS does not require the computation of marginals, which makes it equally easy to apply as cutting-plane methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11226500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5371aab836743ef59e0de15a8de8531648226ae",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Conditional log-linear models are a commonly used method for structured prediction. Efficient learning of parameters in these models is therefore an important problem. This paper describes an exponentiated gradient (EG) algorithm for training such models. EG is applied to the convex dual of the maximum likelihood objective; this results in both sequential and parallel update algorithms, where in the sequential algorithm parameters are updated in an online fashion. We provide a convergence proof for both algorithms. Our analysis also simplifies previous results on EG for max-margin models, and leads to a tighter bound on convergence rates. Experiments on a large-scale parsing task show that the proposed algorithm converges much faster than conjugate-gradient and L-BFGS approaches both in terms of optimization objective and test error."
            },
            "slug": "Exponentiated-gradient-algorithms-for-log-linear-Globerson-Koo",
            "title": {
                "fragments": [],
                "text": "Exponentiated gradient algorithms for log-linear structured prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper describes an exponentiated gradient (EG) algorithm for trainingitional log-linear models, which results in both sequential and parallel update algorithms, and provides a convergence proof for both algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 255
                            }
                        ],
                        "text": "\u2026this provides greater modeling flexibility through avoidance of independence assumptions, and it was shown to provide substantially improved prediction accuracy in many domains (e.g., Lafferty et al, 2001; Taskar et al, 2003; Tsochantaridis et al, 2004; Taskar et al, 2004; Yu et al, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 115
                            }
                        ],
                        "text": "The flexibility in designing \u03a8 allows employing structural SVMs for problems as diverse as natural language parsing (Taskar et al, 2004), protein sequence alignment (Yu et al, 2007), supervised clustering (Finley and Joachims, 2005), learning ranking functions that optimize IR performance measures\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "x and output y. Intuitively, one can think of fw(x,y) as a compatibility function that measures how well the output y matches the given input x. The flexibility in designing% allows us to employ SVMs to learn models for problems as diverse as natural language parsing ( Taskar et al, 2004 ), protein sequence alignment (Yu et al,"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Akin to moving from Naive Bayes to an SVM for classification, this provides greater modeling flexibility through avoidance of independence assumptions, and was shown to provide substantially improvedprediction accuracy in many domains (e.g., Lafferty et al, 2001; Taskar et al, 2003; Tsochantaridis et al, 2004;  Taskar et al, 2004;  Yu et al, 2006))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8313435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e19a94d547ee023837c14c361139185e2353fc0",
            "isKey": true,
            "numCitedBy": 246,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines. Our formulation uses a factorization analogous to the standard dynamic programs for parsing. In particular, it allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates. Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness. We provide an efficient algorithm for learning such models and show experimental evidence of the model\u2019s improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar."
            },
            "slug": "Max-Margin-Parsing-Taskar-Klein",
            "title": {
                "fragments": [],
                "text": "Max-Margin Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines is presented, which allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60502900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c4da62e9e89e65ac78ee271e424e8b498053e8c",
            "isKey": false,
            "numCitedBy": 5544,
            "numCiting": 260,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction to support vector learning roadmap. Part 1 Theory: three remarks on the support vector method of function estimation, Vladimir Vapnik generalization performance of support vector machines and other pattern classifiers, Peter Bartlett and John Shawe-Taylor Bayesian voting schemes and large margin classifiers, Nello Cristianini and John Shawe-Taylor support vector machines, reproducing kernel Hilbert spaces, and randomized GACV, Grace Wahba geometry and invariance in kernel based methods, Christopher J.C. Burges on the annealed VC entropy for margin classifiers - a statistical mechanics study, Manfred Opper entropy numbers, operators and support vector kernels, Robert C. Williamson et al. Part 2 Implementations: solving the quadratic programming problem arising in support vector classification, Linda Kaufman making large-scale support vector machine learning practical, Thorsten Joachims fast training of support vector machines using sequential minimal optimization, John C. Platt. Part 3 Applications: support vector machines for dynamic reconstruction of a chaotic system, Davide Mattera and Simon Haykin using support vector machines for time series prediction, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel. Part 4 Extensions of the algorithm: reducing the run-time complexity in support vector machines, Edgar E. Osuna and Federico Girosi support vector regression with ANOVA decomposition kernels, Mark O. Stitson et al support vector density estimation, Jason Weston et al combining support vector and mathematical programming methods for classification, Bernhard Scholkopf et al."
            },
            "slug": "Advances-in-kernel-methods:-support-vector-learning-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Advances in kernel methods: support vector learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Support vector machines for dynamic reconstruction of a chaotic system, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144106136"
                        ],
                        "name": "S. Keerthi",
                        "slug": "S.-Keerthi",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Keerthi",
                            "middleNames": [
                                "Sathiya"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Keerthi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703049"
                        ],
                        "name": "D. DeCoste",
                        "slug": "D.-DeCoste",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "DeCoste",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. DeCoste"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 22
                            }
                        ],
                        "text": "The L2-SVM-MFN method (Keerthi and DeCoste 2005) avoids explicitly representing N \u00d7 N matrices using conjugate gradient techniques."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 23
                            }
                        ],
                        "text": "The L2-SVM-MFN method (Keerthi and DeCoste, 2005) avoids explicitly representing N \u00d7N matrices using conjugate gradient techniques."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17488612,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c66378bcf0c63a25e78c24b544764e7ee073cb5",
            "isKey": false,
            "numCitedBy": 339,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper develops a fast method for solving linear SVMs with L2 loss function that is suited for large scale data mining tasks such as text classification. This is done by modifying the finite Newton method of Mangasarian in several ways. Experiments indicate that the method is much faster than decomposition methods such as SVMlight, SMO and BSVM (e.g., 4-100 fold), especially when the number of examples is large. The paper also suggests ways of extending the method to other loss functions such as the modified Huber's loss function and the L1 loss function, and also for solving ordinal regression."
            },
            "slug": "A-Modified-Finite-Newton-Method-for-Fast-Solution-Keerthi-DeCoste",
            "title": {
                "fragments": [],
                "text": "A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A fast method for solving linear SVMs with L2 loss function that is suited for large scale data mining tasks such as text classification is developed by modifying the finite Newton method of Mangasarian in several ways."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389955537"
                        ],
                        "name": "S. Shalev-Shwartz",
                        "slug": "S.-Shalev-Shwartz",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Shalev-Shwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shalev-Shwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706280"
                        ],
                        "name": "Nathan Srebro",
                        "slug": "Nathan-Srebro",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Srebro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Srebro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145658292"
                        ],
                        "name": "Andrew Cotter",
                        "slug": "Andrew-Cotter",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Cotter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Cotter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 246
                            }
                        ],
                        "text": "\u2026that fw(x,y) takes the form of a linear function\nfw(x,y) = wT\u03a8 (x,y)\nwhere w \u2208 \u211cN is a parameter vector and \u03a8 (x,y) is a feature vector relating input x and output y. Intuitively, one can think of f w(x,y) as a compatibility function that measures how well the output y matches the given input x."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 61
                            }
                        ],
                        "text": "Recently, subgradient methods and their stochastic variants (Ratliff et al, 2007) have also been proposed to solve the optimization problem in maxmargin structured prediction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 73
                            }
                        ],
                        "text": "Such algorithms either take stochastic subgradi-\nent steps (Collins, 2002; Ratliff et al, 2007; Shalev-Shwartz et al, 2007), or build a cutting-plane model which is easy to solve directly (Tsochantaridis et al, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53306004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb4ffd271451a8fbed752582543bd34925fa4396",
            "isKey": false,
            "numCitedBy": 2004,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe and analyze a simple and effective stochastic sub-gradient descent algorithm for solving the optimization problem cast by Support Vector Machines (SVM). We prove that the number of iterations required to obtain a solution of accuracy $${\\epsilon}$$ is $${\\tilde{O}(1 / \\epsilon)}$$, where each iteration operates on a single training example. In contrast, previous analyses of stochastic gradient descent methods for SVMs require $${\\Omega(1 / \\epsilon^2)}$$ iterations. As in previously devised SVM solvers, the number of iterations also scales linearly with 1/\u03bb, where \u03bb is the regularization parameter of SVM. For a linear kernel, the total run-time of our method is $${\\tilde{O}(d/(\\lambda \\epsilon))}$$, where d is a bound on the number of non-zero features in each example. Since the run-time does not depend directly on the size of the training set, the resulting algorithm is especially suited for learning from large datasets. Our approach also extends to non-linear kernels while working solely on the primal objective function, though in this case the runtime does depend linearly on the training set size. Our algorithm is particularly well suited for large text classification problems, where we demonstrate an order-of-magnitude speedup over previous SVM learning methods."
            },
            "slug": "Pegasos:-primal-estimated-sub-gradient-solver-for-Shalev-Shwartz-Singer",
            "title": {
                "fragments": [],
                "text": "Pegasos: primal estimated sub-gradient solver for SVM"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A simple and effective stochastic sub-gradient descent algorithm for solving the optimization problem cast by Support Vector Machines, which is particularly well suited for large text classification problems, and demonstrates an order-of-magnitude speedup over previous SVM learning methods."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747026"
                        ],
                        "name": "O. Mangasarian",
                        "slug": "O.-Mangasarian",
                        "structuredName": {
                            "firstName": "Olvi",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mangasarian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791069"
                        ],
                        "name": "D. Musicant",
                        "slug": "D.-Musicant",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Musicant",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Musicant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 17
                            }
                        ],
                        "text": ", Lagrangian SVM (Mangasarian and Musicant 2001) (using the \u2211 \u03be 2 i loss), Proximal SVM (Fung and Mangasarian 2001) (using an L2 regression loss), and Interior Point Methods (Ferris and Munson 2003)), they use the Sherman-Morrison-Woodbury formula (or matrix factorizations) for inverting the Hessian of the dual."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 86
                            }
                        ],
                        "text": "While there are training algorithms for linear SVMs that scale linearly with n (e.g., Lagrangian SVM (Mangasarian and Musicant, 2001) (using the \u2211\u03be 2i loss), Proximal SVM (Fung and Mangasarian, 2001) (using an L 2 regression loss), and Interior Point Methods (Ferris and Munson, 2003)), they use the Sherman-Morrison-Woodbury formula (or similar matrix factorizations) for inverting the Hessian of the dual."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 102
                            }
                        ],
                        "text": "While there are training algorithms for linear SVMs that scale linearly with n (e.g., Lagrangian SVM (Mangasarian and Musicant, 2001) (using the \u2211\u03be 2i loss), Proximal SVM (Fung and Mangasarian, 2001) (using an L 2 regression loss), and Interior Point Methods (Ferris and Munson, 2003)), they use the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7794861,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45204c008c8f3d9e9b66ea5623c032ce0b3089e7",
            "isKey": false,
            "numCitedBy": 548,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "An implicit Lagrangian for the dual of a simple reformulation of the standard quadratic program of a linear support vector machine is proposed. This leads to the minimization of an unconstrained differentiable convex function in a space of dimensionality equal to the number of classified points. This problem is solvable by an extremely simple linearly convergent Lagrangian support vector machine (LSVM) algorithm. LSVM requires the inversion at the outset of a single matrix of the order of the much smaller dimensionality of the original input space plus one. The full algorithm is given in this paper in 11 lines of MATLAB code without any special optimization tools such as linear or quadratic programming solvers. This LSVM code can be used \"as is\" to solve classification problems with millions of points. For example, 2 million points in 10 dimensional input space were classified by a linear surface in 82 minutes on a Pentium III 500 MHz notebook with 384 megabytes of memory (and additional swap space), and in 7 minutes on a 250 MHz UltraSPARC II processor with 2 gigabytes of memory. Other standard classification test problems were also solved. Nonlinear kernel classification can also be solved by LSVM. Although it does not scale up to very large problems, it can handle any positive semidefinite kernel and is guaranteed to converge. A short MATLAB code is also given for nonlinear kernels and tested on a number of problems."
            },
            "slug": "Lagrangian-Support-Vector-Machines-Mangasarian-Musicant",
            "title": {
                "fragments": [],
                "text": "Lagrangian Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "An implicit Lagrangian for the dual of a simple reformulation of the standard quadratic program of a linear support vector machine is proposed, which leads to the minimization of an unconstrained differentiable convex function in a space of dimensionality equal to the number of classified points."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143857271"
                        ],
                        "name": "Nigel P. Duffy",
                        "slug": "Nigel-P.-Duffy",
                        "structuredName": {
                            "firstName": "Nigel",
                            "lastName": "Duffy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nigel P. Duffy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 194
                            }
                        ],
                        "text": "Recent years have seen intriguing advances in extending methods like Logistic Regression, Perceptrons, and Support Vector Machines (SVMs) to global training of such structured prediction models (e.g., Lafferty et al, 2001; Collins and Duffy, 2002; Taskar et al, 2003; Tsochantaridis et al, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 225
                            }
                        ],
                        "text": "\u2026provided intriguing advances in extending methods like Logistic Regression, Perceptrons, and Support Vector Machines (SVMs) to global training of such structured prediction models (e.g., Lafferty et al, 2001; Collins, 2004; Collins and Duffy, 2002; Taskar et al, 2003; Tsochantaridis et al, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7506864,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe638b5610475d4524684fb2c2b7b08c119c8700",
            "isKey": false,
            "numCitedBy": 631,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the \"all subtrees\" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data."
            },
            "slug": "New-Ranking-Algorithms-for-Parsing-and-Tagging:-and-Collins-Duffy",
            "title": {
                "fragments": [],
                "text": "New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "New learning algorithms for natural language processing based on the perceptron algorithm are introduced, showing how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the \"all subtrees\" (DOP) representation described by (Bod 1998)."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6134427,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aab43c9c33af00b718cf2ae374b861d49862a563",
            "isKey": false,
            "numCitedBy": 15727,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine Learning is the study of methods for programming computers to learn. Computers are applied to a wide range of tasks, and for most of these it is relatively easy for programmers to design and implement the necessary software. However, there are many tasks for which this is difficult or impossible. These can be divided into four general categories. First, there are problems for which there exist no human experts. For example, in modern automated manufacturing facilities, there is a need to predict machine failures before they occur by analyzing sensor readings. Because the machines are new, there are no human experts who can be interviewed by a programmer to provide the knowledge necessary to build a computer system. A machine learning system can study recorded data and subsequent machine failures and learn prediction rules. Second, there are problems where human experts exist, but where they are unable to explain their expertise. This is the case in many perceptual tasks, such as speech recognition, hand-writing recognition, and natural language understanding. Virtually all humans exhibit expert-level abilities on these tasks, but none of them can describe the detailed steps that they follow as they perform them. Fortunately, humans can provide machines with examples of the inputs and correct outputs for these tasks, so machine learning algorithms can learn to map the inputs to the outputs. Third, there are problems where phenomena are changing rapidly. In finance, for example, people would like to predict the future behavior of the stock market, of consumer purchases, or of exchange rates. These behaviors change frequently, so that even if a programmer could construct a good predictive computer program, it would need to be rewritten frequently. A learning program can relieve the programmer of this burden by constantly modifying and tuning a set of learned prediction rules. Fourth, there are applications that need to be customized for each computer user separately. Consider, for example, a program to filter unwanted electronic mail messages. Different users will need different filters. It is unreasonable to expect each user to program his or her own rules, and it is infeasible to provide every user with a software engineer to keep the rules up-to-date. A machine learning system can learn which mail messages the user rejects and maintain the filtering rules automatically. Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis. Statistics focuses on understanding the phenomena that have generated the data, often with the goal of testing different hypotheses about those phenomena. Data mining seeks to find patterns in the data that are understandable by people. Psychological studies of human learning aspire to understand the mechanisms underlying the various learning behaviors exhibited by people (concept learning, skill acquisition, strategy change, etc.)."
            },
            "slug": "Machine-learning-Dietterich",
            "title": {
                "fragments": [],
                "text": "Machine learning"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786202"
                        ],
                        "name": "H. Bunt",
                        "slug": "H.-Bunt",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Bunt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152299194"
                        ],
                        "name": "G. Satta",
                        "slug": "G.-Satta",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Satta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Satta"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 221252969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e70f27fc0f4631b4b10fc0537d54b391afd6ab44",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface. 1: Developments in Parsing Technology: From Theory to Application H. Bunt, J. Carroll, G. Satta. 1. Introduction. 2. About this book. 2: Parameter Estimation for Statistical Parsing Models: Theory and Practice of Distribution-Free Methods M. Collins. 1. Introduction. 2. Linear Models. 3. Probabilistic Context-Free Grammars. 4. Statistical Learning Theory. 5. Convergence Bounds for Finite Sets of Hypotheses. 6. Convergence Bounds for Hyperplane Classifiers. 7. Application of Margin Analysis to Parsing. 8. Algorithms. 9. Discussion. 10. Conclusions. 3: High Precision Extraction of Grammatical Relations J. Carroll, T. Briscoe. 1. Introduction. 2. The Analysis System. 3. Empirical Results. 4. Conclusions and Further Work. 4: Automated Extraction of TAGs from the Penn Treebank J. Chen, K.V. Shanker. 1. Introduction. 2. Tree Extraction Procedure. 3. Evaluation. 4. Extended Extracted Grammars. 5. Related Work. 6. Conclusions. 5: Computing the Most Probable Parse for a Discontinuous Phrase-Structure Grammar O. Plaehn. 1. Introduction. 2. Discontinuous Phrase-Structure Grammar. 3. The Parsing Algorithm. 4. Computing the Most Probable Parse. 5. Experiments. 6. Conclusion and Future Work. 6: A Neural Network Parser that Handles Sparse Data J. Henderson. 1. Introduction. 2. Simple Synchrony Networks. 3. A Probabilistic Parser for SSNs. 4. Estimating the Probabilities with a Simple Synchrony Network. 5. Generalizing from Sparse Data. 6. Conclusion. 7: An Efficient LR Parser Generator for Tree-Adjoining Grammars C.A. Prolo. 1. Introduction. 2. TAGS. 3. On Some Degenerate LR Models for TAGS. 4. Proposed Algorithm. 5. Implementation. 6. Example. 7. Some Properties Of the Algorithms. 8. Evaluation. 9. Conclusions. 8: Relating Tabular Parsing Algorithms for LIG and TAG M.A. Alonso, E. de la Clergerie, V.J. Diaz, M. Vilares. 1. Introduction. 2. Tree-Adjoining Grammars. 3. Linear Indexed Grammars. 4. Bottom-upParsing Algorithms. 5. Barley-like Parsing Algorithms. 6. Barley-like Parsing Algorithms Preserving the Correct Prefix Property. 7. Bidirectional Parsing. 8. Specialized TAG parsers. 9. Conclusion. 9: Improved Left-Corner Chart Parsing for Large Context-Free Grammars R.C. Moore. 1. Introduction. 2. Evaluating Parsing Algorithms. 3. Terminology and Notation. 4. Test Grammars. 5. Left-Corner Parsing Algorithms and Refinements. 6. Grammar Transformations. 7. Extracting Parses from the Chart. 8. Comparison to Other Algorithms. 9. Conclusions. 10: On Two Classes of Feature Paths in Large-Scale Unification Grammars L. Ciortuz. 1. Introduction. 2. Compiling the Quick Check Filter. 3. Generalised Rule Reduction. 4. Conclusion. 11: A Context-Free Superset Approximation of Unification-Based Grammars B. Kiefer, H.-U. Krieger. 1. Introduction. 2. Basic Inventory. 3. Approximation as Fixpoint Construction. 4. The Basic Algorithm. 5. Implementation Issues and Optimizations. 6. Revisiting the Fixpoint Construction. 7. Three Grammars. 8. Disambiguation of UBGs via Probabilistic Approximations. 12: A Recognizer for Minimalist Languages H. Harkema. 1. Introduction. 2. Minimalist Grammars. 3. Specification of the Recognizer. 4. Correctness. 5. Complexity Results. 6. Conclusions and Future Work. 13: Range Concatenation Grammars P. Boullier. 1. Introduction. 2. Positive Range Concatenation Grammars. 3. Negative Range Concatenation Grammars. 4. A Parsing Algorithm for RCGs. 5. Closure Properties and Modularity. 6. Conclusion. 14: Grammar Induction by MDL-Based Distributional Classification Yikun Guo, Fuliang Weng, Lide Wu. 1. Introduction. 2. Grammar Induction with the MDL Principle. 3. Induction Strategies. 4. MDL Induction by Dynamic Distributional Classification (DCC). 5. Comparison and Conclusion. Appendix. 15: Optimal Ambiguity Packing in Context-Free Parsers with Interleaved Unification A. Lavie, C. Penstein Rose. 1."
            },
            "slug": "New-developments-in-parsing-technology-Bunt-Carroll",
            "title": {
                "fragments": [],
                "text": "New developments in parsing technology"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This book discusses the development of Parsing Technology from Theory to Application, and concludes that computing the Most Probable Parse for a Discontinuous Phrase-Structure Grammar with a Simple Synchrony Network is feasible."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388317459"
                        ],
                        "name": "S. Lacoste-Julien",
                        "slug": "S.-Lacoste-Julien",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lacoste-Julien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lacoste-Julien"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To this special case also applies the method of  Taskar et al (2005) , which casts the training of max-margin structured predictors as a convex-concave saddle-point problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Other methods that exploit the factorization of the structural formulation include extragradient ( Taskar et al, 2005 ) and exponentiated gradient methods (Bartlett et al, 2004; Globerson et al, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 48
                            }
                        ],
                        "text": "To this special case also applies the method of Taskar et al (2005), which casts the training of max-margin structured predictors as a convex-concave saddle-point problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 61
                            }
                        ],
                        "text": "Similar restrictions also apply to the extragradient method (Taskar et al, 2005), which applies only to problems where subgradients of the QP can be computed via a convex real relaxation, as well as exponentiated gradient methods (Bartlett et al, 2004; Globerson et al, 2007), which require the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2298202,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd5590d9696be6d9e0807c6660826f5351093790",
            "isKey": true,
            "numCitedBy": 63,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a simple and scalable algorithm for large-margin estimation of structured models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem and apply the extragradient method, yielding an algorithm with linear convergence using simple gradient and projection calculations. The projection step can be solved using combinatorial algorithms for min-cost quadratic flow. This makes the approach an efficient alternative to formulations based on reductions to a quadratic program (QP). We present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm."
            },
            "slug": "Structured-Prediction-via-the-Extragradient-Method-Taskar-Lacoste-Julien",
            "title": {
                "fragments": [],
                "text": "Structured Prediction via the Extragradient Method"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A simple and scalable algorithm for large-margin estimation of structured models, including an important class of Markov networks and combinatorial models, with linear convergence using simple gradient and projection calculations is presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32440409"
                        ],
                        "name": "T. Rose",
                        "slug": "T.-Rose",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Rose",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Rose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146329150"
                        ],
                        "name": "Fan Li",
                        "slug": "Fan-Li",
                        "structuredName": {
                            "firstName": "Fan",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fan Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 185
                            }
                        ],
                        "text": "\u2026this provides greater modeling flexibility through avoidance of independence assumptions, and it was shown to provide substantially improved prediction accuracy in many domains (e.g., Lafferty et al, 2001; Taskar et al, 2003; Tsochantaridis et al, 2004; Taskar et al, 2004; Yu et al, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 188
                            }
                        ],
                        "text": "\u2026provided intriguing advances in extending methods like Logistic Regression, Perceptrons, and Support Vector Machines (SVMs) to global training of such structured prediction models (e.g., Lafferty et al, 2001; Collins, 2004; Collins and Duffy, 2002; Taskar et al, 2003; Tsochantaridis et al, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11027141,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2abe6b9ea1b13653b7384e9c8ef14b0d87e20cfc",
            "isKey": false,
            "numCitedBy": 2683,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters documentation, we describe the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as RCV1-v1, and the corrected data as RCV1-v2. We benchmark several widely used supervised learning methods on RCV1-v2, illustrating the collection's properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices."
            },
            "slug": "RCV1:-A-New-Benchmark-Collection-for-Text-Research-Lewis-Yang",
            "title": {
                "fragments": [],
                "text": "RCV1: A New Benchmark Collection for Text Categorization Research"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work describes the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 223
                            }
                        ],
                        "text": "In particular, for binary classification X = \u211cN and Y = {\u22121,+1}, and plugging\n\u03a8(x,y) = 1 2\nyx and \u0394(y,y\u2032) = {\n0 if y = y\u2032 1 otherwise\n(28)\ninto either n-slack formulation OP2 or OP3 produces the standard SVM optimization problem OP1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Support Vector Machines select an h # H that minimizes a regularized empirical risk on S. For conventional binary classification where Y = {! 1,+1}, SVM training is typically formulatedas the following convexquadratic optimization problem 2 ( Cortes and Vapnik, 1995;  Vapnik, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 152
                            }
                        ],
                        "text": "For conventional binary classification where Y = {\u22121,+1}, SVM training is typically formulated as the following convex quadratic optimization problem 2 (Cortes and Vapnik, 1995; Vapnik, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In the n-slack formulation, one immediatelyrecoversVapniket al.\u2019s originalclassification SVM formulationofOP1 ( Cortes and Vapnik, 1995;  Vapnik, 1998) (up to the more convenient percentagescale rescaling of the loss function), which we solve using SVM-light."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 116
                            }
                        ],
                        "text": "In the n-slack formulation, one immediately recovers Vapnik et al.\u2019s original classification SVM formulation of OP1 (Cortes and Vapnik, 1995; Vapnik, 1998) (up to the more convenient percentagescale rescaling of the loss function and the absence of the bias term), which we solve using SVMlight ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52874011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52b7bf3ba59b31f362aa07f957f1543a29a4279e",
            "isKey": true,
            "numCitedBy": 33434,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition."
            },
            "slug": "Support-Vector-Networks-Cortes-Vapnik",
            "title": {
                "fragments": [],
                "text": "Support-Vector Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated and the performance of the support- vector network is compared to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145177220"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 41
                            }
                        ],
                        "text": "We use the CKY parser implementation4 of Johnson (1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We use the CKY parser implementation4 of Johnson (1998). For the separation oracle the same CKY parser is used after extending it to also return the second best solution."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7978249,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c9f553e723a40a6713453b734b552c1928bf52b",
            "isKey": false,
            "numCitedBy": 441,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The kinds of tree representations used in a treebank corpus can have a dramatic effect on performance of a parser based on the PCFG estimated from that corpus, causing the estimated likelihood of a tree to differ substantially from its frequency in the training corpus. This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today. This performance variation comes about because any PCFG, and hence the corpus of trees from which the PCFG is induced, embodies independence assumptions about the distribution of words and phrases. The particular independence assumptions implicit in a tree representation can be studied theoretically and investigated empirically by means of a tree transformation / detransformation process."
            },
            "slug": "PCFG-Models-of-Linguistic-Tree-Representations-Johnson",
            "title": {
                "fragments": [],
                "text": "PCFG Models of Linguistic Tree Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A simple node relabeling transformation is described that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113414328"
                        ],
                        "name": "Fernando Pereira",
                        "slug": "Fernando-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 185
                            }
                        ],
                        "text": "\u2026this provides greater modeling flexibility through avoidance of independence assumptions, and it was shown to provide substantially improved prediction accuracy in many domains (e.g., Lafferty et al, 2001; Taskar et al, 2003; Tsochantaridis et al, 2004; Taskar et al, 2004; Yu et al, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Akin to moving from Naive Bayes to an SVM for classification, this provides greater modeling flexibility through avoidance of independence assumptions, and was shown to provide substantially improvedprediction accuracy in many domains (e.g.,  Lafferty et al, 2001;  Taskar et al, 2003; Tsochantaridis et al, 2004; Taskar et al, 2004; Yu et al, 2006))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 188
                            }
                        ],
                        "text": "\u2026provided intriguing advances in extending methods like Logistic Regression, Perceptrons, and Support Vector Machines (SVMs) to global training of such structured prediction models (e.g., Lafferty et al, 2001; Collins, 2004; Collins and Duffy, 2002; Taskar et al, 2003; Tsochantaridis et al, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Regression, Perceptrons, and Support Vector Machines (SVMs) to global training of such structured prediction models (e.g.,  Lafferty et al, 2001;  Collins and Duffy, 2002; Taskar et al, 2003; Tsochantaridis et al, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 219683473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4ba954b0412773d047dc41231c733de0c1f4926",
            "isKey": true,
            "numCitedBy": 13411,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "slug": "Conditional-Random-Fields:-Probabilistic-Models-for-Lafferty-McCallum",
            "title": {
                "fragments": [],
                "text": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents iterative parameter estimation algorithms for conditional random fields and compares the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1838674"
                        ],
                        "name": "Dragomir Anguelov",
                        "slug": "Dragomir-Anguelov",
                        "structuredName": {
                            "firstName": "Dragomir",
                            "lastName": "Anguelov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dragomir Anguelov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2482074"
                        ],
                        "name": "Vassil Chatalbashev",
                        "slug": "Vassil-Chatalbashev",
                        "structuredName": {
                            "firstName": "Vassil",
                            "lastName": "Chatalbashev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vassil Chatalbashev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35195064"
                        ],
                        "name": "D. Gupta",
                        "slug": "D.-Gupta",
                        "structuredName": {
                            "firstName": "Dinkar",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728179"
                        ],
                        "name": "G. Heitz",
                        "slug": "G.-Heitz",
                        "structuredName": {
                            "firstName": "Geremy",
                            "lastName": "Heitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Heitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 150
                            }
                        ],
                        "text": "\u2026(PRIMAL))\nmin w,\u03bei\u22650 1 2 wT w + C n\nn\n\u2211 i=1 \u03bei\ns.t. \u2200i \u2208 {1, ...,n} : yi(wT xi)\u2265 1\u2212 \u03bei\nIt was shown that SVM training can be generalized to structured outputs (Altun et al, 2003; Taskar et al, 2003; Tsochantaridis et al, 2004), leading to an optimiza-\n1 Note, however, that all formal results in\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 138
                            }
                        ],
                        "text": "For training the weights w of the linear discriminant function, the standard SVM optimization problem can be generalized in several ways (Altun et al, 2003; Joachims, 2003; Taskar et al, 2003; Tsochantaridis et al, 2004, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8396595,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55a5e1a4e0068a4f2a8a8bdfbd777c249110ccfe",
            "isKey": false,
            "numCitedBy": 419,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of segmenting 3D scan data into objects or object classes. Our segmentation framework is based on a subclass of Markov random fields (MRFs) which support efficient graph-cut inference. The MRF models incorporate a large set of diverse features and enforce the preference that adjacent scan points have the same classification label. We use a recently proposed maximum-margin framework to discriminatively train the model from a set of labeled scans; as a result we automatically learn the relative importance of the features for the segmentation task. Performing graph-cut inference in the trained MRF can then be used to segment new scenes very efficiently. We test our approach on three large-scale datasets produced by different kinds of 3D sensors, showing its applicability to both outdoor and indoor environments containing diverse objects."
            },
            "slug": "Discriminative-learning-of-Markov-random-fields-for-Anguelov-Taskar",
            "title": {
                "fragments": [],
                "text": "Discriminative learning of Markov random fields for segmentation of 3D scan data"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work addresses the problem of segmenting 3D scan data into objects or object classes by using a recently proposed maximum-margin framework to discriminatively train the model from a set of labeled scans and automatically learn the relative importance of the features for the segmentation task."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693407"
                        ],
                        "name": "K. Crammer",
                        "slug": "K.-Crammer",
                        "structuredName": {
                            "firstName": "Koby",
                            "lastName": "Crammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Crammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 187
                            }
                        ],
                        "text": "\u2026prediction, the PEGASOS algorithm (Shalev-Shwartz et al, 2007) has shown promising performance for binary classification SVMs. Related to such online methods is also the MIRA algorithm (Crammer and Singer, 2003), which has been used for training structured predictors (e.g. McDonald et al, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 58
                            }
                        ],
                        "text": "Related to such online methods is also the MIRA algorithm (Crammer and Singer 2003), which has been used for training structured predictors (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8729730,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28b9bacde6499f8cc6f7e70feee4232107211e39",
            "isKey": false,
            "numCitedBy": 433,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we study online classification algorithms for multiclass problems in the mistake bound model. The hypotheses we use maintain one prototype vector per class. Given an input instance, a multiclass hypothesis computes a similarity-score between each prototype and the input instance and then sets the predicted label to be the index of the prototype achieving the highest similarity. To design and analyze the learning algorithms in this paper we introduce the notion of ultracon-servativeness. Ultraconservative algorithms are algorithms that update only the prototypes attaining similarity-scores which are higher than the score of the correct label's prototype. We start by describing a family of additive ultraconservative algorithms where each algorithm in the family updates its prototypes by finding a feasible solution for a set of linear constraints that depend on the instantaneous similarity-scores. We then discuss a specific online algorithm that seeks a set of prototypes which have a small norm. The resulting algorithm, which we term MIRA (for Margin Infused Relaxed Algorithm) is ultraconservative as well. We derive mistake bounds for all the algorithms and provide further analysis of MIRA using a generalized notion of the margin for multiclass problems."
            },
            "slug": "Ultraconservative-Online-Algorithms-for-Multiclass-Crammer-Singer",
            "title": {
                "fragments": [],
                "text": "Ultraconservative Online Algorithms for Multiclass Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This paper studies online classification algorithms for multiclass problems in the mistake bound model and introduces the notion of ultracon-servativeness, a family of additive ultraconservative algorithms where each algorithm in the family updates its prototypes by finding a feasible solution for a set of linear constraints that depend on the instantaneous similarity-scores."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10888973,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a7958b418bceb48a315384568091ab1898b1640",
            "isKey": false,
            "numCitedBy": 2272,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random fields (CRFs). The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates. We describe theory justifying the algorithms through a modification of the proof of convergence of the perceptron algorithm for classification problems. We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger."
            },
            "slug": "Discriminative-Training-Methods-for-Hidden-Markov-Collins",
            "title": {
                "fragments": [],
                "text": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experimental results on part-of-speech tagging and base noun phrase chunking are given, in both cases showing improvements over results for a maximum-entropy tagger."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37077406"
                        ],
                        "name": "C. Teo",
                        "slug": "C.-Teo",
                        "structuredName": {
                            "firstName": "Choon",
                            "lastName": "Teo",
                            "middleNames": [
                                "Hui"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Teo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145713876"
                        ],
                        "name": "S. Vishwanathan",
                        "slug": "S.-Vishwanathan",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Vishwanathan",
                            "middleNames": [
                                "V.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vishwanathan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1719925,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8ad078927375243a4dd937f745c60e884ebbf6b",
            "isKey": false,
            "numCitedBy": 189,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "A wide variety of machine learning problems can be described as minimizing a regularized risk functional, with different algorithms using different notions of risk and different regularizers. Examples include linear Support Vector Machines (SVMs), Logistic Regression, Conditional Random Fields (CRFs), and Lasso amongst others. This paper describes the theory and implementation of a highly scalable and modular convex solver which solves all these estimation problems. It can be parallelized on a cluster of workstations, allows for data-locality, and can deal with regularizers such as l1 and l2 penalties. At present, our solver implements 20 different estimation problems, can be easily extended, scales to millions of observations, and is up to 10 times faster than specialized solvers for many applications. The open source code is freely available as part of the ELEFANT toolbox."
            },
            "slug": "A-scalable-modular-convex-solver-for-regularized-Teo-Smola",
            "title": {
                "fragments": [],
                "text": "A scalable modular convex solver for regularized risk minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The theory and implementation of a highly scalable and modular convex solver which solves all these estimation problems, can be parallelized on a cluster of workstations, allows for data-locality, and can deal with regularizers such as l1 and l2 penalties."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 210
                            }
                        ],
                        "text": "\u2026provided intriguing advances in extending methods like Logistic Regression, Perceptrons, and Support Vector Machines (SVMs) to global training of such structured prediction models (e.g., Lafferty et al, 2001; Collins, 2004; Collins and Duffy, 2002; Taskar et al, 2003; Tsochantaridis et al, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10576017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a6cda5c73b3da91ce4260b2b70ca5c226b39edf",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "A fundamental problem in statistical parsing is the choice of criteria and algo-algorithms used to estimate the parameters in a model. The predominant approach in computational linguistics has been to use a parametric model with some variant of maximum-likelihood estimation. The assumptions under which maximum-likelihood estimation is justified are arguably quite strong. This chapter discusses the statistical theory underlying various parameter-estimation methods, and gives algorithms which depend on alternatives to (smoothed) maximum-likelihood estimation. We first give an overview of results from statistical learning theory. We then show how important concepts from the classification literature - specifically, generalization results based on margins on training data - can be derived for parsing models. Finally, we describe parameter estimation algorithms which are motivated by these generalization bounds."
            },
            "slug": "Parameter-Estimation-for-Statistical-Parsing-Theory-Collins",
            "title": {
                "fragments": [],
                "text": "Parameter Estimation for Statistical Parsing Models: Theory and Practice of"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This chapter discusses the statistical theory underlying various parameter-estimation methods, and gives algorithms which depend on alternatives to maximum-likelihood estimation, and describes parameter estimation algorithms which are motivated by these generalization bounds."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50256971"
                        ],
                        "name": "Thomas Finley",
                        "slug": "Thomas-Finley",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Finley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Finley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 205
                            }
                        ],
                        "text": "The flexibility in designing \u03a8 allows employing structural SVMs for problems as diverse as natural language parsing (Taskar et al, 2004), protein sequence alignment (Yu et al, 2007), supervised clustering (Finley and Joachims, 2005), learning ranking functions that optimize IR performance measures (Yue et al, 2007), and segmenting images (Anguelov et al, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 146
                            }
                        ],
                        "text": "\u2026SVMs for problems as diverse as natural language parsing (Taskar et al, 2004), protein sequence alignment (Yu et al, 2007), supervised clustering (Finley and Joachims, 2005), learning ranking functions that optimize IR performance measures (Yue et al, 2007), and segmenting images (Anguelov et\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8292657,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9616433b227763c28f31820ec05b174fcd577af",
            "isKey": false,
            "numCitedBy": 238,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Supervised clustering is the problem of training a clustering algorithm to produce desirable clusterings: given sets of items and complete clusterings over these sets, we learn how to cluster future sets of items. Example applications include noun-phrase coreference clustering, and clustering news articles by whether they refer to the same topic. In this paper we present an SVM algorithm that trains a clustering algorithm by adapting the item-pair similarity measure. The algorithm may optimize a variety of different clustering functions to a variety of clustering performance measures. We empirically evaluate the algorithm for noun-phrase and news article clustering."
            },
            "slug": "Supervised-clustering-with-support-vector-machines-Finley-Joachims",
            "title": {
                "fragments": [],
                "text": "Supervised clustering with support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper presents an SVM algorithm that trains a clustering algorithm by adapting the item-pair similarity measure, and empirically evaluates the algorithm for noun-phrase and news article clustering."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957226"
                        ],
                        "name": "Ryan T. McDonald",
                        "slug": "Ryan-T.-McDonald",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "McDonald",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan T. McDonald"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693407"
                        ],
                        "name": "K. Crammer",
                        "slug": "K.-Crammer",
                        "structuredName": {
                            "firstName": "Koby",
                            "lastName": "Crammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Crammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 275
                            }
                        ],
                        "text": "\u2026prediction, the PEGASOS algorithm (Shalev-Shwartz et al, 2007) has shown promising performance for binary classification SVMs. Related to such online methods is also the MIRA algorithm (Crammer and Singer, 2003), which has been used for training structured predictors (e.g. McDonald et al, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12926517,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d3b27746f7a53f2dc5d9b8c2f3d343313622ec36",
            "isKey": false,
            "numCitedBy": 917,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an effective training algorithm for linearly-scored dependency parsers that implements online large-margin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efficient parsing techniques for dependency trees (Eisner, 1996). The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language specific enhancements."
            },
            "slug": "Online-Large-Margin-Training-of-Dependency-Parsers-McDonald-Crammer",
            "title": {
                "fragments": [],
                "text": "Online Large-Margin Training of Dependency Parsers"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "An effective training algorithm for linearly-scored dependency parsers that implements online large-margin multi-class training on top of efficient parsing techniques for dependency trees is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6130401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98eed3f082351c4821d1edb315846207a8fefbe9",
            "isKey": false,
            "numCitedBy": 911,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider two algorithm for on-line prediction based on a linear model. The algorithms are the well-known Gradient Descent (GD) algorithm and a new algorithm, which we call EG(+/-). They both maintain a weight vector using simple updates. For the GD algorithm, the update is based on subtracting the gradient of the squared error made on a prediction. The EG(+/-) algorithm uses the components of the gradient in the exponents of factors that are used in updating the weight vector multiplicatively. We present worst-case loss bounds for EG(+/-) and compare them to previously known bounds for the GD algorithm. The bounds suggest that the losses of the algorithms are in general incomparable, but EG(+/-) has a much smaller loss if only a few components of the input are relevant for the predictions. We have performed experiments, which show that our worst-case upper bounds are quite tight already on simple artificial data."
            },
            "slug": "Exponentiated-Gradient-Versus-Gradient-Descent-for-Kivinen-Warmuth",
            "title": {
                "fragments": [],
                "text": "Exponentiated Gradient Versus Gradient Descent for Linear Predictors"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The bounds suggest that the losses of the algorithms are in general incomparable, but EG(+/-) has a much smaller loss if only a few components of the input are relevant for the predictions, which is quite tight already on simple artificial data."
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Comput."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472298"
                        ],
                        "name": "Chih-Chung Chang",
                        "slug": "Chih-Chung-Chang",
                        "structuredName": {
                            "firstName": "Chih-Chung",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Chung Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 146
                            }
                        ],
                        "text": "The most widely used algorithms for training binary SVMs are decomposition methods like SVM-light (Joachims, 1999), SMO (Platt, 1999), and others (Chang and Lin, 2001; Collobert and Bengio, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 146
                            }
                        ],
                        "text": "The most widely used algorithms for training binary SVMs are decomposition methods like SVMlight (Joachims, 1999), SMO (Platt, 1999), and others (Chang and Lin, 2001; Collobert and Bengio, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 961425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "273dfbcb68080251f5e9ff38b4413d7bd84b10a1",
            "isKey": false,
            "numCitedBy": 40077,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail."
            },
            "slug": "LIBSVM:-A-library-for-support-vector-machines-Chang-Lin",
            "title": {
                "fragments": [],
                "text": "LIBSVM: A library for support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail."
            },
            "venue": {
                "fragments": [],
                "text": "TIST"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13693897"
                        ],
                        "name": "Nathan D. Ratliff",
                        "slug": "Nathan-D.-Ratliff",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Ratliff",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan D. Ratliff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756566"
                        ],
                        "name": "J. Bagnell",
                        "slug": "J.-Bagnell",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Bagnell",
                            "middleNames": [
                                "Andrew"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bagnell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8195063"
                        ],
                        "name": "Martin A. Zinkevich",
                        "slug": "Martin-A.-Zinkevich",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Zinkevich",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin A. Zinkevich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 57
                            }
                        ],
                        "text": "Such algorithms either take stochastic subgradient steps (Collins 2002; Ratliff et al. 2007; Shalev-Shwartz et al. 2007), or build a cutting-plane model which is easy to solve directly (Tsochantaridis et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 61
                            }
                        ],
                        "text": "Recently, subgradient methods and their stochastic variants (Ratliff et al, 2007) have also been proposed to solve the optimization problem in maxmargin structured prediction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 73
                            }
                        ],
                        "text": "Such algorithms either take stochastic subgradi-\nent steps (Collins, 2002; Ratliff et al, 2007; Shalev-Shwartz et al, 2007), or build a cutting-plane model which is easy to solve directly (Tsochantaridis et al, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 60
                            }
                        ],
                        "text": "Recently, subgradient methods and their stochastic variants (Ratliff et al. 2007) have also been proposed to solve the optimization problem in max-margin structured prediction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5929174,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45372f73a0e40da428595597816ac4cae1469cec",
            "isKey": true,
            "numCitedBy": 177,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Promising approaches to structured learning problems have recently been developed in the maximum margin framework. Unfortunately, algorithms that are computationally and memory efficient enough to solve large scale problems have lagged behind. We propose using simple subgradient-based techniques for optimizing a regularized risk formulation of these problems in both online and batch settings, and analyze the theoretical convergence, generalization, and robustness properties of the resulting techniques. These algorithms are are simple, memory efficient, fast to converge, and have small regret in the online setting. We also investigate a novel convex regression formulation of structured learning. Finally, we demonstrate the benefits of the subgradient approach on three structured prediction problems."
            },
            "slug": "Online)-Subgradient-Methods-for-Structured-Ratliff-Bagnell",
            "title": {
                "fragments": [],
                "text": "Online) Subgradient Methods for Structured Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes using simple subgradient-based techniques for optimizing a regularized risk formulation of structured learning problems in both online and batch settings, and analyzes the theoretical convergence, generalization, and robustness properties of the resulting techniques."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5106399"
                        ],
                        "name": "M. Ferris",
                        "slug": "M.-Ferris",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Ferris",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ferris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1882062"
                        ],
                        "name": "T. Munson",
                        "slug": "T.-Munson",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Munson",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Munson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 174
                            }
                        ],
                        "text": ", Lagrangian SVM (Mangasarian and Musicant 2001) (using the \u2211 \u03be 2 i loss), Proximal SVM (Fung and Mangasarian 2001) (using an L2 regression loss), and Interior Point Methods (Ferris and Munson 2003)), they use the Sherman-Morrison-Woodbury formula (or matrix factorizations) for inverting the Hessian of the dual."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 233
                            }
                        ],
                        "text": "While there are training algorithms for linear SVMs that scale linearly with n (e.g., Lagrangian SVM (Mangasarian and Musicant, 2001) (using the \u2211\u03be 2i loss), Proximal SVM (Fung and Mangasarian, 2001) (using an L 2 regression loss), and Interior Point Methods (Ferris and Munson, 2003)), they use the Sherman-Morrison-Woodbury formula (or similar matrix factorizations) for inverting the Hessian of the dual."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 144
                            }
                        ],
                        "text": "\u2026and Musicant, 2001) (using the \u2211\u03be 2i loss), Proximal SVM (Fung and Mangasarian, 2001) (using an L 2 regression loss), and Interior Point Methods (Ferris and Munson, 2003)), they use the Sherman-Morrison-Woodbury formula (or similar matrix factorizations) for inverting the Hessian of the dual."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13563302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20f6ac9427d700e48a5025c9c43e5b6d20a2a79d",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the use of interior-point methods for solving quadratic programming problems with a small number of linear constraints, where the quadratic term consists of a low-rank update to a positive semidefinite matrix. Several formulations of the support vector machine fit into this category. An interesting feature of these particular problems is the volume of data, which can lead to quadratic programs with between 10 and 100 million variables and, if written explicitly, a dense Q matrix. Our code is based on OOQP, an object-oriented interior-point code, with the linear algebra specialized for the support vector machine application. For the targeted massive problems, all of the data is stored out of core and we overlap computation and input/output to reduce overhead. Results are reported for several linear support vector machine formulations demonstrating that the method is reliable and scalable."
            },
            "slug": "Interior-Point-Methods-for-Massive-Support-Vector-Ferris-Munson",
            "title": {
                "fragments": [],
                "text": "Interior-Point Methods for Massive Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work investigates the use of interior-point methods for solving quadratic programming problems with a small number of linear constraints, where the quadratics term consists of a low-rank update to a positive semidefinite matrix."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 130
                            }
                        ],
                        "text": "The argmax in Line 5 has an efficient solution for a wide variety of choices for\u03a8 , Y , and \u0394 (see e.g., Tsochantaridis et al,\n2005; Joachims, 2005; Yu et al, 2007; Yue et al, 2007), and often the algorithm for making predictions (see Eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 94
                            }
                        ],
                        "text": "The argmax in Line 5 has an efficient solution for a wide variety of choices for\u03a8 , Y , and \u2206 (see e.g., Tsochantaridis et al, 2005; Joachims, 2005; Yu et al, 2007; Yue et al, 2007)), and often the same algorithm can be used that is already needed for making a prediction (see Eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 152
                            }
                        ],
                        "text": "\u2026for problems as diverse as natural language parsing (Taskar et al, 2004), protein sequence alignment (Yu et al, 2007), supervised clustering (Finley and Joachims, 2005), learning ranking functions that optimize IR performance measures (Yue et al, 2007), and segmenting images (Anguelov et al, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1115550,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "175c1bb60ee46dac56d942ef8c7339977b4ebb0e",
            "isKey": false,
            "numCitedBy": 845,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a Support Vector Method for optimizing multivariate nonlinear performance measures like the F1-score. Taking a multivariate prediction approach, we give an algorithm with which such multivariate SVMs can be trained in polynomial time for large classes of potentially non-linear performance measures, in particular ROCArea and all measures that can be computed from the contingency table. The conventional classification SVM arises as a special case of our method."
            },
            "slug": "A-support-vector-method-for-multivariate-measures-Joachims",
            "title": {
                "fragments": [],
                "text": "A support vector method for multivariate performance measures"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An algorithm with which such multivariate SVMs can be trained in polynomial time for large classes of potentially non-linear performance measures, in particular ROCArea and all measures that can be computed from the contingency table are given."
            },
            "venue": {
                "fragments": [],
                "text": "K\u00fcnstliche Intell."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063206"
                        ],
                        "name": "Mary Ann Marcinkiewicz",
                        "slug": "Mary-Ann-Marcinkiewicz",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Marcinkiewicz",
                            "middleNames": [
                                "Ann"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mary Ann Marcinkiewicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 82
                            }
                        ],
                        "text": "3 We evaluate on the Part-of-Speech tagging dataset from the Penn Treebank corpus (Marcus et al. 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 106
                            }
                        ],
                        "text": "3 For the following experiments, we use all sentences with at most 15 words from the Penn Treebank corpus (Marcus et al. 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 105
                            }
                        ],
                        "text": "For the following experiments, we use all sentences with at most 15 words from the Penn Treebank corpus (Marcus et al, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 81
                            }
                        ],
                        "text": "We evaluate on the Part-of-Speech tagging dataset from the Penn Treebank corpus (Marcus et al, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 252796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "isKey": true,
            "numCitedBy": 8177,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant."
            },
            "slug": "Building-a-Large-Annotated-Corpus-of-English:-The-Marcus-Santorini",
            "title": {
                "fragments": [],
                "text": "Building a Large Annotated Corpus of English: The Penn Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "As a result of this grant, the researchers have now published on CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, which includes a fully hand-parsed version of the classic Brown corpus."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145713874"
                        ],
                        "name": "S. Vishwanathan",
                        "slug": "S.-Vishwanathan",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Vishwanathan",
                            "middleNames": [
                                "V.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vishwanathan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145610994"
                        ],
                        "name": "Mark W. Schmidt",
                        "slug": "Mark-W.-Schmidt",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Schmidt",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark W. Schmidt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1978101,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "950e8d556e30d2a873172bfa90f4f36da5286c07",
            "isKey": false,
            "numCitedBy": 358,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply Stochastic Meta-Descent (SMD), a stochastic gradient optimization method with gain vector adaptation, to the training of Conditional Random Fields (CRFs). On several large data sets, the resulting optimizer converges to the same quality of solution over an order of magnitude faster than limited-memory BFGS, the leading method reported to date. We report results for both exact and inexact inference techniques."
            },
            "slug": "Accelerated-training-of-conditional-random-fields-Vishwanathan-Schraudolph",
            "title": {
                "fragments": [],
                "text": "Accelerated training of conditional random fields with stochastic gradient methods"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "Stochastic Meta-Descent (SMD), a stochastic gradient optimization method with gain vector adaptation, is applied to the training of Conditional Random Fields (CRFs) and the resulting optimizer converges to the same quality of solution over an order of magnitude faster than limited-memory BFGS."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751569"
                        ],
                        "name": "Samy Bengio",
                        "slug": "Samy-Bengio",
                        "structuredName": {
                            "firstName": "Samy",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samy Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 146
                            }
                        ],
                        "text": "The most widely used algorithms for training binary SVMs are decomposition methods like SVM-light (Joachims, 1999), SMO (Platt, 1999), and others (Chang and Lin, 2001; Collobert and Bengio, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 167
                            }
                        ],
                        "text": "The most widely used algorithms for training binary SVMs are decomposition methods like SVMlight (Joachims, 1999), SMO (Platt, 1999), and others (Chang and Lin, 2001; Collobert and Bengio, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8952183,
            "fieldsOfStudy": [
                "Economics",
                "Education"
            ],
            "id": "7141ea996fc449807b14c071716cecac0999f4ce",
            "isKey": false,
            "numCitedBy": 985,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Keywords: learning Reference EPFL-REPORT-82604 URL: http://publications.idiap.ch/downloads/reports/2000/rr00-17.pdf Record created on 2006-03-10, modified on 2017-05-10"
            },
            "slug": "SVMTorch:-Support-Vector-Machines-for-Large-Scale-Collobert-Bengio",
            "title": {
                "fragments": [],
                "text": "SVMTorch: Support Vector Machines for Large-Scale Regression Problems"
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 53
                            }
                        ],
                        "text": "We follow the Empirical Risk Minimization Principle (Vapnik, 1998) to infer a function h from the training sample S."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 223
                            }
                        ],
                        "text": "In particular, for binary classification X = \u211cN and Y = {\u22121,+1}, and plugging\n\u03a8(x,y) = 1 2\nyx and \u0394(y,y\u2032) = {\n0 if y = y\u2032 1 otherwise\n(28)\ninto either n-slack formulation OP2 or OP3 produces the standard SVM optimization problem OP1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 151
                            }
                        ],
                        "text": "For conventional binary classification where Y = {\u22121,+1}, SVM training is typically formulated as the following convex quadratic optimization problem2 (Cortes and Vapnik 1995; Vapnik 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 177
                            }
                        ],
                        "text": "For conventional binary classification where Y = {\u22121,+1}, SVM training is typically formulated as the following convex quadratic optimization problem 2 (Cortes and Vapnik, 1995; Vapnik, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 52
                            }
                        ],
                        "text": "We follow the Empirical Risk Minimization Principle (Vapnik 1998) to infer a function h from the training sample S."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 141
                            }
                        ],
                        "text": "In the n-slack formulation, one immediately recovers Vapnik et al.\u2019s original classification SVM formulation of OP1 (Cortes and Vapnik, 1995; Vapnik, 1998) (up to the more convenient percentagescale rescaling of the loss function and the absence of the bias term), which we solve using SVMlight ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 50
                            }
                        ],
                        "text": "\u2019s original classification SVM formulation of OP1 (Cortes and Vapnik 1995; Vapnik 1998) (up to the more convenient percentage-scale rescaling of the loss function and the absence of the bias term), which we solve using SVM-light."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": true,
            "numCitedBy": 26320,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 163
                            }
                        ],
                        "text": "For conventional binary classification where Y = {\u22121,+1}, SVM training is typically formulated as the following convex quadratic optimization problem 2 (Cortes and Vapnik, 1995; Vapnik, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 127
                            }
                        ],
                        "text": "In the n-slack formulation, one immediately recovers Vapnik et al.\u2019s original classification SVM formulation of OP1 (Cortes and Vapnik, 1995; Vapnik, 1998) (up to the more convenient percentagescale rescaling of the loss function and the absence of the bias term), which we solve using SVMlight ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38755,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 38
                            }
                        ],
                        "text": "Unlike exponentiated gradient methods (Bartlett et al. 2004; Globerson et al. 2007), PEGASOS does not require the computation of marginals, which makes it equally easy to apply as cutting-plane methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 164
                            }
                        ],
                        "text": "\u2026et al, 2005), which applies only to problems where subgradients of the QP can be computed via a convex real relaxation, as well as exponentiated gradient methods (Bartlett et al, 2004; Globerson et al, 2007), which require the ability to compute \u201cmarginals\u201d (e.g. via the sum-product algorithm)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 155
                            }
                        ],
                        "text": "2005), which applies only to problems where subgradients of the QP can be computed via a convex real relaxation, as well as exponentiated gradient methods (Bartlett et al. 2004; Globerson et al. 2007), which require the ability to compute \u201cmarginals\u201d (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 39
                            }
                        ],
                        "text": "Unlike exponentiated gradient methods (Bartlett et al, 2004; Globerson et al, 2007), PEGASOS does not require the computation of marginals, which makes it equally easy to apply as cutting-plane methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 210
                            }
                        ],
                        "text": "Exponentiated gradient methods, originally proposed for online learning of linear predictors (Kivinen and Warmuth, 1997), have also been applied to the training of structured predictors (Globerson et al, 2007; Bartlett et al, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 185
                            }
                        ],
                        "text": "Exponentiated gradient methods, originally proposed for online learning of linear predictors (Kivinen and Warmuth 1997), have also been applied to the training of structured predictors (Globerson et al. 2007; Bartlett et al. 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exponentiated algorithms for large-margin structured classification. In Advances in neural information processing systems (NIPS) (pp. 305\u2013312)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 207
                            }
                        ],
                        "text": "\u2026this provides greater modeling flexibility through avoidance of independence assumptions, and it was shown to provide substantially improved prediction accuracy in many domains (e.g., Lafferty et al, 2001; Taskar et al, 2003; Tsochantaridis et al, 2004; Taskar et al, 2004; Yu et al, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 173
                            }
                        ],
                        "text": "For training the weights w of the linear discriminant function, the standard SVM optimization problem can be generalized in several ways (Altun et al, 2003; Joachims, 2003; Taskar et al, 2003; Tsochantaridis et al, 2004, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 157
                            }
                        ],
                        "text": "These smaller QPs can then be solved, for example, with general-purpose optimization methods (Anguelov et al, 2005) or decomposition methods similar to SMO (Taskar et al, 2003; Platt, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 106
                            }
                        ],
                        "text": "The first group of algorithms relies on an elegant polynomial-size reformulation of the training problem (Taskar et al, 2003; Anguelov et al, 2005), which is possible for the special case of margin-rescaling (Tsochantaridis et al, 2005) with linearly decomposable loss."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 250
                            }
                        ],
                        "text": "\u2026provided intriguing advances in extending methods like Logistic Regression, Perceptrons, and Support Vector Machines (SVMs) to global training of such structured prediction models (e.g., Lafferty et al, 2001; Collins, 2004; Collins and Duffy, 2002; Taskar et al, 2003; Tsochantaridis et al, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 148
                            }
                        ],
                        "text": "\u20261 2 wT w + C n\nn\n\u2211 i=1 \u03bei\ns.t. \u2200i \u2208 {1, ...,n} : yi(wT xi)\u2265 1\u2212 \u03bei\nIt was shown that SVM training can be generalized to structured outputs (Altun et al, 2003; Taskar et al, 2003; Tsochantaridis et al, 2004), leading to an optimiza-\n1 Note, however, that all formal results in this paper also hold\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "Empirically, the new algorithm is substantially faster than existing methods, in particular decomposition methods like SMO and SVM light , and it includes the training algorithm of Joachims (2006) for linear binary classification SVMs as a special case."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 115
                            }
                        ],
                        "text": "The most widely used algorithms for training binary SVMs are decomposition methods like SVMlight (Joachims, 1999), SMO (Platt, 1999), and others (Chang and Lin, 2001; Collobert and Bengio, 2001)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 100
                            }
                        ],
                        "text": "For the special case of margin-rescaling with linearly decomposable loss functions \u0394 , Taskar et al. (Taskar et al, 2003) have shown that the problem can be reformulated as a quadratic program with only a polynomial number of constraints and variables."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "In the case of binary classification, their SMO algorithm reduces to a variant of the traditional SMO algorithm, which can be seen as a special case of the SVMlight algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Taskar et al (2003) extended the SMO algorithm to structured prediction problems based on their polynomial-size reformulation of the n-slack optimization problem OP2 for the special case of decomposable models and decomposable loss functions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum-margin markov networks. In: Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 210
                            }
                        ],
                        "text": "Exponentiated gradient methods, originally proposed for online learning of linear predictors (Kivinen and Warmuth, 1997), have also been applied to the training of structured predictors (Globerson et al, 2007; Bartlett et al, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 164
                            }
                        ],
                        "text": "\u2026et al, 2005), which applies only to problems where subgradients of the QP can be computed via a convex real relaxation, as well as exponentiated gradient methods (Bartlett et al, 2004; Globerson et al, 2007), which require the ability to compute \u201cmarginals\u201d (e.g. via the sum-product algorithm)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 39
                            }
                        ],
                        "text": "Unlike exponentiated gradient methods (Bartlett et al, 2004; Globerson et al, 2007), PEGASOS does not require the computation of marginals, which makes it equally easy to apply as cutting-plane methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 179
                            }
                        ],
                        "text": "\u0394SR(y,hw(x)) = max y\u0304\u2208Y {\u0394(y, y\u0304)(1\u2212wT\u03a8(x,y)+ wT\u03a8(x, y\u0304))} \u2265 \u0394(y,hw(x)) (2)\nThis leads to the following two training problems, where each slack variable \u03be i is equal to the respective \u0394MR(yi,hw(xi)) or \u0394SR(yi,hw(xi)) for training example (xi,yi)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exponentiated algorithms for large-margin structured classification Kddcup 2004: Results and analysis"
            },
            "venue": {
                "fragments": [],
                "text": "ACM SIGKDD Newsletter"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 210
                            }
                        ],
                        "text": "Exponentiated gradient methods, originally proposed for online learning of linear predictors (Kivinen and Warmuth, 1997), have also been applied to the training of structured predictors (Globerson et al, 2007; Bartlett et al, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 164
                            }
                        ],
                        "text": "\u2026et al, 2005), which applies only to problems where subgradients of the QP can be computed via a convex real relaxation, as well as exponentiated gradient methods (Bartlett et al, 2004; Globerson et al, 2007), which require the ability to compute \u201cmarginals\u201d (e.g. via the sum-product algorithm)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 39
                            }
                        ],
                        "text": "Unlike exponentiated gradient methods (Bartlett et al, 2004; Globerson et al, 2007), PEGASOS does not require the computation of marginals, which makes it equally easy to apply as cutting-plane methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 179
                            }
                        ],
                        "text": "\u0394SR(y,hw(x)) = max y\u0304\u2208Y {\u0394(y, y\u0304)(1\u2212wT\u03a8(x,y)+ wT\u03a8(x, y\u0304))} \u2265 \u0394(y,hw(x)) (2)\nThis leads to the following two training problems, where each slack variable \u03be i is equal to the respective \u0394MR(yi,hw(xi)) or \u0394SR(yi,hw(xi)) for training example (xi,yi)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exponentiated algorithms for large-margin structured classification Kddcup 2004: Results and analysis"
            },
            "venue": {
                "fragments": [],
                "text": "ACM SIGKDD Newsletter"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 276
                            }
                        ],
                        "text": "\u2026for problems as diverse as natural language parsing (Taskar et al, 2004), protein sequence alignment (Yu et al, 2007), supervised clustering (Finley and Joachims, 2005), learning ranking functions that optimize IR performance measures (Yue et al, 2007), and segmenting images (Anguelov et al, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 126
                            }
                        ],
                        "text": "The first group of algorithms relies on an elegant polynomial-size reformulation of the training problem (Taskar et al, 2003; Anguelov et al, 2005), which is possible for the special case of margin-rescaling (Tsochantaridis et al, 2005) with linearly decomposable loss."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 94
                            }
                        ],
                        "text": "These smaller QPs can then be solved, for example, with general-purpose optimization methods (Anguelov et al, 2005) or decomposition methods similar to SMO (Taskar et al, 2003; Platt, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 50
                            }
                        ],
                        "text": "Related to Algorithm 1 is the method proposed in (Anguelov et al, 2005), which applies to the special case where the argmax in Line 5 can be computed as a linear program."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discrim - inative learning of Markov random fields for segmentation of 3 D scan data"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE conference on computer vision and pattern recognition ( CVPR )"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41680909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ded923a192ffbf13e4466c6b7d2ede55724b716",
            "isKey": false,
            "numCitedBy": 726,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sparse-Greedy-Matrix-Approximation-for-Machine-Smola-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Sparse Greedy Matrix Approximation for Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum - margin markov networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 255
                            }
                        ],
                        "text": "\u2026this provides greater modeling flexibility through avoidance of independence assumptions, and it was shown to provide substantially improved prediction accuracy in many domains (e.g., Lafferty et al, 2001; Taskar et al, 2003; Tsochantaridis et al, 2004; Taskar et al, 2004; Yu et al, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 115
                            }
                        ],
                        "text": "The flexibility in designing \u03a8 allows employing structural SVMs for problems as diverse as natural language parsing (Taskar et al, 2004), protein sequence alignment (Yu et al, 2007), supervised clustering (Finley and Joachims, 2005), learning ranking functions that optimize IR performance measures\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Max-margin parsing. In: Empirical Methods in Natural Language Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 145
                            }
                        ],
                        "text": "\u2026algorithms for linear SVMs that scale linearly with n (e.g., Lagrangian SVM (Mangasarian and Musicant, 2001) (using the \u2211\u03be 2i loss), Proximal SVM (Fung and Mangasarian, 2001) (using an L 2 regression loss), and Interior Point Methods (Ferris and Munson, 2003)), they use the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 88
                            }
                        ],
                        "text": ", Lagrangian SVM (Mangasarian and Musicant, 2001) (using the \u2211\u03be 2 i loss), Proximal SVM (Fung and Mangasarian, 2001) (using an L 2 regression loss), and Interior Point Methods (Ferris and Munson, 2003)), they use the Sherman-Morrison-Woodbury formula (or similar matrix factorizations) for inverting the Hessian of the dual."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 155
                            }
                        ],
                        "text": "While there are training algorithms for linear SVMs that scale linearly with n (e.g., Lagrangian SVM (Mangasarian and Musicant, 2001) (using the \u2211\u03be 2i loss), Proximal SVM (Fung and Mangasarian, 2001) (using an L 2 regression loss), and Interior Point Methods (Ferris and Munson, 2003)), they use the Sherman-Morrison-Woodbury formula (or similar matrix factorizations) for inverting the Hessian of the dual."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proximal support vector classifiers"
            },
            "venue": {
                "fragments": [],
                "text": "ACM Conference on Knowledge Discovery and Data Mining (KDD),"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "KDDCup 2004 : Results and analysis"
            },
            "venue": {
                "fragments": [],
                "text": "ACM SIGKDD Newsletter"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Machine Learning Journal"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning Journal"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exponentiated algorithms for large - margin structured classification"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in neural information processing systems ( NIPS )"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 145
                            }
                        ],
                        "text": "\u2026algorithms for linear SVMs that scale linearly with n (e.g., Lagrangian SVM (Mangasarian and Musicant, 2001) (using the \u2211\u03be 2i loss), Proximal SVM (Fung and Mangasarian, 2001) (using an L 2 regression loss), and Interior Point Methods (Ferris and Munson, 2003)), they use the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proximal support vector classifiers Exponentiated gradient algorithm for loglinear structured prediction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 145
                            }
                        ],
                        "text": "\u2026algorithms for linear SVMs that scale linearly with n (e.g., Lagrangian SVM (Mangasarian and Musicant, 2001) (using the \u2211\u03be 2i loss), Proximal SVM (Fung and Mangasarian, 2001) (using an L 2 regression loss), and Interior Point Methods (Ferris and Munson, 2003)), they use the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 155
                            }
                        ],
                        "text": "While there are training algorithms for linear SVMs that scale linearly with n (e.g., Lagrangian SVM (Mangasarian and Musicant, 2001) (using the \u2211\u03be 2i loss), Proximal SVM (Fung and Mangasarian, 2001) (using an L 2 regression loss), and Interior Point Methods (Ferris and Munson, 2003)), they use the Sherman-Morrison-Woodbury formula (or similar matrix factorizations) for inverting the Hessian of the dual."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 88
                            }
                        ],
                        "text": ", Lagrangian SVM (Mangasarian and Musicant, 2001) (using the \u2211\u03be 2 i loss), Proximal SVM (Fung and Mangasarian, 2001) (using an L 2 regression loss), and Interior Point Methods (Ferris and Munson, 2003)), they use the Sherman-Morrison-Woodbury formula (or matrix factorizations) for inverting the Hessian of the dual."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proximal support vector classifiers. In: ACM SIGKDD International Conference On Knowledge Discovery and Data Mining (KDD"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 185
                            }
                        ],
                        "text": "\u2026this provides greater modeling flexibility through avoidance of independence assumptions, and it was shown to provide substantially improved prediction accuracy in many domains (e.g., Lafferty et al, 2001; Taskar et al, 2003; Tsochantaridis et al, 2004; Taskar et al, 2004; Yu et al, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 188
                            }
                        ],
                        "text": "\u2026provided intriguing advances in extending methods like Logistic Regression, Perceptrons, and Support Vector Machines (SVMs) to global training of such structured prediction models (e.g., Lafferty et al, 2001; Collins, 2004; Collins and Duffy, 2002; Taskar et al, 2003; Tsochantaridis et al, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data Rcv1: A new benchmark collection for text categorization research Lagrangian support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Machine Learning"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "KDD-Cup 2004"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 143
                            }
                        ],
                        "text": "\u2026for linear SVMs that scale linearly with n (e.g., Lagrangian SVM (Mangasarian and Musicant, 2001) (using the \u2211\u03be 2i loss), Proximal SVM (Fung and Mangasarian, 2001) (using an L 2 regression loss), and Interior Point Methods (Ferris and Munson, 2003)), they use the Sherman-Morrison-Woodbury\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proximal support vector classifiers"
            },
            "venue": {
                "fragments": [],
                "text": "ACM conference on knowledge discovery and data mining ( KDD )"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 145
                            }
                        ],
                        "text": "\u2026algorithms for linear SVMs that scale linearly with n (e.g., Lagrangian SVM (Mangasarian and Musicant, 2001) (using the \u2211\u03be 2i loss), Proximal SVM (Fung and Mangasarian, 2001) (using an L 2 regression loss), and Interior Point Methods (Ferris and Munson, 2003)), they use the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 88
                            }
                        ],
                        "text": ", Lagrangian SVM (Mangasarian and Musicant 2001) (using the \u2211 \u03be 2 i loss), Proximal SVM (Fung and Mangasarian 2001) (using an L2 regression loss), and Interior Point Methods (Ferris and Munson 2003)), they use the Sherman-Morrison-Woodbury formula (or matrix factorizations) for inverting the Hessian of the dual."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 155
                            }
                        ],
                        "text": "While there are training algorithms for linear SVMs that scale linearly with n (e.g., Lagrangian SVM (Mangasarian and Musicant, 2001) (using the \u2211\u03be 2i loss), Proximal SVM (Fung and Mangasarian, 2001) (using an L 2 regression loss), and Interior Point Methods (Ferris and Munson, 2003)), they use the Sherman-Morrison-Woodbury formula (or similar matrix factorizations) for inverting the Hessian of the dual."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proximal support vector classifiers. In ACM conference on knowledge discovery and data mining (KDD) (pp. 77\u201386)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 255
                            }
                        ],
                        "text": "\u2026this provides greater modeling flexibility through avoidance of independence assumptions, and it was shown to provide substantially improved prediction accuracy in many domains (e.g., Lafferty et al, 2001; Taskar et al, 2003; Tsochantaridis et al, 2004; Taskar et al, 2004; Yu et al, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 115
                            }
                        ],
                        "text": "The flexibility in designing \u03a8 allows employing structural SVMs for problems as diverse as natural language parsing (Taskar et al, 2004), protein sequence alignment (Yu et al, 2007), supervised clustering (Finley and Joachims, 2005), learning ranking functions that optimize IR performance measures\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Max-margin parsing. In Empirical methods in natural language processing (EMNLP)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 27,
            "methodology": 44
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 66,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Cutting-plane-training-of-structural-SVMs-Joachims-Finley/f30aba767d71c1db5ea70b041d9fcc2b9b1ddad4?sort=total-citations"
}