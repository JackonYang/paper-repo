{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1944614"
                        ],
                        "name": "Hannes Schulz",
                        "slug": "Hannes-Schulz",
                        "structuredName": {
                            "firstName": "Hannes",
                            "lastName": "Schulz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hannes Schulz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699019"
                        ],
                        "name": "Sven Behnke",
                        "slug": "Sven-Behnke",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Behnke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sven Behnke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "Perhaps the closest approach to ours is [18] which has similar high level objective but use much smaller network with a different features, loss function and without a machinery to distinguish between multiple instances of the same class."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10635979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "648a65e31a56f54dd906d40872b1a8ac78309b0b",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "After successes at image classification, segmentation is the next step towards image understanding for neural networks. We propose a convolutional network architecture that outperforms current methods on the challenging INRIA-Graz02 dataset with regards to accuracy and speed."
            },
            "slug": "Object-Class-Segmentation-using-Deep-Convolutional-Schulz-Behnke",
            "title": {
                "fragments": [],
                "text": "Object-Class Segmentation using Deep Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "A convolutional network architecture is proposed that outperforms current methods on the challenging INRIA-Graz02 dataset with regards to accuracy and speed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "Contrary to the widely cited DPM approach by [9], DetectorNet excels at deformable objects such as bird, cat, sheep, dog."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 104
                            }
                        ],
                        "text": "One of the most heavily studied paradigms for object detection is the deformable part-based model, with [9] being the most prominent example."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 100
                            }
                        ],
                        "text": "A prominent example of a state-of-the-art detection system is the Deformable Part-based Model (DPM) [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "Finally, we compare against the DPM by [9] and [11]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3198903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e79272fe3d65197100eae8be9fec6469107969ae",
            "isKey": true,
            "numCitedBy": 9374,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function."
            },
            "slug": "Object-Detection-with-Discriminatively-Trained-Part-Felzenszwalb-Girshick",
            "title": {
                "fragments": [],
                "text": "Object Detection with Discriminatively Trained Part Based Models"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An object detection system based on mixtures of multiscale deformable part models that is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 92
                            }
                        ],
                        "text": "This is presented together with state-of-the-art detection results on Pascal VOC challenge [7] in Sec."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4246903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82635fb63640ae95f90ee9bdc07832eb461ca881",
            "isKey": false,
            "numCitedBy": 11688,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension."
            },
            "slug": "The-Pascal-Visual-Object-Classes-(VOC)-Challenge-Everingham-Gool",
            "title": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The state-of-the-art in evaluated methods for both classification and detection are reviewed, whether the methods are statistically different, what they are learning from the images, and what the methods find easy or confuse."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2010660"
                        ],
                        "name": "Iasonas Kokkinos",
                        "slug": "Iasonas-Kokkinos",
                        "structuredName": {
                            "firstName": "Iasonas",
                            "lastName": "Kokkinos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iasonas Kokkinos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "Further examples of compositional models for detection are based on segments as primitives [1], focus on shape [13], use Gabor filters [10] or larger HOG filters [19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15200124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d777a71d36ec929f70c2f7b5eca47456c34a4e0",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we introduce a hierarchical representation for object detection. We represent an object in terms of parts composed of contours corresponding to object boundaries and symmetry axes; these are in turn related to edge and ridge features that are extracted from the image.We propose a coarse-to-fine algorithm for efficient detection which exploits the hierarchical nature of the model. This provides a tractable framework to combine bottom-up and top-down computation. We learn our models from training images where only the bounding box of the object is provided. We automate the decomposition of an object category into parts and contours, and discriminatively learn the cost function that drives the matching of the object to the image using Multiple Instance Learning.Using shape-based information, we obtain state-of-the-art localization results on the UIUC and ETHZ datasets."
            },
            "slug": "Inference-and-Learning-with-Hierarchical-Shape-Kokkinos-Yuille",
            "title": {
                "fragments": [],
                "text": "Inference and Learning with Hierarchical Shape Models"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work automates the decomposition of an object category into parts and contours, and discriminatively learn the cost function that drives the matching of the object to the image using Multiple Instance Learning."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089272"
                        ],
                        "name": "R. Monga",
                        "slug": "R.-Monga",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Monga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Monga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145139947"
                        ],
                        "name": "Matthieu Devin",
                        "slug": "Matthieu-Devin",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Devin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthieu Devin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118440152"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "This has been empirically demonstrated on the challenging ImageNet classification task [5] across thousands of classes [14, 15]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 120
                            }
                        ],
                        "text": "It was not until recently than these models emerged as highly successful on large-scale image classification tasks [14, 15] in the form of DNNs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206741597,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72e93aa6767ee683de7f001fa72f1314e40a8f35",
            "isKey": false,
            "numCitedBy": 2100,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a deep sparse autoencoder on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200\u00d7200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting from these learned features, we trained our network to recognize 22,000 object categories from ImageNet and achieve a leap of 70% relative improvement over the previous state-of-the-art."
            },
            "slug": "Building-high-level-features-using-large-scale-Le-Ranzato",
            "title": {
                "fragments": [],
                "text": "Building high-level features using large scale unsupervised learning"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Contrary to what appears to be a widely-held intuition, the experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256269"
                        ],
                        "name": "C. Farabet",
                        "slug": "C.-Farabet",
                        "structuredName": {
                            "firstName": "Cl\u00e9ment",
                            "lastName": "Farabet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Farabet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2341378"
                        ],
                        "name": "C. Couprie",
                        "slug": "C.-Couprie",
                        "structuredName": {
                            "firstName": "Camille",
                            "lastName": "Couprie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Couprie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688714"
                        ],
                        "name": "Laurent Najman",
                        "slug": "Laurent-Najman",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Najman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laurent Najman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 109
                            }
                        ],
                        "text": "Scene parsing, as a more detailed form of detection, has been attempted using multi-layer Convolutional NNs [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206765110,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "237a04dd8291cbdb59b6dc4b53e689af743fe2a3",
            "isKey": false,
            "numCitedBy": 2404,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320\u00d7240 image labeling in less than a second, including feature extraction."
            },
            "slug": "Learning-Hierarchical-Features-for-Scene-Labeling-Farabet-Couprie",
            "title": {
                "fragments": [],
                "text": "Learning Hierarchical Features for Scene Labeling"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel, alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49835695"
                        ],
                        "name": "Long Zhu",
                        "slug": "Long-Zhu",
                        "structuredName": {
                            "firstName": "Long",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Long Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144034946"
                        ],
                        "name": "Yuanhao Chen",
                        "slug": "Yuanhao-Chen",
                        "structuredName": {
                            "firstName": "Yuanhao",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuanhao Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 162
                            }
                        ],
                        "text": "Further examples of compositional models for detection are based on segments as primitives [1], focus on shape [13], use Gabor filters [10] or larger HOG filters [19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6960041,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "120081166fc0d780c84e198622d638152a7cdf3e",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a latent hierarchical structural learning method for object detection. An object is represented by a mixture of hierarchical tree models where the nodes represent object parts. The nodes can move spatially to allow both local and global shape deformations. The models can be trained discriminatively using latent structural SVM learning, where the latent variables are the node positions and the mixture component. But current learning methods are slow, due to the large number of parameters and latent variables, and have been restricted to hierarchies with two layers. In this paper we describe an incremental concave-convex procedure (iCCCP) which allows us to learn both two and three layer models efficiently. We show that iCCCP leads to a simple training algorithm which avoids complex multi-stage layer-wise training, careful part selection, and achieves good performance without requiring elaborate initialization. We perform object detection using our learnt models and obtain performance comparable with state-of-the-art methods when evaluated on challenging public PASCAL datasets. We demonstrate the advantages of three layer hierarchies - outperforming Felzenszwalb et al.'s two layer models on all 20 classes."
            },
            "slug": "Latent-hierarchical-structural-learning-for-object-Zhu-Chen",
            "title": {
                "fragments": [],
                "text": "Latent hierarchical structural learning for object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper describes an incremental concave-convex procedure (iCCCP) which allows us to learn both two and three layer models efficiently and demonstrates the advantages of three layer hierarchies - outperforming Felzenszwalb et al.'s two layer models on all 20 classes."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 170
                            }
                        ],
                        "text": "Contrary to DNNs, whose layers are generic, the work by [9] exploits domain knowledge \u2013 the parts are based on manually designed Histogram of Gradients (HOG) descriptors [4] and the structure of the parts is kinematically motivated."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206590483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cec734d7097ab6b1e60d95228ffd64248eb89d66",
            "isKey": false,
            "numCitedBy": 29262,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds."
            },
            "slug": "Histograms-of-oriented-gradients-for-human-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Histograms of oriented gradients for human detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection, and the influence of each stage of the computation on performance is studied."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "First, they are deep architectures which have the capacity to learn more complex models than shallow ones [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207178999,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e60ff004dde5c13ec53087872cfcdd12e85beb57",
            "isKey": false,
            "numCitedBy": 7558,
            "numCiting": 345,
            "paperAbstract": {
                "fragments": [],
                "text": "Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult optimization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks."
            },
            "slug": "Learning-Deep-Architectures-for-AI-Bengio",
            "title": {
                "fragments": [],
                "text": "Learning Deep Architectures for AI"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer modelssuch as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Found. Trends Mach. Learn."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37895334"
                        ],
                        "name": "S. Fidler",
                        "slug": "S.-Fidler",
                        "structuredName": {
                            "firstName": "Sanja",
                            "lastName": "Fidler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fidler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732672"
                        ],
                        "name": "A. Leonardis",
                        "slug": "A.-Leonardis",
                        "structuredName": {
                            "firstName": "Ale\u0161",
                            "lastName": "Leonardis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Leonardis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": "Further examples of compositional models for detection are based on segments as primitives [1], focus on shape [13], use Gabor filters [10] or larger HOG filters [19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5934579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f602c33b1762d57223c9f9656579f9d1dc2e30a",
            "isKey": false,
            "numCitedBy": 215,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a novel approach to constructing a hierarchical representation of visual input that aims to enable recognition and detection of a large number of object categories. Inspired by the principles of efficient indexing (bottom-up,), robust matching (top-down,), and ideas of compositionality, our approach learns a hierarchy of spatially flexible compositions, i.e. parts, in an unsupervised, statistics-driven manner. Starting with simple, frequent features, we learn the statistically most significant compositions (parts composed of parts), which consequently define the next layer. Parts are learned sequentially, layer after layer, optimally adjusting to the visual data. Lower layers are learned in a category-independent way to obtain complex, yet sharable visual building blocks, which is a crucial step towards a scalable representation. Higher layers of the hierarchy, on the other hand, are constructed by using specific categories, achieving a category representation with a small number of highly generalizable parts that gained their structural flexibility through composition within the hierarchy. Built in this way, new categories can be efficiently and continuously added to the system by adding a small number of parts only in the higher layers. The approach is demonstrated on a large collection of images and a variety of object categories. Detection results confirm the effectiveness and robustness of the learned parts."
            },
            "slug": "Towards-Scalable-Representations-of-Object-Learning-Fidler-Leonardis",
            "title": {
                "fragments": [],
                "text": "Towards Scalable Representations of Object Categories: Learning a Hierarchy of Parts"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This paper proposes a novel approach to constructing a hierarchical representation of visual input that aims to enable recognition and detection of a large number of object categories by learning a hierarchy of spatially flexible compositions in an unsupervised, statistics-driven manner."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895356"
                        ],
                        "name": "D. Ciresan",
                        "slug": "D.-Ciresan",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Ciresan",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ciresan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33354551"
                        ],
                        "name": "A. Giusti",
                        "slug": "A.-Giusti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Giusti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Giusti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6803671"
                        ],
                        "name": "L. Gambardella",
                        "slug": "L.-Gambardella",
                        "structuredName": {
                            "firstName": "Luca",
                            "lastName": "Gambardella",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gambardella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "Segmentation of medical imagery has been addressed using DNNs [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7725346,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09193e19b59fc8f05bee9d6efbfb1607ca5b6501",
            "isKey": false,
            "numCitedBy": 1223,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity. To segment biological neuron membranes, we use a special type of deep artificial neural network as a pixel classifier. The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classifier is trained by plain gradient descent on a 512 \u00d7 512 \u00d7 30 stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge. Even without problem-specific postprocessing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. rand error, warping error and pixel error. For pixel error, our approach is the only one outperforming a second human observer."
            },
            "slug": "Deep-Neural-Networks-Segment-Neuronal-Membranes-in-Ciresan-Giusti",
            "title": {
                "fragments": [],
                "text": "Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This work addresses a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy images, using a special type of deep artificial neural network as a pixel classifier to segment biological neuron membranes."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237406"
                        ],
                        "name": "N. Ahuja",
                        "slug": "N.-Ahuja",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Ahuja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ahuja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143856428"
                        ],
                        "name": "S. Todorovic",
                        "slug": "S.-Todorovic",
                        "structuredName": {
                            "firstName": "Sinisa",
                            "lastName": "Todorovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Todorovic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 91
                            }
                        ],
                        "text": "Further examples of compositional models for detection are based on segments as primitives [1], focus on shape [13], use Gabor filters [10] or larger HOG filters [19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13240424,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76e5cfe00b96da896107703629077152f315ca18",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes, and presents a solution to, the problem of simultaneous learning of multiple visual categories present in an arbitrary image set and their inter-category relationships. These relationships, also called their taxonomy, allow categories to be defined recursively, as spatial configurations of (simpler) subcategories each of which may be shared by many categories. Each image is represented by a segmentation tree, whose structure captures recursive embedding of image regions in a multiscale segmentation, and whose nodes contain the associated region properties. The presence of any occurring categories is reflected in the occurrence of associated, similar subtrees within the image trees. Similar subtrees across the entire image set are clustered. Each cluster corresponds to a discovered category, represented by the cluster properties. A (subcategory) cluster of small matching subtrees may occur within multiple clusters (categories) of larger matching subtrees, in different spatial relationships with subtrees from other small clusters. Such recursive embedding, grouping and intersection of clusters is captured in a directed acyclic graph (DAG) which represents the discovered taxonomy. Detection, recognition and segmentation of any of the learned categories present in a new image are simultaneously conducted by matching the segmentation tree of the new image with the learned DAG. This matching also yields a semantic explanation of the recognized category, in terms of the presence of its subcategories. Experiments with a newly compiled dataset of four-legged animals demonstrate good cross-category resolvability."
            },
            "slug": "Learning-the-Taxonomy-and-Models-of-Categories-in-Ahuja-Todorovic",
            "title": {
                "fragments": [],
                "text": "Learning the Taxonomy and Models of Categories Present in Arbitrary Images"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "This paper proposes, and presents a solution to, the problem of simultaneous learning of multiple visual categories present in an arbitrary image set and their inter-category relationships, and proposes a directed acyclic graph (DAG) which represents the discovered taxonomy."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143995438"
                        ],
                        "name": "Jorge S\u00e1nchez",
                        "slug": "Jorge-S\u00e1nchez",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "S\u00e1nchez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jorge S\u00e1nchez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723883"
                        ],
                        "name": "F. Perronnin",
                        "slug": "F.-Perronnin",
                        "structuredName": {
                            "firstName": "Florent",
                            "lastName": "Perronnin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Perronnin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 199
                            }
                        ],
                        "text": "Manually engineered representations in conjunction with shallow discriminatively trained models have been among the best performing paradigms for the related problem of object classification as well [17]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16199577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eefcc7bcc05436dac9881acb4ff4e4a0b730e175",
            "isKey": false,
            "numCitedBy": 298,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We address image classification on a large-scale, i.e. when a large number of images and classes are involved. First, we study classification accuracy as a function of the image signature dimensionality and the training set size. We show experimentally that the larger the training set, the higher the impact of the dimensionality on the accuracy. In other words, high-dimensional signatures are important to obtain state-of-the-art results on large datasets. Second, we tackle the problem of data compression on very large signatures (on the order of 105 dimensions) using two lossy compression strategies: a dimensionality reduction technique known as the hash kernel and an encoding technique based on product quantizers. We explain how the gain in storage can be traded against a loss in accuracy and/or an increase in CPU cost. We report results on two large databases \u2014 ImageNet and a dataset of lM Flickr images \u2014 showing that we can reduce the storage of our signatures by a factor 64 to 128 with little loss in accuracy. Integrating the decompression in the classifier learning yields an efficient and scalable training algorithm. On ILSVRC2010 we report a 74.3% accuracy at top-5, which corresponds to a 2.5% absolute improvement with respect to the state-of-the-art. On a subset of 10K classes of ImageNet we report a top-1 accuracy of 16.7%, a relative improvement of 160% with respect to the state-of-the-art."
            },
            "slug": "High-dimensional-signature-compression-for-image-S\u00e1nchez-Perronnin",
            "title": {
                "fragments": [],
                "text": "High-dimensional signature compression for large-scale image classification"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work reports results on two large databases \u2014 ImageNet and a dataset of lM Flickr images \u2014 showing that it can reduce the storage of the authors' signatures by a factor 64 to 128 with little loss in accuracy and integrating the decompression in the classifier learning yields an efficient and scalable training algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "In the last years, however, Deep Neural Networks (DNNs) [12] have emerged as a powerful machine learning model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1658773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e",
            "isKey": false,
            "numCitedBy": 14640,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \u201cautoencoder\u201d networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "slug": "Reducing-the-Dimensionality-of-Data-with-Neural-Hinton-Salakhutdinov",
            "title": {
                "fragments": [],
                "text": "Reducing the Dimensionality of Data with Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work describes an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847596"
                        ],
                        "name": "Wei Dong",
                        "slug": "Wei-Dong",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94451829"
                        ],
                        "name": "K. Li",
                        "slug": "K.-Li",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 87
                            }
                        ],
                        "text": "This has been empirically demonstrated on the challenging ImageNet classification task [5] across thousands of classes [14, 15]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 57246310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b47265245e8db53a553049dcb27ed3e495fd625",
            "isKey": false,
            "numCitedBy": 27401,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond."
            },
            "slug": "ImageNet:-A-large-scale-hierarchical-image-database-Deng-Dong",
            "title": {
                "fragments": [],
                "text": "ImageNet: A large-scale hierarchical image database"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new database called \u201cImageNet\u201d is introduced, a large-scale ontology of images built upon the backbone of the WordNet structure, much larger in scale and diversity and much more accurate than the current image datasets."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734693"
                        ],
                        "name": "John C. Duchi",
                        "slug": "John-C.-Duchi",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Duchi",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Duchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34840427"
                        ],
                        "name": "Elad Hazan",
                        "slug": "Elad-Hazan",
                        "structuredName": {
                            "firstName": "Elad",
                            "lastName": "Hazan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elad Hazan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 538820,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "413c1142de9d91804d6d11c67ff3fed59c9fc279",
            "isKey": false,
            "numCitedBy": 8025,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms."
            },
            "slug": "Adaptive-Subgradient-Methods-for-Online-Learning-Duchi-Hazan",
            "title": {
                "fragments": [],
                "text": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work describes and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal functions that can be chosen in hindsight."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "Applications of NNs to vision problems are decades old, with Convolutional NNs being the most prominent example [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6916627,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "563e821bb5ea825efb56b77484f5287f08cf3753",
            "isKey": false,
            "numCitedBy": 4091,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Convolutional-networks-for-images,-speech,-and-time-LeCun-Bengio",
            "title": {
                "fragments": [],
                "text": "Convolutional networks for images, speech, and time series"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 106
                            }
                        ],
                        "text": "First, they are deep architectures which have the capacity to learn more complex models than shallow ones [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning deep architectures for ai. Foundations and Trends R in Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Learning deep architectures for ai. Foundations and Trends R in Machine Learning"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "A notable example is the And/Or graph [20], where an object is modeled by a tree with And-nodes representing different parts and Or-nodes representing different modes of the same part."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A stochastic grammar of images"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Graphics and Vision"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A notable example is the And/Or graph [20], where an object is modeled by a tree with And-nodes representing different parts and Or-nodes representing different modes of the same part."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Stochastic Grammar of Images"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminatively trained deformable part models, release 5"
            },
            "venue": {
                "fragments": [],
                "text": "Discriminatively trained deformable part models, release 5"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 8,
            "methodology": 9
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 21,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Deep-Neural-Networks-for-Object-Detection-Szegedy-Toshev/713f73ce5c3013d9fb796c21b981dc6629af0bd5?sort=total-citations"
}