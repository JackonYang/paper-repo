{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": false,
            "numCitedBy": 10837,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2542741,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86ab4cae682fbd49c5a5bedb630e5a40fa7529f6",
            "isKey": false,
            "numCitedBy": 2930,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service."
            },
            "slug": "Handwritten-Digit-Recognition-with-a-Network-LeCun-Boser",
            "title": {
                "fragments": [],
                "text": "Handwritten Digit Recognition with a Back-Propagation Network"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task, and has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120242409"
                        ],
                        "name": "J. Bromley",
                        "slug": "J.-Bromley",
                        "structuredName": {
                            "firstName": "Jane",
                            "lastName": "Bromley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bromley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97914531"
                        ],
                        "name": "E. Sackinger",
                        "slug": "E.-Sackinger",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Sackinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Sackinger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 54104056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82430a89d444dd3f09106a233c3943d180d5ca68",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The performance of a state-of-the-art neural network classifier for hand-written digits is compared to that of a k-nearest-neighbor classifier and to human performance. The neural network has a clear advantage over the k-nearest-neighbor method, but at the same time does not yet reach human performance. Two methods for combining neural-network ideas and the k-nearest-neighbor algorithm are proposed. Numerical experiments for these methods show an improvement in performance."
            },
            "slug": "Neural-Network-and-k-Nearest-neighbor-Classifiers-Bromley-Sackinger",
            "title": {
                "fragments": [],
                "text": "Neural-Network and k-Nearest-neighbor Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The performance of a state-of-the-art neural network classifier for hand-written digits is compared to that of a k-nearest-neighbor method and to human performance."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3144,
                                "start": 85
                            }
                        ],
                        "text": "Extracting Support Data for a Given Task Bernhard Sch\u007f olkopf Chris Burgesy Vladimir Vapnik AT&T Bell Laboratories 101 Crawfords Corner Road Holmdel NJ 07733 USA schoel@big.att.com cjcb@big.att.com vlad@big.att.com Abstract We report a novel possibility for extracting a small subset of a data base which contains all the information necessary to solve a given classi cation task: using the Support Vector Algorithm to train three di erent types of handwritten digit classi ers, we observed that these types of classi ers construct their decision surface from strongly overlapping small ( 4%) subsets of the data base. This nding opens up the possibility of compressing data bases signi cantly by disposing of the data which is not important for the solution of a given task. In addition, we show that the theory allows us to predict the classi er that will have the best generalization ability, based solely on performance on the training set and characteristics of the learning machines. This nding is important for cases where the amount of available data is limited. Introduction in: U. M. Fayyad and R. Uthurusamy (eds.): Proceedings, First International Conference on Knowledge Discovery & Data Mining. AAAI Press, Menlo Park, CA, 1995, 252 -- 257 Learning can be viewed as inferring regularities from a set of training examples. Much research has been devoted to the study of various learning algorithms which allow the extraction of these underlying regularities. No matter how di erent the outward appearance of these algorithms is, they all must rely on intrinsic regularities of the data. If the learning has been successful, these intrinsic regularities will be captured in the values of some parameters of a learning machine; for a polynomial classi er, these parameters will be the coe cients of a polynomial, for a neural net they will be the weights and biases, and for a radial basis function classi er they will be weights and centers. This variety of di erent representations of the intrinsic regularities, however, conceals the fact that they all stem from a common root. permanent address: Max{Planck{Institut f\u007f ur biologische Kybernetik, Spemannstra e 38, 72076 T\u007f ubingen, Germany ysupported by ARPA under ONR contract number N00014-94-C-0186 In the present study, we explore the Support Vector Algorithm, an algorithm which gives rise to a number of di erent types of pattern classi ers. We show that the algorithm allows us to construct di erent classi ers (polynomial classi ers, radial basis function classi ers, and neural networks) exhibiting similar performance and relying on almost identical subsets of the training set, their support vector sets. In this sense, the support vector set is a stable characteristic of the data. In the case where the available training data is limited, it is important to have a means for achieving the best possible generalization by controlling characteristics of the learning machine. We use a bound of statistical learning theory (Vapnik, 1995) to predict the degree which yields the best generalization for polynomial classi ers. In the next Section, we follow Vapnik (1995), Boser, Guyon & Vapnik (1992), and Cortes & Vapnik (1995) in brie y recapitulating this algorithm and the idea of Structural Risk Minimization that it is based on."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 187
                            }
                        ],
                        "text": "Then we de ne a decision function fw;b by fw;b : Bx1;:::;xr ! f 1g; fw;b = sgn ((w x) + b) : (4) The possibility of introducing a structure on the set of hyperplanes is based on the fact (Vapnik, 1995) that the set ffw;b : kwk Ag has a VC-dimension h satisfying h R2A2: (5) Note."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 95
                            }
                        ],
                        "text": "(4)\nThe possibility of introducing a structure on the set of hyperplanes is baaed on the fact (Vapnik, 1995) that the set {j;H,b : iiwii 5 A) ha8 a Vcdimension ir sati& fying h < R2A2 ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 46
                            }
                        ],
                        "text": "We use a bound of statistical learning theory (Vapnik, 1995) to predict the degree which yields the best generalization for polynomial classi ers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 31
                            }
                        ],
                        "text": "In the next Section, we follow Vapnik (1995), Baser, Guyon & Vapnik (1992), and Cortes & Vapnik (1995) in briefly recapitulating this algorithm and the idea of Structural Risk Minimization that it is based on."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 988,
                                "start": 188
                            }
                        ],
                        "text": "Then we de ne a decision function fw;b by fw;b : Bx1;:::;xr ! f 1g; fw;b = sgn ((w x) + b) : (4) The possibility of introducing a structure on the set of hyperplanes is based on the fact (Vapnik, 1995) that the set ffw;b : kwk Ag has a VC-dimension h satisfying h R2A2: (5) Note. Dropping the condition kwk A leads to a set of functions whose VC{dimension equals N +1, where N is the dimensionality of Z. Due to kwk A, we can get VC{dimensions which are much smaller than N , enabling us to work in very high dimensional spaces. The SupportVector Algorithm. Now suppose we are given a set of examples (x1; y1); : : : ; (x`; y`); xi 2 RN ; yi 2 f 1;+1g, and we want to nd a decision function fw;b with the property fw;b(xi) = yi; i = 1; : : : ; `: If this function exists, canonicality (3) implies yi((w xi) + b) 1; i = 1; : : : ; `: (6) In many practical applications, a separating hyperplane does not exist. To allow for the possibility of examples violating (6), Cortes & Vapnik (1995) introduce slack variables i 0; i = 1; : : : ; `; (7) to get yi((w xi) + b) 1 i; i = 1; : : : ; `: (8) The support vector approach to minimizing the guaranteed risk bound (1) consists in the following: minimize (w; ) = (w w) + X\u0300i=1 i (9) subject to the constraints (7) and (8)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 47
                            }
                        ],
                        "text": "We use a bound of statistical learning theory (Vapnik, 1995) to predict the degree which yields the best generalization for polynomial classifiers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 65
                            }
                        ],
                        "text": "To allow for the possibility of examples violating (6), Cortes & Vapnik (1995) introduce slack variables\nto get (i>O, i=l,..., J, (7)\n&((W * Xi) + 6) 1 1 - &, i = 1 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59752996,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "5451278e1a11cf3f1be28a05f38d36c8641e68f7",
            "isKey": true,
            "numCitedBy": 4580,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Nature-of-Statistical-Learning-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58744436,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e3034bed8631249e26344149d8fad77e4ac854a",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Proceedings-of-the-fifth-annual-workshop-on-theory-Haussler",
            "title": {
                "fragments": [],
                "text": "Proceedings of the fifth annual workshop on Computational learning theory"
            },
            "venue": {
                "fragments": [],
                "text": "COLT 1992"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural{network and k{nearest{neighbor classiiers"
            },
            "venue": {
                "fragments": [],
                "text": "Neural{network and k{nearest{neighbor classiiers"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "the support vector algorithm allows the construction of various learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Handwritten digit recognition with a backpropagation network"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems, Vol. 2, 396-404, Morgan"
            },
            "year": 1990
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 1,
            "methodology": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 8,
        "totalPages": 1
    },
    "page_url": "https://www.semanticscholar.org/paper/Extracting-Support-Data-for-a-Given-Task-Sch\u00f6lkopf-Burges/7ec8029e5855b6efbac161488a2e68f83298091c?sort=total-citations"
}