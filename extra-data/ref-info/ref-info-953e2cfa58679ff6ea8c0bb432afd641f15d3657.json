{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3171632"
                        ],
                        "name": "A. Quattoni",
                        "slug": "A.-Quattoni",
                        "structuredName": {
                            "firstName": "Ariadna",
                            "lastName": "Quattoni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Quattoni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "linear subspaces) using data from related tasks and can also be used for transfer learning on this data [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2830003,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23c6cb079b9ec45ae462cae743e01a1185fc4c2c",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Current methods for learning visual categories work well when a large amount of labeled data is available, but can run into severe difficulties when the number of labeled examples is small. When labeled data is scarce it may be beneficial to use unlabeled data to learn an image representation that is low-dimensional, but nevertheless captures the information required to discriminate between image categories. This paper describes a method for learning representations from large quantities of unlabeled images which have associated captions; the goal is to improve learning in future image classification problems. Experiments show that our method significantly outperforms (1) a fully-supervised baseline model, (2) a model that ignores the captions and learns a visual representation by performing PCA on the unlabeled images alone and (3) a model that uses the output of word classifiers trained using captions and unlabeled data. Our current work concentrates on captions as the source of meta-data, but more generally other types of meta-data could be used."
            },
            "slug": "Learning-Visual-Representations-using-Images-with-Quattoni-Collins",
            "title": {
                "fragments": [],
                "text": "Learning Visual Representations using Images with Captions"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper describes a method for learning representations from large quantities of unlabeled images which have associated captions, which significantly outperforms a fully-supervised baseline model and a model that ignores the captions and learns a visual representation by performing PCA on the unlabeling images alone."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38070424"
                        ],
                        "name": "R. Ando",
                        "slug": "R.-Ando",
                        "structuredName": {
                            "firstName": "Rie",
                            "lastName": "Ando",
                            "middleNames": [
                                "Kubota"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ando"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 111
                            }
                        ],
                        "text": "Existing methods for transfer learning often learn a prior model or linear manifold over classifier parameters [8, 2], discover a sparse set of common features [13, 3, 20, 6], or use a representation based on classifier outputs from related tasks [24], but do not generally take advantage of unlabeled data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 116
                            }
                        ],
                        "text": "Most transfer learning techniques can be broadly grouped into two categories: learning intermediate representations [19, 5, 16, 2], and learning small sets of relevant features that are shared across tasks [11, 13, 3, 1, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 64
                            }
                        ],
                        "text": "An alternative and complementary approach for transfer learning [2] is based on learning shared hidden representations (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13650160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "944e1a7b2c5c62e952418d7684e3cade89c76f87",
            "isKey": false,
            "numCitedBy": 1414,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting."
            },
            "slug": "A-Framework-for-Learning-Predictive-Structures-from-Ando-Zhang",
            "title": {
                "fragments": [],
                "text": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data, and algorithms for structural learning will be proposed, and computational issues will be investigated."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2979876"
                        ],
                        "name": "R. Raina",
                        "slug": "R.-Raina",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Raina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Raina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078284037"
                        ],
                        "name": "Alexis Battle",
                        "slug": "Alexis-Battle",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Battle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexis Battle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409971380"
                        ],
                        "name": "Ben Packer",
                        "slug": "Ben-Packer",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Packer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Packer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[15] described an approach that learns a sparse set of highlevel features (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6692382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3852f0113fcf8a3913c55ae92393ae6ccde347e",
            "isKey": false,
            "numCitedBy": 1611,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new machine learning framework called \"self-taught learning\" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation."
            },
            "slug": "Self-taught-learning:-transfer-learning-from-data-Raina-Battle",
            "title": {
                "fragments": [],
                "text": "Self-taught learning: transfer learning from unlabeled data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data to form a succinct input representation and significantly improve classification performance."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50033175"
                        ],
                        "name": "Andreas Argyriou",
                        "slug": "Andreas-Argyriou",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Argyriou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Argyriou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801089"
                        ],
                        "name": "T. Evgeniou",
                        "slug": "T.-Evgeniou",
                        "structuredName": {
                            "firstName": "Theodoros",
                            "lastName": "Evgeniou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Evgeniou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 160
                            }
                        ],
                        "text": "Existing methods for transfer learning often learn a prior model or linear manifold over classifier parameters [8, 2], discover a sparse set of common features [13, 3, 20, 6], or use a representation based on classifier outputs from related tasks [24], but do not generally take advantage of unlabeled data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 142
                            }
                        ],
                        "text": "Additionally, while previous feature sharing approaches build a joint sparse classifier on the feature space [13], or a random [13] or hidden [3, 1] projection of that feature space, our method discovers a set of discriminative prototypes that can be transferred to solve a future problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 206
                            }
                        ],
                        "text": "Most transfer learning techniques can be broadly grouped into two categories: learning intermediate representations [19, 5, 16, 2], and learning small sets of relevant features that are shared across tasks [11, 13, 3, 1, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7502194,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbb3342599c9b431a3152a0d5c813d3e56967a27",
            "isKey": false,
            "numCitedBy": 1380,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for learning a low-dimensional representation which is shared across a set of multiple related tasks. The method builds upon the well-known 1-norm regularization problem using a new regularizer which controls the number of learned features common for all the tasks. We show that this problem is equivalent to a convex optimization problem and develop an iterative algorithm for solving it. The algorithm has a simple interpretation: it alternately performs a supervised and an unsupervised step, where in the latter step we learn commonacross-tasks representations and in the former step we learn task-specific functions using these representations. We report experiments on a simulated and a real data set which demonstrate that the proposed method dramatically improves the performance relative to learning each task independently. Our algorithm can also be used, as a special case, to simply select \u2013 not learn \u2013 a few common features across the tasks."
            },
            "slug": "Multi-Task-Feature-Learning-Argyriou-Evgeniou",
            "title": {
                "fragments": [],
                "text": "Multi-Task Feature Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The method builds upon the well-known 1-norm regularization problem using a new regularizer which controls the number of learned features common for all the tasks, and develops an iterative algorithm for solving it."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105534484"
                        ],
                        "name": "Michael Fink",
                        "slug": "Michael-Fink",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Fink",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Fink"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "In the context of discriminative (maximum margin) object models Fink [9] developed a method that learns distance metrics from related problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 101
                            }
                        ],
                        "text": "Transfer learning for visual category recognition has received significant attention in recent years [8, 18, 12, 24, 9, 10, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9248748,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01e098695a5b653ebf0daaef9f8cefc3b02e8e71",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a framework for learning an object classifier from a single example. This goal is achieved by emphasizing the relevant dimensions for classification using available examples of related classes. Learning to accurately classify objects from a single training example is often unfeasible due to overfitting effects. However, if the instance representation provides that the distance between each two instances of the same class is smaller than the distance between any two instances from different classes, then a nearest neighbor classifier could achieve perfect performance with a single training example. We therefore suggest a two stage strategy. First, learn a metric over the instances that achieves the distance criterion mentioned above, from available examples of other related classes. Then, using the single examples, define a nearest neighbor classifier where distance is evaluated by the learned class relevance metric. Finding a metric that emphasizes the relevant dimensions for classification might not be possible when restricted to linear projections. We therefore make use of a kernel based metric learning algorithm. Our setting encodes object instances as sets of locality based descriptors and adopts an appropriate image kernel for the class relevance metric learning. The proposed framework for learning from a single example is demonstrated in a synthetic setting and on a character classification task."
            },
            "slug": "Object-Classification-from-a-Single-Example-Class-Fink",
            "title": {
                "fragments": [],
                "text": "Object Classification from a Single Example Utilizing Class Relevance Metrics"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A framework for learning an object classifier from a single example by emphasizing the relevant dimensions for classification using available examples of related classes by making use of a kernel based metric learning algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2979876"
                        ],
                        "name": "R. Raina",
                        "slug": "R.-Raina",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Raina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Raina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 116
                            }
                        ],
                        "text": "Most transfer learning techniques can be broadly grouped into two categories: learning intermediate representations [19, 5, 16, 2], and learning small sets of relevant features that are shared across tasks [11, 13, 3, 1, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3056583,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "328f88380422573a4ff9ada1fc5aa9f198a32bc5",
            "isKey": false,
            "numCitedBy": 315,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Many applications of supervised learning require good generalization from limited labeled data. In the Bayesian setting, we can try to achieve this goal by using an informative prior over the parameters, one that encodes useful domain knowledge. Focusing on logistic regression, we present an algorithm for automatically constructing a multivariate Gaussian prior with a full covariance matrix for a given supervised learning task. This prior relaxes a commonly used but overly simplistic independence assumption, and allows parameters to be dependent. The algorithm uses other \"similar\" learning problems to estimate the covariance of pairs of individual parameters. We then use a semidefinite program to combine these estimates and learn a good prior for the current learning task. We apply our methods to binary text classification, and demonstrate a 20 to 40% test error reduction over a commonly used prior."
            },
            "slug": "Constructing-informative-priors-using-transfer-Raina-Ng",
            "title": {
                "fragments": [],
                "text": "Constructing informative priors using transfer learning"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An algorithm for automatically constructing a multivariate Gaussian prior with a full covariance matrix for a given supervised learning task, which relaxes a commonly used but overly simplistic independence assumption, and allows parameters to be dependent."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "proposed a Bayesian transfer learning approach for object recognition [8] where a common prior over visual classifier parameters is learnt; their results show a significant improvement when learning from a few labeled examples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 101
                            }
                        ],
                        "text": "Transfer learning for visual category recognition has received significant attention in recent years [8, 18, 12, 24, 9, 10, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 111
                            }
                        ],
                        "text": "Existing methods for transfer learning often learn a prior model or linear manifold over classifier parameters [8, 2], discover a sparse set of common features [13, 3, 20, 6], or use a representation based on classifier outputs from related tasks [24], but do not generally take advantage of unlabeled data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6953475,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "812355cec91fa30bb50e9e992a3549af39e4f6eb",
            "isKey": false,
            "numCitedBy": 2365,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by maximum likelihood (ML) and maximum a posteriori (MAP) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully."
            },
            "slug": "One-shot-learning-of-object-categories-Fei-Fei-Fergus",
            "title": {
                "fragments": [],
                "text": "One-shot learning of object categories"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is found that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774536"
                        ],
                        "name": "T. Hertz",
                        "slug": "T.-Hertz",
                        "structuredName": {
                            "firstName": "Tomer",
                            "lastName": "Hertz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hertz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400556488"
                        ],
                        "name": "Aharon Bar-Hillel",
                        "slug": "Aharon-Bar-Hillel",
                        "structuredName": {
                            "firstName": "Aharon",
                            "lastName": "Bar-Hillel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aharon Bar-Hillel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789171"
                        ],
                        "name": "D. Weinshall",
                        "slug": "D.-Weinshall",
                        "structuredName": {
                            "firstName": "Daphna",
                            "lastName": "Weinshall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Weinshall"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 101
                            }
                        ],
                        "text": "Transfer learning for visual category recognition has received significant attention in recent years [8, 18, 12, 24, 9, 10, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[10] reports a method based on training binary max margin classifiers on the product space of pairs of images, thus creating a distance function based on the output of those classifiers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4473984,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8bbd29b4b1e3364e551c638ee46e0cde3cd3b31d",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Image retrieval critically relies on the distance function used to compare a query image to images in the database. We suggest learning such distance functions by training binary classifiers with margins, where the classifiers are defined over the product space of pairs of images. The classifiers are trained to distinguish between pairs in which the images are from the same class and pairs, which contain images from different classes. The signed margin is used as a distance function. We explore several variants of this idea, based on using SVM and boosting algorithms as product space classifiers. Our main contribution is a distance learning method, which combines boosting hypotheses over the product space with a weak learner based on partitioning the original feature space. The weak learner used is a Gaussian mixture model computed using a constrained EM algorithm, where the constraints are equivalence constraints on pairs of data points. This approach allows us to incorporate unlabeled data into the training process. Using some benchmark databases from the UCI repository, we show that our margin based methods significantly outperform existing metric learning methods, which are based an learning a Mahalanobis distance. We then show comparative results of image retrieval in a distributed learning paradigm, using two databases: a large database of facial images (YaleB), and a database of natural images taken from a commercial CD. In both cases our GMM based boosting method outperforms all other methods, and its generalization to unseen classes is superior."
            },
            "slug": "Learning-distance-functions-for-image-retrieval-Hertz-Bar-Hillel",
            "title": {
                "fragments": [],
                "text": "Learning distance functions for image retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The main contribution is a distance learning method, which combines boosting hypotheses over the product space with a weak learner based on partitioning the original feature space, which outperforms existing metric learning methods, which are based an learning a Mahalanobis distance."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": ", [17]), but don\u2019t generally exploit knowledge learned from previous supervised tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11326549,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6fe31db0f89a81705f6aad16066cb9a5d1885134",
            "isKey": false,
            "numCitedBy": 275,
            "numCiting": 117,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, on the one hand, we aim to give a review on literature dealing with the problem of supervised learning aided by additional unlabeled data. On the other hand, being a part of the author's first year PhD report, the paper serves as a frame to bundle related work by the author as well as numerous suggestions for potential future work. Therefore, this work contains more speculative and partly subjective material than the reader might expect from a literature review. We give a rigorous definition of the problem and relate it to supervised and unsupervised learning. The crucial role of prior knowledge is put forward, and we discuss the important notion of input-dependent regularization. We postulate a number of baseline methods, being algorithms or algorithmic schemes which can more or less straightforwardly be applied to the problem, without the need for genuinely new concepts. However, some of them might serve as basis for a genuine method. In the literature review, we try to cover the wide variety of (recent) work and to classify this work into meaningful categories. We also mention work done on related problems and suggest some ideas towards synthesis. Finally, we discuss some caveats and tradeoffs of central importance to the problem."
            },
            "slug": "Learning-from-Labeled-and-Unlabeled-Data-Seeger",
            "title": {
                "fragments": [],
                "text": "Learning from Labeled and Unlabeled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A rigorous definition of the problem is given and the crucial role of prior knowledge is put forward, and the important notion of input-dependent regularization is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Encyclopedia of Machine Learning"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 101
                            }
                        ],
                        "text": "Transfer learning for visual category recognition has received significant attention in recent years [8, 18, 12, 24, 9, 10, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 160
                            }
                        ],
                        "text": "Existing methods for transfer learning often learn a prior model or linear manifold over classifier parameters [8, 2], discover a sparse set of common features [13, 3, 20, 6], or use a representation based on classifier outputs from related tasks [24], but do not generally take advantage of unlabeled data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[20] on feature sharing for multi-class classification includes a joint boosting algorithm where the weak learners (step functions applied to individual features) are greedily selected so they can both separate well some bipartition of the set of classes and reduce the average empirical risk on all classifiers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "Previous approaches in vision to joint feature learning have employed a greedy boosting approach [20]; our joint regularization exploits a norm derived from simultaneous sparse signal approximation methods [21], leading to an optimization problem that can be expressed as a linear"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2741819,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd626564bd47e9fc67a5b276301282ba2fe3d833",
            "isKey": true,
            "numCitedBy": 793,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (runtime) computational complexity and the (training-time) sample complexity scale linearly with the number of classes to be detected. We present a multitask learning procedure, based on boosted decision stumps, that reduces the computational and sample complexity by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required and, therefore, the runtime cost of the classifier, is observed to scale approximately logarithmically with the number of classes. The features selected by joint training are generic edge-like features, whereas the features chosen by training each class separately tend to be more object-specific. The generic features generalize better and considerably reduce the computational cost of multiclass object detection"
            },
            "slug": "Sharing-Visual-Features-for-Multiclass-and-Object-Torralba-Murphy",
            "title": {
                "fragments": [],
                "text": "Sharing Visual Features for Multiclass and Multiview Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A multitask learning procedure, based on boosted decision stumps, that reduces the computational and sample complexity by finding common features that can be shared across the classes (and/or views) and considerably reduce the computational cost of multiclass object detection."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41224113"
                        ],
                        "name": "E. Miller",
                        "slug": "E.-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1837031"
                        ],
                        "name": "Nicholas E. Matsakis",
                        "slug": "Nicholas-E.-Matsakis",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Matsakis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicholas E. Matsakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 101
                            }
                        ],
                        "text": "Transfer learning for visual category recognition has received significant attention in recent years [8, 18, 12, 24, 9, 10, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2699786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7aac1045e6943b4a7978e260a3035662d5b3bf8d",
            "isKey": false,
            "numCitedBy": 381,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We define a process called congealing in which elements of a dataset (images) are brought into correspondence with each other jointly, producing a data-defined model. It is based upon minimizing the summed component-wise (pixel-wise) entropies over a continuous set of transforms on the data. One of the biproducts of this minimization is a set of transform, one associated with each original training sample. We then demonstrate a procedure for effectively bringing test data into correspondence with the data-defined model produced in the congealing process. Subsequently; we develop a probability density over the set of transforms that arose from the congealing process. We suggest that this density over transforms may be shared by many classes, and demonstrate how using this density as \"prior knowledge\" can be used to develop a classifier based on only a single training example for each class."
            },
            "slug": "Learning-from-one-example-through-shared-densities-Miller-Matsakis",
            "title": {
                "fragments": [],
                "text": "Learning from one example through shared densities on transforms"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A probability density over the set of transforms that arose from the congealing process is developed, and it is suggested that this density over transforms may be shared by many classes, and used to develop a classifier based on only a single training example for each class."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145859952"
                        ],
                        "name": "M. Varma",
                        "slug": "M.-Varma",
                        "structuredName": {
                            "firstName": "Manik",
                            "lastName": "Varma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Varma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789036"
                        ],
                        "name": "Debajyoti Ray",
                        "slug": "Debajyoti-Ray",
                        "structuredName": {
                            "firstName": "Debajyoti",
                            "lastName": "Ray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Debajyoti Ray"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Recent progress has shown that visual category recognition can improve with the use of kernels that are optimized to particular tasks [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13942014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3edd9e15976ca3aa2d627d41384e1f0908a91632",
            "isKey": false,
            "numCitedBy": 582,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the problem of learning optimal descriptors for a given classification task. Many hand-crafted descriptors have been proposed in the literature for measuring visual similarity. Looking past initial differences, what really distinguishes one descriptor from another is the tradeoff that it achieves between discriminative power and invariance. Since this trade-off must vary from task to task, no single descriptor can be optimal in all situations. Our focus, in this paper, is on learning the optimal tradeoff for classification given a particular training set and prior constraints. The problem is posed in the kernel learning framework. We learn the optimal, domain-specific kernel as a combination of base kernels corresponding to base features which achieve different levels of trade-off (such as no invariance, rotation invariance, scale invariance, affine invariance, etc.) This leads to a convex optimisation problem with a unique global optimum which can be solved for efficiently. The method is shown to achieve state-of-the-art performance on the UIUC textures, Oxford flowers and Cal- tech 101 datasets."
            },
            "slug": "Learning-The-Discriminative-Power-Invariance-Varma-Ray",
            "title": {
                "fragments": [],
                "text": "Learning The Discriminative Power-Invariance Trade-Off"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This paper investigates the problem of learning optimal descriptors for a given classification task using the kernel learning framework and learns the optimal, domain-specific kernel as a combination of base kernels corresponding to base features which achieve different levels of trade-off."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 116
                            }
                        ],
                        "text": "Most transfer learning techniques can be broadly grouped into two categories: learning intermediate representations [19, 5, 16, 2], and learning small sets of relevant features that are shared across tasks [11, 13, 3, 1, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12565208,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bd6e929ed8384ea2212d50ab3c103ec018cc9fd",
            "isKey": false,
            "numCitedBy": 382,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A Bayesian model of learning to learn by sampling from multiple tasks is presented. The multiple tasks are themselves generated by sampling from a distribution over an environment of related tasks. Such an environment is shown to be naturally modelled within a Bayesian context by the concept of an objective prior distribution. It is argued that for many common machine learning problems, although in general we do not know the true (objective) prior for the problem, we do have some idea of a set of possible priors to which the true prior belongs. It is shown that under these circumstances a learner can use Bayesian inference to learn the true prior by learning sufficiently many tasks from the environment. In addition, bounds are given on the amount of information required to learn a task when it is simultaneously learnt with several other tasks. The bounds show that if the learner has little knowledge of the true prior, but the dimensionality of the true prior is small, then sampling multiple tasks is highly advantageous. The theory is applied to the problem of learning a common feature set or equivalently a low-dimensional-representation (LDR) for an environment of related tasks."
            },
            "slug": "A-Bayesian/Information-Theoretic-Model-of-Learning-Baxter",
            "title": {
                "fragments": [],
                "text": "A Bayesian/Information Theoretic Model of Learning to Learn via Multiple Task Sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is argued that for many common machine learning problems, although in general the authors do not know the true (objective) prior for the problem, they do have some idea of a set of possible priors to which the true prior belongs."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745410"
                        ],
                        "name": "Maria-Florina Balcan",
                        "slug": "Maria-Florina-Balcan",
                        "structuredName": {
                            "firstName": "Maria-Florina",
                            "lastName": "Balcan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maria-Florina Balcan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737804"
                        ],
                        "name": "S. Vempala",
                        "slug": "S.-Vempala",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Vempala",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vempala"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[4], who proved it has important theoretical properties."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[4], who proposed the use of a representation based on kernel distances to unlabeled datapoints, and the work of Tropp [21] on simultaneous sparse approximation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 137
                            }
                        ],
                        "text": "One of the advantages of our transfer learning method is that the prototype representation is defined using an arbitrary kernel function [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8345579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "faa9b8fef24637f1debc2d161ce591f35d3e527a",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel functions are typically viewed as providing an implicit mapping of points into a high-dimensional space, with the ability to gain much of the power of that space without incurring a high cost if the result is linearly-separable by a large margin \u03b3. However, the Johnson-Lindenstrauss lemma suggests that in the presence of a large margin, a kernel function can also be viewed as a mapping to a low-dimensional space, one of dimension only $$\\tilde{O}(1/\\gamma^2)$$. In this paper, we explore the question of whether one can efficiently produce such low-dimensional mappings, using only black-box access to a kernel function. That is, given just a program that computes K(x,y) on inputs x,y of our choosing, can we efficiently construct an explicit (small) set of features that effectively capture the power of the implicit high-dimensional space? We answer this question in the affirmative if our method is also allowed black-box access to the underlying data distribution (i.e., unlabeled examples). We also give a lower bound, showing that if we do not have access to the distribution, then this is not possible for an arbitrary black-box kernel function; we leave as an open problem, however, whether this can be done for standard kernel functions such as the polynomial kernel. Our positive result can be viewed as saying that designing a good kernel function is much like designing a good feature space. Given a kernel, by running it in a black-box manner on random unlabeled examples, we can efficiently generate an explicit set of $$\\tilde{O}(1/\\gamma^2)$$ features, such that if the data was linearly separable with margin \u03b3 under the kernel, then it is approximately separable in this new feature space."
            },
            "slug": "Kernels-as-features:-On-kernels,-margins,-and-Balcan-Blum",
            "title": {
                "fragments": [],
                "text": "Kernels as features: On kernels, margins, and low-dimensional mappings"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The question of whether one can efficiently produce low-dimensional mappings, using only black-box access to a kernel function, is explored, and it is found that if the data was linearly separable with margin \u03b3 under the kernel, then it is approximately separable in this new feature space."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799035"
                        ],
                        "name": "Erik B. Sudderth",
                        "slug": "Erik-B.-Sudderth",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sudderth",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erik B. Sudderth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701607"
                        ],
                        "name": "A. Willsky",
                        "slug": "A.-Willsky",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Willsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Willsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 101
                            }
                        ],
                        "text": "Transfer learning for visual category recognition has received significant attention in recent years [8, 18, 12, 24, 9, 10, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6153430,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5331349557fababfac48d47e49b44583e3bd5f6",
            "isKey": false,
            "numCitedBy": 365,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a hierarchical probabilistic model for the detection and recognition of objects in cluttered, natural scenes. The model is based on a set of parts which describe the expected appearance and position, in an object centered coordinate frame, of features detected by a low-level interest operator. Each object category then has its own distribution over these parts, which are shared between objects. We learn the parameters of this model via a Gibbs sampler which uses the graphical model's structure to analytically average over many parameters. Applied to a database of images of isolated objects, the sharing of parts among objects improves detection accuracy when few training examples are available. We also extend this hierarchical framework to scenes containing multiple objects"
            },
            "slug": "Learning-hierarchical-models-of-scenes,-objects,-Sudderth-Torralba",
            "title": {
                "fragments": [],
                "text": "Learning hierarchical models of scenes, objects, and parts"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Applied to a database of images of isolated objects, the sharing of parts among objects improves detection accuracy when few training examples are available and this hierarchical probabilistic model is extended to scenes containing multiple objects."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39161025"
                        ],
                        "name": "Alon Zweig",
                        "slug": "Alon-Zweig",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Zweig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alon Zweig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789171"
                        ],
                        "name": "D. Weinshall",
                        "slug": "D.-Weinshall",
                        "structuredName": {
                            "firstName": "Daphna",
                            "lastName": "Weinshall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Weinshall"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 101
                            }
                        ],
                        "text": "Transfer learning for visual category recognition has received significant attention in recent years [8, 18, 12, 24, 9, 10, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 247
                            }
                        ],
                        "text": "Existing methods for transfer learning often learn a prior model or linear manifold over classifier parameters [8, 2], discover a sparse set of common features [13, 3, 20, 6], or use a representation based on classifier outputs from related tasks [24], but do not generally take advantage of unlabeled data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "Also in the context of constellation models, Zweig [24] has investigated transfer learning with a method based on combining object classifiers from different hierarchical levels into a single classifier."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7691438,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "352c53e56c52a49d33dcdbec5690c2ba604b07d0",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigated the computational properties of natural object hierarchy in the context of constellation object class models, and its utility for object class recognition. We first observed an interesting computational property of the object hierarchy: comparing the recognition rate when using models of objects at different levels, the higher more inclusive levels (e.g., closed-frame vehicles or vehicles) exhibit higher recall but lower precision when compared with the class specific level (e.g., bus). These inherent differences suggest that combining object classifiers from different hierarchical levels into a single classifier may improve classification, as it appears like these models capture different aspects of the object. We describe a method to combine these classifiers, and analyze the conditions under which improvement can be guaranteed. When given a small sample of a new object class, we describe a method to transfer knowledge across the tree hierarchy, between related objects. Finally, we describe extensive experiments using object hierarchies obtained from publicly available datasets, and show that the combined classifiers significantly improve recognition results."
            },
            "slug": "Exploiting-Object-Hierarchy:-Combining-Models-from-Zweig-Weinshall",
            "title": {
                "fragments": [],
                "text": "Exploiting Object Hierarchy: Combining Models from Different Category Levels"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "An interesting computational property of the object hierarchy is observed: comparing the recognition rate when using models of objects at different levels, the higher more inclusive levels exhibit higher recall but lower precision when compared with the class specific level."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2533906"
                        ],
                        "name": "G. Obozinski",
                        "slug": "G.-Obozinski",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Obozinski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Obozinski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1453240859"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 160
                            }
                        ],
                        "text": "Existing methods for transfer learning often learn a prior model or linear manifold over classifier parameters [8, 2], discover a sparse set of common features [13, 3, 20, 6], or use a representation based on classifier outputs from related tasks [24], but do not generally take advantage of unlabeled data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "Additionally, while previous feature sharing approaches build a joint sparse classifier on the feature space [13], or a random [13] or hidden [3, 1] projection of that feature space, our method discovers a set of discriminative prototypes that can be transferred to solve a future problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 206
                            }
                        ],
                        "text": "Most transfer learning techniques can be broadly grouped into two categories: learning intermediate representations [19, 5, 16, 2], and learning small sets of relevant features that are shared across tasks [11, 13, 3, 1, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14655047,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71977913bf322052cb84346dc0061ae68470f556",
            "isKey": false,
            "numCitedBy": 293,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of joint feature selection across a group of related classification or regression tasks. We propose a novel type of joint regularization of the model parameters in order to couple feature selection across tasks. Intuitively, we extend the `1 regularization for single-task estimation to the multi-task setting. By penalizing the sum of `2-norms of the blocks of coefficients associated with each feature across different tasks, we encourage multiple predictors to have similar parameter sparsity patterns. To fit parameters under this regularization, we propose a blockwise boosting scheme that follows the regularization path. The algorithm introduces and updates simultaneously the coefficients associated with one feature in all tasks. We show empirically that this approach outperforms independent `1-based feature selection on several datasets."
            },
            "slug": "Multi-task-feature-selection-Obozinski-Taskar",
            "title": {
                "fragments": [],
                "text": "Multi-task feature selection"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This work proposes a novel type of joint regularization of the model parameters in order to couple feature selection across tasks and shows empirically that this approach outperforms independent `1-based feature selection on several datasets."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768120"
                        ],
                        "name": "T. Jebara",
                        "slug": "T.-Jebara",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Jebara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jebara"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 206
                            }
                        ],
                        "text": "Most transfer learning techniques can be broadly grouped into two categories: learning intermediate representations [19, 5, 16, 2], and learning small sets of relevant features that are shared across tasks [11, 13, 3, 1, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14128785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8559ff5718b5a5ff1ffaaf5f91efaf25ee518457",
            "isKey": false,
            "numCitedBy": 241,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We compute a common feature selection or kernel selection configuration for multiple support vector machines (SVMs) trained on different yet inter-related datasets. The method is advantageous when multiple classification tasks and differently labeled datasets exist over a common input space. Different datasets can mutually reinforce a common choice of representation or relevant features for their various classifiers. We derive a multi-task representation learning approach using the maximum entropy discrimination formalism. The resulting convex algorithms maintain the global solution properties of support vector machines. However, in addition to multiple SVM classification/regression parameters they also jointly estimate an optimal subset of features or optimal combination of kernels. Experiments are shown on standardized datasets."
            },
            "slug": "Multi-task-feature-and-kernel-selection-for-SVMs-Jebara",
            "title": {
                "fragments": [],
                "text": "Multi-task feature and kernel selection for SVMs"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A multi-task representation learning approach using the maximum entropy discrimination formalism is derived and the resulting convex algorithms maintain the global solution properties of support vector machines."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3126798"
                        ],
                        "name": "B. Epshtein",
                        "slug": "B.-Epshtein",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Epshtein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Epshtein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743045"
                        ],
                        "name": "S. Ullman",
                        "slug": "S.-Ullman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Ullman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ullman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 160
                            }
                        ],
                        "text": "Existing methods for transfer learning often learn a prior model or linear manifold over classifier parameters [8, 2], discover a sparse set of common features [13, 3, 20, 6], or use a representation based on classifier outputs from related tasks [24], but do not generally take advantage of unlabeled data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15315201,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c812bd2474af91f33950db1b501fc5dc2b98002",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a novel technique for identifying semantically equivalent parts in images belonging to the same object class, (e.g. eyes, license plates, aircraft wings etc.). The visual appearance of such object parts can differ substantially, and therefore, traditional image similarity-based methods are inappropriate for this task. The technique we propose is based on the use of common context. We first retrieve context fragments, which consistently appear together with a given input fragment in a stable geometric relation. We then use the context fragments in new images to infer the most likely position of equivalent parts. Given a set of image examples of objects in a class, the method can automatically learn the part structure of the domain - identify the main parts, and how their appearance changes across objects in the class. Two applications of the proposed algorithm are shown: the detection and identification of object parts and object recognition."
            },
            "slug": "Identifying-semantically-equivalent-object-Epshtein-Ullman",
            "title": {
                "fragments": [],
                "text": "Identifying semantically equivalent object fragments"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A novel technique for identifying semantically equivalent parts in images belonging to the same object class, (e.g. eyes, license plates, aircraft wings etc.) based on the use of common context is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 116
                            }
                        ],
                        "text": "Most transfer learning techniques can be broadly grouped into two categories: learning intermediate representations [19, 5, 16, 2], and learning small sets of relevant features that are shared across tasks [11, 13, 3, 1, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1016169,
            "fieldsOfStudy": [
                "Computer Science",
                "Education"
            ],
            "id": "371c9dc680e916f79d9c78fcf6c894a2dd299095",
            "isKey": false,
            "numCitedBy": 685,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates learning in a lifelong context. Lifelong learning addresses situations in which a learner faces a whole stream of learning tasks. Such scenarios provide the opportunity to transfer knowledge across multiple learning tasks, in order to generalize more accurately from less training data. In this paper, several different approaches to lifelong learning are described, and applied in an object recognition domain. It is shown that across the board, lifelong learning approaches generalize consistently more accurately from less training data, by their ability to transfer knowledge across learning tasks."
            },
            "slug": "Is-Learning-The-n-th-Thing-Any-Easier-Than-Learning-Thrun",
            "title": {
                "fragments": [],
                "text": "Is Learning The n-th Thing Any Easier Than Learning The First?"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that across the board, lifelong learning approaches generalize consistently more accurately from less training data, by their ability to transfer knowledge across learning tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20937580"
                        ],
                        "name": "Yonatan Amit",
                        "slug": "Yonatan-Amit",
                        "structuredName": {
                            "firstName": "Yonatan",
                            "lastName": "Amit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yonatan Amit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105534484"
                        ],
                        "name": "Michael Fink",
                        "slug": "Michael-Fink",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Fink",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Fink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706280"
                        ],
                        "name": "Nathan Srebro",
                        "slug": "Nathan-Srebro",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Srebro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Srebro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743045"
                        ],
                        "name": "S. Ullman",
                        "slug": "S.-Ullman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Ullman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ullman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[1] proposed an alternative joint regularization framework based on a trace-norm penalty on the coefficients matrix, where the trace-norm is defined as the sum of the matrix\u2019s singular values."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[1] on joint sparse approximation for multi-task learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 142
                            }
                        ],
                        "text": "Additionally, while previous feature sharing approaches build a joint sparse classifier on the feature space [13], or a random [13] or hidden [3, 1] projection of that feature space, our method discovers a set of discriminative prototypes that can be transferred to solve a future problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 206
                            }
                        ],
                        "text": "Most transfer learning techniques can be broadly grouped into two categories: learning intermediate representations [19, 5, 16, 2], and learning small sets of relevant features that are shared across tasks [11, 13, 3, 1, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15645633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c09d83387c3ad36cd9d7113c0019d1ea0c5a3c9",
            "isKey": true,
            "numCitedBy": 337,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper suggests a method for multiclass learning with many classes by simultaneously learning shared characteristics common to the classes, and predictors for the classes in terms of these characteristics. We cast this as a convex optimization problem, using trace-norm regularization and study gradient-based optimization both for the linear case and the kernelized setting."
            },
            "slug": "Uncovering-shared-structures-in-multiclass-Amit-Fink",
            "title": {
                "fragments": [],
                "text": "Uncovering shared structures in multiclass classification"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper suggests a method for multiclass learning with many classes by simultaneously learning shared characteristics common to the classes, and predictors for the classes in terms of these characteristics."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "In the context of regression Donoho [7] has proven that the solution with smallest l1 norm is also the sparsest solution, i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 29
                            }
                        ],
                        "text": "In the context of regression Donoho [7] has proven that the solution with smallest l1 norm is also the sparsest solution, i.e. the solution with the least number of non-zero coefficients."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8510060,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "3424286d6d39de51080ddd683646565545d015e2",
            "isKey": false,
            "numCitedBy": 2296,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider linear equations y = \u03a6x where y is a given vector in \u211dn and \u03a6 is a given n \u00d7 m matrix with n < m \u2264 \u03c4n, and we wish to solve for x \u2208 \u211dm. We suppose that the columns of \u03a6 are normalized to the unit \ud835\udcc12\u2010norm, and we place uniform measure on such \u03a6. We prove the existence of \u03c1 = \u03c1(\u03c4) > 0 so that for large n and for all \u03a6's except a negligible fraction, the following property holds: For every y having a representation y = \u03a6x0 by a coefficient vector x0 \u2208 \u211dm with fewer than \u03c1 \u00b7 n nonzeros, the solution x1 of the \ud835\udcc11\u2010minimization problem $${\\rm min} \\|x\\|_{1} \\;\\;{subject \\; to}\\;\\; \\Phi x = y$$ is unique and equal to x0. In contrast, heuristic attempts to sparsely solve such systems\u2014greedy algorithms and thresholding\u2014perform poorly in this challenging setting. The techniques include the use of random proportional embeddings and almost\u2010spherical sections in Banach space theory, and deviation bounds for the eigenvalues of random Wishart matrices. \u00a9 2006 Wiley Periodicals, Inc."
            },
            "slug": "For-most-large-underdetermined-systems-of-linear-is-Donoho",
            "title": {
                "fragments": [],
                "text": "For most large underdetermined systems of linear equations the minimal \ud835\udcc11\u2010norm solution is also the sparsest solution"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The techniques include the use of random proportional embeddings and almost\u2010spherical sections in Banach space theory, and deviation bounds for the eigenvalues of random Wishart matrices."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788107"
                        ],
                        "name": "J. Tropp",
                        "slug": "J.-Tropp",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Tropp",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tropp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 206
                            }
                        ],
                        "text": "Previous approaches in vision to joint feature learning have employed a greedy boosting approach [20]; our joint regularization exploits a norm derived from simultaneous sparse signal approximation methods [21], leading to an optimization problem that can be expressed as a linear"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "[4], who proposed the use of a representation based on kernel distances to unlabeled datapoints, and the work of Tropp [21] on simultaneous sparse approximation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Tropp proposed a joint optimization with a shared regularization norm over the coefficients of each signal and gave theoretical guarantees for this approach."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "Instead of solving it directly we use a convex relaxation of the r0 pseudo-norm suggested in the literature of simultaneous sparse approximation [21], the (l1, l\u221e) norm, which takes the following form:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 162
                            }
                        ],
                        "text": "Our approach builds on the work of Balcan et al. [4], who proposed the use of a representation based on kernel distances to unlabeled datapoints, and the work of Tropp [21] on simultaneous sparse approximation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 79
                            }
                        ],
                        "text": "In our sparse transfer prototype algorithm we make use of the norm proposed by Tropp and Obozinski et al., but we use it to learn a sparse set of prototypes that are discriminative in a given domain."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2478002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61017d4ea4aaa53f78b4dddbf29db65d62d9f811",
            "isKey": true,
            "numCitedBy": 744,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Algorithms-for-simultaneous-sparse-approximation.-Tropp",
            "title": {
                "fragments": [],
                "text": "Algorithms for simultaneous sparse approximation. Part II: Convex relaxation"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117194380"
                        ],
                        "name": "Xinlei",
                        "slug": "Xinlei",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Xinlei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinlei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35384453"
                        ],
                        "name": "I. Edward",
                        "slug": "I.-Edward",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Edward",
                            "middleNames": [
                                "Josef",
                                "Matheus"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Edward"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 206
                            }
                        ],
                        "text": "Most transfer learning techniques can be broadly grouped into two categories: learning intermediate representations [19, 5, 16, 2], and learning small sets of relevant features that are shared across tasks [11, 13, 3, 1, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11205939,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8bd3cd6bd1961d49b699c60fe0d5b619157c2ea9",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "For the problem of variable selection in generalized linear models, we develop various adaptive Bayesian criteria. Using a hierarchical mixture setup for model uncertainty, combined with an integrated Laplace approximation, we derive Empirical Bayes and Fully Bayes criteria that can be computed easily and quickly. The performance of these criteria is assessed via simulation and compared to other criteria such as AIC and BIC on normal, logistic and Poisson regression model classes. A Fully Bayes criterion based on a restricted region hyperprior seems to be the most promising."
            },
            "slug": "A-Hierarchical-Bayes-Approach-to-Variable-Selection-Xinlei-Edward",
            "title": {
                "fragments": [],
                "text": "A Hierarchical Bayes Approach to Variable Selection for Generalized Linear Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Authorized licensed use limited to: International Computer Science Inst (ICSI) Downloaded on May 12"
            },
            "venue": {
                "fragments": [],
                "text": "Authorized licensed use limited to: International Computer Science Inst (ICSI) Downloaded on May 12"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 134
                            }
                        ],
                        "text": "Recent progress has shown that visual category recognition can improve with the use of kernels that are optimized to particular tasks [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning the discriminative powerinvariance trade-off"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of ICCV,"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the experimental Section we show the advantage of our approach compared to the low rank method."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 173
                            }
                        ],
                        "text": "\u2022 Project all points xki in C to the prototype space: z(xki ) = A\n\"\u03d5(xki ) where \u03d5(x) = [k(x, x1), . . . , k(x, xp)]\", xi \u2208 U\nStep 2: Discover relevant prototypes by joint sparse approximation Let W be a p \u00d7 m matrix where Wjk corresponds to the j-th coefficient of the k-th problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "One-shot learning of object categories. Pattern Analysis and Machine Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "One-shot learning of object categories. Pattern Analysis and Machine Intelligence"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Identifying semantically equivalent object fragments For most large underdetermined systems of linear equations the minimal l 1norm solution is also the sparsest solution"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 25,
            "methodology": 10,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 28,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Transfer-learning-for-image-classification-with-Quattoni-Collins/953e2cfa58679ff6ea8c0bb432afd641f15d3657?sort=total-citations"
}