{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143913738"
                        ],
                        "name": "Santiago Fern\u00e1ndez",
                        "slug": "Santiago-Fern\u00e1ndez",
                        "structuredName": {
                            "firstName": "Santiago",
                            "lastName": "Fern\u00e1ndez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Santiago Fern\u00e1ndez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 61
                            }
                        ],
                        "text": "The connectionist temporal classification (CTC) output layer (Graves et al., 2006) removes the need for hidden Markov models by directly training RNNs to label sequences with unknown alignments, using a single discriminative loss function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 139
                            }
                        ],
                        "text": ", 2007; Chen and Chaudhari, 2005), music generation (Eck and Schmidhuber, 2002), reinforcement learning (Bakker, 2002), speech recognition (Graves and Schmidhuber, 2005b; Graves et al., 2006) and handwriting recognition (Liwicki et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 115
                            }
                        ],
                        "text": "This chapter introduces the connectionist temporal classification (CTC) output layer for recurrent neural networks (Graves et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9901844,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96494e722f58705fa20302fe6179d483f52705b4",
            "isKey": false,
            "numCitedBy": 3467,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN."
            },
            "slug": "Connectionist-temporal-classification:-labelling-Graves-Fern\u00e1ndez",
            "title": {
                "fragments": [],
                "text": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems of sequence learning and post-processing."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143913738"
                        ],
                        "name": "Santiago Fern\u00e1ndez",
                        "slug": "Santiago-Fern\u00e1ndez",
                        "structuredName": {
                            "firstName": "Santiago",
                            "lastName": "Fern\u00e1ndez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Santiago Fern\u00e1ndez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1668634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12496bf48ebdb5ab3c92bc911d6ee42369fa70bc",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Modelling data in structured domains requires establishing the relations among patterns at multiple scales. When these patterns arise from sequential data, the multiscale structure also contains a dynamic component that must be modelled, particularly, as is often the case, if the data is unsegmented. Probabilistic graphical models are the predominant framework for labelling unsegmented sequential data in structured domains. Their use requires a certain degree of a priori knowledge about the relations among patterns and about the patterns themselves. This paper presents a hierarchical system, based on the connectionist temporal classification algorithm, for labelling unsegmented sequential data at multiple scales with recurrent neural networks only. Experiments on the recognition of sequences of spoken digits show that the system outperforms hidden Markov models, while making fewer assumptions about the domain."
            },
            "slug": "Sequence-Labelling-in-Structured-Domains-with-Fern\u00e1ndez-Graves",
            "title": {
                "fragments": [],
                "text": "Sequence Labelling in Structured Domains with Hierarchical Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper presents a hierarchical system, based on the connectionist temporal classification algorithm, for labelling unsegmented sequential data at multiple scales with recurrent neural networks only and shows that the system outperforms hidden Markov models, while making fewer assumptions about the domain."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087902035"
                        ],
                        "name": "Patrice Simardy",
                        "slug": "Patrice-Simardy",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simardy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrice Simardy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087902026"
                        ],
                        "name": "Paolo FrasconizyAT",
                        "slug": "Paolo-FrasconizyAT",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "FrasconizyAT",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paolo FrasconizyAT"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1308459,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65c0712514efa9139fce00bb17362c6cc0779950",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks can be used to map input sequences to output sequences , such as for recognition, production or prediction problems. However, practical diiculties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly diicult problem as the duration of the dependencies to be captured increases. These results expose a trade-oo between eecient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered."
            },
            "slug": "Learning-Long-Term-Dependencies-with-Simardy-FrasconizyAT",
            "title": {
                "fragments": [],
                "text": "Learning Long-Term Dependencies with"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work shows why gradient based learning algorithms face an increasingly diicult problem as the duration of the dependencies to be captured increases, and exposes a trade-oo between eecient learning by gradient descent and latching on information for long periods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 130
                            }
                        ],
                        "text": "The two components in a the hybrid can be trained independently, but many authors have proposed methods for combined optimisation (Bengio et al., 1992; Bourlard et al., 1996; Hennebert et al., 1997; Trentin and Gori, 2003) which typically yields improved results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206457500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0be39ee052d246ae99c082a565aba25b811be2d",
            "isKey": false,
            "numCitedBy": 6141,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered."
            },
            "slug": "Learning-long-term-dependencies-with-gradient-is-Bengio-Simard",
            "title": {
                "fragments": [],
                "text": "Learning long-term dependencies with gradient descent is difficult"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work shows why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases, and exposes a trade-off between efficient learning by gradient descent and latching on information for long periods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143913738"
                        ],
                        "name": "Santiago Fern\u00e1ndez",
                        "slug": "Santiago-Fern\u00e1ndez",
                        "structuredName": {
                            "firstName": "Santiago",
                            "lastName": "Fern\u00e1ndez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Santiago Fern\u00e1ndez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "LSTM01 LSTM\u539f\u7406\u4e0e\u5e94\u7528\u53ca\u8bad\u7ec3\u65b9\u6cd5_Constant dripping wears \u2026Accepted papers | EMNLP 2021roberta-base \u00b7 Hugging Face\u6709\u54ea\u4e9bLSTM(Long Short Term Memory)\u548cRNN(Recurrent)\u7f51\u7edc\u7684 \u2026(PDF) Deep Learning - ResearchGateFake news detection: A hybrid CNN-RNN based deep learning GitHub - shawnyuen IEEE ICIP 2021 || Anchorage, Alaska, USA || 19-22 30th ACM International Conference on Information and RNN vs LSTM vs GRU -- \u8be5\u9009\u54ea\u4e2a\uff1f - \u77e5\u4e4e - \u77e5\u4e4e\u4e13\u680fRecurrent neural network - Wikipedia\u4e09\u6b21\u7b80\u5316\u4e00\u5f20\u56fe\uff1a\u4e00\u62db\u7406\u89e3LSTM/GRU\u95e8\u63a7\u673a\u5236 - \u77e5\u4e4eDeep learning in remote sensing applications: A meta Turkish Journal of Physiotherapy and Rehabilitation \u3010\u8bba\u6587\u9605\u8bfb\u7b14\u8bb0\u3011NeurIPS2020\u6587\u7ae0\u5217\u8868Part1_zincrain\u7684\u535a\u5ba2 \u2026Text Classification | Papers With CodeGo-to Guide for Text Classification with Machine LearningGlobal quantification of mammalian gene expression control distilbert-base-uncased \u00b7 Hugging FaceA Gentle Introduction to RNN UnrollingSequence Modeling with CTC - Distill\nMutualNet: Adaptive ConvNet via Mutual Learning from Network Width and Resolution \u6587\u7ae0\u76ee\u5f55\u6458\u89811."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Unlike standard feedforward neural networks, LSTM has feedback connections."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 22
                            }
                        ],
                        "text": "Multidimensional LSTM (Graves et al., 2007) brings the improved memory of LSTM to multidimensional networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "A Beginner's Guide to LSTMs and Recurrent Neural Networks Introduction to Semi-Supervised Learning | Synthesis Sequence Classification with LSTM Recurrent Neural Deep Neural Networks - TutorialspointAccurate and efficient time-domain classification with Long short-term memory - WikipediaCTC (Connectionist Temporal Classification) \u7b97\u6cd5\u539f\u7406 FROM Pre-trained Word Embeddings TO Pre-trained Language Supervised Sequence Labelling with Recurrent Neural \u2026NLP: Pretrained Named Entity Recognition (NER) | by Artificial Intelligence and COVID-19: Deep Learning GitHub - ziyujia/PhysiologicalSignal-Classification What Is Machine Learning and Why Is It Important?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "In this post you discovered how to develop LSTM network models for sequence classification predictive modeling \u2026 (\u539f\u521b\u6587\u7ae0\uff0c\u8f6c\u8f7d\u8bf7\u6ce8\u660e\u51fa\u5904\u54e6~) \u7b80\u5355\u4ecb\u7ecdctc\u7b97\u6cd5 ctc\u662f\u5e8f\u5217\u6807\u6ce8\u95ee\u9898\u4e2d\u7684\u4e00\u79cd\u635f\u5931\u51fd\u6570\u3002 \u4f20\u7edf\u5e8f\u5217\u6807\u6ce8\u7b97\u6cd5\u9700\u8981\u6bcf\u4e00\u65f6\u523b\u8f93\u5165\u4e0e\u8f93\u51fa\u7b26\u53f7\u5b8c\u5168\u5bf9\u9f50\u3002\u800cctc\u6269\u5c55\u4e86\u6807\u7b7e\u96c6\u5408\uff0c\u6dfb\u52a0\u7a7a\u5143\u7d20\u3002 \u5728\u4f7f\u7528\u6269\u5c55\u6807\u7b7e\u96c6 Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "Simple Conversational Data Augmentation for Semi-supervised Abstractive Dialogue \u2026 [2] Supervised Sequence Labelling with Recurrent Neural Networks [3] LSTM\uff1aRNN\u6700\u5e38\u7528\u7684\u53d8\u4f53 [4] Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation [5] Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling Sep 22, 2021 \u00b7 2021 IEEE International Conference on Image Processing 19-22 September 2021 \u2022 Anchorage, Alaska, USA Imaging Without Borders Mar 19, 2020 \u00b7 \u2014 Supervised Sequence Labelling with Recurrent Neural Networks, 2008."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 66
                            }
                        ],
                        "text": "This chapter describes multidimensional recurrent neural networks (MDRNNs; Graves et al., 2007), a special case of directed acyclic graph RNNs (DAG-RNNs; Baldi and Pollastri, 2003), extend the potential applicability of RNNs to vision, video processing, medical imaging and many other areas, while avoiding the scaling problems that have plagued other multidimensional models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 168
                            }
                        ],
                        "text": "BERT uses a bidirectional Transformer vs. GPT uses a left-to-right Transformer vs. ELMo uses the concatenation of independently trained left-to-right and right-to-left LSTM to generate features for downstream task."
                    },
                    "intents": []
                }
            ],
            "corpusId": 47136048,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f70ae50828e3b6166628f5e8edb239b1cca6b471",
            "isKey": false,
            "numCitedBy": 256,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks (RNNs) have proved effective at one dimensional sequence learning tasks, such as speech and online handwriting recognition. Some of the properties that make RNNs suitable for such tasks, for example robustness to input warping, and the ability to access contextual information, are also desirable in multi-dimensional domains. However, there has so far been no direct way of applying RNNs to data with more than one spatio-temporal dimension. This paper introduces multi-dimensional recurrent neural networks, thereby extending the potential applicability of RNNs to vision, video processing, medical imaging and many other areas, while avoiding the scaling problems that have plagued other multi-dimensional models. Experimental results are provided for two image segmentation tasks."
            },
            "slug": "Multi-dimensional-Recurrent-Neural-Networks-Graves-Fern\u00e1ndez",
            "title": {
                "fragments": [],
                "text": "Multi-dimensional Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Multi-dimensional recurrent neural networks are introduced, thereby extending the potential applicability of RNNs to vision, video processing, medical imaging and many other areas, while avoiding the scaling problems that have plagued other multi-dimensional models."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144927151"
                        ],
                        "name": "M. Schuster",
                        "slug": "M.-Schuster",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48099761"
                        ],
                        "name": "K. Paliwal",
                        "slug": "K.-Paliwal",
                        "structuredName": {
                            "firstName": "Kuldip",
                            "lastName": "Paliwal",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Paliwal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 40
                            }
                        ],
                        "text": "Bidirectional recurrent neural networks (BRNNs; Schuster and Paliwal, 1997; Schuster, 1999; Baldi et al., 1999) offer a more elegant solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 19
                            }
                        ],
                        "text": "Bidirectional RNNs (Schuster and Paliwal, 1997) scan the data forwards and backwards with two separate recurrent layers, thereby removing the asymmetry between input directions and providing access to all surrounding context."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18375389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e23c34414e66118ecd9b08cf0cd4d016f59b0b85",
            "isKey": false,
            "numCitedBy": 5372,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported."
            },
            "slug": "Bidirectional-recurrent-neural-networks-Schuster-Paliwal",
            "title": {
                "fragments": [],
                "text": "Bidirectional recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 92
                            }
                        ],
                        "text": ", 1996; Plate, 1993) or time constants (Mozer, 1992), and hierarchical sequence compression (Schmidhuber, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18271205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50c770b425a5bb25c77387f687a9910a9d130722",
            "isKey": false,
            "numCitedBy": 428,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous neural network learning algorithms for sequence processing are computationally expensive and perform poorly when it comes to long time lags. This paper first introduces a simple principle for reducing the descriptions of event sequences without loss of information. A consequence of this principle is that only unexpected inputs can be relevant. This insight leads to the construction of neural architectures that learn to divide and conquer by recursively decomposing sequences. I describe two architectures. The first functions as a self-organizing multilevel hierarchy of recurrent networks. The second, involving only two recurrent networks, tries to collapse a multilevel predictor hierarchy into a single recurrent net. Experiments show that the system can require less computation per time step and many fewer training sequences than conventional training algorithms for recurrent nets."
            },
            "slug": "Learning-Complex,-Extended-Sequences-Using-the-of-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning Complex, Extended Sequences Using the Principle of History Compression"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A simple principle for reducing the descriptions of event sequences without loss of information is introduced and this insight leads to the construction of neural architectures that learn to divide and conquer by recursively decomposing sequences."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143684647"
                        ],
                        "name": "B. Hammer",
                        "slug": "B.-Hammer",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Hammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hammer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 215
                            }
                        ],
                        "text": "Indeed, the equivalent result to the universal approximation theory for MLPs is that an RNN with a sufficient number of hidden units can approximate any measurable sequence-to-sequence mapping to arbitrary accuracy (Hammer, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15352446,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cd938d610d57d056f8c980cba9ae9f5dff21eca",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recurrent-networks-for-structured-data-\u2013-A-unifying-Hammer",
            "title": {
                "fragments": [],
                "text": "Recurrent networks for structured data \u2013 A unifying approach and its properties"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Systems Research"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3234592"
                        ],
                        "name": "N. Beringer",
                        "slug": "N.-Beringer",
                        "structuredName": {
                            "firstName": "Nicole",
                            "lastName": "Beringer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Beringer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055652031"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "Juergen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15515157,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a51dc0b5694af3e54393e20e05f42fc3cbe476b",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A system that could be quickly retrained on different corpora would be of great benefit to speech recognition. Recurrent Neural Networks (RNNs) are able to transfer knowledge by simply storing and then retraining their weights. In this report, we partition the TIDIGITS database into utterances spoken by men, women, boys and girls, and successively retrain a Long Short Term Memory (LSTM) RNN on them. We find that the network rapidly adapts to new subsets of the data, and achieves greater accuracy than when trained on them from scratch. This would be useful for applications requiring either cross corpus adaptation or continually expanding datasets."
            },
            "slug": "Rapid-Retraining-on-Speech-Data-with-LSTM-Recurrent-Graves-Beringer",
            "title": {
                "fragments": [],
                "text": "Rapid Retraining on Speech Data with LSTM Recurrent Networks."
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This report partitions the TIDIGITS database into utterances spoken by men, women, boys and girls, and successively retrain a Long Short Term Memory (LSTM) RNN on them, and finds that the network rapidly adapts to new subsets of the data, and achieves greater accuracy than when trained on them from scratch."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 150
                            }
                        ],
                        "text": "Over the past decade, LSTM has proved successful at a range of synthetic tasks requiring long range memory, including learning context free languages (Gers and Schmidhuber, 2001), recalling high precision real numbers over extended noisy sequences (Hochreiter and Schmidhuber, 1997) and various tasks requiring precise timing and counting (Gers et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 118
                            }
                        ],
                        "text": "In various synthetic tasks, LSTM has been shown capable of storing and accessing information over very long timespans (Gers et al., 2002; Gers and Schmidhuber, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10192330,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f828b401c86e0f8fddd8e77774e332dfd226cb05",
            "isKey": false,
            "numCitedBy": 585,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work on learning regular languages from exemplary training sequences showed that long short-term memory (LSTM) outperforms traditional recurrent neural networks (RNNs). We demonstrate LSTMs superior performance on context-free language benchmarks for RNNs, and show that it works even better than previous hardwired or highly specialized architectures. To the best of our knowledge, LSTM variants are also the first RNNs to learn a simple context-sensitive language, namely a(n)b(n)c(n)."
            },
            "slug": "LSTM-recurrent-networks-learn-simple-context-free-Gers-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "LSTM recurrent networks learn simple context-free and context-sensitive languages"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Long short-term memory (LSTM) variants are also the first RNNs to learn a simple context-sensitive language, namely a(n)b( n)c(n)."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 48
                            }
                        ],
                        "text": ", 2000), along with additional peephole weights (Gers et al., 2002) connecting the gates to the memory cell were added later to give extended LSTM (Gers, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 358,
                                "start": 339
                            }
                        ],
                        "text": "Over the past decade, LSTM has proved successful at a range of synthetic tasks requiring long range memory, including learning context free languages (Gers and Schmidhuber, 2001), recalling high precision real numbers over extended noisy sequences (Hochreiter and Schmidhuber, 1997) and various tasks requiring precise timing and counting (Gers et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 118
                            }
                        ],
                        "text": "In various synthetic tasks, LSTM has been shown capable of storing and accessing information over very long timespans (Gers et al., 2002; Gers and Schmidhuber, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 474078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "047655e733a9eed9a500afd916efa566915b9110",
            "isKey": false,
            "numCitedBy": 1270,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by \"peephole connections\" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars. Without external resets or teacher forcing, our LSTM variant also learns to generate stable streams of precisely timed spikes and other highly nonlinear periodic patterns. This makes LSTM a promising approach for tasks that require the accurate measurement or generation of time intervals."
            },
            "slug": "Learning-Precise-Timing-with-LSTM-Recurrent-Gers-Schraudolph",
            "title": {
                "fragments": [],
                "text": "Learning Precise Timing with LSTM Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work finds that LSTM augmented by \"peephole connections\" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6197973,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5eb96540ef53b49eac2246d6b13635fe6e54451",
            "isKey": false,
            "numCitedBy": 467,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": "A structured organization of information is typically required by symbolic processing. On the other hand, most connectionist models assume that data are organized according to relatively poor structures, like arrays or sequences. The framework described in this paper is an attempt to unify adaptive models like artificial neural nets and belief nets for the problem of processing structured information. In particular, relations between data variables are expressed by directed acyclic graphs, where both numerical and categorical values coexist. The general framework proposed in this paper can be regarded as an extension of both recurrent neural networks and hidden Markov models to the case of acyclic graphs. In particular we study the supervised learning problem as the problem of learning transductions from an input structured space to an output structured space, where transductions are assumed to admit a recursive hidden statespace representation. We introduce a graphical formalism for representing this class of adaptive transductions by means of recursive networks, i.e., cyclic graphs where nodes are labeled by variables and edges are labeled by generalized delay elements. This representation makes it possible to incorporate the symbolic and subsymbolic nature of data. Structures are processed by unfolding the recursive network into an acyclic graph called encoding network. In so doing, inference and learning algorithms can be easily inherited from the corresponding algorithms for artificial neural networks or probabilistic graphical model."
            },
            "slug": "A-general-framework-for-adaptive-processing-of-data-Frasconi-Gori",
            "title": {
                "fragments": [],
                "text": "A general framework for adaptive processing of data structures"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The framework described in this paper is an attempt to unify adaptive models like artificial neural nets and belief nets for the problem of processing structured information, where relations between data variables are expressed by directed acyclic graphs, where both numerical and categorical values coexist."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 172
                            }
                        ],
                        "text": "Hybrids of hidden Markov models (HMMs) and artificial neural networks (ANNs) were proposed by several researchers in the 1990s as a way of overcoming the drawbacks of HMMs (Bourlard and Morgan, 1994; Bengio, 1993; Renals et al., 1993; Robinson, 1994; Bengio, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207116557,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb404305aca0c9ea6195bf1d918efe13e5301a90",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The task discussed in this paper is that of learning to map input sequences to output sequences. In particular, problems of phoneme recognition in continuous speech are considered, but most of the discussed techniques could be applied to other tasks, such as the recognition of sequences of handwritten characters. The systems considered in this paper are based on connectionist models, or artificial neural networks, sometimes combined with statistical techniques for recognition of sequences of patterns, stressing the integration of prior knowledge and learning. Different architectures for sequence and speech recognition are reviewed, including recurrent networks as well as hybrid systems involving hidden Markov models."
            },
            "slug": "A-Connectionist-Approach-to-Speech-Recognition-Bengio",
            "title": {
                "fragments": [],
                "text": "A Connectionist Approach to Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Different architectures for sequence and speech recognition are reviewed, including recurrent networks as well as hybrid systems involving hidden Markov models, sometimes combined with statistical techniques for recognition of sequences of patterns."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780235"
                        ],
                        "name": "Fred Cummins",
                        "slug": "Fred-Cummins",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Cummins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fred Cummins"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 17
                            }
                        ],
                        "text": "The forget gates (Gers et al., 2000), along with additional peephole weights (Gers et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11598600,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11540131eae85b2e11d53df7f1360eeb6476e7f4",
            "isKey": false,
            "numCitedBy": 3203,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Long short-term memory (LSTM; Hochreiter & Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive forget gate that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way."
            },
            "slug": "Learning-to-Forget:-Continual-Prediction-with-LSTM-Gers-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning to Forget: Continual Prediction with LSTM"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This work identifies a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset, and proposes a novel, adaptive forget gate that enables an LSTm cell to learn to reset itself at appropriate times, thus releasing internal resources."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1856462,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f83f6e1afadf0963153974968af6b8342775d82",
            "isKey": false,
            "numCitedBy": 3295,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Framewise-phoneme-classification-with-bidirectional-Graves-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115356552"
                        ],
                        "name": "Tsungnan Lin",
                        "slug": "Tsungnan-Lin",
                        "structuredName": {
                            "firstName": "Tsungnan",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsungnan Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35216199"
                        ],
                        "name": "B. Horne",
                        "slug": "B.-Horne",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Horne",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Horne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4023505"
                        ],
                        "name": "P. Ti\u0148o",
                        "slug": "P.-Ti\u0148o",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Ti\u0148o",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Ti\u0148o"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 43
                            }
                        ],
                        "text": ", 1994), explicitly introduced time delays (Lang et al., 1990; Lin et al., 1996; Plate, 1993) or time constants (Mozer, 1992), and hierarchical sequence compression (Schmidhuber, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6638216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6e91c9e7e8f8a577a98ecfcfa998212a683195a",
            "isKey": false,
            "numCitedBy": 629,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "It has previously been shown that gradient-descent learning algorithms for recurrent neural networks can perform poorly on tasks that involve long-term dependencies, i.e. those problems for which the desired output depends on inputs presented at times far in the past. We show that the long-term dependencies problem is lessened for a class of architectures called nonlinear autoregressive models with exogenous (NARX) recurrent neural networks, which have powerful representational capabilities. We have previously reported that gradient descent learning can be more effective in NARX networks than in recurrent neural network architectures that have \"hidden states\" on problems including grammatical inference and nonlinear system identification. Typically, the network converges much faster and generalizes better than other networks. The results in this paper are consistent with this phenomenon. We present some experimental results which show that NARX networks can often retain information for two to three times as long as conventional recurrent neural networks. We show that although NARX networks do not circumvent the problem of long-term dependencies, they can greatly improve performance on long-term dependency problems. We also describe in detail some of the assumptions regarding what it means to latch information robustly and suggest possible ways to loosen these assumptions."
            },
            "slug": "Learning-long-term-dependencies-in-NARX-recurrent-Lin-Horne",
            "title": {
                "fragments": [],
                "text": "Learning long-term dependencies in NARX recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that the long-term dependencies problem is lessened for a class of architectures called nonlinear autoregressive models with exogenous (NARX) recurrent neural networks, which have powerful representational capabilities."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3318461"
                        ],
                        "name": "M. Gagliolo",
                        "slug": "M.-Gagliolo",
                        "structuredName": {
                            "firstName": "Matteo",
                            "lastName": "Gagliolo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gagliolo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 80
                            }
                        ],
                        "text": "Recently, non gradient-based training methods of LSTM have also been considered (Wierstra et al., 2005; Schmidhuber et al., 2007), but they are outside the scope of this book."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11745761,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75479012461814fd176556a56b32c2392462aef5",
            "isKey": false,
            "numCitedBy": 242,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, gradient-based LSTM recurrent neural networks (RNNs) solved many previously RNN-unlearnable tasks. Sometimes, however, gradient information is of little use for training RNNs, due to numerous local minima. For such cases, we present a novel method: EVOlution of systems with LINear Outputs (Evolino). Evolino evolves weights to the nonlinear, hidden nodes of RNNs while computing optimal linear mappings from hidden state to output, using methods such as pseudo-inverse-based linear regression. If we instead use quadratic programming to maximize the margin, we obtain the first evolutionary recurrent support vector machines. We show that Evolino-based LSTM can solve tasks that Echo State nets (Jaeger, 2004a) cannot and achieves higher accuracy in certain continuous function generation tasks than conventional gradient descent RNNs, including gradient-based LSTM."
            },
            "slug": "Training-Recurrent-Networks-by-Evolino-Schmidhuber-Wierstra",
            "title": {
                "fragments": [],
                "text": "Training Recurrent Networks by Evolino"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that Evolino-based LSTM can solve tasks that Echo State nets cannot and achieves higher accuracy in certain continuous function generation tasks than conventional gradient descent RNNs, including gradient-basedLSTM."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 85
                            }
                        ],
                        "text": "The approach favoured by this book is the Long Short-Term Memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 23
                            }
                        ],
                        "text": "Long Short-Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) is a redesign of the RNN architecture around special \u2018memory cell\u2019 units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 37
                            }
                        ],
                        "text": "The original LSTM training algorithm (Hochreiter and Schmidhuber, 1997) used an approximate error gradient calculated with a combination of Real Time Recurrent Learning (RTRL; Robinson and Fallside, 1987) and Backpropagation Through Time (BPTT; Williams and Zipser, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 248
                            }
                        ],
                        "text": "Over the past decade, LSTM has proved successful at a range of synthetic tasks requiring long range memory, including learning context free languages (Gers and Schmidhuber, 2001), recalling high precision real numbers over extended noisy sequences (Hochreiter and Schmidhuber, 1997) and various tasks requiring precise timing and counting (Gers et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": true,
            "numCitedBy": 51648,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 36
                            }
                        ],
                        "text": ", 2006) and handwriting recognition (Liwicki et al., 2007; Graves et al., 2008)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5756363,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0aca3246845016bd8dc996944476f3dd5a5ba56",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In online handwriting recognition the trajectory of the pen is recorded during writing. Although the trajectory provides a compact and complete representation of the written output, it is hard to transcribe directly, because each letter is spread over many pen locations. Most recognition systems therefore employ sophisticated preprocessing techniques to put the inputs into a more localised form. However these techniques require considerable human effort, and are specific to particular languages and alphabets. This paper describes a system capable of directly transcribing raw online handwriting data. The system consists of an advanced recurrent neural network with an output layer designed for sequence labelling, combined with a probabilistic language model. In experiments on an unconstrained online database, we record excellent results using either raw or preprocessed data, well outperforming a state-of-the-art HMM based system in both cases."
            },
            "slug": "Unconstrained-Online-Handwriting-Recognition-with-Graves-Liwicki",
            "title": {
                "fragments": [],
                "text": "Unconstrained Online Handwriting Recognition with Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A system capable of directly transcribing raw online handwriting data is described, consisting of an advanced recurrent neural network with an output layer designed for sequence labelling, combined with a probabilistic language model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144927151"
                        ],
                        "name": "M. Schuster",
                        "slug": "M.-Schuster",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schuster"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 56
                            }
                        ],
                        "text": ", 2001; Chen and Chaudhari, 2004) and speech processing (Schuster, 1999; Fukada et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 40
                            }
                        ],
                        "text": "Bidirectional recurrent neural networks (BRNNs; Schuster and Paliwal, 1997; Schuster, 1999; Baldi et al., 1999) offer a more elegant solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 35
                            }
                        ],
                        "text": "Network Training Set Test Set BRNN (Schuster, 1999) 17."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60987529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebb53aebcbc8edf6c52d94d5f75f16d1c8cf88f2",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 129,
            "paperAbstract": {
                "fragments": [],
                "text": "Many problems of engineering interest, for example speech recognition, can be formulated in an abstract sense as supervised learning from sequential data, where an input sequence x T 1 = fx 1 ;x 2 ;x 3 ; : : : ;x T 1 ;x T g has to be mapped to an output sequence y T 1 = fy 1 ;y 2 ;y 3 ; : : : ;y T 1 ;y T g. This thesis gives a uni ed view of the abstract problem and presents some models and algorithms for improved sequence recognition and modeling performance, measured on synthetic data and on real speech data. A powerful neural network structure to deal with sequential data is the recurrent neural network (RNN), which allows one to estimate P (y t jx 1 ;x 2 ; : : : ;x t ), the output probability distribution at time t given all previous input. The rst part of this thesis presents various extensions to the basic RNN structure, which are a) a bidirectional recurrent neural network (BRNN), which allows the estimation of expressions of the form P (y t jx T 1 ), the output at t given all sequential input, for uni-modal regression and classi cation problems, b) an extended BRNN to directly estimate the posterior probability of a symbol sequence, P (y T 1 jx T 1 ), by modeling P (y t jy t 1 ;y t 2 ; : : : ;y 1 ;x T 1 ) without explicit assumptions about the shape of the distribution P (y T 1 jx T 1 ), c) a BRNN to model multi-modal input data that can be described by Gaussian mixture distributions conditioned on an output vector sequence, P (x t jy T 1 ), assuming that neighboring x t ;x t+1 are conditionally independent, and d) an extension to c) which removes the independence assumption by modeling P (x t jx t 1 ;x t 2 ; : : : ;x 1 ;y T 1 ) to estimate the likelihood P (x T 1 jy T 1 ) of a given output sequence without any explicit approximations about the use of context. The second part of this thesis describes the details of a fast and memory-e cient one-pass stack decoder for speech recognition to perform the search for the most probable word sequence. The use of this decoder, which can handle arbitrary order N-gram language models and arbitrary order context-dependent acoustic models with full crossword expansion, led to the best reported recognition results on the standard test set of a widely used Japanese newspaper dictation task."
            },
            "slug": "On-supervised-learning-from-sequential-data-with-Schuster",
            "title": {
                "fragments": [],
                "text": "On supervised learning from sequential data with applications for speech regognition"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "The use of this decoder, which can handle arbitrary order N-gram language models and arbitrary order context-dependent acoustic models with full crossword expansion, led to the best reported recognition results on the standard test set of a widely used Japanese newspaper dictation task."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 80
                            }
                        ],
                        "text": "Recently, non gradient-based training methods of LSTM have also been considered (Wierstra et al., 2005; Schmidhuber et al., 2007), but they are outside the scope of this book."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 786647,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a149d240ecde338e91ccb2f001074b792be070b2",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Existing Recurrent Neural Networks (RNNs) are limited in their ability to model dynamical systems with nonlinearities and hidden internal states. Here we use our general framework for sequence learning, EVOlution of recurrent systems with LINear Outputs (Evolino), to discover good RNN hidden node weights through evolution, while using linear regression to compute an optimal linear mapping from hidden state to output. Using the Long Short-Term Memory RNN Architecture, Evolino outperforms previous state-of-the-art methods on several tasks: 1) context-sensitive languages, 2) multiple superimposed sine waves."
            },
            "slug": "Modeling-systems-with-internal-state-using-evolino-Wierstra-Gomez",
            "title": {
                "fragments": [],
                "text": "Modeling systems with internal state using evolino"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work uses the general framework for sequence learning, EVOlution of recurrent systems with LINear Outputs (Evolino), to discover good RNN hidden node weights through evolution, while using linear regression to compute an optimal linear mapping from hidden state to output."
            },
            "venue": {
                "fragments": [],
                "text": "GECCO '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2018972"
                        ],
                        "name": "T. Thireou",
                        "slug": "T.-Thireou",
                        "structuredName": {
                            "firstName": "Trias",
                            "lastName": "Thireou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Thireou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686669"
                        ],
                        "name": "M. Reczko",
                        "slug": "M.-Reczko",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Reczko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Reczko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 64
                            }
                        ],
                        "text": "It has proved especially popular in the field of bioinformatics (Chen and Chaudhari, 2005; Thireou and Reczko, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11787259,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2985b87fc977d8a893da6086b2a18298b9650d96",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm called bidirectional long short-term memory networks (BLSTM) for processing sequential data is introduced. This supervised learning method trains a special recurrent neural network to use very long-range symmetric sequence context using a combination of nonlinear processing elements and linear feedback loops for storing long-range context. The algorithm is applied to the sequence-based prediction of protein localization and predicts 93.3 percent novel nonplant proteins and 88.4 percent novel plant proteins correctly, which is an improvement over feedforward and standard recurrent networks solving the same problem. The BLSTM system is available as a Web service at http://stepc.stepc.gr/-synaptic/blstm.html."
            },
            "slug": "Bidirectional-Long-Short-Term-Memory-Networks-for-Thireou-Reczko",
            "title": {
                "fragments": [],
                "text": "Bidirectional Long Short-Term Memory Networks for Predicting the Subcellular Localization of Eukaryotic Proteins"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The BLSTM algorithm is applied to the sequence-based prediction of protein localization and predicts 93.3 percent novel nonplant proteins and 88.4 percent novel plant proteins correctly, which is an improvement over feedforward and standard recurrent networks solving the same problem."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE/ACM Transactions on Computational Biology and Bioinformatics"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145040409"
                        ],
                        "name": "Justin Bayer",
                        "slug": "Justin-Bayer",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Bayer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin Bayer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1810053"
                        ],
                        "name": "J. Togelius",
                        "slug": "J.-Togelius",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Togelius",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Togelius"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 189
                            }
                        ],
                        "text": "Indeed it has been shown that alternative structures with equally good performance on toy problems such as learning contextfree and context-sensitive languages can be evolved automatically (Bayer et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9803542,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b7405ab47e1b6686cebcbfb1f2efdbe431fc79c",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Long Short-Term Memory (LSTM) is one of the best recent supervised sequence learning methods. Using gradient descent, it trains memory cells represented as differentiable computational graph structures. Interestingly, LSTM's cell structure seems somewhat arbitrary. In this paper we optimize its computational structure using a multi-objective evolutionary algorithm. The fitness function reflects the structure's usefulness for learning various formal languages. The evolved cells help to understand crucial features that aid sequence learning."
            },
            "slug": "Evolving-Memory-Cell-Structures-for-Sequence-Bayer-Wierstra",
            "title": {
                "fragments": [],
                "text": "Evolving Memory Cell Structures for Sequence Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper optimize LSTM's computational structure using a multi-objective evolutionary algorithm, which reflects the structure's usefulness for learning various formal languages."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678493"
                        ],
                        "name": "A. Starita",
                        "slug": "A.-Starita",
                        "structuredName": {
                            "firstName": "Antonina",
                            "lastName": "Starita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Starita"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 67
                            }
                        ],
                        "text": "Various forms of DAG-RNN appeared in publications prior to Baldi\u2019s (Goller, 1997; Sperduti and Starita, 1997; Frasconi et al., 1998; Hammer, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5942593,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e33eca03933caaec671e20692e79d1acc9527e1",
            "isKey": false,
            "numCitedBy": 486,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard neural networks and statistical methods are usually believed to be inadequate when dealing with complex structures because of their feature-based approach. In fact, feature-based approaches usually fail to give satisfactory solutions because of the sensitivity of the approach to the a priori selection of the features, and the incapacity to represent any specific information on the relationships among the components of the structures. However, we show that neural networks can, in fact, represent and classify structured patterns. The key idea underpinning our approach is the use of the so called \"generalized recursive neuron\", which is essentially a generalization to structures of a recurrent neuron. By using generalized recursive neurons, all the supervised networks developed for the classification of sequences, such as backpropagation through time networks, real-time recurrent networks, simple recurrent networks, recurrent cascade correlation networks, and neural trees can, on the whole, be generalized to structures. The results obtained by some of the above networks (with generalized recursive neurons) on the classification of logic terms are presented."
            },
            "slug": "Supervised-neural-networks-for-the-classification-Sperduti-Starita",
            "title": {
                "fragments": [],
                "text": "Supervised neural networks for the classification of structures"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that neural networks can, in fact, represent and classify structured patterns and all the supervised networks developed for the classification of sequences can, on the whole, be generalized to structures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2028880"
                        ],
                        "name": "G. Pollastri",
                        "slug": "G.-Pollastri",
                        "structuredName": {
                            "firstName": "Gianluca",
                            "lastName": "Pollastri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Pollastri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1593083,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b7a0048801f9d43dc48a8f04367be813146b05a",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a general methodology for the design of large-scale recursive neural network architectures (DAG-RNNs) which comprises three fundamental steps: (1) representation of a given domain using suitable directed acyclic graphs (DAGs) to connect visible and hidden node variables; (2) parameterization of the relationship between each variable and its parent variables by feedforward neural networks; and (3) application of weight-sharing within appropriate subsets of DAG connections to capture stationarity and control model complexity. Here we use these principles to derive several specific classes of DAG-RNN architectures based on lattices, trees, and other structured graphs. These architectures can process a wide range of data structures with variable sizes and dimensions. While the overall resulting models remain probabilistic, the internal deterministic dynamics allows efficient propagation of information, as well as training by gradient descent, in order to tackle large-scale problems. These methods are used here to derive state-of-the-art predictors for protein structural features such as secondary structure (1D) and both fine- and coarse-grained contact maps (2D). Extensions, relationships to graphical models, and implications for the design of neural architectures are briefly discussed. The protein prediction servers are available over the Web at: www.igb.uci.edu/tools.htm ."
            },
            "slug": "The-Principled-Design-of-Large-Scale-Recursive-and-Baldi-Pollastri",
            "title": {
                "fragments": [],
                "text": "The Principled Design of Large-Scale Recursive Neural Network Architectures--DAG-RNNs and the Protein Structure Prediction Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "These methods are used to derive state-of-the-art predictors for protein structural features such as secondary structure and both fine- and coarse-grained contact maps and implications for the design of neural architectures are briefly discussed."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110094"
                        ],
                        "name": "J. Bridle",
                        "slug": "J.-Bridle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bridle",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bridle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 153
                            }
                        ],
                        "text": "For classification problems with K > 2 classes, the convention is to have K output units, and normalise the output activations with the softmax function (Bridle, 1990) to obtain the class probabilities:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 116
                            }
                        ],
                        "text": "For a sequence labelling task where the labels are drawn from an alphabet A, CTC consists of a softmax output layer (Bridle, 1990) with one more unit than there are labels in A."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59636530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f462943c8d0af69c12a09058251848324135e5a",
            "isKey": false,
            "numCitedBy": 1100,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We are concerned with feed-forward non-linear networks (multi-layer perceptrons, or MLPs) with multiple outputs. We wish to treat the outputs of the network as probabilities of alternatives (e.g. pattern classes), conditioned on the inputs. We look for appropriate output non-linearities and for appropriate criteria for adaptation of the parameters of the network (e.g. weights). We explain two modifications: probability scoring, which is an alternative to squared error minimisation, and a normalised exponential (softmax) multi-input generalisation of the logistic non-linearity. The two modifications together result in quite simple arithmetic, and hardware implementation is not difficult either. The use of radial units (squared distance instead of dot product) immediately before the softmax output stage produces a network which computes posterior distributions over class labels based on an assumption of Gaussian within-class distributions. However the training, which uses cross-class information, can result in better performance at class discrimination than the usual within-class training method, unless the within-class distribution assumptions are actually correct."
            },
            "slug": "Probabilistic-Interpretation-of-Feedforward-Network-Bridle",
            "title": {
                "fragments": [],
                "text": "Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Two modifications are explained: probability scoring, which is an alternative to squared error minimisation, and a normalised exponential (softmax) multi-input generalisation of the logistic non- linearity of feed-forward non-linear networks with multiple outputs."
            },
            "venue": {
                "fragments": [],
                "text": "NATO Neurocomputing"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144473519"
                        ],
                        "name": "M. Mozer",
                        "slug": "M.-Mozer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mozer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mozer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 39
                            }
                        ],
                        "text": ", 1996; Plate, 1993) or time constants (Mozer, 1992), and hierarchical sequence compression (Schmidhuber, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5355536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e141d68065ce638f9fc4f006eab2f66711e89768",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning structure in temporally-extended sequences is a difficult computational problem because only a fraction of the relevant information is available at any instant. Although variants of back propagation can in principle be used to find structure in sequences, in practice they are not sufficiently powerful to discover arbitrary contingencies, especially those spanning long temporal intervals or involving high order statistics. For example, in designing a connectionist network for music composition, we have encountered the problem that the net is able to learn musical structure that occurs locally in time--e.g., relations among notes within a musical phrase--but not structure that occurs over longer time periods--e.g., relations among phrases. To address this problem, we require a means of constructing a reduced description of the sequence that makes global aspects more explicit or more readily detectable. I propose to achieve this using hidden units that operate with different time constants. Simulation experiments indicate that slower time-scale hidden units are able to pick up global structure, structure that simply can not be learned by standard back propagation."
            },
            "slug": "Induction-of-Multiscale-Temporal-Structure-Mozer",
            "title": {
                "fragments": [],
                "text": "Induction of Multiscale Temporal Structure"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Simulation experiments indicate that slower time-scale hidden units are able to pick up global structure, structure that simply can not be learned by standard back propagation, using hidden units that operate with different time constants."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 75
                            }
                        ],
                        "text": "This chapter introduces hierarchical subsampling recurrent neural networks (HSRNNs; Graves and Schmidhuber, 2009) for large data sequences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 30
                            }
                        ],
                        "text": "Hierarchical subsampling RNNs (Graves and Schmidhuber, 2009) contain a stack of recurrent network layers with progressively lower spatiotemporal resolution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1733286,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b32c9d740e0748ad1d7656dd57121dc4357e5363",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Offline handwriting recognition\u2014the automatic transcription of images of handwritten text\u2014is a challenging task that combines computer vision with sequence learning. In most systems the two elements are handled separately, with sophisticated preprocessing techniques used to extract the image features and sequential models such as HMMs used to provide the transcriptions. By combining two recent innovations in neural networks\u2014multidimensional recurrent neural networks and connectionist temporal classification\u2014this paper introduces a globally trained offline handwriting recogniser that takes raw pixel data as input. Unlike competing systems, it does not require any alphabet specific preprocessing, and can therefore be used unchanged for any language. Evidence of its generality and power is provided by data from a recent international Arabic recognition competition, where it outperformed all entries (91.4% accuracy compared to 87.2% for the competition winner) despite the fact that neither author understands a word of Arabic."
            },
            "slug": "Offline-Handwriting-Recognition-with-Recurrent-Graves-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper introduces a globally trained offline handwriting recogniser that takes raw pixel data as input and does not require any alphabet specific preprocessing, and can therefore be used unchanged for any language."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32536207"
                        ],
                        "name": "D. Camp",
                        "slug": "D.-Camp",
                        "structuredName": {
                            "firstName": "Drew",
                            "lastName": "Camp",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Camp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9346534,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25c9f33aceac6dcff357727cbe2faf145b01d13c",
            "isKey": false,
            "numCitedBy": 934,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Supervised neural networks generalize well if there is much less information in the weights than there is in the output vectors of the training cases. So during learning, it is important to keep the weights simple by penalizing the amount of information they contain. The amount of information in a weight can be controlled by adding Gaussian noise and the noise level can be adapted during learning to optimize the trade-o between the expected squared error of the network and the amount of information in the weights. We describe a method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non-linear hidden units. Provided the output units are linear, the exact derivatives can be computed e ciently without time-consuming Monte Carlo simulations. The idea of minimizing the amount of information that is required to communicate the weights of a neural network leads to a number of interesting schemes for encoding the weights."
            },
            "slug": "Keeping-the-neural-networks-simple-by-minimizing-of-Hinton-Camp",
            "title": {
                "fragments": [],
                "text": "Keeping the neural networks simple by minimizing the description length of the weights"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non-linear hidden units without time-consuming Monte Carlo simulations is described."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '93"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3087941"
                        ],
                        "name": "Pascal Lamblin",
                        "slug": "Pascal-Lamblin",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Lamblin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Lamblin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32384143"
                        ],
                        "name": "D. Popovici",
                        "slug": "D.-Popovici",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Popovici",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Popovici"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 112
                            }
                        ],
                        "text": "It has been repeatedly noted that training neural networks with many layers using gradient descent is difficult (Hinton et al., 2006; Bengio et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14201947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "355d44f53428b1ac4fb2ab468d593c720640e5bd",
            "isKey": false,
            "numCitedBy": 3427,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization."
            },
            "slug": "Greedy-Layer-Wise-Training-of-Deep-Networks-Bengio-Lamblin",
            "title": {
                "fragments": [],
                "text": "Greedy Layer-Wise Training of Deep Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "These experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40268570"
                        ],
                        "name": "A. J. Robinson",
                        "slug": "A.-J.-Robinson",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Robinson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Robinson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 69
                            }
                        ],
                        "text": "However there has also been considerable interest in the use of RNNs (Robinson, 1994; Neto et al., 1995; Kershaw et al., 1996; Senior and Robinson, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7514628,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c25e9ebd8fe9d761f4738f7936ef114f7f6afe5d",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the training of a recurrent neural network as the letter posterior probability estimator for a hidden Markov model, off-line handwriting recognition system. The network estimates posterior distributions for each of a series of frames representing sections of a handwritten word. The supervised training algorithm, backpropagation through time, requires target outputs to be provided for each frame. Three methods for deriving these targets are presented. A novel method based upon the forward-backward algorithm is found to result in the recognizer with the lowest error rate."
            },
            "slug": "Forward-backward-retraining-of-recurrent-neural-Senior-Robinson",
            "title": {
                "fragments": [],
                "text": "Forward-backward retraining of recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper describes the training of a recurrent neural network as the letter posterior probability estimator for a hidden Markov model, off-line handwriting recognition system."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066474202"
                        ],
                        "name": "Dan J. Kershaw",
                        "slug": "Dan-J.-Kershaw",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Kershaw",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan J. Kershaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40268570"
                        ],
                        "name": "A. J. Robinson",
                        "slug": "A.-J.-Robinson",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Robinson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Robinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39213695"
                        ],
                        "name": "M. Hochberg",
                        "slug": "M.-Hochberg",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Hochberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hochberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15348936,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74ab2a9419f5356d77bad207ccb1b2caaf196a4f",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for incorporating context-dependent phone classes in a connectionist-HMM hybrid speech recognition system is introduced. A modular approach is adopted, where single-layer networks discriminate between different context classes given the phone class and the acoustic data. The context networks are combined with a context-independent (CI) network to generate context-dependent (CD) phone probability estimates. Experiments show an average reduction in word error rate of 16% and 13% from the CI system on ARPA 5,000 word and SQALE 20,000 word tasks respectively. Due to improved modelling, the decoding speed of the CD system is more than twice as fast as the CI system."
            },
            "slug": "Context-Dependent-Classes-in-a-Hybrid-Recurrent-Kershaw-Robinson",
            "title": {
                "fragments": [],
                "text": "Context-Dependent Classes in a Hybrid Recurrent Network-HMM Speech Recognition System"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A method for incorporating context-dependent phone classes in a connectionist-HMM hybrid speech recognition system is introduced, where single-layer networks discriminate between different context classes given the phone class and the acoustic data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2486270"
                        ],
                        "name": "T. Fukada",
                        "slug": "T.-Fukada",
                        "structuredName": {
                            "firstName": "Toshiaki",
                            "lastName": "Fukada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Fukada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144927151"
                        ],
                        "name": "M. Schuster",
                        "slug": "M.-Schuster",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678766"
                        ],
                        "name": "Y. Sagisaka",
                        "slug": "Y.-Sagisaka",
                        "structuredName": {
                            "firstName": "Yoshinori",
                            "lastName": "Sagisaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Sagisaka"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32265093,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93c1268cc00bf0fe4ed7a7a5e2d2f272988baadf",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a phoneme boundary estimation method based on bidirectional recurrent neural networks (BRNNs). Experimental results showed that the proposed method could estimate segment boundaries significantly better than an HMM or a multilayer perceptron-based method. Furthermore, we incorporated the BRNN-based segment boundary estimator into the HMM-based and segment model-based recognition systems. As a result, we confirmed that (1) BRNN outputs were effective for improving the recognition rate and reducing computational time in an HMM-based recognition system and (2) segment lattices obtained by the proposed methods dramatically reduce the computational complexity of segment model-based recognition. \u00a9 1999 Scripta Technica, Syst Comp Jpn, 30(4): 20\u201330, 1999"
            },
            "slug": "Phoneme-boundary-estimation-using-bidirectional-and-Fukada-Schuster",
            "title": {
                "fragments": [],
                "text": "Phoneme boundary estimation using bidirectional recurrent neural networks and its applications"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "Experimental results showed that the proposed method could estimate segment boundaries significantly better than an HMM or a multilayer perceptron-based method, and incorporated the BRNN-based segment boundary estimator into the HMM-based and segment model-based recognition systems."
            },
            "venue": {
                "fragments": [],
                "text": "Systems and Computers in Japan"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143913738"
                        ],
                        "name": "Santiago Fern\u00e1ndez",
                        "slug": "Santiago-Fern\u00e1ndez",
                        "structuredName": {
                            "firstName": "Santiago",
                            "lastName": "Fern\u00e1ndez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Santiago Fern\u00e1ndez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 124
                            }
                        ],
                        "text": "This section considers a variant of the previous task, where the number of distinct phoneme labels is reduced from 61 to 39 (Fernndez et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14180535,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "354ac3ca9a404150db56f927d5622f4cd07ccb42",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We compare the performance of a recurrent neural network with the best results published so far on phoneme recognition in the TIMIT database. These published results have been obtained with a combination of classifiers. However, in this paper we apply a single recurrent neural network to the same task. Our recurrent neural network attains an error rate of 24.6%. This result is not significantly different from that obtained by the other best methods, but they rely on a combination of classifiers for achieving comparable performance."
            },
            "slug": "Phoneme-recognition-in-TIMIT-with-BLSTM-CTC-Fern\u00e1ndez-Graves",
            "title": {
                "fragments": [],
                "text": "Phoneme recognition in TIMIT with BLSTM-CTC"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "The performance of a recurrent neural network is compared with the best results published so far on phoneme recognition in the TIMIT database and a single recurrent network is applied to the same task."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055652031"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "Juergen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9594328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "685d42a668413422615519a52ac75d66fded4611",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we apply bidirectional training to a long short term memory (LSTM) network for the first time. We also present a modified, full gradient version of the LSTM learning algorithm. We discuss the significance of framewise phoneme classification to continuous speech recognition, and the validity of using bidirectional networks for online causal tasks. On the TIMIT speech database, we measure the framewise phoneme classification scores of bidirectional and unidirectional variants of both LSTM and conventional recurrent neural networks (RNNs). We find that bidirectional LSTM outperforms both RNNs and unidirectional LSTM."
            },
            "slug": "Framewise-phoneme-classification-with-bidirectional-Graves-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Framewise phoneme classification with bidirectional LSTM networks"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is found that bidirectional LSTM outperforms both RNNs and unidirectionalLSTM, and the significance of framewise phoneme classification to continuous speech recognition and the validity of usingbidirectional networks for online causal tasks is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143913738"
                        ],
                        "name": "Santiago Fern\u00e1ndez",
                        "slug": "Santiago-Fern\u00e1ndez",
                        "structuredName": {
                            "firstName": "Santiago",
                            "lastName": "Fern\u00e1ndez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Santiago Fern\u00e1ndez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 185
                            }
                        ],
                        "text": "The following experiments confirm this observation by recording the word error rate both with and without the requirement that the network find the approximate location of the keywords (Fern\u00e1ndez et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17471730,
            "fieldsOfStudy": [
                "Economics",
                "Computer Science",
                "Education"
            ],
            "id": "f6fc38c0d0c6eb0597c7f45abeafe43b7d1ff382",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of keyword spotting is to detect the presence of specific spoken words in unconstrained speech. The majority of keyword spotting systems are based on generative hidden Markov models and lack discriminative capabilities. However, discriminative keyword spotting systems are currently based on frame-level posterior probabilities of sub-word units. This paper presents a discriminative keyword spotting system based on recurrent neural networks only, that uses information from long time spans to estimate word-level posterior probabilities. In a keyword spotting task on a large database of unconstrained speech the system achieved a keyword spotting accuracy of 84.5%"
            },
            "slug": "An-Application-of-Recurrent-Neural-Networks-to-Fern\u00e1ndez-Graves",
            "title": {
                "fragments": [],
                "text": "An Application of Recurrent Neural Networks to Discriminative Keyword Spotting"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A discriminative keyword spotting system based on recurrent neural networks only, that uses information from long time spans to estimate word-level posterior probabilities of sub-word units, is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113414328"
                        ],
                        "name": "Fernando Pereira",
                        "slug": "Fernando-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 358,
                                "start": 335
                            }
                        ],
                        "text": "Experiments on speech and handwriting recognition show that a BLSTM network with a CTC output layer is an effective sequence labeller, generally outperforming standard HMMs and HMM-neural network hybrids, as well as more recent sequence labelling algorithms such as large margin HMMs (Sha and Saul, 2006) and conditional random fields (Lafferty et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 219683473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4ba954b0412773d047dc41231c733de0c1f4926",
            "isKey": false,
            "numCitedBy": 13406,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "slug": "Conditional-Random-Fields:-Probabilistic-Models-for-Lafferty-McCallum",
            "title": {
                "fragments": [],
                "text": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents iterative parameter estimation algorithms for conditional random fields and compares the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793159"
                        ],
                        "name": "M. W. Kadous",
                        "slug": "M.-W.-Kadous",
                        "structuredName": {
                            "firstName": "Mohammed",
                            "lastName": "Kadous",
                            "middleNames": [
                                "Waleed"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. W. Kadous"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 54
                            }
                        ],
                        "text": "We refer to this situation as temporal classification (Kadous, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 26261569,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4badc3f0ad169ed9ec7d073375e9b168fa9f6c8f",
            "isKey": false,
            "numCitedBy": 234,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine learning research has, to a great extent, ignored an important aspect of many real world applications: time. Existing concept learners predominantly operate on a static set of attributes; for example, classifying flowers described by leaf size, petal colour and petal count. \nHowever, many real datasets are not static; they cannot sensibly be represented as a fixed set of attributes. Rather, the examples are expressed as features that vary temporally, and it is the temporal variation itself that is used for classification. Consider a simple gesture recognition domain, in which the temporal features are the position of the hands, finger bends, and so on. Looking at the position of the hand at one point in time is not likely to lead to a successful classification; it is only by analysing changes in position that recognition is possible. \nThis thesis presents a new technique for temporal classification. By extracting sub-events from the training instances and parameterising them to allow feature construction for a subsequent learning process, it is able to employ background knowledge and express learnt concepts in terms of the background knowledge. \nThe novel results of the thesis are: a temporal learner capable of producing comprehensible and accurate classifiers for multivariate time series that can learn from a small number of instances and can integrate non-temporal features; a feature construction technique that parameterises sub-events of the training set and clusters them to construct features for a propositional learner; and a technique for post-processing classification rules produced by the learner to give a comprehensible description expressed in the same form as the original background knowledge. \nThe thesis discusses the implementation of TClass, a temporal learner. Results show rules that are comprehensible in many cases and accuracy results close to or better than existing technique\u2014over 98 per cent for sign language and 72 per cent for ECGs (equivalent to the accuracy of a human cardiologist). One further surprising result is that a small set of very primitive sub-events proves to be functional, avoiding the need for labour-intensive background knowledge if it is not available."
            },
            "slug": "Temporal-classification:-extending-the-paradigm-to-Kadous",
            "title": {
                "fragments": [],
                "text": "Temporal classification: extending the classification paradigm to multivariate time series"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A temporal learner capable of producing comprehensible and accurate classifiers for multivariate time series that can learn from a small number of instances and can integrate non-temporal features, and a feature construction technique that parameterises sub-events of the training set and clusters them to construct features for a propositional learner."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38767254"
                        ],
                        "name": "David Steinkraus",
                        "slug": "David-Steinkraus",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Steinkraus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Steinkraus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 113
                            }
                        ],
                        "text": "We compared our results with the convolutional neural network that has achieved the best results so far on MNIST (Simard et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 125
                            }
                        ],
                        "text": "Input perturbations tailored towards a particular dataset have been shown to be highly effective at improving generalisation (Simard et al., 2003); however this requires a prior model of the data variations, which is not usually available."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 111
                            }
                        ],
                        "text": "Numerous pattern classification algorithms have been applied to MNIST, including convolutional neural networks (LeCun et al., 1998a; Simard et al., 2003) and support vector machines (LeCun et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 61
                            }
                        ],
                        "text": ", 1998a) to image processing tasks such as digit recognition (Simard et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4659176,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5562a56da3a96dae82add7de705e2bd841eb00fc",
            "isKey": true,
            "numCitedBy": 2432,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks are a powerful technology forclassification of visual inputs arising from documents.However, there is a confusing plethora of different neuralnetwork methods that are used in the literature and inindustry. This paper describes a set of concrete bestpractices that document analysis researchers can use toget good results with neural networks. The mostimportant practice is getting a training set as large aspossible: we expand the training set by adding a newform of distorted data. The next most important practiceis that convolutional neural networks are better suited forvisual document tasks than fully connected networks. Wepropose that a simple \"do-it-yourself\" implementation ofconvolution with a flexible architecture is suitable formany visual document problems. This simpleconvolutional neural network does not require complexmethods, such as momentum, weight decay, structure-dependentlearning rates, averaging layers, tangent prop,or even finely-tuning the architecture. The end result is avery simple yet general architecture which can yieldstate-of-the-art performance for document analysis. Weillustrate our claims on the MNIST set of English digitimages."
            },
            "slug": "Best-practices-for-convolutional-neural-networks-to-Simard-Steinkraus",
            "title": {
                "fragments": [],
                "text": "Best practices for convolutional neural networks applied to visual document analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A set of concrete bestpractices that document analysis researchers can use to get good results with neural networks, including a simple \"do-it-yourself\" implementation of convolution with a flexible architecture suitable for many visual document problems."
            },
            "venue": {
                "fragments": [],
                "text": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47740322"
                        ],
                        "name": "Jinmiao Chen",
                        "slug": "Jinmiao-Chen",
                        "structuredName": {
                            "firstName": "Jinmiao",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinmiao Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2077393706"
                        ],
                        "name": "N. Chaudhari",
                        "slug": "N.-Chaudhari",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Chaudhari",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Chaudhari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 112
                            }
                        ],
                        "text": "BRNNs have previously given improved results in various domains, notably protein secondary structure prediction (Baldi et al., 2001; Chen and Chaudhari, 2004) and speech processing (Schuster, 1999; Fukada et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15111957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b26a6552e646248ed6aba3263182040f4516256",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Bidirectional recurrent neural network (BRNN) is a noncausal system that captures both upstream and downstream information for protein secondary structure prediction. Due to the problem of vanishing gradients, the BRNN can not learn remote information efficiently. To limit this problem, we propose segmented memory recurrent neural network (SMRNN) and obtain a bidirectional segmented-memory recurrent neural network (BSMRNN) by replacing the standard RNNs in BRNN with SMRNNs. Our experiment with BSMRNN for protein secondary structure prediction on the RS126 set indicates improvement in the prediction accuracy."
            },
            "slug": "Capturing-Long-Term-Dependencies-for-Protein-Chen-Chaudhari",
            "title": {
                "fragments": [],
                "text": "Capturing Long-Term Dependencies for Protein Secondary Structure Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "SMRNN is proposed and a bidirectional segmented-memory recurrent neural network (BSMRNN) is obtained by replacing the standard RNNs in BRNN with SMRNNs, and improvement in the prediction accuracy is indicated."
            },
            "venue": {
                "fragments": [],
                "text": "ISNN"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2396681"
                        ],
                        "name": "D. Eck",
                        "slug": "D.-Eck",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Eck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Eck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 52
                            }
                        ],
                        "text": ", 2007; Chen and Chaudhari, 2005), music generation (Eck and Schmidhuber, 2002), reinforcement learning (Bakker, 2002), speech recognition (Graves and Schmidhuber, 2005b; Graves et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 579926,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4ffd2f5dd98ee744c013060c5bc06503336d931",
            "isKey": false,
            "numCitedBy": 218,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of extracting essential ingredients of music signals, such as a well-defined global temporal structure in the form of nested periodicities (or meter). We investigate whether we can construct an adaptive signal processing device that learns by example how to generate new instances of a given musical style. Because recurrent neural networks (RNNs) can, in principle, learn the temporal structure of a signal, they are good candidates for such a task. Unfortunately, music composed by standard RNNs often lacks global coherence. The reason for this failure seems to be that RNNs cannot keep track of temporally distant events that indicate global music structure. Long short-term memory (LSTM) has succeeded in similar domains where other RNNs have failed, such as timing and counting and the learning of context sensitive languages. We show that LSTM is also a good mechanism for learning to compose music. We present experimental results showing that LSTM successfully learns a form of blues music and is able to compose novel (and we believe pleasing) melodies in that style. Remarkably, once the network has found the relevant structure, it does not drift from it: LSTM is able to play the blues with good timing and proper structure as long as one is willing to listen."
            },
            "slug": "Finding-temporal-structure-in-music:-blues-with-Eck-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Finding temporal structure in music: blues improvisation with LSTM recurrent networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Long short-term memory (LSTM) has succeeded in similar domains where other RNNs have failed, such as timing and counting and the learning of context sensitive languages, and it is shown that LSTM is also a good mechanism for learning to compose music."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 12th IEEE Workshop on Neural Networks for Signal Processing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2859277"
                        ],
                        "name": "T. Plate",
                        "slug": "T.-Plate",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Plate",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Plate"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 43
                            }
                        ],
                        "text": ", 1994), explicitly introduced time delays (Lang et al., 1990; Lin et al., 1996; Plate, 1993) or time constants (Mozer, 1992), and hierarchical sequence compression (Schmidhuber, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13629215,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebf62950d733a4a8f9ecd8d3752dee8d13fc8e6d",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Holographic Recurrent Networks (HRNs) are recurrent networks which incorporate associative memory techniques for storing sequential structure. HRNs can be easily and quickly trained using gradient descent techniques to generate sequences of discrete outputs and trajectories through continuous space. The performance of HRNs is found to be superior to that of ordinary recurrent networks on these sequence generation tasks."
            },
            "slug": "Holographic-Recurrent-Networks-Plate",
            "title": {
                "fragments": [],
                "text": "Holographic Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "Holographic Recurrent Networks are recurrent networks which incorporate associative memory techniques for storing sequential structure and the performance of HRNs is found to be superior to that of ordinary recurrent networks on sequence generation tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143684647"
                        ],
                        "name": "B. Hammer",
                        "slug": "B.-Hammer",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Hammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hammer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 214
                            }
                        ],
                        "text": "Indeed, the equivalent result to the universal approximation theory for MLPs is that an RNN with a sufficient number of hidden units can approximate any measurable sequenceto-sequence mapping to arbitrary accuracy (Hammer, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17689053,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "231fc73f645dbad1f4065d3a73e32372ac0bc846",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-approximation-capability-of-recurrent-neural-Hammer",
            "title": {
                "fragments": [],
                "text": "On the approximation capability of recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2309950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "isKey": false,
            "numCitedBy": 13406,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind."
            },
            "slug": "A-Fast-Learning-Algorithm-for-Deep-Belief-Nets-Hinton-Osindero",
            "title": {
                "fragments": [],
                "text": "A Fast Learning Algorithm for Deep Belief Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A fast, greedy algorithm is derived that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 14
                            }
                        ],
                        "text": "The HMM setup (Liwicki et al., 2007) contained a separate, linear HMM with 8 states for each character (8 \u2217 81 = 648 states in total)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 36
                            }
                        ],
                        "text": ", 2006) and handwriting recognition (Liwicki et al., 2007; Graves et al., 2008)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 104
                            }
                        ],
                        "text": "The second representation required a large amount of sophisticated preprocessing and feature extraction (Liwicki et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 24
                            }
                        ],
                        "text": "For the CTC experiments (Liwicki et al., 2007; Graves et al., 2008, 2009), the character error rate is obtaining using best-path decoding, and the word error rate using constrained decoding."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5668166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43fa5235e49fa6f16d047c999234d1b93df360b0",
            "isKey": false,
            "numCitedBy": 194,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce a new connectionist approach to on-line handwriting recognition and address in particular the problem of recognizing handwritten whiteboard notes. The approach uses a bidirectional recurrent neural network with the long short-term memory architecture. We use a recently introduced objective function, known as Connectionist Temporal Classification (CTC), that directly trains the network to label unsegmented sequence data. Our new system achieves a word recognition rate of 74.0%, compared with 65.4% using a previously developed HMMbased recognition system."
            },
            "slug": "A-novel-approach-to-on-line-handwriting-recognition-Liwicki-Graves",
            "title": {
                "fragments": [],
                "text": "A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A new connectionist approach to on-line handwriting recognition and address in particular the problem of recognizing handwritten whiteboard notes using a recently introduced objective function, known as Connectionist Temporal Classification (CTC), that directly trains the network to label unsegmented sequence data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 184
                            }
                        ],
                        "text": "The traditional solution\u2014the so-called hybrid approach\u2014is to use hidden Markov models to generate targets for the RNN, then invert the RNN outputs to provide observation probabilities (Bourlard and Morgan, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 88
                            }
                        ],
                        "text": "In other cases ANNs were used to estimate transition or emission probabilities for HMMs (Bourlard and Morgan, 1994), to re-score the N-best HMM labellings according to localised classifications (Zavaliagkos et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 172
                            }
                        ],
                        "text": "Hybrids of hidden Markov models (HMMs) and artificial neural networks (ANNs) were proposed by several researchers in the 1990s as a way of overcoming the drawbacks of HMMs (Bourlard and Morgan, 1994; Bengio, 1993; Renals et al., 1993; Robinson, 1994; Bengio, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61058350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d82e058a5c40954b8f5db170a298a889a254c37",
            "isKey": false,
            "numCitedBy": 1409,
            "numCiting": 190,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nConnectionist Speech Recognition: A Hybrid Approach describes the theory and implementation of a method to incorporate neural network approaches into state-of-the-art continuous speech recognition systems based on Hidden Markov Models (HMMs) to improve their performance. In this framework, neural networks (and in particular, multilayer perceptrons or MLPs) have been restricted to well-defined subtasks of the whole system, i.e., HMM emission probability estimation and feature extraction. The book describes a successful five year international collaboration between the authors. The lessons learned form a case study that demonstrates how hybrid systems can be developed to combine neural networks with more traditional statistical approaches. The book illustrates both the advantages and limitations of neural networks in the framework of a statistical system. Using standard databases and comparing with some conventional approaches, it is shown that MLP probability estimation can improve recognition performance. Other approaches are discussed, though there is no such unequivocal experimental result for these methods. Connectionist Speech Recognition: A Hybrid Approach is of use to anyone intending to use neural networks for speech recognition or within the framework provided by an existing successful statistical approach. This includes research and development groups working in the field of speech recognition, both with standard and neural network approaches, as well as other pattern recognition and/or neural network researchers. This book is also suitable as a text for advanced courses on neural networks or speech processing."
            },
            "slug": "Connectionist-Speech-Recognition:-A-Hybrid-Approach-Bourlard-Morgan",
            "title": {
                "fragments": [],
                "text": "Connectionist Speech Recognition: A Hybrid Approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40268570"
                        ],
                        "name": "A. J. Robinson",
                        "slug": "A.-J.-Robinson",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Robinson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Robinson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 69
                            }
                        ],
                        "text": "However there has also been considerable interest in the use of RNNs (Robinson, 1994; Neto et al., 1995; Kershaw et al., 1996; Senior and Robinson, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 140
                            }
                        ],
                        "text": "In this chapter we follow an iterative approach, where the alignment provided by the HMM is used to successively retrain the neural network (Robinson, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 172
                            }
                        ],
                        "text": "Hybrids of hidden Markov models (HMMs) and artificial neural networks (ANNs) were proposed by several researchers in the 1990s as a way of overcoming the drawbacks of HMMs (Bourlard and Morgan, 1994; Bengio, 1993; Renals et al., 1993; Robinson, 1994; Bengio, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 167
                            }
                        ],
                        "text": "In their simplest form, hybrid methods used HMMs to align the segment classifications provided by the ANNs into a temporal classification of the entire label sequence (Renals et al., 1993; Robinson, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14787570,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6629770cb6a00ad585918e71fe6dbad829ad0d1",
            "isKey": true,
            "numCitedBy": 543,
            "numCiting": 91,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an application of recurrent networks for phone probability estimation in large vocabulary speech recognition. The need for efficient exploitation of context information is discussed; a role for which the recurrent net appears suitable. An overview of early developments of recurrent nets for phone recognition is given along with the more recent improvements that include their integration with Markov models. Recognition results are presented for the DARPA TIMIT and Resource Management tasks, and it is concluded that recurrent nets are competitive with traditional means for performing phone probability estimation."
            },
            "slug": "An-application-of-recurrent-nets-to-phone-Robinson",
            "title": {
                "fragments": [],
                "text": "An application of recurrent nets to phone probability estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Recognition results are presented for the DARPA TIMIT and Resource Management tasks, and it is concluded that recurrent nets are competitive with traditional means for performing phone probability estimation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2546518"
                        ],
                        "name": "D. Plaut",
                        "slug": "D.-Plaut",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Plaut",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Plaut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 57
                            }
                        ],
                        "text": "This can be mitigated by the addition of a momentum term (Plaut et al., 1986; Bishop, 1995), which effectively adds inertia to the motion of the algorithm through weight space, thereby speeding up convergence and helping to escape from local minima"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15150815,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4a42b2104ca8ff891ae77c40a915d4c94c8f8428",
            "isKey": false,
            "numCitedBy": 390,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Rumelhart, Hinton and Williams (Rumelhart 86) describe a learning procedure for layered networks of deterministic, neuron-like units. This paper describes further research on the learning procedure. We start by describing the units, the way they are connected, the learning procedure, and the extension to iterative nets. We then give an example in which a network learns a set of filters that enable it to discriminate formant-like patterns in the presence of noise. The speed of learning is strongly dependent on the shape of the surface formed by the error measure in weight space . We give examples of the shape of the error surface for a typical task and illustrate how an acceleration method speeds up descent in weight space. The main drawback of the learning procedure is the way it scales as the size of the task and the network increases. We give some preliminary results on scaling and show how the magnitude of the optimal weight changes depends on the fan-in of the units. Additional results illustrate the effects on learning speed of the amount of interaction between the weights. A variation of the learning procedure that back-propagates desired state information rather than error gradients is developed and compared with the standard procedure. Finally, we discuss the relationship between our iterative networks and the analog networks described by Hopefield and Tank (Hopfield 85). The learning procedure can discover appropriate weights in their kind of network, as well as determine an optimal schedule for varying the nonlinearity of the units during a search."
            },
            "slug": "Experiments-on-Learning-by-Back-Propagation.-Plaut-Nowlan",
            "title": {
                "fragments": [],
                "text": "Experiments on Learning by Back Propagation."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The learning procedure can discover appropriate weights in their kind of network, as well as determine an optimal schedule for varying the nonlinearity of the units during a search."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35236,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145898106"
                        ],
                        "name": "James R. Glass",
                        "slug": "James-R.-Glass",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Glass",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James R. Glass"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 219
                            }
                        ],
                        "text": ", 2006) frequency-warped LPC cepstra were used as inputs, while Halberstadt and Glass tried a number of variations and combinations of MFCC, perceptual linear prediction (PLP) cepstral coefficients, energy and duration (Halberstadt, 1998; Glass, 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5847709,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6df5c972ed687aefe41637f3b0433c7dd44a0b6",
            "isKey": false,
            "numCitedBy": 335,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-probabilistic-framework-for-segment-based-speech-Glass",
            "title": {
                "fragments": [],
                "text": "A probabilistic framework for segment-based speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884373"
                        ],
                        "name": "J. Elman",
                        "slug": "J.-Elman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Elman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 95
                            }
                        ],
                        "text": "As with feedforward networks, many varieties of RNN have been proposed, such as Elman networks (Elman, 1990), Jordan networks (Jordan, 1990), time delay neural networks (Lang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2763403,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "668087f0ae7ce1de6e0bd0965dbb480c08103260",
            "isKey": false,
            "numCitedBy": 9858,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
            },
            "slug": "Finding-Structure-in-Time-Elman",
            "title": {
                "fragments": [],
                "text": "Finding Structure in Time"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory and suggests a method for representing lexical categories and the type/token distinction is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 23
                            }
                        ],
                        "text": "7) for neural networks (MacKay, 1995; Neal, 1996), we will here focus on objective functions derived using maximum likelihood."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14332165,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ce9da2d2182a2fbc4b460bdb56d3c34110b3e39",
            "isKey": false,
            "numCitedBy": 894,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "Bayesian probability theory provides a unifying framework for data modelling. In this framework the overall aims are to find models that are well-matched to the data, and to use these models to make optimal predictions. Neural network learning is interpreted as an inference of the most probable parameters for the model, given the training data. The search in model space (i.e., the space of architectures, noise models, preprocessings, regularizers and weight decay constants) can then also be treated as an inference problem, in which we infer the relative probability of alternative models, given the data. This review describes practical techniques based on Gaussian approximations for implementation of these powerful methods for controlling, comparing and using adaptive networks."
            },
            "slug": "Probable-networks-and-plausible-predictions-a-of-Mackay",
            "title": {
                "fragments": [],
                "text": "Probable networks and plausible predictions - a review of practical Bayesian methods for supervised neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Practical techniques based on Gaussian approximations for implementation of these powerful methods for controlling, comparing and using adaptive networks are described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 139
                            }
                        ],
                        "text": "This contrasts with nonlinear networks, which can gain considerable power by using successive hidden layers to re-represent the input data (Hinton et al., 2006; Bengio and LeCun, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15559637,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6fdb77260fc83dff91c44fea0f31a2cb8ed13d04",
            "isKey": false,
            "numCitedBy": 1117,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), reasoning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, with minimal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally limited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very inefficient in terms of required number of computational elements and examples. Second, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learning) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more abstract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence."
            },
            "slug": "Scaling-learning-algorithms-towards-AI-Bengio-LeCun",
            "title": {
                "fragments": [],
                "text": "Scaling learning algorithms towards AI"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is argued that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8169197"
                        ],
                        "name": "S. Brunak",
                        "slug": "S.-Brunak",
                        "structuredName": {
                            "firstName": "S\u00f8ren",
                            "lastName": "Brunak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Brunak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2540925"
                        ],
                        "name": "G. Soda",
                        "slug": "G.-Soda",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Soda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Soda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2028880"
                        ],
                        "name": "G. Pollastri",
                        "slug": "G.-Pollastri",
                        "structuredName": {
                            "firstName": "Gianluca",
                            "lastName": "Pollastri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Pollastri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 40
                            }
                        ],
                        "text": "Bidirectional recurrent neural networks (BRNNs; Schuster and Paliwal, 1997; Schuster, 1999; Baldi et al., 1999) offer a more elegant solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15343954,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ac3c0f5c9cb6632447c314082151b6b45112941",
            "isKey": false,
            "numCitedBy": 487,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "MOTIVATION\nPredicting the secondary structure of a protein (alpha-helix, beta-sheet, coil) is an important step towards elucidating its three-dimensional structure, as well as its function. Presently, the best predictors are based on machine learning approaches, in particular neural network architectures with a fixed, and relatively short, input window of amino acids, centered at the prediction site. Although a fixed small window avoids overfitting problems, it does not permit capturing variable long-rang information.\n\n\nRESULTS\nWe introduce a family of novel architectures which can learn to make predictions based on variable ranges of dependencies. These architectures extend recurrent neural networks, introducing non-causal bidirectional dynamics to capture both upstream and downstream information. The prediction algorithm is completed by the use of mixtures of estimators that leverage evolutionary information, expressed in terms of multiple alignments, both at the input and output levels. While our system currently achieves an overall performance close to 76% correct prediction--at least comparable to the best existing systems--the main emphasis here is on the development of new algorithmic ideas.\n\n\nAVAILABILITY\nThe executable program for predicting protein secondary structure is available from the authors free of charge.\n\n\nCONTACT\npfbaldi@ics.uci.edu, gpollast@ics.uci.edu, brunak@cbs.dtu.dk, paolo@dsi.unifi.it."
            },
            "slug": "Exploiting-the-past-and-the-future-in-protein-Baldi-Brunak",
            "title": {
                "fragments": [],
                "text": "Exploiting the past and the future in protein secondary structure prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A family of novel architectures which can learn to make predictions based on variable ranges of dependencies are introduced, extending recurrent neural networks and introducing non-causal bidirectional dynamics to capture both upstream and downstream information."
            },
            "venue": {
                "fragments": [],
                "text": "Bioinform."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8169197"
                        ],
                        "name": "S. Brunak",
                        "slug": "S.-Brunak",
                        "structuredName": {
                            "firstName": "S\u00f8ren",
                            "lastName": "Brunak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Brunak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2028880"
                        ],
                        "name": "G. Pollastri",
                        "slug": "G.-Pollastri",
                        "structuredName": {
                            "firstName": "Gianluca",
                            "lastName": "Pollastri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Pollastri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2540925"
                        ],
                        "name": "G. Soda",
                        "slug": "G.-Soda",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Soda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Soda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 112
                            }
                        ],
                        "text": "BRNNs have previously given improved results in various domains, notably protein secondary structure prediction (Baldi et al., 2001; Chen and Chaudhari, 2004) and speech processing (Schuster, 1999; Fukada et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 105
                            }
                        ],
                        "text": "This is perhaps why protein structure prediction is the domain where BRNNs have been most widely adopted (Baldi et al., 2001; Thireou and Reczko, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14431872,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "5a0bc896955dbe1fd2db321a754b2895d47355fc",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "In this chapter, we have proposed two novel architectures for dealing with sequence learning problems in which data is not obtained from physical measurements over time. The new architectures remove the causality assumption that characterize current connectionist approaches to learning sequential translations. Using bidirectional recurrent neural networks (BRNNs) on the protein secondary structure prediction task appears to be very promising. Our performance is very close to the best existing systems although our usage of profiles is not as sophisticated. One improvement of our prediction system could be obtained by using profiles from the TrEMBL database."
            },
            "slug": "Bidirectional-Dynamics-for-Protein-Secondary-Baldi-Brunak",
            "title": {
                "fragments": [],
                "text": "Bidirectional Dynamics for Protein Secondary Structure Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This chapter proposes two novel architectures for dealing with sequence learning problems in which data is not obtained from physical measurements over time, and using bidirectional recurrent neural networks on the protein secondary structure prediction task appears to be very promising."
            },
            "venue": {
                "fragments": [],
                "text": "Sequence Learning"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2712002"
                        ],
                        "name": "Kam-Chuen Jim",
                        "slug": "Kam-Chuen-Jim",
                        "structuredName": {
                            "firstName": "Kam-Chuen",
                            "lastName": "Jim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kam-Chuen Jim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35216199"
                        ],
                        "name": "B. Horne",
                        "slug": "B.-Horne",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Horne",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Horne"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15729978,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "030a977bf32e81fb694117d78ac84a3fbe2a1d81",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Concerns the effect of noise on the performance of feedforward neural nets. We introduce and analyze various methods of injecting synaptic noise into dynamically driven recurrent nets during training. Theoretical results show that applying a controlled amount of noise during training may improve convergence and generalization performance. We analyze the effects of various noise parameters and predict that best overall performance can be achieved by injecting additive noise at each time step. Noise contributes a second-order gradient term to the error function which can be viewed as an anticipatory agent to aid convergence. This term appears to find promising regions of weight space in the beginning stages of training when the training error is large and should improve convergence on error surfaces with local minima. The first-order term is a regularization term that can improve generalization. Specifically, it can encourage internal representations where the state nodes operate in the saturated regions of the sigmoid discriminant function. While this effect can improve performance on automata inference problems with binary inputs and target outputs, it is unclear what effect it will have on other types of problems. To substantiate these predictions, we present simulations on learning the dual parity grammar from temporal strings for all noise models, and present simulations on learning a randomly generated six-state grammar using the predicted best noise model."
            },
            "slug": "An-analysis-of-noise-in-recurrent-neural-networks:-Jim-Giles",
            "title": {
                "fragments": [],
                "text": "An analysis of noise in recurrent neural networks: convergence and generalization"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Theoretical results show that applying a controlled amount of noise during training may improve convergence and generalization performance, and it is predicted that best overall performance can be achieved by injecting additive noise at each time step."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723644"
                        ],
                        "name": "A. Acero",
                        "slug": "A.-Acero",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Acero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Acero"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 16
                            }
                        ],
                        "text": "For example, in (Yu et al., 2006) frequency-warped LPC cepstra were used as inputs, while Halberstadt and Glass tried a number of variations and combinations of MFCC, perceptual linear prediction (PLP) cepstral coefficients, energy and duration (Halberstadt, 1998; Glass, 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10022505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f05059db776f04eacdf82de7613bad325751d7c6",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-lattice-search-technique-for-a-hidden-trajectory-Yu-Deng",
            "title": {
                "fragments": [],
                "text": "A lattice search technique for a long-contextual-span hidden trajectory model of speech"
            },
            "venue": {
                "fragments": [],
                "text": "Speech Commun."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709862"
                        ],
                        "name": "E. Trentin",
                        "slug": "E.-Trentin",
                        "structuredName": {
                            "firstName": "Edmondo",
                            "lastName": "Trentin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Trentin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 130
                            }
                        ],
                        "text": "The two components in a the hybrid can be trained independently, but many authors have proposed methods for combined optimisation (Bengio et al., 1992; Bourlard et al., 1996; Hennebert et al., 1997; Trentin and Gori, 2003) which typically yields improved results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8813484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fda748c874d51b33994c41035d2dc0e7509ea881",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Acoustic modeling in state-of-the-art speech recognition systems usually relies on hidden Markov models (HMMs) with Gaussian emission densities. HMMs suffer from intrinsic limitations, mainly due to their arbitrary parametric assumption. Artificial neural networks (ANNs) appear to be a promising alternative in this respect, but they historically failed as a general solution to the acoustic modeling problem. This paper introduces algorithms based on a gradient-ascent technique for global training of a hybrid ANN/HMM system, in which the ANN is trained for estimating the emission probabilities of the states of the HMM. The approach is related to the major hybrid systems proposed by Bourlard and Morgan and by Bengio, with the aim of combining their benefits within a unified framework and to overcome their limitations. Several viable solutions to the \"divergence problem\"-that may arise when training is accomplished over the maximum-likelihood (ML) criterion-are proposed. Experimental results in speaker-independent, continuous speech recognition over Italian digit-strings validate the novel hybrid framework, allowing for improved recognition performance over HMMs with mixtures of Gaussian components, as well as over Bourlard and Morgan's paradigm. In particular, it is shown that the maximum a posteriori (MAP) version of the algorithm yields a 46.34% relative word error rate reduction with respect to standard HMMs."
            },
            "slug": "Robust-combination-of-neural-networks-and-hidden-Trentin-Gori",
            "title": {
                "fragments": [],
                "text": "Robust combination of neural networks and hidden Markov models for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experimental results in speaker-independent, continuous speech recognition over Italian digit-strings validate the novel hybrid framework, allowing for improved recognition performance over HMMs with mixtures of Gaussian components, as well as over Bourlard and Morgan's paradigm."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143913738"
                        ],
                        "name": "Santiago Fern\u00e1ndez",
                        "slug": "Santiago-Fern\u00e1ndez",
                        "structuredName": {
                            "firstName": "Santiago",
                            "lastName": "Fern\u00e1ndez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Santiago Fern\u00e1ndez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10352688,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30bc0ca0b965a7e01f1c4cf20684fd654f975e4a",
            "isKey": false,
            "numCitedBy": 575,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we carry out two experiments on the TIMIT speech corpus with bidirectional and unidirectional Long Short Term Memory (LSTM) networks. In the first experiment (framewise phoneme classification) we find that bidirectional LSTMoutperforms both unidirectional LSTMand conventional Recurrent Neural Networks (RNNs). In the second (phoneme recognition) we find that a hybrid BLSTM-HMM system improves on an equivalent traditional HMM system, as well as unidirectional LSTM-HMM."
            },
            "slug": "Bidirectional-LSTM-Networks-for-Improved-Phoneme-Graves-Fern\u00e1ndez",
            "title": {
                "fragments": [],
                "text": "Bidirectional LSTM Networks for Improved Phoneme Classification and Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "In this paper, two experiments on the TIMIT speech corpus with bidirectional and unidirectional Long Short Term Memory networks are carried out and it is found that a hybrid BLSTM-HMM system improves on an equivalent traditional HMM system."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354787"
                        ],
                        "name": "G. Zavaliagkos",
                        "slug": "G.-Zavaliagkos",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Zavaliagkos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zavaliagkos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143811539"
                        ],
                        "name": "S. Austin",
                        "slug": "S.-Austin",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Austin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Austin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10080270"
                        ],
                        "name": "J. Makhoul",
                        "slug": "J.-Makhoul",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Makhoul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Makhoul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 16
                            }
                        ],
                        "text": "classifications (Zavaliagkos et al., 1993), and to extract observation features that can be more easily modelled by an HMM (Bengio et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8785536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce0c63c0f64ae323599b850e7a02d1b1136a7803",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Up till recently, state-of-the-art, large vocabulary, continuous speech recognition (CSR) had employed hidden Markov modeling (HMM) to model speech sounds. In an attempt to improve over HMM we developed a hybrid system that integrates HMM technology with neural networks. We present the concept of a Segmental Neural Net (SNN) for phonetic modeling in CSR. By taking into account all the frames of a phonetic segment simultaneously, the SNN overcomes the well-known conditional-independence limitation of HMMs. We have developed a novel hybrid SNN/HMM system that combines the advantages of SNNs and HMMs using a multiple hypothesis (or N-best) paradigm. In this system, we generate likely phonetic segmentations from the HMM N-best list of word sequences, which are scored by the SNN. The HMM and SNN scores are then combined to optimize performance. In several speaker-independent, 1000-word CSR tests, the error rate for the hybrid system dropped 20% from that of a state-of-the-art HMM system alone."
            },
            "slug": "A-Hybrid-Continuous-Speech-Recognition-System-Using-Zavaliagkos-Austin",
            "title": {
                "fragments": [],
                "text": "A Hybrid Continuous Speech Recognition System Using Segmental Neural Nets with Hidden Markov Models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The authors have developed a system that combines the SNN with a hidden Markov model (HMM) system and believe that this is the first system incorporating a neural network for which the performance has exceeded the state of the art in large-vocabulary, continuous speech recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144620965"
                        ],
                        "name": "J. Ming",
                        "slug": "J.-Ming",
                        "structuredName": {
                            "firstName": "Ji",
                            "lastName": "Ming",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ming"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145894070"
                        ],
                        "name": "F. Smith",
                        "slug": "F.-Smith",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Smith",
                            "middleNames": [
                                "Jack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17872406,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6248fce7299e2b2871e78dcd506743ab86c7d27",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A crucial issue in triphone based continuous speech recognition is the large number of models to be estimated against the limited availability of training data. This problem can be relieved by composing a triphone model from less context-dependent models. This paper introduces a new statistical framework, derived from the Bayesian principle, to perform such a composition. The potential power of this new framework is explored, both algorithmically and experimentally, by an implementation with hidden Markov modeling techniques. This implementation is applied to the recognition of the 39-phone set on the TIMIT database. The new model achieves 74.4% and 75.6% accuracy, respectively, on the core and complete test sets."
            },
            "slug": "Improved-phone-recognition-using-Bayesian-triphone-Ming-Smith",
            "title": {
                "fragments": [],
                "text": "Improved phone recognition using Bayesian triphone models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new statistical framework, derived from the Bayesian principle, is introduced to perform a triphone model from less context-dependent models, and the potential power of this new framework is explored, both algorithmically and experimentally, by an implementation with hidden Markov modeling techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 91
                            }
                        ],
                        "text": "However, we will frequently refer to the well-known generative method hidden Markov models (Rabiner, 1989; Bengio, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 172
                            }
                        ],
                        "text": "Hybrids of hidden Markov models (HMMs) and artificial neural networks (ANNs) were proposed by several researchers in the 1990s as a way of overcoming the drawbacks of HMMs (Bourlard and Morgan, 1994; Bengio, 1993; Renals et al., 1993; Robinson, 1994; Bengio, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2058857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f316d350c85fa7198878a1149c327218f7d4ea30",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 203,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov Models HMMs are statistical models of sequential data that have been used successfully in many machine learning applications especially for speech recognition Further more in the last few years many new and promising probabilistic models related to HMMs have been proposed We rst summarize the basics of HMMs and then review several recent related learning algorithms and extensions of HMMs including in particular hybrids of HMMs with arti cial neural networks Input Output HMMs which are conditional HMMs using neu ral networks to compute probabilities weighted transducers variable length Markov models and Markov switching state space models Finally we discuss some of the challenges of future research in this very active area"
            },
            "slug": "Markovian-Models-for-Sequential-Data-Bengio",
            "title": {
                "fragments": [],
                "text": "Markovian Models for Sequential Data"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The basics ofHMMs are summarized and several recent related learning algorithms and extensions of HMMs including in particular hybrids of HM Ms with arti cial neural networks are reviewed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714393"
                        ],
                        "name": "R. Mori",
                        "slug": "R.-Mori",
                        "structuredName": {
                            "firstName": "Renato",
                            "lastName": "Mori",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2376916"
                        ],
                        "name": "G. Flammia",
                        "slug": "G.-Flammia",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Flammia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Flammia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688499"
                        ],
                        "name": "R. Kompe",
                        "slug": "R.-Kompe",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "Kompe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kompe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 130
                            }
                        ],
                        "text": "The two components in a the hybrid can be trained independently, but many authors have proposed methods for combined optimisation (Bengio et al., 1992; Bourlard et al., 1996; Hennebert et al., 1997; Trentin and Gori, 2003) which typically yields improved results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 894840,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffbe67c217967b6bfb0a5ecc0dc4cdd5cda65776",
            "isKey": false,
            "numCitedBy": 261,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "An original method for integrating artificial neural networks (ANN) with hidden Markov models (HMM) is proposed. ANNs are suitable for performing phonetic classification, whereas HMMs have been proven successful at modeling the temporal structure of the speech signal. In the approach described, the ANN outputs constitute the sequence of observation vectors for the HMM. An algorithm is proposed for global optimization of all the parameters. Results on speaker-independent recognition experiments using this integrated ANN-HMM system on the TIMIT continuous speech database are reported.<<ETX>>"
            },
            "slug": "Global-optimization-of-a-neural-network-hidden-Bengio-Mori",
            "title": {
                "fragments": [],
                "text": "Global optimization of a neural network-hidden Markov model hybrid"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "An original method for integrating artificial neural networks (ANN) with hidden Markov models (HMM) with results on speaker-independent recognition experiments using this integrated ANN-HMM system on the TIMIT continuous speech database are reported."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 87
                            }
                        ],
                        "text": ", 2002) connecting the gates to the memory cell were added later to give extended LSTM (Gers, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "LSTM01 LSTM\u539f\u7406\u4e0e\u5e94\u7528\u53ca\u8bad\u7ec3\u65b9\u6cd5_Constant dripping wears \u2026Accepted papers | EMNLP 2021roberta-base \u00b7 Hugging Face\u6709\u54ea\u4e9bLSTM(Long Short Term Memory)\u548cRNN(Recurrent)\u7f51\u7edc\u7684 \u2026(PDF) Deep Learning - ResearchGateFake news detection: A hybrid CNN-RNN based deep learning GitHub - shawnyuen IEEE ICIP 2021 || Anchorage, Alaska, USA || 19-22 30th ACM International Conference on Information and RNN vs LSTM vs GRU -- \u8be5\u9009\u54ea\u4e2a\uff1f - \u77e5\u4e4e - \u77e5\u4e4e\u4e13\u680fRecurrent neural network - Wikipedia\u4e09\u6b21\u7b80\u5316\u4e00\u5f20\u56fe\uff1a\u4e00\u62db\u7406\u89e3LSTM/GRU\u95e8\u63a7\u673a\u5236 - \u77e5\u4e4eDeep learning in remote sensing applications: A meta Turkish Journal of Physiotherapy and Rehabilitation \u3010\u8bba\u6587\u9605\u8bfb\u7b14\u8bb0\u3011NeurIPS2020\u6587\u7ae0\u5217\u8868Part1_zincrain\u7684\u535a\u5ba2 \u2026Text Classification | Papers With CodeGo-to Guide for Text Classification with Machine LearningGlobal quantification of mammalian gene expression control distilbert-base-uncased \u00b7 Hugging FaceA Gentle Introduction to RNN UnrollingSequence Modeling with CTC - Distill\nMutualNet: Adaptive ConvNet via Mutual Learning from Network Width and Resolution \u6587\u7ae0\u76ee\u5f55\u6458\u89811."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Unlike standard feedforward neural networks, LSTM has feedback connections."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "A Beginner's Guide to LSTMs and Recurrent Neural Networks Introduction to Semi-Supervised Learning | Synthesis Sequence Classification with LSTM Recurrent Neural Deep Neural Networks - TutorialspointAccurate and efficient time-domain classification with Long short-term memory - WikipediaCTC (Connectionist Temporal Classification) \u7b97\u6cd5\u539f\u7406 FROM Pre-trained Word Embeddings TO Pre-trained Language Supervised Sequence Labelling with Recurrent Neural \u2026NLP: Pretrained Named Entity Recognition (NER) | by Artificial Intelligence and COVID-19: Deep Learning GitHub - ziyujia/PhysiologicalSignal-Classification What Is Machine Learning and Why Is It Important?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "In this post you discovered how to develop LSTM network models for sequence classification predictive modeling \u2026 (\u539f\u521b\u6587\u7ae0\uff0c\u8f6c\u8f7d\u8bf7\u6ce8\u660e\u51fa\u5904\u54e6~) \u7b80\u5355\u4ecb\u7ecdctc\u7b97\u6cd5 ctc\u662f\u5e8f\u5217\u6807\u6ce8\u95ee\u9898\u4e2d\u7684\u4e00\u79cd\u635f\u5931\u51fd\u6570\u3002 \u4f20\u7edf\u5e8f\u5217\u6807\u6ce8\u7b97\u6cd5\u9700\u8981\u6bcf\u4e00\u65f6\u523b\u8f93\u5165\u4e0e\u8f93\u51fa\u7b26\u53f7\u5b8c\u5168\u5bf9\u9f50\u3002\u800cctc\u6269\u5c55\u4e86\u6807\u7b7e\u96c6\u5408\uff0c\u6dfb\u52a0\u7a7a\u5143\u7d20\u3002 \u5728\u4f7f\u7528\u6269\u5c55\u6807\u7b7e\u96c6 Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "Simple Conversational Data Augmentation for Semi-supervised Abstractive Dialogue \u2026 [2] Supervised Sequence Labelling with Recurrent Neural Networks [3] LSTM\uff1aRNN\u6700\u5e38\u7528\u7684\u53d8\u4f53 [4] Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation [5] Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling Sep 22, 2021 \u00b7 2021 IEEE International Conference on Image Processing 19-22 September 2021 \u2022 Anchorage, Alaska, USA Imaging Without Borders Mar 19, 2020 \u00b7 \u2014 Supervised Sequence Labelling with Recurrent Neural Networks, 2008."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 168
                            }
                        ],
                        "text": "BERT uses a bidirectional Transformer vs. GPT uses a left-to-right Transformer vs. ELMo uses the concatenation of independently trained left-to-right and right-to-left LSTM to generate features for downstream task."
                    },
                    "intents": []
                }
            ],
            "corpusId": 144707025,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "7a046308275557bf424fbf4ccae26da2919c848e",
            "isKey": true,
            "numCitedBy": 227,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "These Ecole polytechnique federale de Lausanne EPFL, n\u00b0 2366 (2001)Faculte informatique et communicationsJury: Paolo Frasconi, Roger Hersch, Martin Rajman, Jurgen Schmidhuber Public defense: 2001-4-6 Reference doi:10.5075/epfl-thesis-2366Print copy in library catalog Record created on 2005-03-16, modified on 2016-08-08"
            },
            "slug": "Long-short-term-memory-in-recurrent-neural-networks-Gers",
            "title": {
                "fragments": [],
                "text": "Long short-term memory in recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "These Ecole polytechnique federale de Lausanne EPFL, n\u00b0 2366 (2001)Faculte informatique et communicationsJury: Paolo Frasconi, Roger Hersch, Martin Rajman, Jurgen Schmidhuber Public defense."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 83
                            }
                        ],
                        "text": "To efficiently calculate the gradient, we use a technique known as backpropagation (Rumelhart et al., 1986; Williams and Zipser, 1995; Werbos, 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 104
                            }
                        ],
                        "text": "The most widely used form of FNN, and the one we focus on in this section, is the multilayer perceptron (MLP; Rumelhart et al., 1986; Werbos, 1988; Bishop, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 205118721,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "266e07d0dd9a75b61e3632e9469993dbaf063f1c",
            "isKey": false,
            "numCitedBy": 882,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalization-of-backpropagation-with-application-Werbos",
            "title": {
                "fragments": [],
                "text": "Generalization of backpropagation with application to a recurrent gas market model"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 43
                            }
                        ],
                        "text": ", 1994), explicitly introduced time delays (Lang et al., 1990; Lin et al., 1996; Plate, 1993) or time constants (Mozer, 1992), and hierarchical sequence compression (Schmidhuber, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 169
                            }
                        ],
                        "text": "As with feedforward networks, many varieties of RNN have been proposed, such as Elman networks (Elman, 1990), Jordan networks (Jordan, 1990), time delay neural networks (Lang et al., 1990) and echo state networks (Jaeger, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1234937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e08d090d1e586610d636a46004876e9f3ded8209",
            "isKey": false,
            "numCitedBy": 640,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-time-delay-neural-network-architecture-for-word-Lang-Waibel",
            "title": {
                "fragments": [],
                "text": "A time-delay neural network architecture for isolated word recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5631525"
                        ],
                        "name": "H. Jaeger",
                        "slug": "H.-Jaeger",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Jaeger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Jaeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15467150,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8430c0b9afa478ae660398704b11dca1221ccf22",
            "isKey": false,
            "numCitedBy": 1942,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The report introduces a constructive learning algorithm for recurrent neural networks, which modifies only the weights to output units in order to achieve the learning task. key words: recurrent neural networks, supervised learning Zusammenfassung. Der Report f\u00fchrt ein konstruktives Lernverfahren f\u00fcr rekurrente neuronale Netze ein, welches zum Erreichen des Lernzieles lediglich die Gewichte der zu den Ausgabeneuronen f\u00fchrenden Verbindungen modifiziert. Stichw\u00f6rter: rekurrente neuronale Netze, \u00fcberwachtes Lernen"
            },
            "slug": "The''echo-state''approach-to-analysing-and-training-Jaeger",
            "title": {
                "fragments": [],
                "text": "The''echo state''approach to analysing and training recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The report introduces a constructive learning algorithm for recurrent neural networks, which modifies only the weights to output units in order to achieve the learning task."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3090152"
                        ],
                        "name": "Ruxin Chen",
                        "slug": "Ruxin-Chen",
                        "structuredName": {
                            "firstName": "Ruxin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruxin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2667435"
                        ],
                        "name": "L. Jamieson",
                        "slug": "L.-Jamieson",
                        "structuredName": {
                            "firstName": "Leah",
                            "lastName": "Jamieson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jamieson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 68
                            }
                        ],
                        "text": "Several alternative error functions have been studied for this task (Chen and Jamieson, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 19197079,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d0050f220b755fe3319fe1e7011b6796c7ad2ea",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reports on an extensive set of experiments that explore training methods and criteria for recurrent neural networks (RNNs) used for speech phone recognition. Seven different criterion functions are evaluated for speech recognition. A new criterion function that allows direct minimization of the frame error rate is proposed. Two new optimization methods for RNN weight updating are investigated. Experiments have been carried out on the Intel Paragon parallel processing system. The performance of the resulting phone recognition system is competitive with the best results in the literature."
            },
            "slug": "Experiments-on-the-implementation-of-recurrent-for-Chen-Jamieson",
            "title": {
                "fragments": [],
                "text": "Experiments on the implementation of recurrent neural networks for speech phone recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "An extensive set of experiments that explore training methods and criteria for recurrent neural networks (RNNs) used for speech phone recognition and proposes a new criterion function that allows direct minimization of the frame error rate."
            },
            "venue": {
                "fragments": [],
                "text": "Conference Record of The Thirtieth Asilomar Conference on Signals, Systems and Computers"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2316518"
                        ],
                        "name": "Yasser Hifny",
                        "slug": "Yasser-Hifny",
                        "structuredName": {
                            "firstName": "Yasser",
                            "lastName": "Hifny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yasser Hifny"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145086187"
                        ],
                        "name": "S. Renals",
                        "slug": "S.-Renals",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Renals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Renals"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 951317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df5b82595a29724467a98eed4d7e2a45e804579e",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "Acoustic modeling based on hidden Markov models (HMMs) is employed by state-of-the-art stochastic speech recognition systems. Although HMMs are a natural choice to warp the time axis and model the temporal phenomena in the speech signal, their conditional independence properties limit their ability to model spectral phenomena well. In this paper, a new acoustic modeling paradigm based on augmented conditional random fields (ACRFs) is investigated and developed. This paradigm addresses some limitations of HMMs while maintaining many of the aspects which have made them successful. In particular, the acoustic modeling problem is reformulated in a data driven, sparse, augmented space to increase discrimination. Acoustic context modeling is explicitly integrated to handle the sequential phenomena of the speech signal. We present an efficient framework for estimating these models that ensures scalability and generality. In the TIMIT phone recognition task, a phone error rate of 23.0% was recorded on the full test set, a significant improvement over comparable HMM-based systems."
            },
            "slug": "Speech-Recognition-Using-Augmented-Conditional-Hifny-Renals",
            "title": {
                "fragments": [],
                "text": "Speech Recognition Using Augmented Conditional Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A new acoustic modeling paradigm based on augmented conditional random fields (ACRFs) is investigated and developed, which addresses some limitations of HMMs while maintaining many of the aspects which have made them successful."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Audio, Speech, and Language Processing"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113861474"
                        ],
                        "name": "C. Bishop",
                        "slug": "C.-Bishop",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bishop"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 191
                            }
                        ],
                        "text": "Adding zero-mean, fixed-variance Gaussian noise to the network inputs during training (sometimes referred to as training with jitter) is a well-established method for improved generalisation (An, 1996; Koistinen and Holmstr\u00f6m, 1991; Bishop, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 104
                            }
                        ],
                        "text": "The most widely used form of FNN, and the one we focus on in this section, is the multilayer perceptron (MLP; Rumelhart et al., 1986; Werbos, 1988; Bishop, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 73
                            }
                        ],
                        "text": ", 2000), and certain pattern classifiers, such as multilayer perceptrons (Rumelhart et al., 1986; Bishop, 1995) and support vector machines (Vapnik, 1995) have become familiar to the scientific community at large."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60563397,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9b1b1654ce0eea729c4160bfedcbb3246460b1d",
            "isKey": false,
            "numCitedBy": 8595,
            "numCiting": 250,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. Also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and Bayesian techniques and their applications. Designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition."
            },
            "slug": "Neural-networks-for-pattern-recognition-Bishop",
            "title": {
                "fragments": [],
                "text": "Neural networks for pattern recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition, and is designed as a text, with over 100 exercises, to benefit anyone involved in the fields of neural computation and pattern recognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60809283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db869fa192a3222ae4f2d766674a378e47013b1b",
            "isKey": false,
            "numCitedBy": 3643,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial \"neural networks\" are widely used as flexible models for classification and regression applications, but questions remain about how the power of these models can be safely exploited when training data is limited. This book demonstrates how Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional training methods. Insight into the nature of these complex Bayesian models is provided by a theoretical investigation of the priors over functions that underlie them. A practical implementation of Bayesian neural network learning using Markov chain Monte Carlo methods is also described, and software for it is freely available over the Internet. Presupposing only basic knowledge of probability and statistics, this book should be of interest to researchers in statistics, engineering, and artificial intelligence."
            },
            "slug": "Bayesian-Learning-for-Neural-Networks-Neal",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Bayesian Learning for Neural Networks shows that Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional neural network learning methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3011589"
                        ],
                        "name": "B. Bakker",
                        "slug": "B.-Bakker",
                        "structuredName": {
                            "firstName": "Bram",
                            "lastName": "Bakker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Bakker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 104
                            }
                        ],
                        "text": ", 2007; Chen and Chaudhari, 2005), music generation (Eck and Schmidhuber, 2002), reinforcement learning (Bakker, 2002), speech recognition (Graves and Schmidhuber, 2005b; Graves et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6627108,
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "id": "3b90b73fa0f904a2dc84bca4b3f80cbb51d7025f",
            "isKey": false,
            "numCitedBy": 226,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents reinforcement learning with a Long Short-Term Memory recurrent neural network: RL-LSTM. Model-free RL-LSTM using Advantage (\u03bb) learning and directed exploration can solve non-Markovian tasks with long-term dependencies between relevant events. This is demonstrated in a T-maze task, as well as in a difficult variation of the pole balancing task."
            },
            "slug": "Reinforcement-Learning-with-Long-Short-Term-Memory-Bakker",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning with Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Model-free RL-LSTM using Advantage (\u03bb) learning and directed exploration can solve non-Markovian tasks with long-term dependencies between relevant events."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784851"
                        ],
                        "name": "T. Sainath",
                        "slug": "T.-Sainath",
                        "structuredName": {
                            "firstName": "Tara",
                            "lastName": "Sainath",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sainath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720857"
                        ],
                        "name": "B. Ramabhadran",
                        "slug": "B.-Ramabhadran",
                        "structuredName": {
                            "firstName": "Bhuvana",
                            "lastName": "Ramabhadran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ramabhadran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774515"
                        ],
                        "name": "M. Picheny",
                        "slug": "M.-Picheny",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Picheny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Picheny"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 120
                            }
                        ],
                        "text": "However the benchmark has since been substantially lowered by the application of improved language modelling techniques (Sainath et al., 2009) and Deep Belief Networks (Mohamed et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 36
                            }
                        ],
                        "text": "4 Discriminative BMMI Triphone HMMs (Sainath et al., 2009) 22."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2141386,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "956b80fe8a56f90782fcb4fe1c536b24496094ba",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "While research in large vocabulary continuous speech recognition (LVCSR) has sparked the development of many state of the art research ideas, research in this domain suffers from two main drawbacks. First, because of the large number of parameters and poorly labeled transcriptions, gaining insight into further improvements based on error analysis is very difficult. Second, LVCSR systems often take a significantly longer time to train and test new research ideas compared to small vocabulary tasks. A small vocabulary task like TIMIT provides a phonetically rich and hand-labeled corpus and offers a good test bed to study algorithmic improvements. However, oftentimes research ideas explored for small vocabulary tasks do not always provide gains on LVCSR systems. In this paper, we address these issues by taking the standard \u021crecipe\u021d used in typical LVCSR systems and applying it to the TIMIT phonetic recognition corpus, which provides a standard benchmark to compare methods. We find that at the speaker-independent (SI) level, our results offer comparable performance to other SI HMM systems. By taking advantage of speaker adaptation and discriminative training techniques commonly used in LVCSR systems, we achieve an error rate of 20%, the best results reported on the TIMIT task to date, moving us closer to the human reported phonetic recognition error rate of 15%. We propose the use of this system as the baseline for future research and believe that it will serve as a good framework to explore ideas that will carry over to LVCSR systems."
            },
            "slug": "An-exploration-of-large-vocabulary-tools-for-small-Sainath-Ramabhadran",
            "title": {
                "fragments": [],
                "text": "An exploration of large vocabulary tools for small vocabulary phonetic recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper takes the standard \u021crecipe\u021d used in typical LVCSR systems and applies it to the TIMIT phonetic recognition corpus, which provides a standard benchmark to compare methods and finds that at the speaker-independent (SI) level, the results offer comparable performance to other SI HMM systems."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Workshop on Automatic Speech Recognition & Understanding"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98637704"
                        ],
                        "name": "X. Pang",
                        "slug": "X.-Pang",
                        "structuredName": {
                            "firstName": "Xiaozhong",
                            "lastName": "Pang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Pang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17877021,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a52222a989a8677825d6a6e45b6dc479f449b4de",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows that a new type of artificial neural network (ANN) -- the Simultaneous Recurrent Network (SRN) -- can, if properly trained, solve a difficult function approximation problem which conventional ANNs -- either feedforward or Hebbian -- cannot. This problem, the problem of generalized maze navigation, is typical of problems which arise in building true intelligent control systems using neural networks. (Such systems are discussed in the chapter by Werbos in K.Pribram, Brain and Values, Erlbaum 1998.) The paper provides a general review of other types of recurrent networks and alternative training techniques, including a flowchart of the Error Critic training design, arguable the only plausible approach to explain how the brain adapts time-lagged recurrent systems in real-time. The C code of the test is appended. As in the first tests of backprop, the training here was slow, but there are ways to do better after more experience using this type of network."
            },
            "slug": "Neural-network-design-for-J-function-approximation-Pang-Werbos",
            "title": {
                "fragments": [],
                "text": "Neural network design for J function approximation in dynamic programming"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This paper shows that a new type of artificial neural network -- the Simultaneous Recurrent Network (SRN) -- can, if properly trained, solve a difficult function approximation problem which conventional ANNs -- either feedforward or Hebbian -- cannot."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2975061"
                        ],
                        "name": "C. Nohl",
                        "slug": "C.-Nohl",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Nohl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Nohl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 183
                            }
                        ],
                        "text": "Although most hybrid HMM-ANN research has focused on speech recognition, the framework is equally applicable to other sequence labelling tasks, such as online handwriting recognition (Bengio et al., 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3179635,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4a4192fd6efb5661eca197cce24289776a4fbcc2",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new approach for on-line recognition of handwritten words written in unconstrained mixed style. The preprocessor performs a word-level normalization by fitting a model of the word structure using the EM algorithm. Words are then coded into low resolution \"annotated images\" where each pixel contains information about trajectory direction and curvature. The recognizer is a convolution network that can be spatially replicated. From the network output, a hidden Markov model produces word scores. The entire system is globally trained to minimize word-level errors."
            },
            "slug": "LeRec:-A-NN/HMM-Hybrid-for-On-Line-Handwriting-Bengio-LeCun",
            "title": {
                "fragments": [],
                "text": "LeRec: A NN/HMM Hybrid for On-Line Handwriting Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A new approach for on-line recognition of handwritten words written in unconstrained mixed style by fitting a model of the word structure using the EM algorithm to minimize word-level errors."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2028880"
                        ],
                        "name": "G. Pollastri",
                        "slug": "G.-Pollastri",
                        "structuredName": {
                            "firstName": "Gianluca",
                            "lastName": "Pollastri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Pollastri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3255005"
                        ],
                        "name": "Alessandro Vullo",
                        "slug": "Alessandro-Vullo",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Vullo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alessandro Vullo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 125
                            }
                        ],
                        "text": "A more efficient way of building multidimensional context into recurrent networks is provided by directed acyclic graph RNNs (DAG-RNNs; Baldi and Pollastri, 2003; Pollastri et al., 2006)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8773848,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10aead64da91c44d2d084a072a367bcbf8146d64",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop and test machine learning methods for the prediction of coarse 3D protein structures, where a protein is represented by a set of rigid rods associated with its secondary structure elements (alpha-helices and beta-strands). First, we employ cascades of recursive neural networks derived from graphical models to predict the relative placements of segments. These are represented as discretized distance and angle maps, and the discretization levels are statistically inferred from a large and curated dataset. Coarse 3D folds of proteins are then assembled starting from topological information predicted in the first stage. Reconstruction is carried out by minimizing a cost function taking the form of a purely geometrical potential. We show that the proposed architecture outperforms simpler alternatives and can accurately predict binary and multiclass coarse maps. The reconstruction procedure proves to be fast and often leads to topologically correct coarse structures that could be exploited as a starting point for various protein modeling strategies. The fully integrated rod-shaped protein builder (predictor of contact maps + reconstruction algorithm) can be accessed at http://distill.ucd.ie/."
            },
            "slug": "Modular-DAG-RNN-Architectures-for-Assembling-Coarse-Pollastri-Vullo",
            "title": {
                "fragments": [],
                "text": "Modular DAG-RNN Architectures for Assembling Coarse Protein Structures"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed architecture outperforms simpler alternatives and can accurately predict binary and multiclass coarse maps and the reconstruction procedure proves to be fast and often leads to topologically correct coarse structures that could be exploited as a starting point for various protein modeling strategies."
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Biol."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116666912"
                        ],
                        "name": "Lin Wu",
                        "slug": "Lin-Wu",
                        "structuredName": {
                            "firstName": "Lin",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lin Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 106
                            }
                        ],
                        "text": "In two dimensions, this structure has been successfully used to evaluate positions in the board game \u2018Go\u2019 (Wu and Baldi, 2006) and to determine two dimensional protein contact maps (Baldi and Pollastri, 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18431486,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d9bf73bfe2e84e41accec12b9cd1c1fe2b3a087",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Go is an ancient board game that poses unique opportunities and challenges for AI and machine learning. Here we develop a machine learning approach to Go, and related board games, focusing primarily on the problem of learning a good evaluation function in a scalable way. Scalability is essential at multiple levels, from the library of local tactical patterns, to the integration of patterns across the board, to the size of the board itself. The system we propose is capable of automatically learning the propensity of local patterns from a library of games. Propensity and other local tactical information are fed into a recursive neural network, derived from a Bayesian network architecture. The network integrates local information across the board and produces local outputs that represent local territory ownership probabilities. The aggregation of these probabilities provides an effective strategic evaluation function that is an estimate of the expected area at the end (or at other stages) of the game. Local area targets for training can be derived from datasets of human games. A system trained using only 9 \u00d7 9 amateur game data performs surprisingly well on a test set derived from 19 \u00d7 19 professional game data. Possible directions for further improvements are briefly discussed."
            },
            "slug": "A-Scalable-Machine-Learning-Approach-to-Go-Wu-Baldi",
            "title": {
                "fragments": [],
                "text": "A Scalable Machine Learning Approach to Go"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A machine learning approach to Go, and related board games, focusing primarily on the problem of learning a good evaluation function in a scalable way, is developed, capable of automatically learning the propensity of local patterns from a library of games."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51072723"
                        ],
                        "name": "Guozhong An",
                        "slug": "Guozhong-An",
                        "structuredName": {
                            "firstName": "Guozhong",
                            "lastName": "An",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guozhong An"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 49741739,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8d37cf4c4a2f15acd7c6ab2b4b4f25a3f3d7f9c",
            "isKey": false,
            "numCitedBy": 372,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the effects of adding noise to the inputs, outputs, weight connections, and weight changes of multilayer feedforward neural networks during backpropagation training. We rigorously derive and analyze the objective functions that are minimized by the noise-affected training processes. We show that input noise and weight noise encourage the neural-network output to be a smooth function of the input or its weights, respectively. In the weak-noise limit, noise added to the output of the neural networks only changes the objective function by a constant. Hence, it cannot improve generalization. Input noise introduces penalty terms in the objective function that are related to, but distinct from, those found in the regularization approaches. Simulations have been performed on a regression and a classification problem to further substantiate our analysis. Input noise is found to be effective in improving the generalization performance for both problems. However, weight noise is found to be effective in improving the generalization performance only for the classification problem. Other forms of noise have practically no effect on generalization."
            },
            "slug": "The-Effects-of-Adding-Noise-During-Backpropagation-An",
            "title": {
                "fragments": [],
                "text": "The Effects of Adding Noise During Backpropagation Training on a Generalization Performance"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that input noise and weight noise encourage the neural-network output to be a smooth function of the input or its weights, respectively, and in the weak-noise limit, noise added to the output of the neural networks only changes the objective function by a constant, it cannot improve generalization."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38071249"
                        ],
                        "name": "P. Koistinen",
                        "slug": "P.-Koistinen",
                        "structuredName": {
                            "firstName": "Petri",
                            "lastName": "Koistinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Koistinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799764"
                        ],
                        "name": "Lasse Holmstr\u00f6m",
                        "slug": "Lasse-Holmstr\u00f6m",
                        "structuredName": {
                            "firstName": "Lasse",
                            "lastName": "Holmstr\u00f6m",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lasse Holmstr\u00f6m"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 191
                            }
                        ],
                        "text": "Adding zero-mean, fixed-variance Gaussian noise to the network inputs during training (sometimes referred to as training with jitter) is a well-established method for improved generalisation (An, 1996; Koistinen and Holmstr\u00f6m, 1991; Bishop, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10487631,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "233bb54014b65ad208effdf25293028ae8c7525f",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "One method proposed for improving the generalization capability of a feedforward network trained with the backpropagation algorithm is to use artificial training vectors which are obtained by adding noise to the original training vectors. The authors discuss the connection of such backpropagation training with noise to kernel density and kernel regression estimation. They compare by simulated examples backpropagation, backpropagation with noise, and kernel regression in mapping estimation and pattern classification contexts. It is concluded that additive noise can improve the generalization capability of a feedforward network trained with the backpropagation approach. The magnitude of the noise cannot be selected blindly, though. Cross-validation-type procedures seem to be well suited for the selection of noise magnitude. Kernel regression, however, seems to perform well whenever backpropagation with noise performs well.<<ETX>>"
            },
            "slug": "Kernel-regression-and-backpropagation-training-with-Koistinen-Holmstr\u00f6m",
            "title": {
                "fragments": [],
                "text": "Kernel regression and backpropagation training with noise"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "It is concluded that additive noise can improve the generalization capability of a feedforward network trained with the backpropagation approach, but the magnitude of the noise cannot be selected blindly, though."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] 1991 IEEE International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 139
                            }
                        ],
                        "text": ", 1994) makes it hard for an RNN to learn tasks containing delays of more than about 10 timesteps between relevant input and target events (Hochreiter et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 29
                            }
                        ],
                        "text": "In practice this shortcoming (referred to in the literature as the vanishing gradient problem; Hochreiter, 1991; Hochreiter et al., 2001; Bengio et al., 1994) makes it hard for an RNN to learn tasks containing delays of more than about 10 timesteps between relevant input and target events (Hochreiter et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17278462,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "aed054834e2c696807cc8b227ac7a4197196e211",
            "isKey": false,
            "numCitedBy": 1566,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "D3EGF(FIH)J KMLONPEGQSRPETN UCV.WYX(Z R.[ V R6\\M[ X N@]_^O\\`JaNcb V RcQ W d EGKeL(^(QgfhKeLOE?i)^(QSj ETNPfPQkRl[ V R)m\"[ X ^(KeLOEG^ npo qarpo m\"[ X ^(KeLOEG^tsAu EGNPb V ^ v wyx zlwO{(|(}<~O\u007fC}\u0081\u0080(\u0082(xp{a\u0083y\u0084.~A}\u0086\u0085\u0088\u0087_~ \u0089C\u008al\u00833\u0089#|<\u0080Az\u0086w#|l\u00806\u0087 \u008b(| \u008c JpfhL X\u008dV\u008f\u008e EG^O\u0090 QgJ \u0091 ETFOR\u0086\u0092\u0093] ^O\\\u0094J\u0095NPb V RcQ\u0097\u0096 X E)ETR \u00986EGKeLOETNcKMLOE\u009a\u0099 F\u0088\u009b ETN V RcQgJp^(^OE ZgZ E i ^(Qkj EGNPfhQSRO\u009b E \u009cOE2m1Jp^ RcNY\u009b E V\u0095Z sO\u009d\u009f\u009e! \u008d\u00a1 q.n sCD X KGKa\u00928\u009d\u00a2EG^ RPNhE\u00a4\u00a3 \u00a5\u00a6Q ZgZ E\u0095s m\u00a7J\u0095^ RPNO\u009b E V\u0095Z s( \u0308 X \u009b EG\u00a9#E\u0081Kas#\u009d V ^ V \u009c V s(H a \u009d\u00aba\u0095\u00ac3\u00ad \u00ae#|.\u0080Y \u0304y} xa\u00b0O\u007fC}l{\u008dx\u0093\u0087 \u0089 \u0083yxl\u0080Y~3{\u008d| \u0084 \u00b12\u0087Pz \u0084 \u009e V J Z J U N V fhKTJp^(Q \u0091 ETFOR\u0086\u0092 J\u0095\\ D vYf3RPEGb \u0301f V ^(\u009c\u00a7\u009d\u0088Jpb\u008fF X RPETN@D KTQ\u0097EG^(KTE i ^(QSjpEGNPfhQSR4v\u03bcJ\u0095\\ U\u00b6Z JaNPEG^(K\u00b7E jYQ V \u009c(Q \u0327D V ^ R V m V N3R V aOs#1 o \u00a1Ga r U Q\u0097NhE\u0081^OoTE1\u20444\u00bb,] R V\u0095Z vC1\u20442 3\u20444 \u0084 x \u00b1 x \u007f \u008b#\u00bf }\u00c0\u0087 \u00893\u0080t}l\u0082C}2\u0087P}<~ \u00act[ X NP\u0090\u0095E\u0081^\u00a7D KeL(b \u0301Qg\u009c(L X \u00a9yETN ] \u0091 DY]_\u00c1 \u009d\u0088J\u0095NPfhJ\u00c3\u00c2 Z j ETo\u0081Q V a\u0095 rpopo2\u00c4 X \u0090 V ^(J(sCD \u00c5)QSRPoTEGN ZgV ^(\u009c \u00c6 \u0089#|\u0095{3 \u0304\u008d|.\u0080(\u007fC}.\u008bC\u00bfY}p\u0084 \u0087Pz\u0086w"
            },
            "slug": "Gradient-Flow-in-Recurrent-Nets:-the-Difficulty-of-Hochreiter-Bengio",
            "title": {
                "fragments": [],
                "text": "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 174
                            }
                        ],
                        "text": "Well known examples of FNNs include perceptrons (Rosenblatt, 1958), radial basis function networks (Broomhead and Lowe, 1988), Kohonen maps (Kohonen, 1989) and Hopfield nets (Hopfield, 1982)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 784288,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98b4d4e24aab57ab4e1124ff8106909050645cfa",
            "isKey": false,
            "numCitedBy": 14025,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices."
            },
            "slug": "Neural-networks-and-physical-systems-with-emergent-Hopfield",
            "title": {
                "fragments": [],
                "text": "Neural networks and physical systems with emergent collective computational abilities."
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A model of a system having a large number of simple equivalent components, based on aspects of neurobiology but readily adapted to integrated circuits, produces a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145757665"
                        ],
                        "name": "Fei Sha",
                        "slug": "Fei-Sha",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Sha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Sha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6759386,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "685f806fb9ea12ab2795ada01b57a414a9d3e45d",
            "isKey": false,
            "numCitedBy": 212,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of parameter estimation in continuous density hidden Markov models (CD-HMMs) for automatic speech recognition (ASR). As in support vector machines, we propose a learning algorithm based on the goal of margin maximization. Unlike earlier work on max-margin Markov networks, our approach is specifically geared to the modeling of real-valued observations (such as acoustic feature vectors) using Gaussian mixture models. Unlike previous discriminative frameworks for ASR, such as maximum mutual information and minimum classification error, our framework leads to a convex optimization, without any spurious local minima. The objective function for large margin training of CD-HMMs is defined over a parameter space of positive semidefinite matrices. Its optimization can be performed efficiently with simple gradient-based methods that scale well to large problems. We obtain competitive results for phonetic recognition on the TIMIT speech corpus."
            },
            "slug": "Large-Margin-Hidden-Markov-Models-for-Automatic-Sha-Saul",
            "title": {
                "fragments": [],
                "text": "Large Margin Hidden Markov Models for Automatic Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This work proposes a learning algorithm based on the goal of margin maximization in continuous density hidden Markov models for automatic speech recognition (ASR) using Gaussian mixture models, and obtains competitive results for phonetic recognition on the TIMIT speech corpus."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145259603"
                        ],
                        "name": "S. Young",
                        "slug": "S.-Young",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Young",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Young"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145731155"
                        ],
                        "name": "N. Russell",
                        "slug": "N.-Russell",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Russell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145984105"
                        ],
                        "name": "J. Thornton",
                        "slug": "J.-Thornton",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Thornton",
                            "middleNames": [
                                "H.",
                                "Simon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Thornton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 76
                            }
                        ],
                        "text": "We now describe an algorithm, based on the token passing algorithm for HMMs (Young et al., 1989), that finds an approximate solution to (7."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59705956,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "963cf8f238745100ac6cc5cf730653a6e1849b62",
            "isKey": false,
            "numCitedBy": 364,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a simple but powerful abstract model in which connected word recognition is viewed as a process of passing tokens around a transition network The advantages of this unifying view are many The various apparently di erent connected word algorithms can be represented within the same conceptual framework simply by changing the network topology the application of grammatical constraints is straightforward and perhaps most importantly the entire structure is independent of the actual underlying pattern matching technology To illustrate the power of this conceptual model the paper concludes by describing some work done under the UK Alvey sponsored VODIS Project in which the Token Passing paradigm enabled the One Pass algorithm to be straightforwardly extended to include the generation of multiple alternatives and context free syntactic constraints"
            },
            "slug": "Token-passing:-a-simple-conceptual-model-for-speech-Young-Russell",
            "title": {
                "fragments": [],
                "text": "Token passing: a simple conceptual model for connected speech recognition systems"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "This paper describes a simple but powerful abstract model in which connected word recognition is viewed as a process of passing tokens around a transition network and some work done under the UK Alvey sponsored VODIS Project is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51083130"
                        ],
                        "name": "F. Rosenblatt",
                        "slug": "F.-Rosenblatt",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Rosenblatt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rosenblatt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 48
                            }
                        ],
                        "text": "Well known examples of FNNs include perceptrons (Rosenblatt, 1958), radial basis function networks (Broomhead and Lowe, 1988), Kohonen maps (Kohonen, 1989) and Hopfield nets (Hopfield, 1982)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12781225,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "5d11aad09f65431b5d3cb1d85328743c9e53ba96",
            "isKey": false,
            "numCitedBy": 9066,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The first of these questions is in the province of sensory physiology, and is the only one for which appreciable understanding has been achieved. This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory. With regard to the second question, two alternative positions have been maintained. The first suggests that storage of sensory information is in the form of coded representations or images, with some sort of one-to-one mapping between the sensory stimulus"
            },
            "slug": "The-perceptron:-a-probabilistic-model-for-storage-Rosenblatt",
            "title": {
                "fragments": [],
                "text": "The perceptron: a probabilistic model for information storage and organization in the brain."
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1958
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745906"
                        ],
                        "name": "Y. Konig",
                        "slug": "Y.-Konig",
                        "structuredName": {
                            "firstName": "Yochai",
                            "lastName": "Konig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Konig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1980449"
                        ],
                        "name": "C. Ris",
                        "slug": "C.-Ris",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Ris",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ris"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 130
                            }
                        ],
                        "text": "The two components in a the hybrid can be trained independently, but many authors have proposed methods for combined optimisation (Bengio et al., 1992; Bourlard et al., 1996; Hennebert et al., 1997; Trentin and Gori, 2003) which typically yields improved results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16891274,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f2be19f013554bd6c6ac99eabfe6ebc2c2d2c9f",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we briefly describe REMAP, an approach for the training and estimation of posterior probabilities, and report its application to speech recognition. REMAP is a recursive algorithm that is reminiscent of the Expectation Maximization (EM) [5] algorithm for the estimation of data likelihoods. Although very general, the method is developed in the context of a statistical model for transition-based speech recognition using Artificial Neural Networks (ANN) to generate probabilities for Hidden Markov Models (HMMs). In the new approach, we use local conditional posterior probabilities of transitions to estimate global posterior probabilities of word sequences. As with earlier hybrid HMM/ANN systems we have developed, ANNs are used to estimate posterior probabilities. In the new approach, however, the network is trained with targets that are themselves estimates of local posterior probabilities. Initial experimental results support the theory by showing an increase in the estimates of posterior probabilities of the correct sentences after REMAP iterations, and a decrease in error rate for an independent test set."
            },
            "slug": "A-new-training-algorithm-for-hybrid-HMM/ANN-speech-Bourlard-Konig",
            "title": {
                "fragments": [],
                "text": "A new training algorithm for hybrid HMM/ANN speech recognition systems"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Initial experimental results support the theory by showing an increase in the estimates of posterior probabilities of the correct sentences after REMAP iterations, and a decrease in error rate for an independent test set."
            },
            "venue": {
                "fragments": [],
                "text": "1996 8th European Signal Processing Conference (EUSIPCO 1996)"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 144
                            }
                        ],
                        "text": "A similar, but more general, framework for combining neural networks with other sequential algorithms is provided by graph transformer networks (LeCun et al., 1997, 1998a; Bottou and LeCun, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14704082,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f78017ab52a9e10d206da41363ec0c11a10e4757",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Consider a system that takes the image of a check and returns the check amount. This system locates the numerical amount, recognizes digits or other symbols, and parses the check amount. Accuracy should remain high despite countless variations in check layout, writing style or amount grammar. From an engineering perspective, one must design components for locating the amount, segmenting characters, recognizing digits, and parsing the amount text. Yet it is very difficult to locate the amount without identifying that it is composed of characters that mostly resemble digits and form a meaningful check amount (not a date or a routing number). Purely sequential approaches do not work. Components must interact, form hypotheses and backtrack erroneous decisions. The orchestration is difficult to design and costly to maintain. From a statistical perspective, one seeks to estimate and compare the posterior probabilities P (Y |X) where variable X represents a check image and variable Y represents a check amount. Let us define a suitable parametric model p\u03b8(y|x), gather data pairs (xi, yi), and maximize the likelihood \u2211 i log p\u03b8(yi|xi). Such a direct approach leads to problems of unpractical sizes. It is therefore common to manually annotate some pairs (xi, yi) with detailled information such as isolated character images T , character codes C, or sequences S of character codes. One can then model P (C|T ) and P (Y |S) and obtain components such as a character recognizer or an amount parser. The statistical perspective suggests a principled way to orchestrate the interaction of these components: let the global model p\u03b8(y|x) be expressed as a composition of submodels such as p\u03b8(c|t) and p\u03b8(y|s). The submodels are first fit using the detailled data. The resulting parameters are used as a bias when fitting the global model p\u03b8(y|x) using the initial data pairs (yi|xi). This bias can be viewed as a capacity control tool for structural risk minimization (Vapnik, 1982). Model composition works nicely with generative models where one seeks to estimate the joint density P (X,Y ) instead of the posterior P (Y |X). For instance, Hidden Markov Models (HMM) for speech recognition (Rabiner, 1989) use the decomposition"
            },
            "slug": "Graph-transformer-networks-for-image-recognition-Bottou-LeCun",
            "title": {
                "fragments": [],
                "text": "Graph transformer networks for image recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The statistical perspective suggests a principled way to orchestrate the interaction of these components: let the global model p\u03b8(y|x) be expressed as a composition of submodels such as p \u03b8(c|t) and p\u03b7s, and the submodels are first fit using the detailled data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144711425"
                        ],
                        "name": "T. Robinson",
                        "slug": "T.-Robinson",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Robinson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Robinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48889683"
                        ],
                        "name": "L. Almeida",
                        "slug": "L.-Almeida",
                        "structuredName": {
                            "firstName": "Lu\u00eds",
                            "lastName": "Almeida",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Almeida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3342906"
                        ],
                        "name": "Jean-Marc Boite",
                        "slug": "Jean-Marc-Boite",
                        "structuredName": {
                            "firstName": "Jean-Marc",
                            "lastName": "Boite",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Marc Boite"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1998157"
                        ],
                        "name": "F. Fallside",
                        "slug": "F.-Fallside",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Fallside",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fallside"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39213695"
                        ],
                        "name": "M. Hochberg",
                        "slug": "M.-Hochberg",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Hochberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hochberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066474202"
                        ],
                        "name": "Dan J. Kershaw",
                        "slug": "Dan-J.-Kershaw",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Kershaw",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan J. Kershaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143869759"
                        ],
                        "name": "P. Kohn",
                        "slug": "P.-Kohn",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Kohn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kohn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745906"
                        ],
                        "name": "Y. Konig",
                        "slug": "Y.-Konig",
                        "structuredName": {
                            "firstName": "Yochai",
                            "lastName": "Konig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Konig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723288"
                        ],
                        "name": "J. Neto",
                        "slug": "J.-Neto",
                        "structuredName": {
                            "firstName": "Joao",
                            "lastName": "Neto",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Neto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145086187"
                        ],
                        "name": "S. Renals",
                        "slug": "S.-Renals",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Renals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Renals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2356015"
                        ],
                        "name": "Marco Saerens",
                        "slug": "Marco-Saerens",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Saerens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Saerens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705919"
                        ],
                        "name": "Chuck Wooters",
                        "slug": "Chuck-Wooters",
                        "structuredName": {
                            "firstName": "Chuck",
                            "lastName": "Wooters",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuck Wooters"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7671475,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d419843f2b77b6e0403513f96489b41360373426",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "International Computer Science Institute (ICSI), USA(Author list is alphabetical with the exception of the typist.)ABSTRACTThis paper describes the research underway for the ESPRITWERNICKE project. The project brings together a num-ber of different groups from Europe and the US and focuseson extending the state-of-the-art for hybrid hidden Markovmodel/connectionist approaches to large vocabulary, continu-ous speech recognition. Thispaper describes the speci\ufb01c goalsoftheresearchandpresentstheworkperformedtodate. Resultsare reported for the resource management talker-independentrecognition task. The paper concludes with a discussion of theprojected future work.Keywords: Recognition, Neural Nets, HMM.1. BACKGROUNDW"
            },
            "slug": "A-neural-network-based,-speaker-independent,-large-Robinson-Almeida",
            "title": {
                "fragments": [],
                "text": "A neural network based, speaker independent, large vocabulary, continuous speech recognition system: the WERNICKE project"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The ESPRITWERNICKE project brings together a number of different groups from Europe and the US and focuses on extending the state-of-the-art for hybrid hidden Markov model/connectionist approaches to large vocabulary, continu-ous speech recognition."
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109474526"
                        ],
                        "name": "Jeremy Morris",
                        "slug": "Jeremy-Morris",
                        "structuredName": {
                            "firstName": "Jeremy",
                            "lastName": "Morris",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeremy Morris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398481836"
                        ],
                        "name": "E. Fosler-Lussier",
                        "slug": "E.-Fosler-Lussier",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Fosler-Lussier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Fosler-Lussier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11305607,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0125b014ff5171c74bd6d8365f4cffe3714c0b0",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "A Conditional Random Field is a mathematical model for sequences that is similar in many ways to a Hidden Markov Model, but is discriminative rather than generative in nature. In this paper, we explore the application of the CRF model to ASR processing of discriminative phonetic features by building a system that performs first-pass phonetic recognition using discriminatively trained phonetic features. With this system, we show that this CRF model achieves an accuracy level in a phone recognition task that is superior to a similarly trained HMM model. Index Terms: speech recognition, conditional random fields."
            },
            "slug": "Combining-phonetic-attributes-using-conditional-Morris-Fosler-Lussier",
            "title": {
                "fragments": [],
                "text": "Combining phonetic attributes using conditional random fields"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper builds a system that performs first-pass phonetic recognition using discriminatively trained phonetic features and shows that this CRF model achieves an accuracy level in a phone recognition task that is superior to a similarly trained HMM model."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686820"
                        ],
                        "name": "B. M\u00e9rialdo",
                        "slug": "B.-M\u00e9rialdo",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "M\u00e9rialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. M\u00e9rialdo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1968526"
                        ],
                        "name": "J. Jiten",
                        "slug": "J.-Jiten",
                        "structuredName": {
                            "firstName": "Joakim",
                            "lastName": "Jiten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jiten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2086066"
                        ],
                        "name": "B. Huet",
                        "slug": "B.-Huet",
                        "structuredName": {
                            "firstName": "Benoit",
                            "lastName": "Huet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Huet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 34
                            }
                        ],
                        "text": ", 2005), and dependency tree HMMs (Jiten et al., 2006)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15161049,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c51336e8259e0d9269d4c10120eb49ebf9882973",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a new type of multi-dimensional hidden Markov model based on the idea of dependency tree between positions. This simplification leads to an efficient implementation of the re-estimation algorithms, while keeping a mix of horizontal and vertical dependencies between positions. We explain DT-HMM and we present the formulas for the maximum likelihood re-estimation. We illustrate the algorithm by training a 2-dimensional model on a set of coherent images"
            },
            "slug": "Multi-Dimensional-Dependency-Tree-Hidden-Markov-M\u00e9rialdo-Jiten",
            "title": {
                "fragments": [],
                "text": "Multi-Dimensional Dependency-Tree Hidden Markov Models"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "This paper proposes a new type of multi-dimensional hidden Markov model based on the idea of dependency tree between positions that leads to an efficient implementation of the re-estimation algorithms, while keeping a mix of horizontal and vertical dependencies between positions."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145086187"
                        ],
                        "name": "S. Renals",
                        "slug": "S.-Renals",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Renals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Renals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111403728"
                        ],
                        "name": "Michael Cohen",
                        "slug": "Michael-Cohen",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Cohen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145660147"
                        ],
                        "name": "H. Franco",
                        "slug": "H.-Franco",
                        "structuredName": {
                            "firstName": "Horacio",
                            "lastName": "Franco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Franco"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2077908,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a08c99425ad94eed67d059813511fe9ca55e73eb",
            "isKey": false,
            "numCitedBy": 292,
            "numCiting": 131,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors are concerned with integrating connectionist networks into a hidden Markov model (HMM) speech recognition system. This is achieved through a statistical interpretation of connectionist networks as probability estimators. They review the basis of HMM speech recognition and point out the possible benefits of incorporating connectionist networks. Issues necessary to the construction of a connectionist HMM recognition system are discussed, including choice of connectionist probability estimator. They describe the performance of such a system using a multilayer perceptron probability estimator evaluated on the speaker-independent DARPA Resource Management database. In conclusion, they show that a connectionist component improves a state-of-the-art HMM system. >"
            },
            "slug": "Connectionist-probability-estimators-in-HMM-speech-Renals-Morgan",
            "title": {
                "fragments": [],
                "text": "Connectionist probability estimators in HMM speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is shown that a connectionist component improves a state-of-the-art HMM system through a statistical interpretation of connectionist networks as probability estimators."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Speech Audio Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 196
                            }
                        ],
                        "text": "Two well-known algorithms have been devised to efficiently calculate weight derivatives for RNNs: real time recurrent learning (RTRL; Robinson and Fallside, 1987) and backpropagation through time (BPTT; Williams and Zipser, 1995; Werbos, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18470994,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1a3d22599028a05669e884f3eaf19a342e190a87",
            "isKey": false,
            "numCitedBy": 4036,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Backpropagation is now the most widely used tool in the field of artificial neural networks. At the core of backpropagation is a method for calculating derivatives exactly and efficiently in any large system made up of elementary subsystems or calculations which are represented by known, differentiable functions; thus, backpropagation has many applications which do not involve neural networks as such. This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis. Next, it presents the basic equations for backpropagation through time, and discusses applications to areas like pattern recognition involving dynamic systems, systems identification, and control. Finally, i t describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method. Pseudocode is provided to clarify the algorithms. The chain rule for ordered derivatives-the theorem which underlies backpropagation-is briefly discussed."
            },
            "slug": "Backpropagation-Through-Time:-What-It-Does-and-How-Werbos",
            "title": {
                "fragments": [],
                "text": "Backpropagation Through Time: What It Does and How to Do It"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis, and describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143847857"
                        ],
                        "name": "Jane W. Chang",
                        "slug": "Jane-W.-Chang",
                        "structuredName": {
                            "firstName": "Jane",
                            "lastName": "Chang",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jane W. Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 39
                            }
                        ],
                        "text": "6 Near-miss Probabilistic Segmentation (Chang, 1998) 25."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1597965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78b8d6afee6a1a46ed3b89ccfc76cdc4a2fdbff6",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "Currently, most approaches to speech recognition are frame-based in that they represent speech as a temporal sequence of feature vectors. Although these approaches have been successful, they cannot easily incorporate complex modeling strategies that may further improve speech recognition performance. In contrast, segment-based approaches represent speech as a temporal graph of feature vectors and facilitate the incorporation of a wide range of modeling strategies. However, difficulties in segmentbased recognition have impeded the realization of potential advantages in modeling. This thesis describes an approach called near-miss modeling that addresses the major difficulties in segment-based recognition. Probabilistically, each path should account for the entire graph including the segments that are off the path as well as the segments that are on the path. Near-miss modeling is based on the idea that an off-path segment can be modeled as a \"near-miss\" of an on-path segment. Each segment is associated with a near-miss subset of segments that contains the on-path segment as well as zero or more off-path segments such that the near-miss subsets that are associated with any path account for the entire graph. Computationally, the graph should contain only a small number of segments without introducing a large number of segmentation errors. Near-miss modeling runs a recognizer and produces a graph that contains only the segments on paths that score within a threshold of the best scoring path. A near-miss recognizer using context-independent segment-based acoustic models, diphone context-dependent frame-based models, and a phone bigram language model achieves a 25.5% error rate on the TIMIT core test set over 39 classes. This is a 16% reduction in error rate from our best previously reported result and, to our knowledge, is the lowest error rate that has been reported under comparable conditions. Additional experiments using the ATIS corpus verify that these improvements generalize to word recognition. Thesis Supervisor: James R. Glass Title: Principal Research Scientist"
            },
            "slug": "Near-miss-modeling:-a-segment-based-approach-to-Chang",
            "title": {
                "fragments": [],
                "text": "Near-miss modeling: a segment-based approach to speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This thesis describes an approach called near-miss modeling that addresses the major difficulties in segment-based recognition and runs a recognizer and produces a graph that contains only the segments on paths that score within a threshold of the best scoring path."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40116905"
                        ],
                        "name": "Jia Li",
                        "slug": "Jia-Li",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145248524"
                        ],
                        "name": "A. Najmi",
                        "slug": "A.-Najmi",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Najmi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Najmi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144790332"
                        ],
                        "name": "R. Gray",
                        "slug": "R.-Gray",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gray",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gray"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 28
                            }
                        ],
                        "text": ", 2001), isolating elements (Li et al., 2000), approximate Viterbi algorithms (Joshi et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1613124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79368bfbeab606c13c29f59492b88af4e031220d",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "For block-based classification, an image is divided into blocks, and a feature vector is formed for each block by grouping statistics extracted from the block. Conventional block-based classification algorithms decide the class of a block by examining only the feature vector of this block and ignoring context information. In order to improve classification by context, an algorithm is proposed that models images by two dimensional (2-D) hidden Markov models (HMMs). The HMM considers feature vectors statistically dependent through an underlying state process assumed to be a Markov mesh, which has transition probabilities conditioned on the states of neighboring blocks from both horizontal and vertical directions. Thus, the dependency in two dimensions is reflected simultaneously. The HMM parameters are estimated by the EM algorithm. To classify an image, the classes with maximum a posteriori probability are searched jointly for all the blocks. Applications of the HMM algorithm to document and aerial image segmentation show that the algorithm outperforms CART/sup TM/, LVQ, and Bayes VQ."
            },
            "slug": "Image-classification-by-a-two-dimensional-hidden-Li-Najmi",
            "title": {
                "fragments": [],
                "text": "Image classification by a two-dimensional hidden Markov model"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "An algorithm is proposed that models images by two dimensional (2-D) hidden Markov models (HMMs) that outperforms CART/sup TM/, LVQ, and Bayes VQ in classification by context."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723288"
                        ],
                        "name": "J. Neto",
                        "slug": "J.-Neto",
                        "structuredName": {
                            "firstName": "Joao",
                            "lastName": "Neto",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Neto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48889683"
                        ],
                        "name": "L. Almeida",
                        "slug": "L.-Almeida",
                        "structuredName": {
                            "firstName": "Lu\u00eds",
                            "lastName": "Almeida",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Almeida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39213695"
                        ],
                        "name": "M. Hochberg",
                        "slug": "M.-Hochberg",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Hochberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hochberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112813430"
                        ],
                        "name": "Ciro Martins",
                        "slug": "Ciro-Martins",
                        "structuredName": {
                            "firstName": "Ciro",
                            "lastName": "Martins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ciro Martins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053277981"
                        ],
                        "name": "Lu\u00eds Nunes",
                        "slug": "Lu\u00eds-Nunes",
                        "structuredName": {
                            "firstName": "Lu\u00eds",
                            "lastName": "Nunes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lu\u00eds Nunes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145086187"
                        ],
                        "name": "S. Renals",
                        "slug": "S.-Renals",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Renals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Renals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144711425"
                        ],
                        "name": "T. Robinson",
                        "slug": "T.-Robinson",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Robinson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Robinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 69
                            }
                        ],
                        "text": "However there has also been considerable interest in the use of RNNs (Robinson, 1994; Neto et al., 1995; Kershaw et al., 1996; Senior and Robinson, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10013344,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83fab3b9a434cd3a93e2367bd52af5c3ea66c7b2",
            "isKey": false,
            "numCitedBy": 210,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "It is well known that recognition performance degrades signi cantly when moving from a speakerdependent to a speaker-independent system. Traditional hidden Markov model (HMM) systems have successfully applied speaker-adaptation approaches to reduce this degradation. In this paper we present and evaluate some techniques for speaker-adaptation of a hybrid HMM-arti cial neural network (ANN) continuous speech recognition system. These techniques are applied to a well trained, speaker-independent, hybrid HMM-ANN system and the recognizer parameters are adapted to a new speaker through o -line procedures. The techniques are evaluated on the DARPA RM corpus using varying amounts of adaptation material and different ANN architectures. The results show that speaker-adaptation within the hybrid framework can substantially improve system performance."
            },
            "slug": "Speaker-adaptation-for-hybrid-HMM-ANN-continuous-Neto-Almeida",
            "title": {
                "fragments": [],
                "text": "Speaker-adaptation for hybrid HMM-ANN continuous speech recognition system"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "These techniques are applied to a well trained, speaker-independent, hybrid HMM-ANN system and the recognizer parameters are adapted to a new speaker through o -line procedures and show that speaker-adaptation within the hybrid framework can substantially improve system performance."
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102429215"
                        ],
                        "name": "Kai-Fu Lee",
                        "slug": "Kai-Fu-Lee",
                        "structuredName": {
                            "firstName": "Kai-Fu",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai-Fu Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145058181"
                        ],
                        "name": "H. Hon",
                        "slug": "H.-Hon",
                        "structuredName": {
                            "firstName": "Hsiao-Wuen",
                            "lastName": "Hon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 37373402,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3034afcd45fc190ed71982828b77f6e4154bdc5c",
            "isKey": false,
            "numCitedBy": 1044,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov modeling is extended to speaker-independent phone recognition. Using multiple codebooks of various linear-predictive-coding (LPC) parameters and discrete hidden Markov models (HMMs) the authors obtain a speaker-independent phone recognition accuracy of 58.8-73.8% on the TIMIT database, depending on the type of acoustic and language models used. In comparison, the performance of expert spectrogram readers is only 69% without use of higher level knowledge. The authors introduce the co-occurrence smoothing algorithm, which enables accurate recognition even with very limited training data. Since the results were evaluated on a standard database, they can be used as benchmarks to evaluate future systems. >"
            },
            "slug": "Speaker-independent-phone-recognition-using-hidden-Lee-Hon",
            "title": {
                "fragments": [],
                "text": "Speaker-independent phone recognition using hidden Markov models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The authors introduce the co-occurrence smoothing algorithm, which enables accurate recognition even with very limited training data, and can be used as benchmarks to evaluate future systems."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37526757"
                        ],
                        "name": "Urs-Viktor Marti",
                        "slug": "Urs-Viktor-Marti",
                        "structuredName": {
                            "firstName": "Urs-Viktor",
                            "lastName": "Marti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Urs-Viktor Marti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 49
                            }
                        ],
                        "text": "Substantial preprocessing was used for this task (Marti and Bunke, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10207300,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e15725948c2ea8b190b825a0887e430dc4898428",
            "isKey": false,
            "numCitedBy": 486,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, a system for the reading of totally unconstrained handwritten text is presented. The kernel of the system is a hidden Markov model (HMM) for handwriting recognition. The HMM is enhanced by a statistical language model. Thus linguistic knowledge beyond the lexicon level is incorporated in the recognition process. Another novel feature of the system is that the HMM is applied in such a way that the difficult problem of segmenting a line of text into individual words is avoided. A number of experiments with various language models and large vocabularies have been conducted. The language models used in the system were also analytically compared based on their perplexity."
            },
            "slug": "Using-a-Statistical-Language-Model-to-Improve-the-Marti-Bunke",
            "title": {
                "fragments": [],
                "text": "Using a Statistical Language Model to Improve the Performance of an HMM-Based Cursive Handwriting Recognition System"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel feature of the system is that the HMM is applied in such a way that the difficult problem of segmenting a line of text into individual words is avoided and linguistic knowledge beyond the lexicon level is incorporated in the recognition process."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2512508"
                        ],
                        "name": "Andrew K. Halberstadt",
                        "slug": "Andrew-K.-Halberstadt",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Halberstadt",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew K. Halberstadt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 95
                            }
                        ],
                        "text": "The TIMIT corpus was divided into a training set, a validation set and a test set according to (Halberstadt, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 864370,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0ff60013a985f932e2bdf4223b86c2d3ce25d77",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 103,
            "paperAbstract": {
                "fragments": [],
                "text": "The acoustic-phonetic modeling component of most current speech recognition systems calculates a small set of homogeneous frame-based measurements at a single, fixed time-frequency resolution. This thesis presents evidence indicating that recognition performance can be significantly improved through a contrasting approach using more detailed and more diverse acoustic measurements, which we refer to as heterogeneous measurements. \nThis investigation has three principal goals. The first goal is to develop heterogeneous acoustic measurements to increase the amount of acoustic-phonetic information I extracted from the speech signal. Diverse measurements are obtained by varying the time-frequency resolution, the spectral representation, the choice of temporal basis vectors, and other aspects of the preprocessing of the speech waveform. The second goal is to develop classifier systems for successfully utilizing high-dimensional heterogeneous acoustic measurement spaces. This is accomplished through hierarchical and committee-based techniques for combining multiple classifiers. The third goal is to increase understanding of the weaknesses of current automatic phonetic classification systems. This is accomplished through perceptual experiments on stop consonants which facilitate comparisons between humans and machines. \nSystems using heterogeneous measurements and multiple classifiers were evaluated in phonetic classification, phonetic recognition, and word recognition tasks. On the TIMIT core test set, these systems achieved error rates of 18.3% and 24.4% for, context-independent phonetic classification and context-dependent phonetic recognition, respectively. These results are the best that we have seen reported on these tasks. Word recognition experiments using the corpus associated with the JUPITER telephone-based weather information system showed 10\u201316% word error rate reduction, thus demonstrating that these techniques generalize to word recognition in a telephone-bandwidth acoustic environment. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)"
            },
            "slug": "Heterogeneous-acoustic-measurements-and-multiple-Halberstadt",
            "title": {
                "fragments": [],
                "text": "Heterogeneous acoustic measurements and multiple classifiers for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Evidence is presented indicating that recognition performance can be significantly improved through a contrasting approach using more detailed and more diverse acoustic measurements, which are referred to as heterogeneous measurements, as well as understanding of the weaknesses of current automatic phonetic classification systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3137672"
                        ],
                        "name": "Martin A. Riedmiller",
                        "slug": "Martin-A.-Riedmiller",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Riedmiller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin A. Riedmiller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145326760"
                        ],
                        "name": "H. Braun",
                        "slug": "H.-Braun",
                        "structuredName": {
                            "firstName": "Heinrich",
                            "lastName": "Braun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Braun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 95
                            }
                        ],
                        "text": "A large number of sophisticated gradient descent algorithms have been developed, such as RPROP (Riedmiller and Braun, 1993), quickprop (Fahlman, 1989), conjugate gradients (Hestenes and Stiefel, 1952; Shewchuk, 1994) and L-BFGS (Byrd et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16848428,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "916ceefae4b11dadc3ee754ce590381c568c90de",
            "isKey": false,
            "numCitedBy": 4443,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A learning algorithm for multilayer feedforward networks, RPROP (resilient propagation), is proposed. To overcome the inherent disadvantages of pure gradient-descent, RPROP performs a local adaptation of the weight-updates according to the behavior of the error function. Contrary to other adaptive techniques, the effect of the RPROP adaptation process is not blurred by the unforeseeable influence of the size of the derivative, but only dependent on the temporal behavior of its sign. This leads to an efficient and transparent adaptation process. The capabilities of RPROP are shown in comparison to other adaptive techniques.<<ETX>>"
            },
            "slug": "A-direct-adaptive-method-for-faster-backpropagation-Riedmiller-Braun",
            "title": {
                "fragments": [],
                "text": "A direct adaptive method for faster backpropagation learning: the RPROP algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A learning algorithm for multilayer feedforward networks, RPROP (resilient propagation), is proposed that performs a local adaptation of the weight-updates according to the behavior of the error function to overcome the inherent disadvantages of pure gradient-descent."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Conference on Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769923"
                        ],
                        "name": "T. Lindblad",
                        "slug": "T.-Lindblad",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Lindblad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lindblad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7483079"
                        ],
                        "name": "J. Kinser",
                        "slug": "J.-Kinser",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Kinser",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kinser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 62
                            }
                        ],
                        "text": "Other neural network based approaches to two-dimensional data (Pang and Werbos, 1996; Lindblad and Kinser, 2005) have been structured like cellular automata."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2539705,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b8ad477688ec51485e09b40e0897dce7a6f73d5",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 91,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nPCNN's represent a new advance in imaging technology, allowing images to be refined to levels well beyond that of the original. This volume provides an introduction to the topic by reviewing the theoretical foundations as well as a number of image processing applications, including segmentation, edge extraction, texture extraction, object identification, object isolation, motion processing, noise suppression, and image fusion. This is the first book to cover PCNN technology, an area which will have many applications in medical, military and industrial imaging."
            },
            "slug": "Image-Processing-using-Pulse-Coupled-Neural-Lindblad-Kinser",
            "title": {
                "fragments": [],
                "text": "Image Processing using Pulse-Coupled Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This volume provides an introduction to the topic by reviewing the theoretical foundations as well as a number of image processing applications, including segmentation, edge extraction, texture extraction, object identification, object isolation, motion processing, noise suppression, and image fusion."
            },
            "venue": {
                "fragments": [],
                "text": "Perspectives in Neural Computing"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37526757"
                        ],
                        "name": "Urs-Viktor Marti",
                        "slug": "Urs-Viktor-Marti",
                        "structuredName": {
                            "firstName": "Urs-Viktor",
                            "lastName": "Marti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Urs-Viktor Marti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29622813,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "04a10e1b25f35a9ac1a4d4344bfbdb34b253cb59",
            "isKey": false,
            "numCitedBy": 1058,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. In this paper we describe a database that consists of handwritten English sentences. It is based on the Lancaster-Oslo/Bergen (LOB) corpus. This corpus is a collection of texts that comprise about one million word instances. The database includes 1,066 forms produced by approximately 400 different writers. A total of 82,227 word instances out of a vocabulary of 10,841 words occur in the collection. The database consists of full English sentences. It can serve as a basis for a variety of handwriting recognition tasks. However, it is expected that the database would be particularly useful for recognition tasks where linguistic knowledge beyond the lexicon level is used, because this knowledge can be automatically derived from the underlying corpus. The database also includes a few image-processing procedures for extracting the handwritten text from the forms and the segmentation of the text into lines and words."
            },
            "slug": "The-IAM-database:-an-English-sentence-database-for-Marti-Bunke",
            "title": {
                "fragments": [],
                "text": "The IAM-database: an English sentence database for offline handwriting recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A database that consists of handwritten English sentences based on the Lancaster-Oslo/Bergen corpus, which is expected that the database would be particularly useful for recognition tasks where linguistic knowledge beyond the lexicon level is used."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal on Document Analysis and Recognition"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2445103"
                        ],
                        "name": "Martin Heusel",
                        "slug": "Martin-Heusel",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Heusel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin Heusel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743272"
                        ],
                        "name": "K. Obermayer",
                        "slug": "K.-Obermayer",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Obermayer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Obermayer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 131
                            }
                        ],
                        "text": "It has also proved advantageous in real-world domains such as speech processing (Graves and Schmidhuber, 2005b) and bioinformatics (Hochreiter et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 115
                            }
                        ],
                        "text": "Additionally, LSTM has been applied to various real-world problems, such as protein secondary structure prediction (Hochreiter et al., 2007; Chen and Chaudhari, 2005), music generation (Eck and Schmidhuber, 2002), reinforcement learning (Bakker, 2002), speech recognition (Graves and Schmidhuber, 2005b; Graves et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18823115,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "004910391854fc7a507ffb27c65b4a265cd197a3",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "MOTIVATION\nAs more genomes are sequenced, the demand for fast gene classification techniques is increasing. To analyze a newly sequenced genome, first the genes are identified and translated into amino acid sequences which are then classified into structural or functional classes. The best-performing protein classification methods are based on protein homology detection using sequence alignment methods. Alignment methods have recently been enhanced by discriminative methods like support vector machines (SVMs) as well as by position-specific scoring matrices (PSSM) as obtained from PSI-BLAST. However, alignment methods are time consuming if a new sequence must be compared to many known sequences-the same holds for SVMs. Even more time consuming is to construct a PSSM for the new sequence. The best-performing methods would take about 25 days on present-day computers to classify the sequences of a new genome (20,000 genes) as belonging to just one specific class--however, there are hundreds of classes. Another shortcoming of alignment algorithms is that they do not build a model of the positive class but measure the mutual distance between sequences or profiles. Only multiple alignments and hidden Markov models are popular classification methods which build a model of the positive class but they show low classification performance. The advantage of a model is that it can be analyzed for chemical properties common to the class members to obtain new insights into protein function and structure. We propose a fast model-based recurrent neural network for protein homology detection, the 'Long Short-Term Memory' (LSTM). LSTM automatically extracts indicative patterns for the positive class, but in contrast to profile methods it also extracts negative patterns and uses correlations between all detected patterns for classification. LSTM is capable to automatically extract useful local and global sequence statistics like hydrophobicity, polarity, volume, polarizability and combine them with a pattern. These properties make LSTM complementary to alignment-based approaches as it does not use predefined similarity measures like BLOSUM or PAM matrices.\n\n\nRESULTS\nWe have applied LSTM to a well known benchmark for remote protein homology detection, where a protein must be classified as belonging to a SCOP superfamily. LSTM reaches state-of-the-art classification performance but is considerably faster for classification than other approaches with comparable classification performance. LSTM is five orders of magnitude faster than methods which perform slightly better in classification and two orders of magnitude faster than the fastest SVM-based approaches (which, however, have lower classification performance than LSTM). Only PSI-BLAST and HMM-based methods show comparable time complexity as LSTM, but they cannot compete with LSTM in classification performance. To test the modeling capabilities of LSTM, we applied LSTM to PROSITE classes and interpreted the extracted patterns. In 8 out of 15 classes, LSTM automatically extracted the PROSITE motif. In the remaining 7 cases alternative motifs are generated which give better classification results on average than the PROSITE motifs.\n\n\nAVAILABILITY\nThe LSTM algorithm is available from http://www.bioinf.jku.at/software/LSTM_protein/."
            },
            "slug": "Fast-model-based-protein-homology-detection-without-Hochreiter-Heusel",
            "title": {
                "fragments": [],
                "text": "Fast model-based protein homology detection without alignment"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A fast model-based recurrent neural network for protein homology detection, the 'Long Short-Term Memory' (LSTM), which reaches state-of-the-art classification performance but is considerably faster for classification than other approaches with comparable classification performance."
            },
            "venue": {
                "fragments": [],
                "text": "Bioinform."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775200"
                        ],
                        "name": "W. Pirovano",
                        "slug": "W.-Pirovano",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Pirovano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Pirovano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699372"
                        ],
                        "name": "J. Heringa",
                        "slug": "J.-Heringa",
                        "structuredName": {
                            "firstName": "Jaap",
                            "lastName": "Heringa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Heringa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 40
                            }
                        ],
                        "text": "Bidirectional recurrent neural networks (BRNNs; Schuster and Paliwal, 1997; Schuster, 1999; Baldi et al., 1999) offer a more elegant solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5082916,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "c3fa67ed37dc3ad4c85222b8a5b0423312ac4ac3",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "While the prediction of a native protein structure from sequence continues to remain a challenging problem, over the past decades computational methods have become quite successful in exploiting the mechanisms behind secondary structure formation. The great effort expended in this area has resulted in the development of a vast number of secondary structure prediction methods. Especially the combination of well-optimized/sensitive machine-learning algorithms and inclusion of homologous sequence information has led to increased prediction accuracies of up to 80%. In this chapter, we will first introduce some basic notions and provide a brief history of secondary structure prediction advances. Then a comprehensive overview of state-of-the-art prediction methods will be given. Finally, we will discuss open questions and challenges in this field and provide some practical recommendations for the user."
            },
            "slug": "Protein-secondary-structure-prediction.-Pirovano-Heringa",
            "title": {
                "fragments": [],
                "text": "Protein secondary structure prediction."
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This chapter introduces some basic notions and provides a brief history of secondary structure prediction advances and a comprehensive overview of state-of-the-art prediction methods will be given."
            },
            "venue": {
                "fragments": [],
                "text": "Methods in molecular biology"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115505215"
                        ],
                        "name": "M. T. Johnson",
                        "slug": "M.-T.-Johnson",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Johnson",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. T. Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 123
                            }
                        ],
                        "text": "Exponential decay is in general a poor model of state duration, and various measures have been suggested to alleviate this (Johnson, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2880351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "802264a22e7398abaf57d04b74204b9b17767bef",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability of a standard hidden Markov model (HMM) or expanded state HMM (ESHMM) to accurately model duration distributions of phonemes is compared with specific duration-focused approaches such as semi-Markov models or variable transition probabilities. It is demonstrated that either a three-state ESHMM or a standard HMM with an increased number of states is capable of closely matching both Gamma distributions and duration distributions of phonemes from the TIMIT corpus, as measured by Bhattacharyya distance to the true distributions. Standard HMMs are easily implemented with off-the-shelf tools, whereas duration models require substantial algorithmic development and have higher computational costs when implemented, suggesting that a simple adjustment to HMM topologies is perhaps a more efficient solution to the problem of duration than more complex approaches."
            },
            "slug": "Capacity-and-complexity-of-HMM-duration-modeling-Johnson",
            "title": {
                "fragments": [],
                "text": "Capacity and complexity of HMM duration modeling techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Standard HMMs are easily implemented with off-the-shelf tools, whereas duration models require substantial algorithmic development and have higher computational costs when implemented, suggesting that a simple adjustment to HMM topologies is perhaps a more efficient solution to the problem of duration than more complex approaches."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Letters"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996960"
                        ],
                        "name": "M. Riesenhuber",
                        "slug": "M.-Riesenhuber",
                        "structuredName": {
                            "firstName": "Maximilian",
                            "lastName": "Riesenhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Riesenhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 165
                            }
                        ],
                        "text": "So-called hierarchical subsampling is commonly used in fields such as computer vision where the volume of data is too great to be processed by a \u2018flat\u2019 architecture (LeCun et al., 1998b; Reisenhuber and Poggio, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8920227,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "85abadb689897997f1e37baa7b5fc6f7d497518b",
            "isKey": false,
            "numCitedBy": 3317,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual processing in cortex is classically modeled as a hierarchy of increasingly sophisticated representations, naturally extending the model of simple to complex cells of Hubel and Wiesel. Surprisingly, little quantitative modeling has been done to explore the biological feasibility of this class of models to explain aspects of higher-level visual processing such as object recognition. We describe a new hierarchical model consistent with physiological data from inferotemporal cortex that accounts for this complex visual task and makes testable predictions. The model is based on a MAX-like operation applied to inputs to certain cortical neurons that may have a general role in cortical function."
            },
            "slug": "Hierarchical-models-of-object-recognition-in-cortex-Riesenhuber-Poggio",
            "title": {
                "fragments": [],
                "text": "Hierarchical models of object recognition in cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A new hierarchical model consistent with physiological data from inferotemporal cortex that accounts for this complex visual task and makes testable predictions is described."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Neuroscience"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144423965"
                        ],
                        "name": "A. Murray",
                        "slug": "A.-Murray",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Murray",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053507626"
                        ],
                        "name": "P. J. Edwards",
                        "slug": "P.-J.-Edwards",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Edwards",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. J. Edwards"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 113
                            }
                        ],
                        "text": "An alternative regularisation strategy is to add zero-mean, fixed variance Gaussian noise to the network weights (Murray and Edwards, 1994; Jim et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8666351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "60e66781bf17f8103bbc57fc5daeb6fbc5e4b910",
            "isKey": false,
            "numCitedBy": 190,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze the effects of analog noise on the synaptic arithmetic during multilayer perceptron training, by expanding the cost function to include noise-mediated terms. Predictions are made in the light of these calculations that suggest that fault tolerance, training quality and training trajectory should be improved by such noise-injection. Extensive simulation experiments on two distinct classification problems substantiate the claims. The results appear to be perfectly general for all training schemes where weights are adjusted incrementally, and have wide-ranging implications for all applications, particularly those involving \"inaccurate\" analog neural VLSI."
            },
            "slug": "Enhanced-MLP-performance-and-fault-tolerance-from-Murray-Edwards",
            "title": {
                "fragments": [],
                "text": "Enhanced MLP performance and fault tolerance resulting from synaptic weight noise during training"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "The effects of analog noise on the synaptic arithmetic during multilayer perceptron training is analyzed, by expanding the cost function to include noise-mediated terms, and predictions are made that fault tolerance, training quality and training trajectory should be improved by such noise-injection."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4605464"
                        ],
                        "name": "W. McCulloch",
                        "slug": "W.-McCulloch",
                        "structuredName": {
                            "firstName": "Warren",
                            "lastName": "McCulloch",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. McCulloch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50314979"
                        ],
                        "name": "W. Pitts",
                        "slug": "W.-Pitts",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Pitts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Pitts"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 147
                            }
                        ],
                        "text": "Artificial neural networks (ANNs) were originally developed as mathematical models of the information processing capabilities of biological brains (McCulloch and Pitts, 1988; Rosenblatt, 1963; Rumelhart et al., 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15619658,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "090c5a5df345ab60c41d6de02b3e366e1a27cf43",
            "isKey": false,
            "numCitedBy": 5975,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed."
            },
            "slug": "A-logical-calculus-of-the-ideas-immanent-in-nervous-McCulloch-Pitts",
            "title": {
                "fragments": [],
                "text": "A logical calculus of the ideas immanent in nervous activity"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time."
            },
            "venue": {
                "fragments": [],
                "text": "The Philosophy of Artificial Intelligence"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17101958,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e68d2aaa09337c5d5d7d0f4e9348ea9ff1626a4d",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a new system for processing on-line whiteboard notes. Notes written on a whiteboard is a new modality in handwriting recognition research that has received relatively little attention in the past. For the recognition we use an off-line HMM-recognizer, which has been developed in the context of our previous work. The recognizer is supplemented with methods for processing the on-line data and generating the images. The system consists of six main modules: on-line preprocessing, transformation to off-line data, off-line preprocessing, feature extraction, classification and post-processing. The recognition rate of the basic recognizer in a writer independent experiment is 59,5%. By applying state-of-the-art methods, such as optimizing the number of states and Gaussian components, and by including a language model we could achieve a statistically significant increase of the recognition rate to 64.3%."
            },
            "slug": "Handwriting-Recognition-of-Whiteboard-Notes-Liwicki-Bunke",
            "title": {
                "fragments": [],
                "text": "Handwriting Recognition of Whiteboard Notes"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A new system for processing on-line whiteboard notes and using an off-line HMM-recognizer, which has been developed in the context of previous work, to achieve a statistically significant increase of the recognition rate."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4605464"
                        ],
                        "name": "W. McCulloch",
                        "slug": "W.-McCulloch",
                        "structuredName": {
                            "firstName": "Warren",
                            "lastName": "McCulloch",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. McCulloch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50314979"
                        ],
                        "name": "W. Pitts",
                        "slug": "W.-Pitts",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Pitts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Pitts"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 120118103,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "3573a93812370debd39ba8c40288ffd59abe8ff2",
            "isKey": false,
            "numCitedBy": 8114,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Logical-Calculus-of-the-Ideas-Immanent-in-Nervous-McCulloch-Pitts",
            "title": {
                "fragments": [],
                "text": "A logical calculus of the ideas immanent in nervous activity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398517004"
                        ],
                        "name": "C. Giraud-Carrier",
                        "slug": "C.-Giraud-Carrier",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Giraud-Carrier",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Giraud-Carrier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34706692"
                        ],
                        "name": "R. Vilalta",
                        "slug": "R.-Vilalta",
                        "structuredName": {
                            "firstName": "Ricardo",
                            "lastName": "Vilalta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vilalta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712908"
                        ],
                        "name": "P. Brazdil",
                        "slug": "P.-Brazdil",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Brazdil",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brazdil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17996962,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79b765d91c6ecd0a7d9dda734fa2505552f62478",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent advances in meta-learning are providing the foundations to construct meta-learning assistants and task-adaptive learners. The goal of this special issue is to foster an interest in meta-learning by compiling representative work in the field. The contributions to this special issue provide strong insights into the construction of future meta-learning tools. In this introduction we present a common frame of reference to address work in meta-learning through the concept of meta-knowledge. We show how meta-learning can be simply defined as the process of exploiting knowledge about learning that enables us to understand and improve the performance of learning algorithms."
            },
            "slug": "Introduction-to-the-Special-Issue-on-Meta-Learning-Giraud-Carrier-Vilalta",
            "title": {
                "fragments": [],
                "text": "Introduction to the Special Issue on Meta-Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown how meta-learning can be simply defined as the process of exploiting knowledge about learning that enables us to understand and improve the performance of learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145204681"
                        ],
                        "name": "L. Lamel",
                        "slug": "L.-Lamel",
                        "structuredName": {
                            "firstName": "Lori",
                            "lastName": "Lamel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lamel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685010"
                        ],
                        "name": "J. Gauvain",
                        "slug": "J.-Gauvain",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Gauvain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauvain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 34
                            }
                        ],
                        "text": "6 Triphone Continuous Density HMM (Lamel and Gauvain, 1993) 27."
                    },
                    "intents": []
                }
            ],
            "corpusId": 251250,
            "fieldsOfStudy": [
                "Physics",
                "Computer Science"
            ],
            "id": "b250c7238711b5203f2ab9a5e5693d097e38b027",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we report high phone accuracies on three corpora: WSJ0, BREF and TIMIT. The main characteristics of the phone recognizerare: high dimensional feature vector (48), contextand genderdependent phone models with duration distribution, continuous density HMM with Gaussian mixtures, and n-gram probabilities for the phonotatic constraints. These models are trained on speech data that have either phonetic or orthographic transcriptions using maximum likelihood and maximum a posteriori estimation techniques. On the WSJ0 corpus with a 46 phone set we obtain phone accuraciesof 72.4% and 74.4% using 500 and 1600 CD phone units, respectively. Accuracy on BREF with 35 phones is as high as 78.7% with only 428 CD phone units. On TIMIT using the 61 phone symbols and only 500 CD phone units, we obtain a phone accuracyof 67.2% which correspond to 73.4% when the recognizer output is mapped to the commonly used 39 phone set. Making reference to our work on large vocabulary CSR, we show that it is worthwhile to perform phone recognition experiments as opposed to only focusing attention on word recognition results."
            },
            "slug": "High-performance-speaker-independent-phone-using-Lamel-Gauvain",
            "title": {
                "fragments": [],
                "text": "High performance speaker-independent phone recognition using CDHMM"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that it is worthwhile to perform phone recognition experiments as opposed to only focusing attention on word recognition results, and high phone accuracies on three corpora: WSJ0, BREF and TIMIT are reported."
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2964655"
                        ],
                        "name": "M. Stinchcombe",
                        "slug": "M.-Stinchcombe",
                        "structuredName": {
                            "firstName": "Maxwell",
                            "lastName": "Stinchcombe",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stinchcombe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 26
                            }
                        ],
                        "text": "Indeed it has been proven (Hornik et al., 1989) that an MLP with a single hidden layer containing a sufficient number of nonlinear units can approximate any continuous function on a compact input domain to arbitrary precision."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2757547,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f22f6972e66bdd2e769fa64b0df0a13063c0c101",
            "isKey": false,
            "numCitedBy": 17341,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multilayer-feedforward-networks-are-universal-Hornik-Stinchcombe",
            "title": {
                "fragments": [],
                "text": "Multilayer feedforward networks are universal approximators"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1745101,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9eb7daa88879f283ae05e359d6c502a320b833c9",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present IAM-OnDB - a new large online handwritten sentences database. It is publicly available and consists of text acquired via an electronic interface from a whiteboard. The database contains about 86 K word instances from an 11 K dictionary written by more than 200 writers. We also describe a recognizer for unconstrained English text that was trained and tested using this database. This recognizer is based on hidden Markov models (HMMs). In our experiments we show that by using larger training sets we can significantly increase the word recognition rate. This recognizer may serve as a benchmark reference for future research."
            },
            "slug": "IAM-OnDB-an-on-line-English-sentence-database-from-Liwicki-Bunke",
            "title": {
                "fragments": [],
                "text": "IAM-OnDB - an on-line English sentence database acquired from handwritten text on a whiteboard"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "IAM-OnDB is a new large online handwritten sentences database that consists of text acquired via an electronic interface from a whiteboard and a recognizer for unconstrained English text that was trained and tested using this database."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054206556"
                        ],
                        "name": "Matthias Zimmermann",
                        "slug": "Matthias-Zimmermann",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Zimmermann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthias Zimmermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1874188"
                        ],
                        "name": "Jean-C\u00e9dric Chappelier",
                        "slug": "Jean-C\u00e9dric-Chappelier",
                        "structuredName": {
                            "firstName": "Jean-C\u00e9dric",
                            "lastName": "Chappelier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-C\u00e9dric Chappelier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1363996,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "047f8e5d493fa7043dedb2679e120bd8acb682a1",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a sequential coupling of a hidden Markov model (HMM) recognizer for offline handwritten English sentences with a probabilistic bottom-up chart parser using stochastic context-free grammars (SCFG) extracted from a text corpus. Based on extensive experiments, we conclude that syntax analysis helps to improve recognition rates significantly."
            },
            "slug": "Offline-grammar-based-recognition-of-handwritten-Zimmermann-Chappelier",
            "title": {
                "fragments": [],
                "text": "Offline grammar-based recognition of handwritten sentences"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "This paper proposes a sequential coupling of a hidden Markov model (HMM) recognizer for offline handwritten English sentences with a probabilistic bottom-up chart parser using stochastic context-free grammars extracted from a text corpus and concludes that syntax analysis helps to improve recognition rates significantly."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784519"
                        ],
                        "name": "Peter Norvig",
                        "slug": "Peter-Norvig",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Norvig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Norvig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53142908,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3524cdf7cf8344e7eb74886f71fcbb5c6732c337",
            "isKey": false,
            "numCitedBy": 26732,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For computer professionals, linguists, and cognitive scientists interested in artificial intelligence."
            },
            "slug": "Artificial-Intelligence:-A-Modern-Approach-Russell-Norvig",
            "title": {
                "fragments": [],
                "text": "Artificial Intelligence: A Modern Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2862682"
                        ],
                        "name": "G. Doddington",
                        "slug": "G.-Doddington",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Doddington",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Doddington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69475390"
                        ],
                        "name": "B. M. Hydrick",
                        "slug": "B.-M.-Hydrick",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Hydrick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. M. Hydrick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120171848,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "99b49ade23b71e218d15d0dbd1a74780fdf29b6b",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Speaker\u2010independent recognition of words spoken in isolation was performed using a very large vocabulary of over 26 000 words taken from the \u201cBrown\u201d data set. (Computational Analysis of Present\u2010Day American English by Kucera and Francis). After discarding 4% of the data judged to be spoken incorrectly, experimental recognition error rate was 2.3% (1.8% substitution and 0.5% rejection), with negligible difference in performance between male and female speakers. Experimental error rate for vocabulary subsets, ordered by frequency of usage, was 1.0% for the first 50 words, 0.8% for the first 120 words, and 1.2% error for the first 1500 words. An analysis of recognition errors and a discussion of ultimate performance limitations will be presented."
            },
            "slug": "High-performance-speaker\u2010independent-word-Doddington-Hydrick",
            "title": {
                "fragments": [],
                "text": "High performance speaker\u2010independent word recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3010427"
                        ],
                        "name": "F. H\u00fclsken",
                        "slug": "F.-H\u00fclsken",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "H\u00fclsken",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. H\u00fclsken"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717858"
                        ],
                        "name": "F. Wallhoff",
                        "slug": "F.-Wallhoff",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Wallhoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Wallhoff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145512909"
                        ],
                        "name": "G. Rigoll",
                        "slug": "G.-Rigoll",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Rigoll",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Rigoll"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 124
                            }
                        ],
                        "text": "Numerous approximate methods have been proposed to alleviate one or both of these problems, including pseudo 2D and 3D HMMs (H\u00fclsken et al., 2001), isolating elements (Li et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10572153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b3459d5c7ecab56675b75eb68c75aa36fe8f856",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel approach to gesture recognition, based on Pseudo-3D Hidden Markov Models (P3DHMMs). This technique is capable of integrating spatially and temporally derived features in an elegant way, thus enabling the recognition of different dynamic face-expressions. Pseudo-2D Hidden Markov Models have been utilized for two dimensional problems such as face recognition. P3DHMMs can be considered as an extension of the 2D case, where the so-called superstates in P3DHMM encapsulate P2DHMMs. With our new approach image sequences can efficiently and successfully be processed. Because the 'ordinary' training of P3DHMMs is time expensive and can destroy the 3D approach, an improved training is presented in this paper. The feasibility of the usage of P3DHMMs is demonstrated by a number of experiments on a person independent database, which consists of different image sequences of 4face-expressions from 6 persons."
            },
            "slug": "Facial-Expression-Recognition-with-Pseudo-3D-Hidden-H\u00fclsken-Wallhoff",
            "title": {
                "fragments": [],
                "text": "Facial Expression Recognition with Pseudo-3D Hidden Markov Models"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A novel approach to gesture recognition, based on Pseudo-3D Hidden Markov Models (P3DHMMs), which is capable of integrating spatially and temporally derived features in an elegant way, thus enabling the recognition of different dynamic face-expressions."
            },
            "venue": {
                "fragments": [],
                "text": "DAGM-Symposium"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143678972"
                        ],
                        "name": "G. Navarro",
                        "slug": "G.-Navarro",
                        "structuredName": {
                            "firstName": "Gonzalo",
                            "lastName": "Navarro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Navarro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 44
                            }
                        ],
                        "text": "ED(p,q) can be calculated in O(|p||q|) time (Navarro, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207551224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e001232a2876989f52a7b4ac12bd9ed550d41240",
            "isKey": false,
            "numCitedBy": 2518,
            "numCiting": 237,
            "paperAbstract": {
                "fragments": [],
                "text": "We survey the current techniques to cope with the problem of string matching that allows errors. This is becoming a more and more relevant issue for many fast growing areas such as information retrieval and computational biology. We focus on online searching and mostly on edit distance, explaining the problem and its relevance, its statistical behavior, its history and current developments, and the central ideas of the algorithms and their complexities. We present a number of experiments to compare the performance of the different algorithms and show which are the best choices. We conclude with some directions for future work and open problems."
            },
            "slug": "A-guided-tour-to-approximate-string-matching-Navarro",
            "title": {
                "fragments": [],
                "text": "A guided tour to approximate string matching"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "This work surveys the current techniques to cope with the problem of string matching that allows errors, and focuses on online searching and mostly on edit distance, explaining the problem and its relevance, its statistical behavior, its history and current developments, and the central ideas of the algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759124"
                        ],
                        "name": "C. Goller",
                        "slug": "C.-Goller",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Goller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Goller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 67
                            }
                        ],
                        "text": "Various forms of DAG-RNN appeared in publications prior to Baldi\u2019s (Goller, 1997; Sperduti and Starita, 1997; Frasconi et al., 1998; Hammer, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35388031,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b19ab7f0d9a5ef60be7045bc80539590b00605a",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Automated deduction has a long tradition in computer science and most of the symbolic AI systems perform some kind of logic-based deductive inference. The central problem in automated deduction is the explosive growth of search spaces with deduction length. Methods of guiding and controlling the search process are indispensable. I will present a connectionist approach for learning search-control heuristics from examples of successful deductions. It originated in the context of the relatively new and still emerging eld of hybrid (symbolic/connectionist) systems. The goal of this eld is the identiication of strengths and weaknesses in the symbolic and the connectionist paradigms, and based on such insights, the design of hybrid systems that can beneet from the strengths of both. I begin with brieey comparing search-control heuristics for automated deduction systems with other methods that improve the eeciency of search, such as search strategies, calculus reenements and extensions, and analogical reasoning. The most important characteristics of heuristics are that they are simple and useful in most cases, however, they are not guaranteed to help. Search-control heuristics can be viewed as approximations to unknown and almost never realizable perfect search-control strategies. Approximation on the other hand is an inherent characteristic of connectionist methods, especially of the most successful and widespread backpropagation (BP) networks. Search-control heuristics for automated deduction systems are traditionally represented by heuristic evaluation functions for states or inference steps in the search space. These heuristic evaluation functions compute ratings for symbolic structures (e.g. graphs, trees, terms, formulas) of arbitrary size. Representing and processing the symbolic structures traditionally used in AI, belong to the most important and challenging open questions in the connectionist community. Most of the previous connectionist approaches for learning heuris-tic evaluation functions use a xed, a priori deened set of features for symbolic structures SF71, SE90, SE91, SG93, Gol94, Fuc96]. The deenition and selection of features, however, introduces a very strong bias and severely limits the class of relations that can be expressed and learned. It can be stated that the selection of a good set of features is already more than half the way to nding a good evaluation function. Compared to the feature-selection and deenition problem, the particular learning method which is applied is only of minor importance. I present a new and very powerful approach for adaptive structure processing which automatically nds the features which are relevant for the task at hand: folding architecture networks (FA-networks) \u2026"
            },
            "slug": "A-connectionist-approach-for-learning-heuristics-Goller",
            "title": {
                "fragments": [],
                "text": "A connectionist approach for learning search-control heuristics for automated deduction systems"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A connectionist approach for learning search-control heuristics for automated deduction systems with other methods that improve the eeciency of search, such as search strategies, calculus reenements and extensions, and analogical reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "DISKI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2168488"
                        ],
                        "name": "Roman Bertolami",
                        "slug": "Roman-Bertolami",
                        "structuredName": {
                            "firstName": "Roman",
                            "lastName": "Bertolami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roman Bertolami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 58
                            }
                        ],
                        "text": "The HMM-based recogniser was identical to the one used in (Bertolami and Bunke, 2007), with each character modelled by a linear HMM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 46212785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81ead2caa25de9010cdcf590133a707a7e6906dd",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates the use of multiple classifier methods for offline handwritten text line recognition. To obtain ensembles of recognisers we implement a random feature subspace method. The word sequences returned by the individual ensemble members are first aligned. Then the final word sequence is produced. For this purpose we use a voting method and two novel statistical combination methods. The conducted experiments show that the proposed multiple classifier methods have the potential to improve the recognition accuracy of single recognisers."
            },
            "slug": "Multiple-Classifier-Methods-for-Offline-Handwritten-Bertolami-Bunke",
            "title": {
                "fragments": [],
                "text": "Multiple Classifier Methods for Offline Handwritten Text Line Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The conducted experiments show that the proposed multiple classifier methods have the potential to improve the recognition accuracy of single recognisers."
            },
            "venue": {
                "fragments": [],
                "text": "MCS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145259603"
                        ],
                        "name": "S. Young",
                        "slug": "S.-Young",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Young",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Young"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056933609"
                        ],
                        "name": "J. Jansen",
                        "slug": "J.-Jansen",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Jansen",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jansen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144687429"
                        ],
                        "name": "J. Odell",
                        "slug": "J.-Odell",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Odell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Odell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116518564"
                        ],
                        "name": "Dg Ollason",
                        "slug": "Dg-Ollason",
                        "structuredName": {
                            "firstName": "Dg",
                            "lastName": "Ollason",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dg Ollason"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716393"
                        ],
                        "name": "P. Woodland",
                        "slug": "P.-Woodland",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Woodland",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Woodland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16197350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a98e481ce418a437cdfae107d85f009a5da6a790",
            "isKey": false,
            "numCitedBy": 2086,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "1 The Fundamentals of HTK 2 1.1 General Principles of HMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 Isolated Word Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.3 Output Probability Specification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.4 Baum-Welch Re-Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.5 Recognition and Viterbi Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 1.6 Continuous Speech Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 1.7 Speaker Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13"
            },
            "slug": "The-HTK-book-Young-Jansen",
            "title": {
                "fragments": [],
                "text": "The HTK book"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The Fundamentals of HTK: General Principles of HMMs, Recognition and Viterbi Decoding, and Continuous Speech Recognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2801094"
                        ],
                        "name": "H. E. Abed",
                        "slug": "H.-E.-Abed",
                        "structuredName": {
                            "firstName": "Haikal",
                            "lastName": "Abed",
                            "middleNames": [
                                "El"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. E. Abed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774130"
                        ],
                        "name": "V. M\u00e4rgner",
                        "slug": "V.-M\u00e4rgner",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "M\u00e4rgner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. M\u00e4rgner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 826071,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9750c0850a7590b936a2337d395226a47888636",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the Arabic handwriting recognition competition held at ICDAR 2009. This third competition (the first two were held at ICDAR 2005 and 2007, respectively) again used the IfN/ENIT-database with Arabic handwritten Tunisian town names. This very successful database is used today by more than 82 research groups from universities, research centers, and industries worldwide. At ICDAR 2009, 7 groups with 17 systems participated in the competition. The system evaluation was made on one known dataset and on two datasets unknown to the participants. The systems were compared based on the recognition rates achieved. Additionally, the relative speeds of the systems were compared. A description of the participating groups, their systems, and the results achieved are presented. As a very important result of this competition, a continuous improvement of the recognition rate from competition to competition of more than 5% can be observed."
            },
            "slug": "ICDAR-2009-Arabic-handwriting-recognition-Abed-M\u00e4rgner",
            "title": {
                "fragments": [],
                "text": "ICDAR 2009-Arabic handwriting recognition competition"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A continuous improvement of the recognition rate from competition to competition of more than 5% can be observed and this is a very important result of this competition."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2801094"
                        ],
                        "name": "H. E. Abed",
                        "slug": "H.-E.-Abed",
                        "structuredName": {
                            "firstName": "Haikal",
                            "lastName": "Abed",
                            "middleNames": [
                                "El"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. E. Abed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774130"
                        ],
                        "name": "V. M\u00e4rgner",
                        "slug": "V.-M\u00e4rgner",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "M\u00e4rgner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. M\u00e4rgner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2139481"
                        ],
                        "name": "M. Kherallah",
                        "slug": "M.-Kherallah",
                        "structuredName": {
                            "firstName": "Monji",
                            "lastName": "Kherallah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kherallah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144000830"
                        ],
                        "name": "A. Alimi",
                        "slug": "A.-Alimi",
                        "structuredName": {
                            "firstName": "Adel",
                            "lastName": "Alimi",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Alimi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7666694,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62d3cced863287bd81c19f739d0cdda1e4809bcd",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the Online Arabic handwriting recognition competition held at ICDAR 2009. This first competition uses the ADAB-database with Arabic online handwritten words. This year, 3 groups with 7 systems are participating in the competition. The systems were tested on known data (sets 1 to 3) and on one test dataset which is unknown to all participants (set 4). The systems are compared on the most important characteristic of classification systems, the recognition rate. Additionally, the relative speed of the different systems were compared. A short description of the participating groups, their systems, the experimental setup, and the performed results are presented."
            },
            "slug": "ICDAR-2009-Online-Arabic-Handwriting-Recognition-Abed-M\u00e4rgner",
            "title": {
                "fragments": [],
                "text": "ICDAR 2009 Online Arabic Handwriting Recognition Competition"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This paper describes the Online Arabic handwriting recognition competition held at ICDAR 2009, which uses the ADAB-database with Arabic online handwritten words and compares the systems on the most important characteristic of classification systems, the recognition rate."
            },
            "venue": {
                "fragments": [],
                "text": "2009 10th International Conference on Document Analysis and Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5113463"
                        ],
                        "name": "D. Joshi",
                        "slug": "D.-Joshi",
                        "structuredName": {
                            "firstName": "Dhiraj",
                            "lastName": "Joshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Joshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40116905"
                        ],
                        "name": "Jia Li",
                        "slug": "Jia-Li",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48094094"
                        ],
                        "name": "James Ze Wang",
                        "slug": "James-Ze-Wang",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Wang",
                            "middleNames": [
                                "Ze"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Ze Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3756004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "471697fdca8c34b79d27fe951b15c16e6823d4b5",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Parameter estimation is a key computational issue in all statistical image modeling techniques. In this paper, we explore a computationally efficient parameter estimation algorithm for multi-dimensional hidden Markov models. 2-D HMM has been applied to supervised aerial image classification and comparisons have been made with the first proposed estimation algorithm. An extensive parametric study has been performed with 3-D HMM and the scalability of the estimation algorithm has been discussed. Results show the great applicability of the explored algorithm to multi-dimensional HMM based image modeling applications."
            },
            "slug": "Parameter-estimation-of-multi-dimensional-hidden-a-Joshi-Li",
            "title": {
                "fragments": [],
                "text": "Parameter estimation of multi-dimensional hidden Markov models - a scalable approach"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Results show the great applicability of the explored algorithm to multi-dimensional HMM based image modeling applications."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Conference on Image Processing 2005"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096070"
                        ],
                        "name": "J. Shewchuk",
                        "slug": "J.-Shewchuk",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Shewchuk",
                            "middleNames": [
                                "Richard"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shewchuk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6491967,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "70000f7791ab8519429ce939bc897738a05939c3",
            "isKey": false,
            "numCitedBy": 2496,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The Conjugate Gradient Method is the most prominent iterative method for solving sparse systems of linear equations. Unfortunately, many textbook treatments of the topic are written so that even their own authors would be mystified, if they bothered to read their own writing. For this reason, an understanding of the method has been reserved for the elite brilliant few who have painstakingly decoded the mumblings of their forebears. Nevertheless, the Conjugate Gradient Method is a composite of simple, elegant ideas that almost anyone can understand. Of course, a reader as intelligent as yourself will learn them almost effortlessly. The idea of quadratic forms is introduced and used to derive the methods of Steepest Descent, Conjugate Directions, and Conjugate Gradients. Eigenvectors are explained and used to examine the convergence of the Jacobi Method, Steepest Descent, and Conjugate Gradients. Other topics include preconditioning and the nonlinear Conjugate Gradient Method. I have taken pains to make this article easy to read. Sixty-two illustrations are provided. Dense prose is avoided. Concepts are explained in several different ways. Most equations are coupled with an intuitive interpretation."
            },
            "slug": "An-Introduction-to-the-Conjugate-Gradient-Method-Shewchuk",
            "title": {
                "fragments": [],
                "text": "An Introduction to the Conjugate Gradient Method Without the Agonizing Pain"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774130"
                        ],
                        "name": "V. M\u00e4rgner",
                        "slug": "V.-M\u00e4rgner",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "M\u00e4rgner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. M\u00e4rgner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2801094"
                        ],
                        "name": "H. E. Abed",
                        "slug": "H.-E.-Abed",
                        "structuredName": {
                            "firstName": "Haikal",
                            "lastName": "Abed",
                            "middleNames": [
                                "El"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. E. Abed"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 142
                            }
                        ],
                        "text": "The offline Arabic handwriting recognition competition at the 2009 International Conference on Document Analysis and Recognition (ICDAR 2009) (M\u00e4rgner and Abed, 2009) was based on the publicly available IFN/ENIT database of handwritten Arabic words (Pechwitz et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1766575,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "217c52c94cb37deafffe915fc30b4b959605456e",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the Arabic handwriting recognition competition held at ICDAR 2007. This second competition (the first was at ICDAR 2005) again uses the IFN/ENIT-database with Arabic handwritten Tunisian town names. Today, more than 54 research groups from universities, research centers, and industry are working with this database worldwide. This year, 8 groups with 14 systems are participating in the competition. The systems were tested on known data and on two datasets which are unknown to the participants. The systems are compared on the most important characteristic, the recognition rate. Additionally, the relative speed of the different systems were compared. A short description of the participating groups, their systems, and the results achieved are finally presented."
            },
            "slug": "Arabic-Handwriting-Recognition-Competition-M\u00e4rgner-Abed",
            "title": {
                "fragments": [],
                "text": "Arabic Handwriting Recognition Competition"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper describes the Arabic handwriting recognition competition held at ICDAR 2007, again uses the IFN/ENIT-database with Arabic handwritten Tunisian town names, and 8 groups with 14 systems are participating in the competition."
            },
            "venue": {
                "fragments": [],
                "text": "ICDAR"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3191442"
                        ],
                        "name": "E. Grosicki",
                        "slug": "E.-Grosicki",
                        "structuredName": {
                            "firstName": "Emmanu\u00e8le",
                            "lastName": "Grosicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Grosicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071182278"
                        ],
                        "name": "Matthieu Carr\u00e9",
                        "slug": "Matthieu-Carr\u00e9",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Carr\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthieu Carr\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644348559"
                        ],
                        "name": "J. Brodin",
                        "slug": "J.-Brodin",
                        "structuredName": {
                            "firstName": "J.-M.",
                            "lastName": "Brodin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Brodin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2196683"
                        ],
                        "name": "E. Geoffrois",
                        "slug": "E.-Geoffrois",
                        "structuredName": {
                            "firstName": "Edouard",
                            "lastName": "Geoffrois",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Geoffrois"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 131
                            }
                        ],
                        "text": "The French handwriting recognition competition at ICDAR 2009 (Grosicki and Abed, 2009) was based on a subset of the RIMES database (Grosicki et al., 2009) of French mail snippets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 212607,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e90bd63bb7a51036875c030cc73c16fdb90499e2",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the results of the second test phase of the RIMES evaluation campaign. The latter is the first large-scale evaluation campaign intended to all the key players of the handwritten recognition and document analysis communities. It proposes various tasks around recognition and indexing of handwritten letters such as those sent by postal mail or fax by individuals to companies or administrations. In this second evaluation test, automatic systems have been evaluated on three themes: layout analysis, handwriting recognition and writer identification. The databases used are part of the RIMES database of 5605 real mails completely annotated as well as secondary databases of isolated characters and handwritten words (250,000 snippets). The paper reports on protocols and gives the results obtained in the campaign.(RIMES : Reconnaissance et Indexation de donn\u00e9es Manuscrites et de fac simil\u00c9S / Recognition and Indexing of handwritten documents and faxes)"
            },
            "slug": "Results-of-the-RIMES-Evaluation-Campaign-for-Mail-Grosicki-Carr\u00e9",
            "title": {
                "fragments": [],
                "text": "Results of the RIMES Evaluation Campaign for Handwritten Mail Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This paper presents the results of the second test phase of the RIMES evaluation campaign, where automatic systems have been evaluated on three themes: layout analysis, handwriting recognition and writer identification."
            },
            "venue": {
                "fragments": [],
                "text": "2009 10th International Conference on Document Analysis and Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35262302"
                        ],
                        "name": "H. Khosravi",
                        "slug": "H.-Khosravi",
                        "structuredName": {
                            "firstName": "Hossein",
                            "lastName": "Khosravi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Khosravi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145161075"
                        ],
                        "name": "E. Kabir",
                        "slug": "E.-Kabir",
                        "structuredName": {
                            "firstName": "Ehsanollah",
                            "lastName": "Kabir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Kabir"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 476300,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e39e8a05131bbbcfcc03b98edd9761a244b0db8",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introducing-a-very-large-dataset-of-handwritten-and-Khosravi-Kabir",
            "title": {
                "fragments": [],
                "text": "Introducing a very large dataset of handwritten Farsi digits and a study on their varieties"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722800"
                        ],
                        "name": "J. Hennebert",
                        "slug": "J.-Hennebert",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Hennebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hennebert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1980449"
                        ],
                        "name": "C. Ris",
                        "slug": "C.-Ris",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Ris",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145086187"
                        ],
                        "name": "S. Renals",
                        "slug": "S.-Renals",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Renals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Renals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 130
                            }
                        ],
                        "text": "The two components in a the hybrid can be trained independently, but many authors have proposed methods for combined optimisation (Bengio et al., 1992; Bourlard et al., 1996; Hennebert et al., 1997; Trentin and Gori, 2003) which typically yields improved results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12465945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3bc9be16cfd119e822bc1d572fb028053a3d4caa",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Keywords: Non-Linear Signal Processing ; Speech Processing Reference LANOS-CONF-1997-001 Record created on 2004-12-03, modified on 2017-05-12"
            },
            "slug": "Estimation-of-global-posteriors-and-training-of-Hennebert-Ris",
            "title": {
                "fragments": [],
                "text": "Estimation of global posteriors and forward-backward training of hybrid HMM/ANN systems"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "Non-Linear Signal Processing ; Speech Processing Reference LANOS-CONF-1997-001 Record created on 2004-12-03, modified on 2017-05-12."
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703049"
                        ],
                        "name": "D. DeCoste",
                        "slug": "D.-DeCoste",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "DeCoste",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. DeCoste"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 36
                            }
                        ],
                        "text": ", 2003) and support vector machines (LeCun et al., 1998a; Decoste and Sch\u00f6lkopf, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 85843,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd14aa8bef5afc7d248a04de681242f2e64c6a1e",
            "isKey": false,
            "numCitedBy": 594,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Practical experience has shown that in order to obtain the best possible performance, prior knowledge about invariances of a classification problem at hand ought to be incorporated into the training procedure. We describe and review all known methods for doing so in support vector machines, provide experimental results, and discuss their respective merits. One of the significant new results reported in this work is our recent achievement of the lowest reported test error on the well-known MNIST digit recognition benchmark task, with SVM training times that are also significantly faster than previous SVM methods."
            },
            "slug": "Training-Invariant-Support-Vector-Machines-DeCoste-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Training Invariant Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work reports the recent achievement of the lowest reported test error on the well-known MNIST digit recognition benchmark task, with SVM training times that are also significantly faster than previous SVM methods."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2315777"
                        ],
                        "name": "Farshid Solimanpour",
                        "slug": "Farshid-Solimanpour",
                        "structuredName": {
                            "firstName": "Farshid",
                            "lastName": "Solimanpour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Farshid Solimanpour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806639"
                        ],
                        "name": "J. Sadri",
                        "slug": "J.-Sadri",
                        "structuredName": {
                            "firstName": "Javad",
                            "lastName": "Sadri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sadri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 150
                            }
                        ],
                        "text": "The Farsi/Arabic character classification competition at ICDAR 2009 was based on data drawn from the Holda (Khosravi and Kabir, 2007), Farsi CENPARMI (Solimanpour et al., 2006) and Extended IFHCDB (Mozaffari et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62318385,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dcf221f7f766c14520cb1afb3965e4d3f9d0dc26",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an important step towards the standardization of the research on Optical Character Recognition (OCR) in Farsi language. It describes formations of novel and standard handwritten databases including isolated digits, letters, numerical strings, Legal amounts (used for cheques), and dates. Despite conventional research and an Internet search, no publicly accessible Farsi database was found. Hence, it was decided that it would be a worthwhile academic effort to create several Farsi databases that could stand on their own merit functioning as useful tools for OCR researchers. Also, in order to show the potential uses of our new databases we also conducted some experiments on the recognition of handwritten isolated Farsi digits."
            },
            "slug": "Standard-Databases-for-Recognition-of-Handwritten-Solimanpour-Sadri",
            "title": {
                "fragments": [],
                "text": "Standard Databases for Recognition of Handwritten Digits, Numerical Strings, Legal Amounts, Letters and Dates in Farsi Language"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An important step towards the standardization of the research on Optical Character Recognition (OCR) in Farsi language is described, which describes formations of novel and standard handwritten databases including isolated digits, letters, numerical strings, Legal amounts, and dates."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 123
                            }
                        ],
                        "text": "The loss function is sometimes said to match the output layer activation function when the output derivative has this form (Schraudolph, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 79
                            }
                        ],
                        "text": "A recently proposed alternative for online learning is stochastic meta-descent (Schraudolph, 2002), which has been shown to give faster convergence and improved results for a variety of neural network tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11017566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffa94bba647817fa5e8f8d3250fc977435b5ca76",
            "isKey": false,
            "numCitedBy": 275,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a generic method for iteratively approximating various second-order gradient steps-Newton, Gauss-Newton, Levenberg-Marquardt, and natural gradient-in linear time per iteration, using special curvature matrix-vector products that can be computed in O(n). Two recent acceleration techniques for on-line learning, matrix momentum and stochastic meta-descent (SMD), implement this approach. Since both were originally derived by very different routes, this offers fresh insight into their operation, resulting in further improvements to SMD."
            },
            "slug": "Fast-Curvature-Matrix-Vector-Products-for-Gradient-Schraudolph",
            "title": {
                "fragments": [],
                "text": "Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A generic method for iteratively approximating various second-order gradient steps-Newton, Gauss- newton, Levenberg-Marquardt, and natural gradient-in linear time per iteration, using special curvature matrix-vector products that can be computed in O(n)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683324"
                        ],
                        "name": "M. Pechwitz",
                        "slug": "M.-Pechwitz",
                        "structuredName": {
                            "firstName": "Mario",
                            "lastName": "Pechwitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pechwitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681421"
                        ],
                        "name": "S. Maddouri",
                        "slug": "S.-Maddouri",
                        "structuredName": {
                            "firstName": "Samia",
                            "lastName": "Maddouri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Maddouri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774130"
                        ],
                        "name": "V. M\u00e4rgner",
                        "slug": "V.-M\u00e4rgner",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "M\u00e4rgner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. M\u00e4rgner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683433"
                        ],
                        "name": "N. Ellouze",
                        "slug": "N.-Ellouze",
                        "structuredName": {
                            "firstName": "Noureddine",
                            "lastName": "Ellouze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ellouze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145198897"
                        ],
                        "name": "H. Amiri",
                        "slug": "H.-Amiri",
                        "structuredName": {
                            "firstName": "Hamid",
                            "lastName": "Amiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Amiri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 249
                            }
                        ],
                        "text": "The offline Arabic handwriting recognition competition at the 2009 International Conference on Document Analysis and Recognition (ICDAR 2009) (M\u00e4rgner and Abed, 2009) was based on the publicly available IFN/ENIT database of handwritten Arabic words (Pechwitz et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15106190,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "732c94369298f0d3df48ed4035703e56aaf39892",
            "isKey": false,
            "numCitedBy": 505,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Dans cet article on presente une nouvelle base de donnees, qui contient des noms manuscrits de villes/villages arabes. Pour chaque nom les informations de base, par exemple l'ordre des formes de caractere, les informations sur le style de l'ecriture et la ligne de base, sont codees. 411 auteurs ont rempli des formulaires avec plus de 26400 noms contenant plus de 210 000 caracteres. La base de donnees est decrite en detail, et elle est concue pour la formation et l'essai des systemes d'identification pour les mots arabes manuscrits. La base de donnees IFA/ENIT est disponible pour la recherche."
            },
            "slug": "IFN/ENIT:-database-of-handwritten-arabic-words-Pechwitz-Maddouri",
            "title": {
                "fragments": [],
                "text": "IFN/ENIT: database of handwritten arabic words"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 91
                            }
                        ],
                        "text": "However, we will frequently refer to the well-known generative method hidden Markov models (Rabiner, 1989; Bengio, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 126
                            }
                        ],
                        "text": "Fortunately the problem can be solved with a dynamic-programming algorithm similar to the forward-backward algorithm for HMMs (Rabiner, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 52
                            }
                        ],
                        "text": "Note that rescaling the variables at every timestep (Rabiner, 1989) is less robust, and can fail for very long sequences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13618539,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5",
            "isKey": false,
            "numCitedBy": 24802,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests. The fabric is defined of voids having depth as well as width and length. The fabric is usable as a material from which to form clothing for wear, or bed coverings, or sleeping bags, etc., besides use simply as a netting."
            },
            "slug": "A-Tutorial-on-Hidden-Markov-Models-and-Selected-Rabiner",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Hidden Markov Models and Selected Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144402025"
                        ],
                        "name": "S. Mozaffari",
                        "slug": "S.-Mozaffari",
                        "structuredName": {
                            "firstName": "Saeed",
                            "lastName": "Mozaffari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mozaffari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692435"
                        ],
                        "name": "K. Faez",
                        "slug": "K.-Faez",
                        "structuredName": {
                            "firstName": "Karim",
                            "lastName": "Faez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Faez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2147944"
                        ],
                        "name": "F. Faradji",
                        "slug": "F.-Faradji",
                        "structuredName": {
                            "firstName": "Farhad",
                            "lastName": "Faradji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Faradji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2394658"
                        ],
                        "name": "M. Ziaratban",
                        "slug": "M.-Ziaratban",
                        "structuredName": {
                            "firstName": "Majid",
                            "lastName": "Ziaratban",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ziaratban"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2102950155"
                        ],
                        "name": "S. M. Golzan",
                        "slug": "S.-M.-Golzan",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Golzan",
                            "middleNames": [
                                "Mohamad"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. M. Golzan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 28
                            }
                        ],
                        "text": ", 2006) and Extended IFHCDB (Mozaffari et al., 2006) databases."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11768456,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9ef606cf556cb9010824f5cc665df270d71464e",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new comprehensive database for isolated offline handwritten Farsi/Arabic numbers and characters for use in optical character recognition research. The database is freely available for academic use. So far no such a freely database in Farsi language is available. Grayscale images of 52,380 characters and 17,740 numerals are included. Each image was scanned from Iranian school entrance exam forms during the years 2004-2006 at 300 dpi. The only restriction imposed on the writers is to write each character within a rectangular box. The number of samples in each class of the database is non-uniform corresponding to their real life distributions. Also, for comparison purposes, each dataset has been properly divided into respective training and test sets."
            },
            "slug": "A-Comprehensive-Isolated-Farsi/Arabic-Character-for-Mozaffari-Faez",
            "title": {
                "fragments": [],
                "text": "A Comprehensive Isolated Farsi/Arabic Character Database for Handwritten OCR Research"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A new comprehensive database for isolated offline handwritten Farsi/Arabic numbers and characters for use in optical character recognition research and is freely available for academic use."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 83
                            }
                        ],
                        "text": "To efficiently calculate the gradient, we use a technique known as backpropagation (Rumelhart et al., 1986; Williams and Zipser, 1995; Werbos, 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 104
                            }
                        ],
                        "text": "The most widely used form of FNN, and the one we focus on in this section, is the multilayer perceptron (MLP; Rumelhart et al., 1986; Werbos, 1988; Bishop, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 147
                            }
                        ],
                        "text": "Artificial neural networks (ANNs) were originally developed as mathematical models of the information processing capabilities of biological brains (McCulloch and Pitts, 1988; Rosenblatt, 1963; Rumelhart et al., 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 87
                            }
                        ],
                        "text": ", 2000), and various pattern classification algorithms, such as multilayer perceptrons (Rumelhart et al., 1986; Bishop, 1995) and support vector machines (Vapnik, 1995), are routinely used for commercial tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": true,
            "numCitedBy": 19355,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113861474"
                        ],
                        "name": "C. Bishop",
                        "slug": "C.-Bishop",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bishop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8147588"
                        ],
                        "name": "N. Nasrabadi",
                        "slug": "N.-Nasrabadi",
                        "structuredName": {
                            "firstName": "Nasser",
                            "lastName": "Nasrabadi",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Nasrabadi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 124
                            }
                        ],
                        "text": "Pattern classification, also known as pattern recognition, is one of the most extensively studied areas of machine learning (Bishop, 2006; Duda et al., 2000), and certain pattern classifiers, such as multilayer perceptrons (Rumelhart et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 162
                            }
                        ],
                        "text": "The issue of whether training set performance carries over to the test set is referred to as generalisation, and is of fundamental importance to machine learning (see e.g. Vapnik, 1995; Bishop, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63317738,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "668b1277fbece28c4841eeab1c97e4ebd0079700",
            "isKey": false,
            "numCitedBy": 10185,
            "numCiting": 389,
            "paperAbstract": {
                "fragments": [],
                "text": "Probability Distributions.- Linear Models for Regression.- Linear Models for Classification.- Neural Networks.- Kernel Methods.- Sparse Kernel Machines.- Graphical Models.- Mixture Models and EM.- Approximate Inference.- Sampling Methods.- Continuous Latent Variables.- Sequential Data.- Combining Models."
            },
            "slug": "Pattern-Recognition-and-Machine-Learning-Bishop-Nasrabadi",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "Probability Distributions, linear models for Regression, Linear Models for Classification, Neural Networks, Graphical Models, Mixture Models and EM, Sampling Methods, Continuous Latent Variables, Sequential Data are studied."
            },
            "venue": {
                "fragments": [],
                "text": "J. Electronic Imaging"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69401634"
                        ],
                        "name": "S. Tosi",
                        "slug": "S.-Tosi",
                        "structuredName": {
                            "firstName": "Sandro",
                            "lastName": "Tosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Tosi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 124
                            }
                        ],
                        "text": "The spectrograms were calculated from the sample sequences using the \u2018specgram\u2019 function of the \u2018matplotlib\u2019 python toolkit (Tosi, 2009), based on Welch\u2019s \u2018Periodogram\u2019 algorithm (Welch, 1967), with the following parameters: The Fourier transform windows were 254 samples wide with an overlap of 127 samples (corresponding to 15."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61804350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5444764717392eb971f503de5f58ac220f770f76",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Build remarkable publication-quality plots the easy way Create high quality 2D plots by using Matplotlib productivelyIncremental introduction to Matplotlib, from the ground up to advanced levelsEmbed Matplotlib in GTK+, Qt, and wxWidgets applications as well as web sites to utilize them in Python applicationsDeploy Matplotlib in web applications and expose it on the Web using popular web frameworks such as Pylons and DjangoGet to grips with hands-on code and complete realistic case study examples along with highly informative plot screenshots In Detail Providing appealing plots and graphs is an essential part of various fields such as scientific research, data analysis, and so on. Matplotlib, the Python 2D plotting library, is used to produce publication-quality figures in a variety of hardcopy formats and interactive environments across platforms. This book explains creating various plots, histograms, power spectra, bar charts, error charts, scatter-plots and much more using the powerful Matplotlib library to get impressive out-of-the-box results.This book gives you a comprehensive tour of the key features of the Matplotlib Python 2D plotting library, right from the simplest concepts to the most advanced topics. You will discover how easy it is to produce professional-quality plots when you have this book to hand.The book introduces the library in steps. First come the basics: introducing what the library is, its important prerequisites (and terminology), installing and configuring Matplotlib, and going through simple plots such as lines, grids, axes, and charts. Then we start with some introductory examples, and move ahead by discussing the various programming styles that Matplotlib allows, and several key features.Further, the book presents an important section on embedding applications. You will be introduced to three of the best known GUI libraries--GTK+, Qt, and wxWidgets--and presented with the steps to implement to include Matplotlib in an application written using each of them. You will learn through an incremental approach: from a simple example that presents the peculiarities of the GUI library, to more complex ones, using GUI designer tools.Because the Web permeates all of our activities, a part of the book is dedicated to showing how Matplotlib can be used in a web environment, and another section focuses on using Matplotlib with common Python web frameworks, namely, Pylons and Django. Last, but not least, you will go through real-world examples, where you will see some real situations in which you can use Matplotlib. What you will learn from this book? Exploit the interactive computing environment of IPython to its fullest in collaboration with MatplotlibLearn line and point styles and master their customization, customization of axis ticks, and develop several plot types available in Matplotlib, such as histograms, bars, pie charts, polar charts, and so onExplore Object Oriented Matplotlib and learn how to add subplots, multiple figures, additional and shared axes, logarithmic scaled axes, data plotting with tick formatting and locators, text properties, fonts, LaTeX typewriting, and contour plotsGet comfortable with Glade--a RAD tool--to quickly design a GUI for GTK+ and embed Matplotlib into itMake the most of Matplotlib within the wxWidgets framework, in particular using the wxPython bindings and design a GUI with wxGladeUse the Qt Designer to draw a simple GUI and refer it to your Python code to fit your needsExpose Matplotlib on the Web using CGI (through Apache mod_cgi), mod_python, Django, and Pylons in no time at allProfit from the real-world examples by simply following the stream--identify the data source, elaborate the data and generate the resulting plot Approach This is a practical, hands-on book, with a lot of code and images. It presents the real code that generates every image and describes almost every single line of it, so that you know exactly what's going on.Introductory, descriptive, and theoretical parts are mixed with examples, so that reading and understanding them is easy. All of the examples build gradually with code snippets, their explanations, and plot images where necessary with the complete code and output presented at the end. Who this book is written for? This book is essentially for Python developers who have a good knowledge of Python; no knowledge of Matplotlib is required. You will be creating 2D plots using Matplotlib in no time at all."
            },
            "slug": "Matplotlib-for-Python-Developers-Tosi",
            "title": {
                "fragments": [],
                "text": "Matplotlib for Python Developers"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This book gives you a comprehensive tour of the key features of the Matplotlib Python 2D plotting library, right from the simplest concepts to the most advanced topics, and how easy it is to produce professional-quality plots when you have this book to hand."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448267"
                        ],
                        "name": "R. Byrd",
                        "slug": "R.-Byrd",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Byrd",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Byrd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2337678"
                        ],
                        "name": "P. Lu",
                        "slug": "P.-Lu",
                        "structuredName": {
                            "firstName": "Peihuang",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34728746"
                        ],
                        "name": "Ciyou Zhu",
                        "slug": "Ciyou-Zhu",
                        "structuredName": {
                            "firstName": "Ciyou",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ciyou Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 82
                            }
                        ],
                        "text": "1989), conjugate gradients (Hestenes and Stiefel, 1952; Shewchuk, 1994) and LBFGS (Byrd et al., 1995), that generally outperform steepest descent at batch learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6398414,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47a9ab6f97ab05fcd49aaf2864c97538b55e6268",
            "isKey": false,
            "numCitedBy": 3768,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for solving large nonlinear optimization problems with simple bounds is described. It is based on the gradient projection method and uses a limited memory BFGS matrix to approximate the Hessian of the objective function. It is shown how to take advantage of the form of the limited memory approximation to implement the algorithm efficiently. The results of numerical tests on a set of large problems are reported."
            },
            "slug": "A-Limited-Memory-Algorithm-for-Bound-Constrained-Byrd-Lu",
            "title": {
                "fragments": [],
                "text": "A Limited Memory Algorithm for Bound Constrained Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "An algorithm for solving large nonlinear optimization problems with simple bounds is described, based on the gradient projection method and uses a limited memory BFGS matrix to approximate the Hessian of the objective function."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31821767"
                        ],
                        "name": "P. Welch",
                        "slug": "P.-Welch",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Welch",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Welch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13900622,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "e63345b4243e2376720a4e66373fdffe7a7d6be0",
            "isKey": false,
            "numCitedBy": 8565,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of the fast Fourier transform in power spectrum analysis is described. Principal advantages of this method are a reduction in the number of computations and in required core storage, and convenient application in nonstationarity tests. The method involves sectioning the record and averaging modified periodograms of the sections."
            },
            "slug": "The-use-of-fast-Fourier-transform-for-the-of-power-Welch",
            "title": {
                "fragments": [],
                "text": "The use of fast Fourier transform for the estimation of power spectra: A method based on time averaging over short, modified periodograms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48306096"
                        ],
                        "name": "M. Hestenes",
                        "slug": "M.-Hestenes",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Hestenes",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hestenes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50079822"
                        ],
                        "name": "E. Stiefel",
                        "slug": "E.-Stiefel",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Stiefel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Stiefel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 27
                            }
                        ],
                        "text": "1989), conjugate gradients (Hestenes and Stiefel, 1952; Shewchuk, 1994) and LBFGS (Byrd et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2207234,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "466daddfb6340c28cb8da548007028c8cc5df687",
            "isKey": false,
            "numCitedBy": 7183,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "An iterative algorithm is given for solving a system Ax=k of n linear equations in n unknowns. The solution is given in n steps. It is shown that this method is a special case of a very general method which also includes Gaussian elimination. These general algorithms are essentially algorithms for finding an n dimensional ellipsoid. Connections are made with the theory of orthogonal polynomials and continued fractions."
            },
            "slug": "Methods-of-conjugate-gradients-for-solving-linear-Hestenes-Stiefel",
            "title": {
                "fragments": [],
                "text": "Methods of conjugate gradients for solving linear systems"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "An iterative algorithm is given for solving a system Ax=k of n linear equations in n unknowns and it is shown that this method is a special case of a very general method which also includes Gaussian elimination."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1952
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38594228"
                        ],
                        "name": "M. Blomberg",
                        "slug": "M.-Blomberg",
                        "structuredName": {
                            "firstName": "Mats",
                            "lastName": "Blomberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Blomberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145674781"
                        ],
                        "name": "R. Carlson",
                        "slug": "R.-Carlson",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "Carlson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Carlson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2612031"
                        ],
                        "name": "K. Elenius",
                        "slug": "K.-Elenius",
                        "structuredName": {
                            "firstName": "Kjell",
                            "lastName": "Elenius",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Elenius"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88378600"
                        ],
                        "name": "B. Granstrom",
                        "slug": "B.-Granstrom",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Granstrom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Granstrom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61788108,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "1e0cb6750c67590e3ebdb1c2e3a913930c1da2e9",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Recording Before three two t t-1 t-2 one WLR's logP logP logP logP one t-3"
            },
            "slug": "ISOLATED-WORD-RECOGNITION-Blomberg-Carlson",
            "title": {
                "fragments": [],
                "text": "ISOLATED WORD RECOGNITION"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "Recording Before three two t t-1 t-2 one WLR's logPlogP logP log P logP one t-3 is recorded."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 50
                            }
                        ],
                        "text": ", 1986; Bishop, 1995) and support vector machines (Vapnik, 1995) have become familiar to the scientific community at large."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38755,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47378595"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 142281124,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "fd68c2e9e69822f2f4b12acaab6f9269a1a61d74",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "FORGETTING AND REMEMBERINGWhen remembering runs amok, past pain can disrupt someone's present. New drugs, psychotherapeutic approaches, and other strategies might temper traumatic memories."
            },
            "slug": "Learning-to-Forget-Miller",
            "title": {
                "fragments": [],
                "text": "Learning to Forget"
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 58
                            }
                        ],
                        "text": "The experiment was carried out on the TIMIT speech corpus (Garofolo et al., 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 56
                            }
                        ],
                        "text": "The data for the experiments came from the TIMIT corpus (Garofolo et al., 1993) of prompted speech, collected by Texas Instruments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 157
                            }
                        ],
                        "text": "In this section we compare a CTC network with the best HMM and HMMBLSTM hybrid results given in Chapter 6 for phoneme recognition on the TIMIT speech corpus (Garofolo et al., 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Darpa timit acoustic phonetic continuous speech corpus cdrom"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758714"
                        ],
                        "name": "S. Fahlman",
                        "slug": "S.-Fahlman",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Fahlman",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fahlman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 135
                            }
                        ],
                        "text": "A large number of sophisticated gradient descent algorithms have been developed, such as RPROP (Riedmiller and Braun, 1993), quickprop (Fahlman, 1989), conjugate gradients (Hestenes and Stiefel, 1952; Shewchuk, 1994) and L-BFGS (Byrd et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 238073001,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "dab1baffed954d5d25d0a4b03a3999af02d0b3f9",
            "isKey": false,
            "numCitedBy": 253,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fast-learning-variations-on-back-propagation:-an-Fahlman",
            "title": {
                "fragments": [],
                "text": "Fast-learning variations on back propagation: an empirical study."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118372009"
                        ],
                        "name": "Jia Li",
                        "slug": "Jia-Li",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145248524"
                        ],
                        "name": "A. Najmi",
                        "slug": "A.-Najmi",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Najmi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Najmi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144790332"
                        ],
                        "name": "R. Gray",
                        "slug": "R.-Gray",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gray",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gray"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124668892,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc0aba67d3ed55e0abf10219ce7f0252d5768021",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Image-Classification-by-a-Two-Dimensional-Hidden-Li-Najmi",
            "title": {
                "fragments": [],
                "text": "Image Classification by a Two-Dimensional Hidden"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2467151"
                        ],
                        "name": "John S. Garofolo",
                        "slug": "John-S.-Garofolo",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Garofolo",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John S. Garofolo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145204681"
                        ],
                        "name": "L. Lamel",
                        "slug": "L.-Lamel",
                        "structuredName": {
                            "firstName": "Lori",
                            "lastName": "Lamel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lamel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144982775"
                        ],
                        "name": "W. Fisher",
                        "slug": "W.-Fisher",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Fisher",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Fisher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3241934"
                        ],
                        "name": "J. Fiscus",
                        "slug": "J.-Fiscus",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Fiscus",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Fiscus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786370"
                        ],
                        "name": "D. Pallett",
                        "slug": "D.-Pallett",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pallett",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pallett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35669756"
                        ],
                        "name": "Nancy L. Dahlgren",
                        "slug": "Nancy-L.-Dahlgren",
                        "structuredName": {
                            "firstName": "Nancy",
                            "lastName": "Dahlgren",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nancy L. Dahlgren"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 65148724,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "47128bb3ce4ed00691c0d7d58c02791c3e963ab7",
            "isKey": false,
            "numCitedBy": 2183,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Darpa-Timit-Acoustic-Phonetic-Continuous-Speech-|-Garofolo-Lamel",
            "title": {
                "fragments": [],
                "text": "Darpa Timit Acoustic-Phonetic Continuous Speech Corpus CD-ROM {TIMIT} | NIST"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49637194"
                        ],
                        "name": "A. Mullin",
                        "slug": "A.-Mullin",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Mullin",
                            "middleNames": [
                                "A."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mullin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51083130"
                        ],
                        "name": "F. Rosenblatt",
                        "slug": "F.-Rosenblatt",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Rosenblatt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rosenblatt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 147
                            }
                        ],
                        "text": "Artificial neural networks (ANNs) were originally developed as mathematical models of the information processing capabilities of biological brains (McCulloch and Pitts, 1988; Rosenblatt, 1963; Rumelhart et al., 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61566132,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cccc0a4817fd5f6d8758c66b4065a23897d49f1d",
            "isKey": false,
            "numCitedBy": 2372,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Principles-of-neurodynamics-Mullin-Rosenblatt",
            "title": {
                "fragments": [],
                "text": "Principles of neurodynamics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144586498"
                        ],
                        "name": "R. Plamondon",
                        "slug": "R.-Plamondon",
                        "structuredName": {
                            "firstName": "R\u00e9jean",
                            "lastName": "Plamondon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Plamondon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1828940"
                        ],
                        "name": "D. Lopresti",
                        "slug": "D.-Lopresti",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lopresti",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lopresti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52643489"
                        ],
                        "name": "L. Schomaker",
                        "slug": "L.-Schomaker",
                        "structuredName": {
                            "firstName": "Lambertus",
                            "lastName": "Schomaker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Schomaker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748081"
                        ],
                        "name": "R. Srihari",
                        "slug": "R.-Srihari",
                        "structuredName": {
                            "firstName": "Rohini",
                            "lastName": "Srihari",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Srihari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61069336,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "6000a47b21a4ac0c256e2a5bb82c93cd31d8dedb",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-line-handwriting-recognition.-Plamondon-Lopresti",
            "title": {
                "fragments": [],
                "text": "On-line handwriting recognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066667188"
                        ],
                        "name": "Sepp Hochreiter",
                        "slug": "Sepp-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sepp Hochreiter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 85
                            }
                        ],
                        "text": "This effect is often referred to in the literature as the vanishing gradient problem (Hochreiter, 1991; Hochreiter et al., 2001a; Bengio et al., 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60091947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f3d13e95c25a8f6a753e38dfce88885097cbd43",
            "isKey": false,
            "numCitedBy": 603,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Untersuchungen-zu-dynamischen-neuronalen-Netzen-Hochreiter",
            "title": {
                "fragments": [],
                "text": "Untersuchungen zu dynamischen neuronalen Netzen"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688681"
                        ],
                        "name": "T. Kohonen",
                        "slug": "T.-Kohonen",
                        "structuredName": {
                            "firstName": "Teuvo",
                            "lastName": "Kohonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kohonen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 140
                            }
                        ],
                        "text": "Well known examples of FNNs include perceptrons (Rosenblatt, 1958), radial basis function networks (Broomhead and Lowe, 1988), Kohonen maps (Kohonen, 1989) and Hopfield nets (Hopfield, 1982)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59773108,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "015e5c48abbd59309e6986aaa94550e40562f100",
            "isKey": false,
            "numCitedBy": 3153,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Self-organization-and-associative-memory:-3rd-Kohonen",
            "title": {
                "fragments": [],
                "text": "Self-organization and associative memory: 3rd edition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 110
                            }
                        ],
                        "text": "As with FNNs, many varieties of RNN have been proposed, such as Elman networks (Elman, 1990), Jordan networks (Jordan, 1990), time delay neural networks (Lang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 56723681,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d76aafbeb54575859441a442376766c597f6bb52",
            "isKey": false,
            "numCitedBy": 1102,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Attractor-dynamics-and-parallelism-in-a-sequential-Jordan",
            "title": {
                "fragments": [],
                "text": "Attractor dynamics and parallelism in a connectionist sequential machine"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144480181"
                        ],
                        "name": "S. Johansson",
                        "slug": "S.-Johansson",
                        "structuredName": {
                            "firstName": "Stig",
                            "lastName": "Johansson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Johansson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144214753"
                        ],
                        "name": "E. Atwell",
                        "slug": "E.-Atwell",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Atwell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Atwell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153733790"
                        ],
                        "name": "R. Garside",
                        "slug": "R.-Garside",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Garside",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Garside"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143998726"
                        ],
                        "name": "G. Leech",
                        "slug": "G.-Leech",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Leech",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Leech"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 63
                            }
                        ],
                        "text": "The writers were asked to write forms from the LOB text corpus (Johansson et al., 1986), and the position of their pen was tracked using an infra-red device in the corner of the board."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53754601,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c2dff1c9b04da49f50fd85ba010d2a041fc5c39",
            "isKey": false,
            "numCitedBy": 162,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-tagged-LOB-Corpus-:-user's-manual-Johansson-Atwell",
            "title": {
                "fragments": [],
                "text": "The tagged LOB Corpus : user's manual"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144623827"
                        ],
                        "name": "W. Nauta",
                        "slug": "W.-Nauta",
                        "structuredName": {
                            "firstName": "Walle",
                            "lastName": "Nauta",
                            "middleNames": [
                                "J.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Nauta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6802116"
                        ],
                        "name": "M. Feirtag",
                        "slug": "M.-Feirtag",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Feirtag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Feirtag"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42194561,
            "fieldsOfStudy": [
                "Biology",
                "Medicine"
            ],
            "id": "ef1505e7597ec325a04bf3774e80ef5808b23321",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-organization-of-the-brain.-Nauta-Feirtag",
            "title": {
                "fragments": [],
                "text": "The organization of the brain."
            },
            "venue": {
                "fragments": [],
                "text": "Scientific American"
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144711425"
                        ],
                        "name": "T. Robinson",
                        "slug": "T.-Robinson",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Robinson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Robinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144251385"
                        ],
                        "name": "J. Holdsworth",
                        "slug": "J.-Holdsworth",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Holdsworth",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Holdsworth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2093174717"
                        ],
                        "name": "R. Patterson",
                        "slug": "R.-Patterson",
                        "structuredName": {
                            "firstName": "Roy",
                            "lastName": "Patterson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Patterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1998157"
                        ],
                        "name": "F. Fallside",
                        "slug": "F.-Fallside",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Fallside",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fallside"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 246
                            }
                        ],
                        "text": "Neural networks, however, tend to be relatively robust to the choice of input representation: for example, in previous work on phoneme recognition, RNNs were shown to perform almost equally well using a wide range of speech preprocessing methods (Robinson et al., 1990)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 32864013,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "095616007d31b1857764ba5aa4fe53c842774e1a",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-comparison-of-preprocessors-for-the-cambridge-Robinson-Holdsworth",
            "title": {
                "fragments": [],
                "text": "A comparison of preprocessors for the cambridge recurrent error propagation network speech recognition system"
            },
            "venue": {
                "fragments": [],
                "text": "ICSLP"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701942"
                        ],
                        "name": "N. Cercone",
                        "slug": "N.-Cercone",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Cercone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cercone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846792"
                        ],
                        "name": "B. Sirinaovakul",
                        "slug": "B.-Sirinaovakul",
                        "structuredName": {
                            "firstName": "Booncharoen",
                            "lastName": "Sirinaovakul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sirinaovakul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693016"
                        ],
                        "name": "V. Wuwongse",
                        "slug": "V.-Wuwongse",
                        "structuredName": {
                            "firstName": "Vilas",
                            "lastName": "Wuwongse",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Wuwongse"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 731177,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89737c6726f1270e8e4cf34e75f66a66201a36d6",
            "isKey": false,
            "numCitedBy": 1477,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-the-Special-Issue-Cercone-Sirinaovakul",
            "title": {
                "fragments": [],
                "text": "Introduction to the Special Issue"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145768864"
                        ],
                        "name": "D. Broomhead",
                        "slug": "D.-Broomhead",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Broomhead",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Broomhead"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144159852"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 99
                            }
                        ],
                        "text": "Well known examples of FNNs include perceptrons (Rosenblatt, 1958), radial basis function networks (Broomhead and Lowe, 1988), Kohonen maps (Kohonen, 1989) and Hopfield nets (Hopfield, 1982)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3686496,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "b08ba914037af6d88d16e2657a65cd9dc5cf5da1",
            "isKey": false,
            "numCitedBy": 2307,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multivariable-Functional-Interpolation-and-Adaptive-Broomhead-Lowe",
            "title": {
                "fragments": [],
                "text": "Multivariable Functional Interpolation and Adaptive Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98782082"
                        ],
                        "name": "J. Benes",
                        "slug": "J.-Benes",
                        "structuredName": {
                            "firstName": "Jir\u00ed",
                            "lastName": "Benes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Benes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16819094,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "c54c3b6f602ca674169d660151dcd72fd14d7b95",
            "isKey": false,
            "numCitedBy": 650,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-neural-networks-Benes",
            "title": {
                "fragments": [],
                "text": "On neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Kybernetika"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143913738"
                        ],
                        "name": "Santiago Fern\u00e1ndez",
                        "slug": "Santiago-Fern\u00e1ndez",
                        "structuredName": {
                            "firstName": "Santiago",
                            "lastName": "Fern\u00e1ndez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Santiago Fern\u00e1ndez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2168488"
                        ],
                        "name": "Roman Bertolami",
                        "slug": "Roman-Bertolami",
                        "structuredName": {
                            "firstName": "Roman",
                            "lastName": "Bertolami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roman Bertolami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14635907,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "375214ac340226e23ec428e92ec499fb89f508b8",
            "isKey": false,
            "numCitedBy": 1587,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance."
            },
            "slug": "A-Novel-Connectionist-System-for-Unconstrained-Graves-Liwicki",
            "title": {
                "fragments": [],
                "text": "A Novel Connectionist System for Unconstrained Handwriting Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies, significantly outperforming a state-of-the-art HMM-based system."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 238
                            }
                        ],
                        "text": "The original LSTM training algorithm (Hochreiter and Schmidhuber, 1997) used an approximate error gradient calculated with a combination of Real Time Recurrent Learning (RTRL; Robinson and Fallside, 1987) and Backpropagation Through Time (BPTT; Williams and Zipser, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 83
                            }
                        ],
                        "text": "To efficiently calculate the gradient, we use a technique known as backpropagation (Rumelhart et al., 1986; Williams and Zipser, 1995; Werbos, 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 196
                            }
                        ],
                        "text": "Two well-known algorithms have been devised to efficiently calculate weight derivatives for RNNs: real time recurrent learning (RTRL; Robinson and Fallside, 1987) and backpropagation through time (BPTT; Williams and Zipser, 1995; Werbos, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14792754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10dae7fca6b65b61d155a622f0c6ca2bc3922251",
            "isKey": false,
            "numCitedBy": 567,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Gradient-based-learning-algorithms-for-recurrent-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning algorithms for recurrent networks and their computational complexity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Offline Handwriting Recognition with Multidimen"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast Model-based Protein Homology"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multi-dimensional Dependency-tree Hidden"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 69
                            }
                        ],
                        "text": "The following preprocessing, which is standard in speech recognition (Young and Woodland, 2002) was used for the audio data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "HTK Version 3.2: User"
            },
            "venue": {
                "fragments": [],
                "text": "Reference and Programmer Manual,"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 85
                            }
                        ],
                        "text": "This effect is often referred to in the literature as the vanishing gradient problem (Hochreiter, 1991; Hochreiter et al., 2001a; Bengio et al., 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 114
                            }
                        ],
                        "text": "These included non-gradient based training algorithms, such as simulated annealing and discrete error propagation (Bengio et al., 1994), explicitly introduced time delays (Lang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "LeRec: A NN/HMM Hybrid"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The desired effect is to artificially enhance the size of the train"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 169
                            }
                        ],
                        "text": "The original LSTM training algorithm (Hochreiter and Schmidhuber, 1997) used an approximate error gradient calculated with a combination of Real Time Recurrent Learning (RTRL; Robinson and Fallside, 1987) and Backpropagation Through Time (BPTT; Williams and Zipser, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 127
                            }
                        ],
                        "text": "Two well-known algorithms have been devised to efficiently calculate weight derivatives for RNNs: real time recurrent learning (RTRL; Robinson and Fallside, 1987) and backpropagation through time (BPTT; Williams and Zipser, 1995; Werbos, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The utility driven dynamic error propagation network"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report CUED/F-INFENG/TR.1,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Standard Databases for Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An Exploration of Large Vocabu"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ICDAR 2009 Handwriting Recognition Competition . In 10th International Conference on Document Analysis and Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "ICDAR 2009 Handwriting Recognition Competition . In 10th International Conference on Document Analysis and Recognition"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Intuitively this is because noise reduces the precision with"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kernel Regression and Backpropagation Training"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 23
                            }
                        ],
                        "text": "5) for neural networks (MacKay, 1995; Neal, 1996), we will here focus on loss functions derived using maximum likelihood."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Database version 2"
            },
            "venue": {
                "fragments": [],
                "text": "Database version 2"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bidirectional LSTM Networks"
            },
            "venue": {
                "fragments": [],
                "text": "LSTM Recurrent Networks. Technical Report IDSIA-09-05,"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Offline Grammar-based Recog"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of Global"
            },
            "venue": {
                "fragments": [],
                "text": "Its Properties. Cognitive Systems Research"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gradient-Based Learning Algorithms for"
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Computation,"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "do not assume features came from particular distribution. -RNN, LSTM can model non-linear relationship among features"
            },
            "venue": {
                "fragments": [],
                "text": "do not assume features came from particular distribution. -RNN, LSTM can model non-linear relationship among features"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evolving Memory Cell"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gradient flow"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 114
                            }
                        ],
                        "text": "Although primarily a means to reduce training time, we have also found that retraining improves final performance (Graves et al., 2005a; Beringer, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Human Language Acquisition in a Machine Learning Task"
            },
            "venue": {
                "fragments": [],
                "text": "In International Conference on Spoken Language Processing,"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gradient-Based Learning Applied"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modeling systems with internal state"
            },
            "venue": {
                "fragments": [],
                "text": "Gas Market Model. Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modular DAG-RNN Architec"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 25
                            }
                        ],
                        "text": "The Air Freight database (McCarter and Storkey, 2007) is a ray-traced colour image sequence that comes with a ground truth segmentation into the different textures mapped onto the 3-d models (Figure 8."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Air Freight Image Segmentation Database"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "LNCS, vol"
            },
            "venue": {
                "fragments": [],
                "text": "4668, pp. 549\u2013558. Springer, Heidelberg (2007)"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Precise Timing with LSTM"
            },
            "venue": {
                "fragments": [],
                "text": "Ecole Polytechnique Fe\u0301de\u0301rale de Lausanne"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Framewise Phoneme Classification with Bidirectional"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Novel Ap"
            },
            "venue": {
                "fragments": [],
                "text": "Document Analysis and Recognition,"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "HTK Version 3.2: User, Reference and Programmer Manual"
            },
            "venue": {
                "fragments": [],
                "text": "HTK Version 3.2: User, Reference and Programmer Manual"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Several Improvements to a Recurrent Error Propagation Network"
            },
            "venue": {
                "fragments": [],
                "text": "ICSLP"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 131
                            }
                        ],
                        "text": "The French handwriting recognition competition at ICDAR 2009 (Grosicki and Abed, 2009) was based on a subset of the RIMES database (Grosicki et al., 2009) of French mail snippets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Results of the RIMES Evalu"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Alex Graves is a Junior Fellow of the Canadian Institute for Advanced Re- search"
            },
            "venue": {
                "fragments": [],
                "text": "Alex Graves is a Junior Fellow of the Canadian Institute for Advanced Re- search"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phoneme Recognition in TIMIT with"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Image Processing Using Pulse-Coupled"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Audio, Speech and Lang"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 17,"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 89
                            }
                        ],
                        "text": "In continuous speech recognition, it is common practice to use a reduced set of phonemes (Robinson, 1991), by merging those with similar sounds, and not separating closures from stops."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Several improvements to a recurrent error propagation network phone recognition system"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report CUED/FINFENG/TR82,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Universal Approximators. Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 57
                            }
                        ],
                        "text": "This can be mitigated by the addition of a momentum term (Plaut et al., 1986), which effectively adds inertia to the motion of the algorithm through weight space, thereby speeding up convergence and helping to escape from local minima:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Experiments on Learning by BackPropagation"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report CMU\u2013CS\u201386\u2013126, Carnegie\u2013Mellon University,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "@BULLET GMM-HMM with diagonal covariance matrix assume features are not correlated"
            },
            "venue": {
                "fragments": [],
                "text": "@BULLET GMM-HMM with diagonal covariance matrix assume features are not correlated"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The tagged LOB corpus"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 115
                            }
                        ],
                        "text": "Additionally, LSTM has been applied to various real-world problems, such as protein secondary structure prediction (Hochreiter et al., 2007; Chen and Chaudhari, 2005), music generation (Eck and Schmidhuber, 2002), reinforcement learning (Bakker, 2002) and speech recognition (Graves and Schmidhuber, 2005b; Graves et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 64
                            }
                        ],
                        "text": "It has proved especially popular in the field of bioinformatics (Chen and Chaudhari, 2005; Thireou and Reczko, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Protein secondary structure prediction with bidirectional lstm networks. In Post-Conference Workshop on Computational Intelligence Approaches for the Analysis of Bio-data (CI-BIO)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Utility Driven Dynamic Error Propagation"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 38
                            }
                        ],
                        "text": "1 Augmented Conditional Random Fields (Hifny and Renals, 2009) 26."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speech Recognition using Augmented Conditional"
            },
            "venue": {
                "fragments": [],
                "text": "Random Fields. Trans. Audio, Speech and Lang. Proc.,"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "As would be expected, its advantages are most pronounced for problems requiring the use of long range contextual information"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 115
                            }
                        ],
                        "text": "Additionally, LSTM has been applied to various real-world problems, such as protein secondary structure prediction (Hochreiter et al., 2007; Chen and Chaudhari, 2005), music generation (Eck and Schmidhuber, 2002), reinforcement learning (Bakker, 2002), speech recognition (Graves and Schmidhuber, 2005b; Graves et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Protein Secondary Structure Prediction with bidirectional LSTM networks"
            },
            "venue": {
                "fragments": [],
                "text": "In International Joint Conference on Neural Networks: Post-Conference Workshop on Computational Intelligence Approaches for the Analysis of Bio-data (CI-BIO),"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic Speech Recognition Understanding, ASRU"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mellon University Discussion -HMM v.s. CTC @BULLET HMM is generative model, LSTM-CTC is discriminative model -Discriminative Model achieve better performance on Labeling task"
            },
            "venue": {
                "fragments": [],
                "text": "Mellon University Discussion -HMM v.s. CTC @BULLET HMM is generative model, LSTM-CTC is discriminative model -Discriminative Model achieve better performance on Labeling task"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phoneme Boundary Estimation Using Bidi"
            },
            "venue": {
                "fragments": [],
                "text": "Data Structures. IEEE Transactions on Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 61
                            }
                        ],
                        "text": "The French handwriting recognition competition at ICDAR 2009 (Grosicki and Abed, 2009) was based on a subset of the RIMES database (Grosicki et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Handwriting Recognition Competition"
            },
            "venue": {
                "fragments": [],
                "text": "ICDAR"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 114
                            }
                        ],
                        "text": "Although primarily a means to reduce training time, we have also found that retraining improves final performance (Graves et al., 2005a; Beringer, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Human Language Acquisition in a Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Task. In: International Conference on Spoken Language Processing"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Institut f\u00fcr Informatik, Technische Universit\u00e4t M\u00fcnchen"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reading checks with graph transformer networks"
            },
            "venue": {
                "fragments": [],
                "text": "In International Conference on Acoustics, Speech, and Signal Processing,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exploiting the Past and"
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of Machine Learning Research"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Identification and forecasting of large dynamical systems by dynamical consistent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "New Directions in Statistical Signal Processing: From Systems"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A comparison of prepro"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speech Recognition using Augmented Conditional Random"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1952
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Conf. on Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Conf. on Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Acoustic Modeling using Deep Belief"
            },
            "venue": {
                "fragments": [],
                "text": "els. In: ICASSP,"
            },
            "year": 1998
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 72,
            "methodology": 57,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 220,
        "totalPages": 22
    },
    "page_url": "https://www.semanticscholar.org/paper/Supervised-Sequence-Labelling-with-Recurrent-Neural-Graves/a97b5db17acc731ef67321832dbbaf5766153135?sort=total-citations"
}