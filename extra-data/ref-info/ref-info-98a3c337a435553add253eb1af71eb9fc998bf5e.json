{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 134
                            }
                        ],
                        "text": "After performing a forward pass through the network to compute the activities of all the units, we do a backward pass as described in Rumelhart et al. (1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 205001834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "isKey": false,
            "numCitedBy": 20334,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."
            },
            "slug": "Learning-representations-by-back-propagating-errors-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating errors"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211061"
                        ],
                        "name": "P. Hoyer",
                        "slug": "P.-Hoyer",
                        "structuredName": {
                            "firstName": "Patrik",
                            "lastName": "Hoyer",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hoyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 71
                            }
                        ],
                        "text": "Other methods of learning topographic maps from natural image patches (Hyvarinen & Hoyer, 2001) are also hard to extend to more hidden layers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6302770,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "7c032d555a9d7096f7bb88441f10e33d3302d5be",
            "isKey": false,
            "numCitedBy": 291,
            "numCiting": 130,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-two-layer-sparse-coding-model-learns-simple-and-Hyv\u00e4rinen-Hoyer",
            "title": {
                "fragments": [],
                "text": "A two-layer sparse coding model learns simple and complex cell receptive fields and topography from natural images"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 249
                            }
                        ],
                        "text": "\u20262 1 1 1 1( ) ( ) ( ) 0 (6)i i i i i i i ix x y y z z l\n2log(1 )j jy \u03bb\ncells found in the primary visual cortex of most mammals and are also similar to the features learned in other models that seek to capture statistical structure in natural images (Bell & Sejnowski, 1997; Olshausen & Field, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6219133,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca1d23be869380ac9e900578c601c2d1febcc0c9",
            "isKey": false,
            "numCitedBy": 2373,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-\u201cindependent-components\u201d-of-natural-scenes-are-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "The \u201cindependent components\u201d of natural scenes are edge filters"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 80
                            }
                        ],
                        "text": "These vectors then underwent standard and biologically motivated preprocessing (Olshausen & Field, 1996; van Hateren & van der Schaaf, 1998) that involved subtracting the mean value from each pixel and then taking the variance normalized projection onto the leading 256 eigenvectors of the pixel\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 273
                            }
                        ],
                        "text": "\u20262 1 1 1 1( ) ( ) ( ) 0 (6)i i i i i i i ix x y y z z l\n2log(1 )j jy \u03bb\ncells found in the primary visual cortex of most mammals and are also similar to the features learned in other models that seek to capture statistical structure in natural images (Bell & Sejnowski, 1997; Olshausen & Field, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4358477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8012c4a1e2ca663f1a04e80cbb19631a00cbab27",
            "isKey": false,
            "numCitedBy": 5639,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1\u20134 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7\u201312. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13\u201318, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs."
            },
            "slug": "Emergence-of-simple-cell-receptive-field-properties-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6699891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8d01934cb26064b253dbd0f1627519133c3df3e",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an energy-based model that uses a product of generalized Student-t distributions to capture the statistical structure in data sets. This model is inspired by and particularly applicable to natural data sets such as images. We begin by providing the mathematical framework, where we discuss complete and overcomplete models and provide algorithms for training these models from data. Using patches of natural scenes, we demonstrate that our approach represents a viable alternative to independent component analysis as an interpretive model of biological visual systems. Although the two approaches are similar in flavor, there are also important differences, particularly when the representations are overcomplete. By constraining the interactions within our model, we are also able to study the topographic organization of Gabor-like receptive fields that our model learns. Finally, we discuss the relation of our new approach to previous workin particular, gaussian scale mixture models and variants of independent components analysis."
            },
            "slug": "Topographic-Product-Models-Applied-to-Natural-Scene-Osindero-Welling",
            "title": {
                "fragments": [],
                "text": "Topographic Product Models Applied to Natural Scene Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "An energy-based model is presented that uses a product of generalized Student-t distributions to capture the statistical structure in data sets to study the topographic organization of Gabor-like receptive fields that the model learns."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 79
                            }
                        ],
                        "text": "Numerical errors up to second order are eliminated by using a leapfrog method (Neal, 1996) that uses the potential energy gradient at time t to compute the velocity increment between time t \u2013 \u00bd and t + \u00bd and uses the velocity at time t + \u00bd to compute the position increment between time t and t + 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60809283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db869fa192a3222ae4f2d766674a378e47013b1b",
            "isKey": false,
            "numCitedBy": 3642,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial \"neural networks\" are widely used as flexible models for classification and regression applications, but questions remain about how the power of these models can be safely exploited when training data is limited. This book demonstrates how Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional training methods. Insight into the nature of these complex Bayesian models is provided by a theoretical investigation of the priors over functions that underlie them. A practical implementation of Bayesian neural network learning using Markov chain Monte Carlo methods is also described, and software for it is freely available over the Internet. Presupposing only basic knowledge of probability and statistics, this book should be of interest to researchers in statistics, engineering, and artificial intelligence."
            },
            "slug": "Bayesian-Learning-for-Neural-Networks-Neal",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Bayesian Learning for Neural Networks shows that Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional neural network learning methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 30
                            }
                        ],
                        "text": "Backpropagation through time (Werbos, 1990) can then be used to obtain the derivatives of the energy with respect to the connection weights and also the energy gradients required for generating a whole confabulated sequence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18470994,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1a3d22599028a05669e884f3eaf19a342e190a87",
            "isKey": false,
            "numCitedBy": 4036,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Backpropagation is now the most widely used tool in the field of artificial neural networks. At the core of backpropagation is a method for calculating derivatives exactly and efficiently in any large system made up of elementary subsystems or calculations which are represented by known, differentiable functions; thus, backpropagation has many applications which do not involve neural networks as such. This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis. Next, it presents the basic equations for backpropagation through time, and discusses applications to areas like pattern recognition involving dynamic systems, systems identification, and control. Finally, i t describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method. Pseudocode is provided to clarify the algorithms. The chain rule for ordered derivatives-the theorem which underlies backpropagation-is briefly discussed."
            },
            "slug": "Backpropagation-Through-Time:-What-It-Does-and-How-Werbos",
            "title": {
                "fragments": [],
                "text": "Backpropagation Through Time: What It Does and How to Do It"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis, and describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474176"
                        ],
                        "name": "J. H. Hateren",
                        "slug": "J.-H.-Hateren",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hateren",
                            "middleNames": [
                                "H.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Hateren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46498936"
                        ],
                        "name": "A. Schaaf",
                        "slug": "A.-Schaaf",
                        "structuredName": {
                            "firstName": "Arjen",
                            "lastName": "Schaaf",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schaaf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "motivated pre-processing ( van Hateren and van der Schaaf, 1998;  Olshausen and"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15666050,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "31b0e2c5bec857e497ab545ff808ce9ccba9f3d1",
            "isKey": false,
            "numCitedBy": 802,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis (ICA) on a large set of natural images. Histograms of spatial frequency bandwidth, orientation tuning bandwidth, aspect ratio and length of the receptive fields match well. This indicates that simple cells are well tuned to the expected statistics of natural stimuli. There is no match, however, in calculated and measured distributions for the peak of the spatial frequency response: the filters produced by ICA do not vary their spatial scale as much as simple cells do, but are fixed to scales close to the finest ones allowed by the sampling lattice. Possible ways to resolve this discrepancy are discussed."
            },
            "slug": "Independent-component-filters-of-natural-images-in-Hateren-Schaaf",
            "title": {
                "fragments": [],
                "text": "Independent component filters of natural images compared with simple cells in primary visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis on a large set of natural images: there is no match, however, in calculated and measured distributions for the peak of the spatial frequency response."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3741160"
                        ],
                        "name": "J. V. van Hateren",
                        "slug": "J.-V.-van-Hateren",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "van Hateren",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. V. van Hateren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51922762"
                        ],
                        "name": "A. van der Schaaf",
                        "slug": "A.-van-der-Schaaf",
                        "structuredName": {
                            "firstName": "Arjen",
                            "lastName": "van der Schaaf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. van der Schaaf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 105
                            }
                        ],
                        "text": "These vectors then underwent standard and biologically motivated preprocessing (Olshausen & Field, 1996; van Hateren & van der Schaaf, 1998) that involved subtracting the mean value from each pixel and then taking the variance normalized projection onto the leading 256 eigenvectors of the pixel\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 144
                            }
                        ],
                        "text": "The original data for this model were vectors representing the pixel intensities in 20 \u00d7 20 patches extracted from photographs of natural scenes (van Hateren & van der Schaaf, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1789554,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "117175a54263cc2ae693fe82c9b3fe0553931cd8",
            "isKey": false,
            "numCitedBy": 639,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis (ICA) on a large set of natural images. Histograms of spatial frequency bandwidth, orientation tuning bandwidth, aspect ratio and length of the receptive fields match well. This indicates that simple cells are well tuned to the expected statistics of natural stimuli. There is no match, however, in calculated and measured distributions for the peak of the spatial frequency response: the filters produced by ICA do not vary their spatial scale as much as simple cells do, but are fixed to scales close to the finest ones allowed by the sampling lattice. Possible ways to resolve this discrepancy are discussed."
            },
            "slug": "Independent-component-filters-of-natural-images-in-Hateren-Schaaf",
            "title": {
                "fragments": [],
                "text": "Independent component filters of natural images compared with simple cells in primary visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis on a large set of natural images: there is no match, however, in calculated and measured distributions for the peak of the spatial frequency response."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Royal Society of London. Series B: Biological Sciences"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 164
                            }
                        ],
                        "text": "If we start the process at an observed data vector and just run it for a few steps, we can generate a \u201cconfabulation\u201d that works very well for adjusting the weights (Hinton, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207596505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9360e5ce9c98166bb179ad479a9d2919ff13d022",
            "isKey": false,
            "numCitedBy": 4572,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data."
            },
            "slug": "Training-Products-of-Experts-by-Minimizing-Hinton",
            "title": {
                "fragments": [],
                "text": "Training Products of Experts by Minimizing Contrastive Divergence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A product of experts (PoE) is an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary because it is hard even to approximate the derivatives of the renormalization term in the combination rule."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "a stochastic sampling procedure ( Osindero et al., 2006 )."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12318053,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "363cee34e32cea1b5f40258e76ec932e03984f0b",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an energy-based model that uses a product of generalised Student-t distributions to capture the statistical structure in datasets. This model is inspired by and particularly applicable to \u201cnatural\u201d datasets such as images. We begin by providing the mathematical framework, where we discuss complete as well as undercomplete"
            },
            "slug": "Modelling-the-Statistics-of-Natural-Images-with-of-Osindero",
            "title": {
                "fragments": [],
                "text": "Modelling the Statistics of Natural Images with Topographic Product of Student-t Models"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "An energy-based model that uses a product of generalised Student-t distributions to capture the statistical structure in datasets, inspired by and particularly applicable to \u201cnatural\u201d datasets such as images is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 273
                            }
                        ],
                        "text": "\u2026v\nx\n( )log ( ) ( ) log (2)Ep E e v v x x\nlog ( ) ( ) ( ) ( ) (3)ij\nij ij ij\np E E w p\nw w w\nv\nx x v v\n2( ) (1 ) (4)j j j E log y x \u03bb\nBy using the energy contributions in Equation 4 we encourage the network to model the data distribution by finding constraints of this type (Hinton & Teh, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9951467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66e65f81f1f76fb3a7c8ab2d813362b924e2fa9b",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Some high-dimensional datasets can be modelled by assuming that there are many different linear constraints, each of which is Frequently Approximately Satisfied (FAS) by the data. The probability of a data vector under the model is then proportional to the product of the probabilities of its constraint violations. We describe three methods of learning products of constraints using a heavy-tailed probability distribution for the violations."
            },
            "slug": "Discovering-Multiple-Constraints-that-are-Satisfied-Hinton-Teh",
            "title": {
                "fragments": [],
                "text": "Discovering Multiple Constraints that are Frequently Approximately Satisfied"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work describes three methods of learning products of constraints using a heavy-tailed probability distribution for the violations of constraint violations."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114422872"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "Abram"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9311196,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "9d81ed652828fd536d86cc371278efcb502572cd",
            "isKey": false,
            "numCitedBy": 1518,
            "numCiting": 190,
            "paperAbstract": {
                "fragments": [],
                "text": "There are many reasons why a budding academic might want to avoid interdisciplinary research. It is diffi cult enough to acquire expertise in one fi eld of research, let alone two or more. The time required to read the literature in a fi eld outside your own main area can be hard to fi nd, and the additional time investment to learn novel methods from another fi eld can be huge. Moreover, the hiring and reward systems in academia still run strongly along disciplinary lines, so that work that draws on or contributes to other fi elds may not be fully valued in one\u2019s own fi eld. Interdisciplinary research may not be appreciated by narrow-minded colleagues. Some interdisciplinary projects have a bogus air about them, looking like they were designed more to bring in big research grants than to accomplish intellectual goals. The interdisciplinary scholar can look a bit like a dilettante, dabbling in multiple fi elds in order to avoid tackling the diffi cult problems in an established fi eld. Grants for interdisciplinary research can be diffi cult to get, because most granting agencies are organized along disciplinary lines. Despite these deterrents to interdisciplinary research, there are powerful intellectual reasons why work that oversteps the ossifi ed boundaries of established fi elds can have great intellectual benefi ts. Such benefi ts are vividly apparent in the interdisciplinary fi eld of cognitive science, which attempts to understand the mind by combining insights from the fi elds of psychology, philosophy, linguistics, neuroscience, anthropology, and artifi cial intelligence. After a brief review of the history of the fi eld and its contributing disciplines, this chapter will examine some of the main theoretical and experimental advances that cognitive science has accomplished over the past half century, deriving lessons that might be useful for researchers in any emerging interdisciplinary area."
            },
            "slug": "Cognitive-science.-Miller",
            "title": {
                "fragments": [],
                "text": "Cognitive science."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This chapter will examine some of the main theoretical and experimental advances that cognitive science has accomplished over the past half century, deriving lessons that might be useful for researchers in any emerging interdisciplinary area."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057748250"
                        ],
                        "name": "B. Warner",
                        "slug": "B.-Warner",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Warner",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Warner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 79
                            }
                        ],
                        "text": "Numerical errors up to second order are eliminated by using a leapfrog method (Neal, 1996) that uses the potential energy gradient at time t to compute the velocity increment between time t \u2013 \u00bd and t + \u00bd and uses the velocity at time t + \u00bd to compute the position increment between time t and t + 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 78
                            }
                        ],
                        "text": "Numerical errors up to second order are eliminated by using a leapfrog method (Neal, 1996) that uses the potential energy gradient at time t to compute the velocity increment between time t \u2013 1\u20442 and t + 1\u20442 and uses the velocity at time t + 1\u20442 to compute the position increment between time t and t + 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 124315035,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36957eff7dec27b3f7307b3043e272a9dcee9152",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bayesian-Learning-for-Neural-Networks-(Lecture-in-Warner-Neal",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks (Lecture Notes in Statistical Vol. 118)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 80
                            }
                        ],
                        "text": "Numerical errors up to second order are eliminated by using a \u201cleapfrog\u201d method (Neal, 1996) which uses the potential energy gradient at time t to compute the velocity increment between time t\u2212 1 2 and t+ 1 2 and uses the velocity at time t+ 1 2 to compute the position increment between time t and t + 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 79
                            }
                        ],
                        "text": "Numerical errors up to second order are eliminated by using a leapfrog method (Neal, 1996) that uses the potential energy gradient at time t to compute the velocity increment between time t \u2013 \u00bd and t + \u00bd and uses the velocity at time t + \u00bd to compute the position increment between time t and t + 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks, Lecture Notes in Statis12"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 80
                            }
                        ],
                        "text": "Numerical errors up to second order are eliminated by using a \u201cleapfrog\u201d method (Neal, 1996) which uses the potential energy gradient at time t to compute the velocity increment between time t\u2212 1 2 and t+ 1 2 and uses the velocity at time t+ 1 2 to compute the position increment between time t and t + 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 79
                            }
                        ],
                        "text": "Numerical errors up to second order are eliminated by using a leapfrog method (Neal, 1996) that uses the potential energy gradient at time t to compute the velocity increment between time t \u2013 \u00bd and t + \u00bd and uses the velocity at time t + \u00bd to compute the position increment between time t and t + 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks, Lecture Notes in Statistics No"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 2,
            "methodology": 12
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 16,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Unsupervised-Discovery-of-Nonlinear-Structure-Using-Hinton-Osindero/98a3c337a435553add253eb1af71eb9fc998bf5e?sort=total-citations"
}