{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47861681"
                        ],
                        "name": "Binyamin Rosenfeld",
                        "slug": "Binyamin-Rosenfeld",
                        "structuredName": {
                            "firstName": "Binyamin",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Binyamin Rosenfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145864794"
                        ],
                        "name": "Ronen Feldman",
                        "slug": "Ronen-Feldman",
                        "structuredName": {
                            "firstName": "Ronen",
                            "lastName": "Feldman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronen Feldman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090792"
                        ],
                        "name": "Y. Aumann",
                        "slug": "Y.-Aumann",
                        "structuredName": {
                            "firstName": "Yonatan",
                            "lastName": "Aumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Aumann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "A preliminary description of this work appeared in [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11008046,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d7fe0bd3854f90c1249062c079e34c30422936d",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Most information extraction systems focus on the textual content of the documents. They treat documents as sequences or of words, disregarding the physical and typographical layout of the information.. While this strategy helps in focusing the extraction process on the key semantic content of the document, much valuable information can also be derived form the document physical appearance. Often, fonts, physical positioning and other graphical characteristics are used to provide additional context to the information. This information is lost with pure-text analysis. In this paper we describe a general procedure for structural extraction, which allows for automatic extraction of entities from the document based on their visual characteristics and relative position in the document layout. Our structural extraction procedure is a learning algorithm, which knows how to automatically generalizes from examples. The procedure is a general one, applicable to any document format with visual and typographical information. We also then describe a specific implementation of the procedure to PDF documents, called PES (PDF Extraction System). PES works with PDF documents and is able to extract such fields such as Author(s), Title, Date, etc. with very high accuracy."
            },
            "slug": "Structural-extraction-from-visual-layout-of-Rosenfeld-Feldman",
            "title": {
                "fragments": [],
                "text": "Structural extraction from visual layout of documents"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A general procedure for structural extraction, which allows for automatic extraction of entities from the document based on their visual characteristics and relative position in the document layout, applicable to any document format with visual and typographical information."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '02"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700821"
                        ],
                        "name": "F. Esposito",
                        "slug": "F.-Esposito",
                        "structuredName": {
                            "firstName": "Floriana",
                            "lastName": "Esposito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Esposito"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738657"
                        ],
                        "name": "D. Malerba",
                        "slug": "D.-Malerba",
                        "structuredName": {
                            "firstName": "Donato",
                            "lastName": "Malerba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Malerba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145091756"
                        ],
                        "name": "Francesca A. Lisi",
                        "slug": "Francesca-A.-Lisi",
                        "structuredName": {
                            "firstName": "Francesca",
                            "lastName": "Lisi",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francesca A. Lisi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The system developed as part of the WISDOM project is of special interest in this regard (see [ 14 , 6, 9])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1542143,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f23b61f04d450ffc49ec6371bb5b30d198cdc5b",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "A paper document processing system is an information system component which transforms information on printed or handwritten documents into a computer-revisable form. In intelligent systems for paper document processing this information capture process is based on knowledge of the specific layout and logical structures of the documents. This article proposes the application of machine learning techniques to acquire the specific knowledge required by an intelligent document processing system, named WISDOM++, that manages printed documents, such as letters and journals. Knowledge is represented by means of decision trees and first-order rules automatically generated from a set of training documents. In particular, an incremental decision tree learning system is applied for the acquisition of decision trees used for the classification of segmented blocks, while a first-order learning system is applied for the induction of rules used for the layout-based classification and understanding of documents. Issues concerning the incremental induction of decision trees and the handling of both numeric and symbolic data in first-order rule learning are discussed, and the validity of the proposed solutions is empirically evaluated by processing a set of real printed documents."
            },
            "slug": "Machine-Learning-for-Intelligent-Processing-of-Esposito-Malerba",
            "title": {
                "fragments": [],
                "text": "Machine Learning for Intelligent Processing of Printed Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This article proposes the application of machine learning techniques to acquire the specific knowledge required by an intelligent document processing system, named WISDOM++, that manages printed documents, such as letters and journals."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Intelligent Information Systems"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3220211"
                        ],
                        "name": "O. Altamura",
                        "slug": "O.-Altamura",
                        "structuredName": {
                            "firstName": "Oronzo",
                            "lastName": "Altamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Altamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700821"
                        ],
                        "name": "F. Esposito",
                        "slug": "F.-Esposito",
                        "structuredName": {
                            "firstName": "Floriana",
                            "lastName": "Esposito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Esposito"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738657"
                        ],
                        "name": "D. Malerba",
                        "slug": "D.-Malerba",
                        "structuredName": {
                            "firstName": "Donato",
                            "lastName": "Malerba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Malerba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The system developed as part of the WISDOM project is of special interest in this regard (see [14,  6 , 9])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6797426,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb107e69981fd59f6ffa225569fe1a7c361b886e",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. The transformation of scanned paper documents to a form suitable for an Internet browser is a complex process that requires solutions to several problems. The application of an OCR to some parts of the document image is only one of the problems. In fact, the generation of documents in HTML format is easier when the layout structure of a page has been extracted by means of a document analysis process. The adoption of an XML format is even better, since it can facilitate the retrieval of documents in the Web. Nevertheless, an effective transformation of paper documents into this format requires further processing steps, namely document image classification and understanding. WISDOM++ is a document processing system that operates in five steps: document analysis, document classification, document understanding, text recognition with an OCR, and transformation into HTML/XML format. The innovative aspects described in the paper are: the preprocessing algorithm, the adaptive page segmentation, the acquisition of block classification rules using techniques from machine learning, the layout analysis based on general layout principles, and a method that uses document layout information for conversion to HTML/XML formats. A benchmarking of the system components implementing these innovative aspects is reported."
            },
            "slug": "Transforming-paper-documents-into-XML-format-with-Altamura-Esposito",
            "title": {
                "fragments": [],
                "text": "Transforming paper documents into XML format with WISDOM++"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The innovative aspects described in the paper are: the preprocessing algorithm, the adaptive page segmentation, the acquisition of block classification rules using techniques from machine learning, the layout analysis based on general layout principles, and a method that uses document layout information for conversion to HTML/XML formats."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Several of the algorithms allow the handling of both semi-structured and unstructured text [ 30 , 11, 16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8359747,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22fb3b3b2bdf768dd435eedfc5ef5155d3e56b1a",
            "isKey": false,
            "numCitedBy": 1071,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "A wealth of on-line text information can be made available to automatic processing by information extraction (IE) systems. Each IE application needs a separate set of rules tuned to the domain and writing style. WHISK helps to overcome this knowledge-engineering bottleneck by learning text extraction rules automatically.WHISK is designed to handle text styles ranging from highly structured to free text, including text that is neither rigidly formatted nor composed of grammatical sentences. Such semi-structured text has largely been beyond the scope of previous systems. When used in conjunction with a syntactic analyzer and semantic tagging, WHISK can also handle extraction from free text such as news stories."
            },
            "slug": "Learning-Information-Extraction-Rules-for-and-Free-Soderland",
            "title": {
                "fragments": [],
                "text": "Learning Information Extraction Rules for Semi-Structured and Free Text"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "WHISK is designed to handle text styles ranging from highly structured to free text, including text that is neither rigidly formatted nor composed of grammatical sentences, and can also handle extraction from free text such as news stories."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Several of the algorithms allow the handling of both semi-structured and unstructured text [30, 11,  16 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 46248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9272961455fa09b5f561e55638621f11a5883345",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Two trends are evident in the recent evolution of the field of information extraction: a preference for simple, often corpus-driven techniques over linguistically sophisticated ones; and a broadening of the central problem definition to include many non-traditional text domains. This development calls for information extraction systems which are as retargetable and general as possible. Here, we describe SRV, a learning architecture for information extraction which is designed for maximum generality and flexibility. SRV can exploit domain-specific information, including linguistic syntax and lexical information, in the form of features provided to the system explicitly as input for training. This process is illustrated using a domain created from Reuters corporate acquisitions articles. Features are derived from two general-purpose NLP systems, Sleator and Temperly's link grammar parser and Wordnet. Experiments compare the learner's performance with and without such linguistic information. Surprisingly, in many cases, the system performs as well without this information as with it."
            },
            "slug": "Toward-General-Purpose-Learning-for-Information-Freitag",
            "title": {
                "fragments": [],
                "text": "Toward General-Purpose Learning for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "SRV is described, a learning architecture for information extraction which is designed for maximum generality and flexibility and can exploit domain-specific information, including linguistic syntax and lexical information, in the form of features provided to the system explicitly as input for training."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692413"
                        ],
                        "name": "Margherita Berardi",
                        "slug": "Margherita-Berardi",
                        "structuredName": {
                            "firstName": "Margherita",
                            "lastName": "Berardi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Margherita Berardi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716542"
                        ],
                        "name": "M. Lapi",
                        "slug": "M.-Lapi",
                        "structuredName": {
                            "firstName": "Michele",
                            "lastName": "Lapi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lapi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738657"
                        ],
                        "name": "D. Malerba",
                        "slug": "D.-Malerba",
                        "structuredName": {
                            "firstName": "Donato",
                            "lastName": "Malerba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Malerba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The system developed as part of the WISDOM project is of special interest in this regard (see [14, 6,  9 ])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We note that Barardi et al [ 9 ] describe an integrated approach based on the WISDOM system."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10787397,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "07c45849bb15ee046a1d5d6d78d14870b881457c",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present an integrated approach for semantic structure extraction in document images. Document images are initially processed to extract both their layout and logical structures on the base of geometrical and spatial information. Then, textual content of logical components is employed for automatic semantic labeling of layout structures. To support the whole process different machine learning techniques are applied. Experimental results on a set of biomedical multi-page documents are discussed and future directions are drawn."
            },
            "slug": "An-Integrated-Approach-for-Automatic-Semantic-in-Berardi-Lapi",
            "title": {
                "fragments": [],
                "text": "An Integrated Approach for Automatic Semantic Structure Extraction in Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "An integrated approach for semantic structure extraction in document images is presented and experimental results on a set of biomedical multi-page documents are discussed and future directions are drawn."
            },
            "venue": {
                "fragments": [],
                "text": "Document Analysis Systems"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3022071"
                        ],
                        "name": "R. Futrelle",
                        "slug": "R.-Futrelle",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Futrelle",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Futrelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2466836"
                        ],
                        "name": "Mingyan Shao",
                        "slug": "Mingyan-Shao",
                        "structuredName": {
                            "firstName": "Mingyan",
                            "lastName": "Shao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingyan Shao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48780620"
                        ],
                        "name": "C. Cieslik",
                        "slug": "C.-Cieslik",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Cieslik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Cieslik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153833686"
                        ],
                        "name": "Andrea Grimes",
                        "slug": "Andrea-Grimes",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Grimes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrea Grimes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Futrelle et al. [ 18 ] describe a system for analysis of diagrams in PDF documents, using support vector machines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2606157,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3bc5618082c98792a1b6be5691e2ba651f746f33",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Diagrams are a critical part of virtually all scientificand technical documents. Analyzing diagrams will beimportant for building comprehensive document retrievalsystems. This paper focuses on the extraction andclassification of diagrams from PDF documents. Westudy diagrams available in vector (not raster) format inonline research papers.PDF files are parsed and their vector graphicscomponents installed in a spatial index. Subdiagrams arefound by analyzing white space gaps. A set of statistics isgenerated for each diagram, e.g., the number ofhorizontal lines and vertical lines. The statistics form afeature vector description of the diagram. The vectorsare used in a kernel-based machine learning system(Support Vector Machine). Separating a set of bargraphs from non-bar-graphs gathered from 20,000biology research papers gave a classification accuracy of91.7%. The approach is directly applicable to diagramsvectorized from images."
            },
            "slug": "Extraction,layout-analysis-and-classification-of-in-Futrelle-Shao",
            "title": {
                "fragments": [],
                "text": "Extraction,layout analysis and classification of diagrams in PDF documents"
            },
            "tldr": {
                "abstractSimilarityScore": 32,
                "text": "Separating a set of bargraphs from non-bar-graphs gathered from 20,000biology research papers gave a classification accuracy of91.7%."
            },
            "venue": {
                "fragments": [],
                "text": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1967815"
                        ],
                        "name": "M. E. Califf",
                        "slug": "M.-E.-Califf",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Califf",
                            "middleNames": [
                                "Elaine"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. E. Califf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Several of the algorithms allow the handling of both semi-structured and unstructured text [30,  11 , 16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 489775,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16bd1fbe3694173eda4ad4338a85f8288d19bf02",
            "isKey": false,
            "numCitedBy": 700,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Information extraction is a form of shallow text processing that locates a specified set of relevant items in a natural-language document. Systems for this task require significant domain-specific knowledge and are time-consuming and difficult to build by hand, making them a good application for machine learning. We present a system, RAPIER, that uses pairs of sample documents and filled templates to induce pattern-match rules that directly extract fillers for the slots in the template. RAPIER employs a bottom-up learning algorithm which incorporates techniques from several inductive logic programming systems and acquires unbounded patterns that include constraints on the words, part-of-speech tags, and semantic classes present in the filler and the surrounding text. We present encouraging experimental results on two domains."
            },
            "slug": "Relational-Learning-of-Pattern-Match-Rules-for-Califf-Mooney",
            "title": {
                "fragments": [],
                "text": "Relational Learning of Pattern-Match Rules for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "RAPIER employs a bottom-up learning algorithm which incorporates techniques from several inductive logic programming systems and acquires unbounded patterns that include constraints on the words, part-of-speech tags, and semantic classes present in the filler and the surrounding text."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116814833"
                        ],
                        "name": "W. Lovegrove",
                        "slug": "W.-Lovegrove",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Lovegrove",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Lovegrove"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3246207"
                        ],
                        "name": "D. Brailsford",
                        "slug": "D.-Brailsford",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Brailsford",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Brailsford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Lovegrove and Brailsford [ 23 ] describe a system for analysis of PDF documents based on the blackboard method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7436734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77dca5291b7bfdb717241ce6225fa500b3ec0c35",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY A strategy for document analysis is presented which uses Portable Document Format (PDF \u2014 the underlying file structure for Adobe Acrobat software) as its starting point. This strategy examines the appearance and geometric position of text and image blocks distributed over an entire document. A blackboard system is used to tag the blocks as a first stage in deducing the fundamental relationships existing between them. PDF is shown to be a useful intermediate stage in the bottom-up analysis of document structure. Its information on line spacing and font usage gives important clues in bridging the \u2018semantic gap\u2019 between the scanned bitmap page and its fully analysed, block-structured form. Analysis of PDF can yield not only accurate page decomposition but also sufficient document information for the later stages of structural analysis and document understanding."
            },
            "slug": "Document-analysis-of-PDF-files:-methods,-results-Lovegrove-Brailsford",
            "title": {
                "fragments": [],
                "text": "Document analysis of PDF files: methods, results and implications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3276863"
                        ],
                        "name": "Ion Muslea",
                        "slug": "Ion-Muslea",
                        "structuredName": {
                            "firstName": "Ion",
                            "lastName": "Muslea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ion Muslea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145293454"
                        ],
                        "name": "Steven Minton",
                        "slug": "Steven-Minton",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Minton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Minton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745117"
                        ],
                        "name": "Craig A. Knoblock",
                        "slug": "Craig-A.-Knoblock",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Knoblock",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig A. Knoblock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Machine learning techniques have been employed extensively for the automatic generation of wrappers (also called wrapper induction), using a multitude of techniques (see [21, 20,  24 ])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2801554,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "855eaab4505fea76ab21402839670a483e0ae339",
            "isKey": false,
            "numCitedBy": 406,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "With the tremendous amount of information that becomes available on the Web on a daily basis, the ability to quickly develop information agents has become a crucial problem. A vital component of any Web-based information agent is a set of wrappers that can extract the relevant data from semistructured information sources. Our novel approach to wrapper induction is based on the idea of hierarchical information extraction, which turns the hard problem of extracting data from an arbitrarily complex document into a series of simpler extraction tasks. We introduce an inductive algorithm, STALKER, that generates high accuracy extraction rules based on user-labeled training examples. Labeling the training data represents the major bottleneck in using wrapper induction techniques, and our experimental results show that STALKER requires up to two orders of magnitude fewer examples than other algorithms. Furthermore, STALKER can wrap information sources that could not be wrapped by existing inductive techniques."
            },
            "slug": "Hierarchical-Wrapper-Induction-for-Semistructured-Muslea-Minton",
            "title": {
                "fragments": [],
                "text": "Hierarchical Wrapper Induction for Semistructured Information Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work introduces an inductive algorithm, STALKER, that generates high accuracy extraction rules based on user-labeled training examples that can wrap information sources that could not be wrapped by existing inductive techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Autonomous Agents and Multi-Agent Systems"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8551365"
                        ],
                        "name": "N. Kushmerick",
                        "slug": "N.-Kushmerick",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Kushmerick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kushmerick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Machine learning techniques have been employed extensively for the automatic generation of wrappers (also called wrapper induction), using a multitude of techniques (see [ 21 , 20, 24])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11075952,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f052f40a3307de1e45e11a3007a7552b36ebfc8",
            "isKey": false,
            "numCitedBy": 641,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Wrapper-induction:-Efficiency-and-expressiveness-Kushmerick",
            "title": {
                "fragments": [],
                "text": "Wrapper induction: Efficiency and expressiveness"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34607455"
                        ],
                        "name": "Chun-Nan Hsu",
                        "slug": "Chun-Nan-Hsu",
                        "structuredName": {
                            "firstName": "Chun-Nan",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chun-Nan Hsu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094164369"
                        ],
                        "name": "Ming-Tzung Dung",
                        "slug": "Ming-Tzung-Dung",
                        "structuredName": {
                            "firstName": "Ming-Tzung",
                            "lastName": "Dung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Tzung Dung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Machine learning techniques have been employed extensively for the automatic generation of wrappers (also called wrapper induction), using a multitude of techniques (see [21,  20 , 24])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17895561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9478265afd280486299a5b8f1dbaaf6769422de",
            "isKey": false,
            "numCitedBy": 507,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generating-Finite-State-Transducers-for-Data-from-Hsu-Dung",
            "title": {
                "fragments": [],
                "text": "Generating Finite-State Transducers for Semi-Structured Data Extraction from the Web"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Syst."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145030811"
                        ],
                        "name": "C. Papageorgiou",
                        "slug": "C.-Papageorgiou",
                        "structuredName": {
                            "firstName": "Constantine",
                            "lastName": "Papageorgiou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papageorgiou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Learning techniques have also been used extensively in the general theory of computer vision, and for visual object recognition in particular (see [27,  25 ])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13308232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6c20ed0c3f375f403ab5d750a6e9699d5c3af6a",
            "isKey": false,
            "numCitedBy": 1404,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a general, trainable system for object detection in unconstrained, cluttered scenes. The system derives much of its power from a representation that describes an object class in terms of an overcomplete dictionary of local, oriented, multiscale intensity differences between adjacent regions, efficiently computable as a Haar wavelet transform. This example-based learning approach implicitly derives a model of an object class by training a support vector machine classifier using a large set of positive and negative examples. We present results on face, people, and car detection tasks using the same architecture. In addition, we quantify how the representation affects detection performance by considering several alternate representations including pixels and principal components. We also describe a real-time application of our person detection system as part of a driver assistance system."
            },
            "slug": "A-Trainable-System-for-Object-Detection-Papageorgiou-Poggio",
            "title": {
                "fragments": [],
                "text": "A Trainable System for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A general, trainable system for object detection in unconstrained, cluttered scenes that derives much of its power from a representation that describes an object class in terms of an overcomplete dictionary of local, oriented, multiscale intensity differences between adjacent regions, efficiently computable as a Haar wavelet transform."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2077174403"
                        ],
                        "name": "M. I. Mauldin",
                        "slug": "M.-I.-Mauldin",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Mauldin",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. I. Mauldin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60521362,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d23684e422688703aa347bb7d21abea2563895f",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the enabling technologies of the World Wide Web, along with browsers, domain name servers, and hypertext markup language, is the search engine. Although the Web contains over 100 million pages of information, those millions of pages are useless if you cannot find the pages you need. All major Web search engines operate the same way: a gathering program explores the hyperlinked documents of the Web, foraging for Web pages to index. These pages are stockpiled by storing them in some kind of database or repository. Finally, a retrieval program takes a user query and creates a list of links to Web documents matching the words, phrases, or concepts in the query. Although the retrieval program itself is correctly called a search engine, by popular usage the term now means a database combined with a retrieval program. For example, the Lycos search engine comprises the Lycos Catalog of the Internet and the Pursuit retrieval program. This paper describes the Lycos system for collecting, storing, and retrieving information about pages on the Web. After outlining the history and precursors of the Lycos system, the paper discusses some of the design choices made in building this Web indexer and touches briefly on the economic issues involved in working with very large retrieval systems."
            },
            "slug": "Lycos:-design-choices-in-an-Internet-search-service-Mauldin",
            "title": {
                "fragments": [],
                "text": "Lycos: design choices in an Internet search service"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The history and precursors of the Lycos system for collecting, storing, and retrieving information about pages on the Web are outlined and some of the design choices made in building this Web indexer are discussed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720135"
                        ],
                        "name": "A. Anjewierden",
                        "slug": "A.-Anjewierden",
                        "structuredName": {
                            "firstName": "Anjo",
                            "lastName": "Anjewierden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Anjewierden"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The AIDAS system [ 7 ] is designed to automatically index technical manuals provided in PDF form."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16469563,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d92ab24a84412ff83e87c994a1174856aebe1895",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "AIDAS is part of a research project in which the aim is to turn technical manuals into a database of indexed training material. We describe the approach AIDAS uses to extract the logical document structure from PDF documents. The approach is based on the idea that the layout structure contains cues about the logical structure and that the logical structure can be discovered incrementally."
            },
            "slug": "AIDAS:-incremental-logical-structure-discovery-in-Anjewierden",
            "title": {
                "fragments": [],
                "text": "AIDAS: incremental logical structure discovery in PDF documents"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "AIDAS is part of a research project in which the aim is to turn technical manuals into a database of indexed training material and the approach AIDAS uses to extract the logical document structure from PDF documents is described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2331213"
                        ],
                        "name": "S. Edelman",
                        "slug": "S.-Edelman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Edelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Edelman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4361875,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50899b2355d6908a304bacb5e406f800f3dde558",
            "isKey": false,
            "numCitedBy": 1019,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "THE visual recognition of three-dimensional (3-D) objects on the basis of their shape poses at least two difficult problems. First, there is the problem of variable illumination, which can be addressed by working with relatively stable features such as intensity edges rather than the raw intensity images1,2. Second, there is the problem of the initially unknown pose of the object relative to the viewer. In one approach to this problem, a hypothesis is first made about the viewpoint, then the appearance of a model object from such a viewpoint is computed and compared with the actual image3\u20137. Such recognition schemes generally employ 3-D models of objects, but the automatic learning of 3-D models is itself a difficult problem8,9. To address this problem in computational vision, we have developed a scheme, based on the theory of approximation of multivariate functions, that learns from a small set of perspective views a function mapping any viewpoint to a standard view. A network equivalent to this scheme will thus 'recognize' the object on which it was trained from any viewpoint."
            },
            "slug": "A-network-that-learns-to-recognize-objects-Poggio-Edelman",
            "title": {
                "fragments": [],
                "text": "A network that learns to recognize three-dimensional objects"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A scheme is developed, based on the theory of approximation of multivariate functions, that learns from a small set of perspective views a function mapping any viewpoint to a standard view, and a network equivalent to this scheme will 'recognize' the object on which it was trained from any viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144363438"
                        ],
                        "name": "Laura Bright",
                        "slug": "Laura-Bright",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Bright",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laura Bright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66544222"
                        ],
                        "name": "MarylandCollege",
                        "slug": "MarylandCollege",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "MarylandCollege",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "MarylandCollege"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060738576"
                        ],
                        "name": "Park",
                        "slug": "Park",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Park"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 116
                            }
                        ],
                        "text": "Graphical interfaces for semi-automatic wrapper generation allow for interactive visual definition of wrappers (see [8, 10])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18939156,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ba3a2c4aaa513950b4c666825a15fe50a17dae9",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "There is an increase in the number of data sources that can be queried across the WWW. Such sources typically support HTML forms-based interfaces and search engines query collections of suitably indexed data. The data is displayed via a browser. One drawback to these sources is that there is no standard programming interface suitable for applications to submit queries. Second, the output (answer to a query) is not well structured. Structured objects have to be extracted from the HTML documents which contain irrelevant data and which may be volatile. Third, domain knowledge about the data source is also embedded in HTML documents and must be extracted. To solve these problems, we present technology to deene and generate wrappers for Web accessible sources (WebSources). Our contributions are as follows: (1) Deening a wrapper interface to specify the capability of WebSources. (2) Developing a wrapper generation toolkit of graphical interfaces and speciication languages to specify the capability of sources and the functionality of the wrapper. The toolkit provides a graphical interface to specify the capabilities of the sources and to deene a simple query translation and answer extraction process. It supports a language to specify a URLConstructor expression, for some query. It supports a declarative Qualiied-path-expression Extractor Language, QEL, to describe a simple Extractor that can extract data from a single HTML document. The toolkit also supports a Complex Extractor Speciication Language, CESL to specify extractors with more complex capability. The third contribution is (3) Developing the technology to generate a wrapper appropriate to the WebSource, from the speciications."
            },
            "slug": "A-Wrapper-Generation-toolkit-to-specify-and-Web-(-)-Bright-MarylandCollege",
            "title": {
                "fragments": [],
                "text": "A Wrapper Generation toolkit to specify and construct Wrappersfor Web Accessible Data Sources ( WebSources )"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents technology to deene and generate wrappers for Web accessible sources (WebSources) and a wrapper generation toolkit of graphical interfaces and speciication languages to specify the capability of sources and the functionality of the wrapper."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786049"
                        ],
                        "name": "Y. Papakonstantinou",
                        "slug": "Y.-Papakonstantinou",
                        "structuredName": {
                            "firstName": "Yannis",
                            "lastName": "Papakonstantinou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Papakonstantinou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50179113"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398574232"
                        ],
                        "name": "H. Garcia-Molina",
                        "slug": "H.-Garcia-Molina",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Garcia-Molina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Garcia-Molina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742391"
                        ],
                        "name": "J. Ullman",
                        "slug": "J.-Ullman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Ullman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ullman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To aid with manual creation of wrappers special tool-kits have been developed [19,  26 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5323612,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf3255ab186c34b3f3c92946add1b472e1a41028",
            "isKey": false,
            "numCitedBy": 234,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Wrappers provide access to heterogeneous information sources by converting application queries into source specific queries or commands. In this paper we present a wrapper implementation toolkit that facilitates rapid development of wrappers. We focus on the query translation component of the toolkit, called the converter. The converter takes as input a Query Description and Translation Language (QDTL) description of the queries that can be processed by the underlying source. Based on this the converter decides if an application query is (a) directly supported, i.e., it can be translated to a query of the underlying system following instructions in the QDTL description; (b) logically supported, i.e., logically equivalent to a directly supported query; (c) indirectly supported, i.e., it can be computed by applying a filter, automatically generated by the converter, to the result of a directly supported query."
            },
            "slug": "A-Query-Translation-Scheme-for-Rapid-Implementation-Papakonstantinou-Gupta",
            "title": {
                "fragments": [],
                "text": "A Query Translation Scheme for Rapid Implementation of Wrappers"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A query translation component of the toolkit, called the converter, is presented that facilitates rapid development of wrappers and takes as input a Query Description and Translation Language (QDTL) description of the queries that can be processed by the underlying source."
            },
            "venue": {
                "fragments": [],
                "text": "DOOD"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143894263"
                        ],
                        "name": "Hui Chao",
                        "slug": "Hui-Chao",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Chao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hui Chao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7232863"
                        ],
                        "name": "G. Beretta",
                        "slug": "G.-Beretta",
                        "structuredName": {
                            "firstName": "Giordano",
                            "lastName": "Beretta",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Beretta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065949787"
                        ],
                        "name": "H. Sang",
                        "slug": "H.-Sang",
                        "structuredName": {
                            "firstName": "Hyung",
                            "lastName": "Sang",
                            "middleNames": [
                                "Jong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "Chao, Beretta and Sang [12] describe a system for the analysis of PDF documents aimed at reuse of its logical elements."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59997520,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c953ba9f5fb3794da40102130d9486180844ee5",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The Portable Document Format (PDF) has been mostly used for posting the final form of documents. The aim of our project is to analyze the layout, to modify the layout or to re-use elements of PDF documents for different media. Using PDFEdit in Adobe SDK, we built tool to study the layout of documents and tool to select page elements to compose a new page. We demonstrate problems we encountered and propose possible solutions."
            },
            "slug": "PDF-Document-Layout-Study-with-Page-Elements-and-Chao-Beretta",
            "title": {
                "fragments": [],
                "text": "PDF Document Layout Study with Page Elements and Bounding Boxes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The aim of this project is to analyze the layout, to modify the layout or to re-use elements of PDF documents for different media using PDFEdit in Adobe SDK."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144477659"
                        ],
                        "name": "J. Hammer",
                        "slug": "J.-Hammer",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Hammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398574232"
                        ],
                        "name": "H. Garcia-Molina",
                        "slug": "H.-Garcia-Molina",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Garcia-Molina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Garcia-Molina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771093"
                        ],
                        "name": "Svetlozar Nestorov",
                        "slug": "Svetlozar-Nestorov",
                        "structuredName": {
                            "firstName": "Svetlozar",
                            "lastName": "Nestorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Svetlozar Nestorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806536"
                        ],
                        "name": "Ramana Yerneni",
                        "slug": "Ramana-Yerneni",
                        "structuredName": {
                            "firstName": "Ramana",
                            "lastName": "Yerneni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ramana Yerneni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34062373"
                        ],
                        "name": "M. Breunig",
                        "slug": "M.-Breunig",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Breunig",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Breunig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715463"
                        ],
                        "name": "V. Vassalos",
                        "slug": "V.-Vassalos",
                        "structuredName": {
                            "firstName": "Vasilis",
                            "lastName": "Vassalos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vassalos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 78
                            }
                        ],
                        "text": "To aid with manual creation of wrappers special tool kits have been developed [19, 26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4469846,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fcb851963ad5bb3530ecd14671c6cc737aeab255",
            "isKey": false,
            "numCitedBy": 273,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In order to access information from a variety of heterogeneous information sources, one has to be able to translate queries and data from one data model into another. This functionality is provided by so-called (source) wrappers [4,8] which convert queries into one or more commands/queries understandable by the underlying source and transform the native results into a format understood by the application. As part of the TSIMMIS project [1, 6] we have developed hard-coded wrappers for a variety of sources (e.g., Sybase DBMS, WWW pages, etc.) including legacy systems (Folio). However, anyone who has built a wrapper before can attest that a lot of effort goes into developing and writing such a wrapper. In situations where it is important or desirable to gain access to new sources quickly, this is a major drawback. Furthermore, we have also observed that only a relatively small part of the code deals with the specific access details of the source. The rest of the code is either common among wrappers or implements query and data transformation that could be expressed in a high level, declarative fashion.\nBased on these observations, we have developed a wrapper implementation toolkit [7] for quickly building wrappers. The toolkit contains a library for commonly used functions, such as for receiving queries from the application and packaging results. It also contains a facility for translating queries into source-specific commands, and for translating results into a model useful to the application. The philosophy behind our \u201ctemplate-based\u201d translation methodology is as follows. The wrapper implementor specifies a set of templates (rules) written in a high level declarative language that describe the queries accepted by the wrapper as well as the objects that it returns. If an application query matches a template, an implementor-provided action associated with the template is executed to provide the native query for the underlying source1. When the source returns the result of the query, the wrapper transforms the answer which is represented in the data model of the source into a representation that is used by the application. Using this toolkit one can quickly design a simple wrapper with a few templates that cover some of the desired functionality, probably the one that is most urgently needed. However, templates can be added gradually as more functionality is required later on.\nAnother important use of wrappers is in extending the query capabilities of a source. For instance, some sources may not be capable of answering queries that have multiple predicates. In such cases, it is necessary to pose a native query to such a source using only predicates that the source is capable of handling. The rest of the predicates are automatically separated from the user query and form a filter query. When the wrapper receives the results, a post-processing engine applies the filter query. This engine supports a set of built-in predicates based on the comparison operators =,\u2260,<,>, etc. In addition, the engine supports more complex predicates that can be specified as part of the filter query. The postprocessing engine is common to wrappers of all sources and is part of the wrapper toolkit. Note that because of postprocessing, the wrapper can handle a much larger class of queries than those that exactly match the templates it has been given.\n Figure 1 shows an overview of the wrapper architecture as it is currently implemented in our TSIMMIS testbed. Shaded components are provided by the toolkit, the white component is source-specific and must be generated by the implementor. The driver component controls the translation process and invokes the following services: the parser which parses the templates, the native schema, as well as the incoming queries into internal data structures, the matcher which matches a query against the set of templates and creates a filter query for postprocessing if necessary, the native component which submits the generated action string to the source, and extracts the data from the native result using the information given in the source schema, and the engine, which transforms and packages the result and applies a postprocessing filter if one has been created by the matcher. We now describe the sequence of events that occur at the wrapper during the translation of a query and its result using an example from our prototype system. The queries are formulated using a rule-based language called MSL that has been developed as a template specification and query language for the TSIMMIS project. Data is represented using our Object Exchange Model (OEM). We will briefly describe MSL and OEM in the next section. Details on MSL can be found in [5], a full introduction to OEM is given in [1]."
            },
            "slug": "Template-based-wrappers-in-the-TSIMMIS-system-Hammer-Garcia-Molina",
            "title": {
                "fragments": [],
                "text": "Template-based wrappers in the TSIMMIS system"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A wrapper implementation toolkit for quickly building wrappers that can quickly design a simple wrapper with a few templates that cover some of the desired functionality, probably the one that is most urgently needed."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 46
                            }
                        ],
                        "text": "Wrappers are commonly used by software agents [15, 17, 29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2447472,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "050584e3b74d1d7f5396d379510ca5204f02bfc1",
            "isKey": false,
            "numCitedBy": 602,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The Internet Softbot (software robot) is a fullyimplemented AI agent developed at the University of Washington (Etzioni, Lcsh, & Segal 1993). The softbot uses a UNIX shell and the World-Wide Web to interact with a wide range of internet resources. The softbot\u2019s effectors include ftp, telnet, mail, and numerous file manipulation commaslds. Its sensors include internet facilities such as archie, gopher, netfind, and many more. The softbot is designed to incorporate new facilities into its repertoirc as they become available. The softbot\u2019s \"added value\" is three-fold. First, it provides an integrated and expressive interface to the internet. Second, the softbot dynamically chooses which facilities to invoke, and in what sequence. For example, the softbot might use netfind to determine David McAllester\u2019s e-mail address. Since it knows that netfind requires a person\u2019s institution as input, the softbot would first search bibliographic databases for a technical report by McAllester which would reveal his institutkm, and then feed that information to netfind. Third, the softbot fluidly backtracks from one facility to another based on information collected at run time. As a result., the softbot\u2019s behavior changes in response to transient system conditions (e.g., the UUCP gateway is down). In this article, we focus on the ideas underlying the softbot-based interface."
            },
            "slug": "A-softbot-based-interface-to-the-Internet-Etzioni-Weld",
            "title": {
                "fragments": [],
                "text": "A softbot-based interface to the Internet"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "The Internet Softbot (software robot) is a fullyimplemented AI agent developed at the University of Washington that uses a UNIX shell and the World-Wide Web to interact with a wide range of internet resources."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3236427"
                        ],
                        "name": "E. Selberg",
                        "slug": "E.-Selberg",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Selberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Selberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 46
                            }
                        ],
                        "text": "Wrappers are commonly used by software agents [15, 17, 29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14643096,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77e2e742d53ef382df9f896abadf291646eed3cc",
            "isKey": false,
            "numCitedBy": 314,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper discusses the MetaCrawler Softbot parallel Web search service that has been available at the University of Washington since June 1995. It provides users with a single interface for querying popular general-purpose Web search services, such as Lycos and AltaVista, and has some sophisticated features that allow it to obtain results of much higher quality than simply regurgitating the output from each search service."
            },
            "slug": "The-MetaCrawler-architecture-for-resource-on-the-Selberg-Etzioni",
            "title": {
                "fragments": [],
                "text": "The MetaCrawler architecture for resource aggregation on the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The paper discusses the MetaCrawler Softbot parallel Web search service that has been available at the University of Washington since June 1995 and has some sophisticated features that allow it to obtain results of much higher quality than simply regurgitating the output from each search service."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "3.1 Basic algorithm"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Network that learns to recognize 3D objects"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143634544"
                        ],
                        "name": "Marc Friedman",
                        "slug": "Marc-Friedman",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Friedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Wrappers are commonly used by software agents [15,  17 , 29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31921995,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9fc31270bdf7dc57424875f3377bc163cfcebf9",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficiently-Executing-Information-Gathering-Plans-Friedman-Weld",
            "title": {
                "fragments": [],
                "text": "Efficiently Executing Information-Gathering Plans"
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2178614"
                        ],
                        "name": "N. Ashish",
                        "slug": "N.-Ashish",
                        "structuredName": {
                            "firstName": "Naveen",
                            "lastName": "Ashish",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ashish"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745117"
                        ],
                        "name": "Craig A. Knoblock",
                        "slug": "Craig-A.-Knoblock",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Knoblock",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig A. Knoblock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 116
                            }
                        ],
                        "text": "Graphical interfaces for semi-automatic wrapper generation allow for interactive visual definition of wrappers (see [8, 10])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10413529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a60a737b40ceb1a50bc8616ae6155bf50e07c07",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "With the current explosion of information on the World Wide Web (WWW) a wealth of information on many different subjects has become available on-line. Numerous sources contain information that can be classified as semi-structured. At present, however, the only way to access the information is by browsing individual pages. We cannot query web documents in a database-like fashion based on their underlying structure. However, we can provide database-like querying for semi-structured WWW sources by building wrappers around these sources. We present an approach for semi-automatically generating such wrappers. The key idea is to exploit the formatting information in pages from the source to hypothesize the underlying structure of a page. From this structure the system generates a wrapper that facilitates querying of a source and possibly integrating it with other sources. We demonstrate the ease with which we are able to build wrappers for a number of internet sources in different domains using our implemented wrapper generation toolkit."
            },
            "slug": "Wrapper-generation-for-semi-structured-Internet-Ashish-Knoblock",
            "title": {
                "fragments": [],
                "text": "Wrapper generation for semi-structured Internet sources"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The key idea is to exploit the formatting information in pages from the source to hypothesize the underlying structure of a page and generate a wrapper that facilitates querying of a source and possibly integrating it with other sources."
            },
            "venue": {
                "fragments": [],
                "text": "SGMD"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108699"
                        ],
                        "name": "L. Eikvil",
                        "slug": "L.-Eikvil",
                        "structuredName": {
                            "firstName": "Line",
                            "lastName": "Eikvil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Eikvil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "Developing tools to facilitate wrapper generation has been the subject of much research (see [13])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16136177,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12865fdc27f87c3c415483f059d39f53ef2ebb49",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Information-Extraction-from-World-Wide-Web-A-Survey-Eikvil",
            "title": {
                "fragments": [],
                "text": "Information Extraction from World Wide Web - A Survey"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 129
                            }
                        ],
                        "text": "Y. Aumann \u00b7 R. Feldman (B) \u00b7 J. Schler Department of Computer Science, Bar Ilan University, Ramat Gan 52900, Israel E-mail: {aumann, feldman}@cs.biu.ac.il Y. Aumann \u00b7 R. Feldman \u00b7 Y. Liberzon \u00b7 B. Rosenfeld ClearForest Ltd., 6 Yoni Netanyahu Street, OR Yehuda 60376, Israel\ne.g. font type, size and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "Keywords Information extraction \u00b7 PDF analysis \u00b7 Text analysis \u00b7 Wrapper induction"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "MUC-3)"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Third Message Understanding Conference"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proceedings of the Forth Message Understanding Conference"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Forth Message Understanding Conference"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 46
                            }
                        ],
                        "text": "Wrappers are commonly used by software agents [15, 17, 29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficiently executing information-gathering plans. In: 15th international joint conference on artificial intelligence, Nagoya, Japan, pp 785\u2013791  Visual information extraction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "MUC-7) Available at"
            },
            "venue": {
                "fragments": [],
                "text": "[1] Proceedings of the Seventh Message Understanding Conference"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "[5] Proceedings of the Sixth Message Understanding Conference (MUC-6)"
            },
            "venue": {
                "fragments": [],
                "text": "[5] Proceedings of the Sixth Message Understanding Conference (MUC-6)"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 137
                            }
                        ],
                        "text": "Y. Aumann \u00b7 R. Feldman (B) \u00b7 J. Schler Department of Computer Science, Bar Ilan University, Ramat Gan 52900, Israel E-mail: {aumann, feldman}@cs.biu.ac.il Y. Aumann \u00b7 R. Feldman \u00b7 Y. Liberzon \u00b7 B. Rosenfeld ClearForest Ltd., 6 Yoni Netanyahu Street, OR Yehuda 60376, Israel\ne.g. font type, size and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 48
                            }
                        ],
                        "text": "Keywords Information extraction \u00b7 PDF analysis \u00b7 Text analysis \u00b7 Wrapper induction"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proceedings of the Fifth Message Understanding Conference (MUC-5)"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifth Message Understanding Conference (MUC-5)"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "Extraction from structured and semi-structured text is frequently performed using wrappers [22], most commonly for extraction from world-wide-web sources."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Wrappers: integration utilities and services for the DICE architecture"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the Second National Symposium on Concurrent Engineering,"
            },
            "year": 1991
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 8,
            "methodology": 21
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 33,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Visual-information-extraction-Aumann-Feldman/37b5296d40fd7ad553a87932e2cc088b3970cce2?sort=total-citations"
}