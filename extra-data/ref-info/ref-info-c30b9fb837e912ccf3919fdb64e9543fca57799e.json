{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144421225"
                        ],
                        "name": "Michael Stark",
                        "slug": "Michael-Stark",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Stark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Stark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3081065"
                        ],
                        "name": "Gy\u00f6rgy Szarvas",
                        "slug": "Gy\u00f6rgy-Szarvas",
                        "structuredName": {
                            "firstName": "Gy\u00f6rgy",
                            "lastName": "Szarvas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gy\u00f6rgy Szarvas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730400"
                        ],
                        "name": "Iryna Gurevych",
                        "slug": "Iryna-Gurevych",
                        "structuredName": {
                            "firstName": "Iryna",
                            "lastName": "Gurevych",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iryna Gurevych"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8181220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4a4b5ae5793696b861aa009932e7a121d36ad67a",
            "isKey": false,
            "numCitedBy": 308,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Remarkable performance has been reported to recognize single object classes. Scalability to large numbers of classes however remains an important challenge for today's recognition methods. Several authors have promoted knowledge transfer between classes as a key ingredient to address this challenge. However, in previous work the decision which knowledge to transfer has required either manual supervision or at least a few training examples limiting the scalability of these approaches. In this work we explicitly address the question of how to automatically decide which information to transfer between classes without the need of any human intervention. For this we tap into linguistic knowledge bases to provide the semantic link between sources (what) and targets (where) of knowledge transfer. We provide a rigorous experimental evaluation of different knowledge bases and state-of-the-art techniques from Natural Language Processing which goes far beyond the limited use of language in related work. We also give insights into the applicability (why) of different knowledge sources and similarity measures for knowledge transfer."
            },
            "slug": "What-helps-where-\u2013-and-why-Semantic-relatedness-for-Rohrbach-Stark",
            "title": {
                "fragments": [],
                "text": "What helps where \u2013 and why? Semantic relatedness for knowledge transfer"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work addresses the question of how to automatically decide which information to transfer between classes without the need of any human intervention and taps into linguistic knowledge bases to provide the semantic link between sources (what) and targets (where) of knowledge transfer."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723883"
                        ],
                        "name": "F. Perronnin",
                        "slug": "F.-Perronnin",
                        "structuredName": {
                            "firstName": "Florent",
                            "lastName": "Perronnin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Perronnin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143995438"
                        ],
                        "name": "Jorge S\u00e1nchez",
                        "slug": "Jorge-S\u00e1nchez",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "S\u00e1nchez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jorge S\u00e1nchez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722052"
                        ],
                        "name": "Thomas Mensink",
                        "slug": "Thomas-Mensink",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Mensink",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Mensink"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10402702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39f3b1804b8df5be645a1dcb4a876e128385d9be",
            "isKey": false,
            "numCitedBy": 2663,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The Fisher kernel (FK) is a generic framework which combines the benefits of generative and discriminative approaches. In the context of image classification the FK was shown to extend the popular bag-of-visual-words (BOV) by going beyond count statistics. However, in practice, this enriched representation has not yet shown its superiority over the BOV. In the first part we show that with several well-motivated modifications over the original framework we can boost the accuracy of the FK. On PASCAL VOC 2007 we increase the Average Precision (AP) from 47.9% to 58.3%. Similarly, we demonstrate state-of-the-art accuracy on CalTech 256. A major advantage is that these results are obtained using only SIFT descriptors and costless linear classifiers. Equipped with this representation, we can now explore image classification on a larger scale. In the second part, as an application, we compare two abundant resources of labeled images to learn classifiers: ImageNet and Flickr groups. In an evaluation involving hundreds of thousands of training images we show that classifiers learned on Flickr groups perform surprisingly well (although they were not intended for this purpose) and that they can complement classifiers learned on more carefully annotated datasets."
            },
            "slug": "Improving-the-Fisher-Kernel-for-Large-Scale-Image-Perronnin-S\u00e1nchez",
            "title": {
                "fragments": [],
                "text": "Improving the Fisher Kernel for Large-Scale Image Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "In an evaluation involving hundreds of thousands of training images, it is shown that classifiers learned on Flickr groups perform surprisingly well and that they can complement classifier learned on more carefully annotated datasets."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787591"
                        ],
                        "name": "Christoph H. Lampert",
                        "slug": "Christoph-H.-Lampert",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Lampert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph H. Lampert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748758"
                        ],
                        "name": "H. Nickisch",
                        "slug": "H.-Nickisch",
                        "structuredName": {
                            "firstName": "Hannes",
                            "lastName": "Nickisch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Nickisch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734990"
                        ],
                        "name": "S. Harmeling",
                        "slug": "S.-Harmeling",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Harmeling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Harmeling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "Table 1: Evaluation of the probabilistic product model suggested by [14] vs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 107
                            }
                        ],
                        "text": "A second line of research uses an intermediate layer of descriptive attributes to represent object classes [8, 14, 22], encoding high-level visual properties that can be shared among object classes, hence promoting scalability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "Our attribute-based object class model is inspired by [14], and uses linguistic knowledge bases to determine both an attribute inventory and the associations between object classes and attributes fully automatically [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[14] provided a first benchmark for zero-shot recognition in the form of the Animals-with-Attributes (AwA) data set, consisting of approximately 30,000 images, divided into 40 known animal classes for training and 10 unseen animal classes for testing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 115
                            }
                        ],
                        "text": "The second direction is based on representing object classes relative to an inventory of generic visual attributes [8, 14, 22], where classes are characterized by distinct patterns of attribute activations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10301835,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0566bf06a0368b518b8b474166f7b1dfef3f9283",
            "isKey": true,
            "numCitedBy": 1951,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of object classification when training and test classes are disjoint, i.e. no training examples of the target classes are available. This setup has hardly been studied in computer vision research, but it is the rule rather than the exception, because the world contains tens of thousands of different object classes and for only a very few of them image, collections have been formed and annotated with suitable class labels. In this paper, we tackle the problem by introducing attribute-based classification. It performs object detection based on a human-specified high-level description of the target objects instead of training images. The description consists of arbitrary semantic attributes, like shape, color or even geographic information. Because such properties transcend the specific learning task at hand, they can be pre-learned, e.g. from image datasets unrelated to the current task. Afterwards, new classes can be detected based on their attribute representation, without the need for a new training phase. In order to evaluate our method and to facilitate research in this area, we have assembled a new large-scale dataset, \u201cAnimals with Attributes\u201d, of over 30,000 animal images that match the 50 classes in Osherson's classic table of how strongly humans associate 85 semantic attributes with animal classes. Our experiments show that by using an attribute layer it is indeed possible to build a learning object detection system that does not require any training images of the target classes."
            },
            "slug": "Learning-to-detect-unseen-object-classes-by-Lampert-Nickisch",
            "title": {
                "fragments": [],
                "text": "Learning to detect unseen object classes by between-class attribute transfer"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The experiments show that by using an attribute layer it is indeed possible to build a learning object detection system that does not require any training images of the target classes, and assembled a new large-scale dataset, \u201cAnimals with Attributes\u201d, of over 30,000 animal images that match the 50 classes in Osherson's classic table of how strongly humans associate 85 semantic attributes with animal classes."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47655614"
                        ],
                        "name": "G. Griffin",
                        "slug": "G.-Griffin",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Griffin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Griffin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144160673"
                        ],
                        "name": "Alex Holub",
                        "slug": "Alex-Holub",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Holub",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Holub"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "Caltech256 [11] is frequently used, however, it consists only of 256 classes and 30k images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118828957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a5effa909cdeafaddbbb7855037e02f8e25d632",
            "isKey": false,
            "numCitedBy": 2545,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions."
            },
            "slug": "Caltech-256-Object-Category-Dataset-Griffin-Holub",
            "title": {
                "fragments": [],
                "text": "Caltech-256 Object Category Dataset"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A challenging set of 256 object categories containing a total of 30607 images is introduced and the clutter category is used to train an interest detector which rejects uninformative background regions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 132
                            }
                        ],
                        "text": "Related work Knowledge transfer for object class recognition comes in different flavors, such as joint learning of multiple classes [28] or transferring object class priors [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2741819,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd626564bd47e9fc67a5b276301282ba2fe3d833",
            "isKey": false,
            "numCitedBy": 793,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (runtime) computational complexity and the (training-time) sample complexity scale linearly with the number of classes to be detected. We present a multitask learning procedure, based on boosted decision stumps, that reduces the computational and sample complexity by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required and, therefore, the runtime cost of the classifier, is observed to scale approximately logarithmically with the number of classes. The features selected by joint training are generic edge-like features, whereas the features chosen by training each class separately tend to be more object-specific. The generic features generalize better and considerably reduce the computational cost of multiclass object detection"
            },
            "slug": "Sharing-Visual-Features-for-Multiclass-and-Object-Torralba-Murphy",
            "title": {
                "fragments": [],
                "text": "Sharing Visual Features for Multiclass and Multiview Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A multitask learning procedure, based on boosted decision stumps, that reduces the computational and sample complexity by finding common features that can be shared across the classes (and/or views) and considerably reduce the computational cost of multiclass object detection."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "119916460"
                        ],
                        "name": "Yuanqing Lin",
                        "slug": "Yuanqing-Lin",
                        "structuredName": {
                            "firstName": "Yuanqing",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuanqing Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39157653"
                        ],
                        "name": "Fengjun Lv",
                        "slug": "Fengjun-Lv",
                        "structuredName": {
                            "firstName": "Fengjun",
                            "lastName": "Lv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fengjun Lv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682028"
                        ],
                        "name": "Shenghuo Zhu",
                        "slug": "Shenghuo-Zhu",
                        "structuredName": {
                            "firstName": "Shenghuo",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shenghuo Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41216159"
                        ],
                        "name": "Ming Yang",
                        "slug": "Ming-Yang",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2807482"
                        ],
                        "name": "Timoth\u00e9e Cour",
                        "slug": "Timoth\u00e9e-Cour",
                        "structuredName": {
                            "firstName": "Timoth\u00e9e",
                            "lastName": "Cour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timoth\u00e9e Cour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782042"
                        ],
                        "name": "Kai Yu",
                        "slug": "Kai-Yu",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48749954"
                        ],
                        "name": "Liangliang Cao",
                        "slug": "Liangliang-Cao",
                        "structuredName": {
                            "firstName": "Liangliang",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liangliang Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 106
                            }
                        ],
                        "text": "In order to benefit from modern multi-core hardware, we further implemented a parallelized version of MeanSGD based on Bouttou\u2019s SGD [4], exploiting data parallelism."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 228
                            }
                        ],
                        "text": "Motivated by the potential of stochastic gradient-based optimization for rapid convergence, and in line with the two best performing ILSVRC10 approaches, we use linear SVM classifiers, trained using stochastic gradient descent (SGD) [3]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "As can be seen in Figure 3a, using MeanSGD (solid lines) instead of SGD (dashed lines) significantly speeds up convergence and improves performance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 282
                            }
                        ],
                        "text": "BoW [2] Sift LibLinear 1,000 80 91 BoW Sift MeanSGD 1,000 72 86 BoW + SPM rgSift MeanSGD 8,000 59 76 LLC + SPM rgSift MeanSGD 21,000 50 69 Fisher vector rgSift MeanSGD 32,768 43 61 LLC+SPM, Fisher rgSift MeanSGD 53,768 38 57 Fisher+SPM [25] Sift, Color SGD 262,144 34 \u2013 LLC,SVC+SPM [16] Hog, Lbp ASGD 1,179,648 28 47"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "However, in contrast to [16] we do not average after each step, but take the mean of the results after each epoch (one pass over the data)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 147
                            }
                        ],
                        "text": "In an attempt to regulate the performance-runtime tradeoff, we explore different combinations of techniques used by the best performing approaches [25, 16] such as spatial pyramid matching (SPM [15]), locality-constrained"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 244
                            }
                        ],
                        "text": "BoW: bag of visual words, SPM: spatial pyramid matching [15], LLC: locality-constrained linear coding [30], Fisher vector [19], SVC: Super-Vector Coding [31], Lbp: local binary patterns, SGD: stochastic gradient decent [3], ASGD: averaging SGD [16]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "We use a SVM (MeanSGD) stacked on top of the scores of all nodes and both features to learn the relative importance of the nodes, i.e. we learn one-vs-all classifiers which use the classifier scores as feature vectors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "While the score of the normal SGD after n epochs only depends on the weights and bias after the final epoch\nfSGD(x) = \u3008wn, x\u3009+ bn, (7)\nwe compute the mean over all epochs in MeanSGD:\nfMeanSGD(x) = \u2211n\ni=1\u3008wi, x\u3009+ bi n\n(8)\n(where \u3008w, x\u3009 is the scaler product of w an x)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 42
                            }
                        ],
                        "text": "Similar in spirit to averaging SGD (ASGD) [20, 16], we average the SVM\u2019s weight and bias."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1346314,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54d48a42a34f368240b79e7c98c7d9283f79b350",
            "isKey": true,
            "numCitedBy": 412,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Most research efforts on image classification so far have been focused on medium-scale datasets, which are often defined as datasets that can fit into the memory of a desktop (typically 4G\u223c48G). There are two main reasons for the limited effort on large-scale image classification. First, until the emergence of ImageNet dataset, there was almost no publicly available large-scale benchmark data for image classification. This is mostly because class labels are expensive to obtain. Second, large-scale classification is hard because it poses more challenges than its medium-scale counterparts. A key challenge is how to achieve efficiency in both feature extraction and classifier training without compromising performance. This paper is to show how we address this challenge using ImageNet dataset as an example. For feature extraction, we develop a Hadoop scheme that performs feature extraction in parallel using hundreds of mappers. This allows us to extract fairly sophisticated features (with dimensions being hundreds of thousands) on 1.2 million images within one day. For SVM training, we develop a parallel averaging stochastic gradient descent (ASGD) algorithm for training one-against-all 1000-class SVM classifiers. The ASGD algorithm is capable of dealing with terabytes of training data and converges very fast\u2013typically 5 epochs are sufficient. As a result, we achieve state-of-the-art performance on the ImageNet 1000-class classification, i.e., 52.9% in classification accuracy and 71.8% in top 5 hit rate."
            },
            "slug": "Large-scale-image-classification:-Fast-feature-and-Lin-Lv",
            "title": {
                "fragments": [],
                "text": "Large-scale image classification: Fast feature extraction and SVM training"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A parallel averaging stochastic gradient descent (ASGD) algorithm for training one-against-all 1000-class SVM classifiers and a Hadoop scheme that performs feature extraction in parallel using hundreds of mappers, which achieves state-of-the-art performance on the ImageNet 1000- class classification."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 173
                            }
                        ],
                        "text": "Related work Knowledge transfer for object class recognition comes in different flavors, such as joint learning of multiple classes [28] or transferring object class priors [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6953475,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "812355cec91fa30bb50e9e992a3549af39e4f6eb",
            "isKey": false,
            "numCitedBy": 2365,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by maximum likelihood (ML) and maximum a posteriori (MAP) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully."
            },
            "slug": "One-shot-learning-of-object-categories-Fei-Fei-Fergus",
            "title": {
                "fragments": [],
                "text": "One-shot learning of object categories"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is found that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3502855"
                        ],
                        "name": "Marcin Marszalek",
                        "slug": "Marcin-Marszalek",
                        "structuredName": {
                            "firstName": "Marcin",
                            "lastName": "Marszalek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcin Marszalek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 168
                            }
                        ],
                        "text": "A first line of research exploits the hierarchical structure of the object class space imposed by a general-to-specific ordering, either based on an existing hierarchy [17, 32] or learned from visual features [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 155
                            }
                        ],
                        "text": "The first direction imposes a hierarchical structure on the space of object classes, according to the general-to-specific ordering defined by the data set [12, 17, 32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14412825,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "530b99c3819bd4c4f1884fa89c9a0d6e024156bd",
            "isKey": false,
            "numCitedBy": 309,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose to use lexical semantic networks to extend the state-of-the-art object recognition techniques. We use the semantics of image labels to integrate prior knowledge about inter-class relationships into the visual appearance learning. We show how to build and train a semantic hierarchy of discriminative classifiers and how to use it to perform object detection. We evaluate how our approach influences the classification accuracy and speed on the Pascal VOC challenge 2006 dataset, a set of challenging real-world images. We also demonstrate additional features that become available to object recognition due to the extension with semantic inference tools- we can classify high-level categories, such as animals, and we can train part detectors, for example a window detector, by pure inference in the semantic network."
            },
            "slug": "Semantic-Hierarchies-for-Visual-Object-Recognition-Marszalek-Schmid",
            "title": {
                "fragments": [],
                "text": "Semantic Hierarchies for Visual Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The semantics of image labels are used to integrate prior knowledge about inter-class relationships into the visual appearance learning and to build and train a semantic hierarchy of discriminative classifiers and how to use it to perform object detection."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2831988"
                        ],
                        "name": "Ian Endres",
                        "slug": "Ian-Endres",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Endres",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian Endres"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 107
                            }
                        ],
                        "text": "A second line of research uses an intermediate layer of descriptive attributes to represent object classes [8, 14, 22], encoding high-level visual properties that can be shared among object classes, hence promoting scalability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 115
                            }
                        ],
                        "text": "The second direction is based on representing object classes relative to an inventory of generic visual attributes [8, 14, 22], where classes are characterized by distinct patterns of attribute activations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14171860,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "5d33a10752af9ea30993139ac6e3a323992a5831",
            "isKey": false,
            "numCitedBy": 212,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an approach to find and describe objects within broad domains. We introduce a new dataset that provides annotation for sharing models of appearance and correlation across categories. We use it to learn part and category detectors. These serve as the visual basis for an integrated model of objects. We describe objects by the spatial arrangement of their attributes and the interactions between them. Using this model, our system can find animals and vehicles that it has not seen and infer attributes, such as function and pose. Our experiments demonstrate that we can more reliably locate and describe both familiar and unfamiliar objects, compared to a baseline that relies purely on basic category detectors."
            },
            "slug": "Attribute-centric-recognition-for-cross-category-Farhadi-Endres",
            "title": {
                "fragments": [],
                "text": "Attribute-centric recognition for cross-category generalization"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This work introduces a new dataset that provides annotation for sharing models of appearance and correlation across categories and uses it to learn part and category detectors that serve as the visual basis for an integrated model of objects."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94451829"
                        ],
                        "name": "K. Li",
                        "slug": "K.-Li",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1274537,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9800e3c3394c569be83379ee2ebe3424e09c2919",
            "isKey": false,
            "numCitedBy": 528,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Image classification is a critical task for both humans and computers. One of the challenges lies in the large scale of the semantic space. In particular, humans can recognize tens of thousands of object classes and scenes. No computer vision algorithm today has been tested at this scale. This paper presents a study of large scale categorization including a series of challenging experiments on classification with more than 10, 000 image classes. We find that a) computational issues become crucial in algorithm design; b) conventional wisdom from a couple of hundred image categories on relative performance of different classifiers does not necessarily hold when the number of categories increases; c) there is a surprisingly strong relationship between the structure of WordNet (developed for studying language) and the difficulty of visual categorization; d) classification can be improved by exploiting the semantic hierarchy. Toward the future goal of developing automatic vision algorithms to recognize tens of thousands or even millions of image categories, we make a series of observations and arguments about dataset scale, category density, and image hierarchy."
            },
            "slug": "What-Does-Classifying-More-Than-10,-000-Image-Tell-Deng-Berg",
            "title": {
                "fragments": [],
                "text": "What Does Classifying More Than 10, 000 Image Categories Tell Us?"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A study of large scale categorization including a series of challenging experiments on classification with more than 10,000 image classes finds that computational issues become crucial in algorithm design and conventional wisdom from a couple of hundred image categories does not necessarily hold when the number of categories increases."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2192178"
                        ],
                        "name": "Olga Russakovsky",
                        "slug": "Olga-Russakovsky",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Russakovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olga Russakovsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11420934,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "c77f1d5c9460409cdab8cf53432cb238a8105c05",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the task of learning visual connections between object categories using the ImageNet dataset, which is a large-scale dataset ontology containing more than 15 thousand object classes. We want to discover visual relationships between the classes that are currently missing (such as similar colors or shapes or textures). In this work we learn 20 visual attributes and use them in a zero-shot transfer learning experiment as well as to make visual connections between semantically unrelated object categories."
            },
            "slug": "Attribute-Learning-in-Large-Scale-Datasets-Russakovsky-Fei-Fei",
            "title": {
                "fragments": [],
                "text": "Attribute Learning in Large-Scale Datasets"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work learns 20 visual attributes and uses them in a zero-shot transfer learning experiment as well as to make visual connections between semantically unrelated object categories."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV Workshops"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105534484"
                        ],
                        "name": "Michael Fink",
                        "slug": "Michael-Fink",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Fink",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Fink"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 129
                            }
                        ],
                        "text": "The third direction is based on direct similarities to related classes effectively using the classifiers of most similar classes [1, 10, 22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 52
                            }
                        ],
                        "text": "While most work based on similarity between classes [1, 10] require a few training samples for new classes, we employ our unsupervised approach [22] where class similarities are mined automatically using semantic relatedness measures with linguistic knowledge bases like Wikipedia or web search."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18864357,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67fdc1e0d878675e9ac765830f85b461777e49ec",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a framework for learning an object classifier from a single example, by emphasizing relevant dimensions using available examples of related classes. Learning to accurately classify objects from a single training example is often unfeasible due to overfitting effects. However, if the instance representation provides that the distance between each two instances of the same class is smaller than the distance between any two instances from different classes, then a nearest neighbor classifier could achieve perfect performance with a single training example. We therefore suggest a two stage strategy. First, learn a metric over the instances that achieves the distance criterion mentioned above, from available examples of other related classes. Then, using the single examples, define a nearest neighbor classifier where distance is evaluated by the learned class relevance metric. Finding a metric that emphasizes the relevant dimensions for classification might not be possible when restricted to linear projections. We therefore make use of a kernel based metric learning algorithm. Our setting encodes object instances as sets of locality based descriptors and adopts an appropriate image kernel for the class relevance metric learning. The proposed framework for learning from a single example is demonstrated in a synthetic setting and on a character classification task."
            },
            "slug": "Object-Classication-from-a-Single-Example-Utilizing-Fink",
            "title": {
                "fragments": [],
                "text": "Object Classication from a Single Example Utilizing Class Relevance Pseudo-Metrics"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A framework for learning an object classifier from a single example, by emphasizing relevant dimensions using available examples of related classes, by making use of a kernel based metric learning algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 2004"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "The tiny image data set [27] (80 million images, loosely labeled with 75,062 WordNet nouns) provides a significantly larger number of images but is mostly restricted to 32x32 pixel images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7487588,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54d2b5c64a67f65c5dd812b89e07973f97699552",
            "isKey": false,
            "numCitedBy": 1868,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 x 32 color images. Each image is loosely labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes that are particularly prevalent in the dataset, such as people, we are able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors."
            },
            "slug": "80-Million-Tiny-Images:-A-Large-Data-Set-for-Object-Torralba-Fergus",
            "title": {
                "fragments": [],
                "text": "80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "For certain classes that are particularly prevalent in the dataset, such as people, this work is able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681054"
                        ],
                        "name": "H. J\u00e9gou",
                        "slug": "H.-J\u00e9gou",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "J\u00e9gou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. J\u00e9gou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3271933"
                        ],
                        "name": "M. Douze",
                        "slug": "M.-Douze",
                        "structuredName": {
                            "firstName": "Matthijs",
                            "lastName": "Douze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Douze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144565371"
                        ],
                        "name": "P. P\u00e9rez",
                        "slug": "P.-P\u00e9rez",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "P\u00e9rez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. P\u00e9rez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "linear coding (LCC [30]), and the Fisher vector [19] (we adapted the implementation of [13]), in connection with the color sift variant rgSift [29] (Tab."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1912782,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "400e09ceca374f0621335f84a4daf2049d5902be",
            "isKey": false,
            "numCitedBy": 2305,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of image search on a very large scale, where three constraints have to be considered jointly: the accuracy of the search, its efficiency, and the memory usage of the representation. We first propose a simple yet efficient way of aggregating local image descriptors into a vector of limited dimension, which can be viewed as a simplification of the Fisher kernel representation. We then show how to jointly optimize the dimension reduction and the indexing algorithm, so that it best preserves the quality of vector comparison. The evaluation shows that our approach significantly outperforms the state of the art: the search accuracy is comparable to the bag-of-features approach for an image representation that fits in 20 bytes. Searching a 10 million image dataset takes about 50ms."
            },
            "slug": "Aggregating-local-descriptors-into-a-compact-image-J\u00e9gou-Douze",
            "title": {
                "fragments": [],
                "text": "Aggregating local descriptors into a compact image representation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a simple yet efficient way of aggregating local image descriptors into a vector of limited dimension, which can be viewed as a simplification of the Fisher kernel representation, and shows how to jointly optimize the dimension reduction and the indexing algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1938475"
                        ],
                        "name": "E. Bart",
                        "slug": "E.-Bart",
                        "structuredName": {
                            "firstName": "Evgeniy",
                            "lastName": "Bart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743045"
                        ],
                        "name": "S. Ullman",
                        "slug": "S.-Ullman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Ullman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ullman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[1] encodes instances of previously unknown classes as collections of \u201cfamiliar\u201d classifier responses, i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 129
                            }
                        ],
                        "text": "The third direction is based on direct similarities to related classes effectively using the classifiers of most similar classes [1, 10, 22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 52
                            }
                        ],
                        "text": "While most work based on similarity between classes [1, 10] require a few training samples for new classes, we employ our unsupervised approach [22] where class similarities are mined automatically using semantic relatedness measures with linguistic knowledge bases like Wikipedia or web search."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12278974,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a09ad471425f6c945d0811a493e2a334e766e9a3",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an object classification method that can learn from a single training example. In this method, a novel class is characterized by its similarity to a number of previously learned, familiar classes. We demonstrate that this similarity is well-preserved across different class instances. As a result, it generalizes well to new instances of the novel class. A simple comparison of the similarity patterns is therefore sufficient to obtain useful classification performance from a single training example. The similarity between the novel class and the familiar classes in the proposed method can be evaluated using a wide variety of existing classification schemes. It can therefore combine the merits of many different classification methods. Experiments on a database of 107 widely varying object classes demonstrate that the proposed method significantly improves the performance of the baseline algorithm."
            },
            "slug": "Single-example-Learning-of-Novel-Classes-using-by-Bart-Ullman",
            "title": {
                "fragments": [],
                "text": "Single-example Learning of Novel Classes using Representation by Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "An object classification method that can learn from a single training example that significantly improves the performance of the baseline algorithm and can combine the merits of many different classification methods."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144421225"
                        ],
                        "name": "Michael Stark",
                        "slug": "Michael-Stark",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Stark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Stark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3081065"
                        ],
                        "name": "Gy\u00f6rgy Szarvas",
                        "slug": "Gy\u00f6rgy-Szarvas",
                        "structuredName": {
                            "firstName": "Gy\u00f6rgy",
                            "lastName": "Szarvas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gy\u00f6rgy Szarvas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1261437,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce5ec6d255134967de60f0917476255e5831b594",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Knowledge transfer between object classes has been identified as an important tool for scalable recognition. However, determining which knowledge to transfer where remains a key challenge. While most approaches employ varying levels of human supervision, we follow the idea of mining linguistic knowledge bases to automatically infer transferable knowledge. In contrast to previous work, we explicitly aim to design robust semantic relatedness measures and to combine different language sources for attribute-based knowledge transfer. On the challenging Animals with Attributes (AwA) data set, we report largely improved attribute-based zero-shot object class recognition performance that matches the performance of human supervision."
            },
            "slug": "Combining-Language-Sources-and-Robust-Semantic-for-Rohrbach-Stark",
            "title": {
                "fragments": [],
                "text": "Combining Language Sources and Robust Semantic Relatedness for Attribute-Based Knowledge Transfer"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "On the challenging Animals with Attributes (AwA) data set, this work reports largely improved attribute-based zero-shot object class recognition performance that matches the performance of human supervision."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV Workshops"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39161025"
                        ],
                        "name": "Alon Zweig",
                        "slug": "Alon-Zweig",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Zweig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alon Zweig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789171"
                        ],
                        "name": "D. Weinshall",
                        "slug": "D.-Weinshall",
                        "structuredName": {
                            "firstName": "Daphna",
                            "lastName": "Weinshall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Weinshall"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 168
                            }
                        ],
                        "text": "A first line of research exploits the hierarchical structure of the object class space imposed by a general-to-specific ordering, either based on an existing hierarchy [17, 32] or learned from visual features [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 155
                            }
                        ],
                        "text": "The first direction imposes a hierarchical structure on the space of object classes, according to the general-to-specific ordering defined by the data set [12, 17, 32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7691438,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "352c53e56c52a49d33dcdbec5690c2ba604b07d0",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigated the computational properties of natural object hierarchy in the context of constellation object class models, and its utility for object class recognition. We first observed an interesting computational property of the object hierarchy: comparing the recognition rate when using models of objects at different levels, the higher more inclusive levels (e.g., closed-frame vehicles or vehicles) exhibit higher recall but lower precision when compared with the class specific level (e.g., bus). These inherent differences suggest that combining object classifiers from different hierarchical levels into a single classifier may improve classification, as it appears like these models capture different aspects of the object. We describe a method to combine these classifiers, and analyze the conditions under which improvement can be guaranteed. When given a small sample of a new object class, we describe a method to transfer knowledge across the tree hierarchy, between related objects. Finally, we describe extensive experiments using object hierarchies obtained from publicly available datasets, and show that the combined classifiers significantly improve recognition results."
            },
            "slug": "Exploiting-Object-Hierarchy:-Combining-Models-from-Zweig-Weinshall",
            "title": {
                "fragments": [],
                "text": "Exploiting Object Hierarchy: Combining Models from Different Category Levels"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "An interesting computational property of the object hierarchy is observed: comparing the recognition rate when using models of objects at different levels, the higher more inclusive levels exhibit higher recall but lower precision when compared with the class specific level."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105534484"
                        ],
                        "name": "Michael Fink",
                        "slug": "Michael-Fink",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Fink",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Fink"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "While most work based on similarity between classes [1,  10 ] require a few training samples for new classes, we employ our unsupervised approach [22] where class similarities are mined automatically using semantic relatedness measures with linguistic knowledge bases like Wikipedia or web search."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The third direction is based on direct similarities to related classes effectively using the classifiers of most similar classes [1,  10 , 22]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9248748,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01e098695a5b653ebf0daaef9f8cefc3b02e8e71",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a framework for learning an object classifier from a single example. This goal is achieved by emphasizing the relevant dimensions for classification using available examples of related classes. Learning to accurately classify objects from a single training example is often unfeasible due to overfitting effects. However, if the instance representation provides that the distance between each two instances of the same class is smaller than the distance between any two instances from different classes, then a nearest neighbor classifier could achieve perfect performance with a single training example. We therefore suggest a two stage strategy. First, learn a metric over the instances that achieves the distance criterion mentioned above, from available examples of other related classes. Then, using the single examples, define a nearest neighbor classifier where distance is evaluated by the learned class relevance metric. Finding a metric that emphasizes the relevant dimensions for classification might not be possible when restricted to linear projections. We therefore make use of a kernel based metric learning algorithm. Our setting encodes object instances as sets of locality based descriptors and adopts an appropriate image kernel for the class relevance metric learning. The proposed framework for learning from a single example is demonstrated in a synthetic setting and on a character classification task."
            },
            "slug": "Object-Classification-from-a-Single-Example-Class-Fink",
            "title": {
                "fragments": [],
                "text": "Object Classification from a Single Example Utilizing Class Relevance Metrics"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A framework for learning an object classifier from a single example by emphasizing the relevant dimensions for classification using available examples of related classes by making use of a kernel based metric learning algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847596"
                        ],
                        "name": "Wei Dong",
                        "slug": "Wei-Dong",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94451829"
                        ],
                        "name": "K. Li",
                        "slug": "K.-Li",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "Our hierarchical classification is closest to [7], combining classifier scores of distinct subgraphs to yield final classification scores."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "We evaluate knowledge transfer on the recently proposed ImageNet data set [7], specifically, on the associated ImageNet Large Scale Visual Recognition Competition 2010 (ILSVRC10) subset [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57246310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b47265245e8db53a553049dcb27ed3e495fd625",
            "isKey": false,
            "numCitedBy": 27420,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond."
            },
            "slug": "ImageNet:-A-large-scale-hierarchical-image-database-Deng-Dong",
            "title": {
                "fragments": [],
                "text": "ImageNet: A large-scale hierarchical image database"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new database called \u201cImageNet\u201d is introduced, a large-scale ontology of images built upon the backbone of the WordNet structure, much larger in scale and diversity and much more accurate than the current image datasets."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71563118"
                        ],
                        "name": "Jinjun Wang",
                        "slug": "Jinjun-Wang",
                        "structuredName": {
                            "firstName": "Jinjun",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinjun Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706007"
                        ],
                        "name": "Jianchao Yang",
                        "slug": "Jianchao-Yang",
                        "structuredName": {
                            "firstName": "Jianchao",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianchao Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782042"
                        ],
                        "name": "Kai Yu",
                        "slug": "Kai-Yu",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39157653"
                        ],
                        "name": "Fengjun Lv",
                        "slug": "Fengjun-Lv",
                        "structuredName": {
                            "firstName": "Fengjun",
                            "lastName": "Lv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fengjun Lv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144768792"
                        ],
                        "name": "Yihong Gong",
                        "slug": "Yihong-Gong",
                        "structuredName": {
                            "firstName": "Yihong",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yihong Gong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "BoW: bag of visual words, SPM: spatial pyramid matching [15], LLC: locality-constrained linear coding [30], Fisher vector [19], SVC: Super-Vector Coding [31], Lbp: local binary patterns, SGD: stochastic gradient decent [3], ASGD: averaging SGD [16]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "linear coding (LCC [30]), and the Fisher vector [19] (we adapted the implementation of [13]), in connection with the color sift variant rgSift [29] (Tab."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6718692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f7713dcc35e7c05becf3be5522f36c9546b0364",
            "isKey": false,
            "numCitedBy": 3240,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "The traditional SPM approach based on bag-of-features (BoF) requires nonlinear classifiers to achieve good image classification performance. This paper presents a simple but effective coding scheme called Locality-constrained Linear Coding (LLC) in place of the VQ coding in traditional SPM. LLC utilizes the locality constraints to project each descriptor into its local-coordinate system, and the projected coordinates are integrated by max pooling to generate the final representation. With linear classifier, the proposed approach performs remarkably better than the traditional nonlinear SPM, achieving state-of-the-art performance on several benchmarks. Compared with the sparse coding strategy [22], the objective function used by LLC has an analytical solution. In addition, the paper proposes a fast approximated LLC method by first performing a K-nearest-neighbor search and then solving a constrained least square fitting problem, bearing computational complexity of O(M + K2). Hence even with very large codebooks, our system can still process multiple frames per second. This efficiency significantly adds to the practical values of LLC for real applications."
            },
            "slug": "Locality-constrained-Linear-Coding-for-image-Wang-Yang",
            "title": {
                "fragments": [],
                "text": "Locality-constrained Linear Coding for image classification"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This paper presents a simple but effective coding scheme called Locality-constrained Linear Coding (LLC) in place of the VQ coding in traditional SPM, using the locality constraints to project each descriptor into its local-coordinate system, and the projected coordinates are integrated by max pooling to generate the final representation."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144078686"
                        ],
                        "name": "Tat-Seng Chua",
                        "slug": "Tat-Seng-Chua",
                        "structuredName": {
                            "firstName": "Tat-Seng",
                            "lastName": "Chua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tat-Seng Chua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8053308"
                        ],
                        "name": "Jinhui Tang",
                        "slug": "Jinhui-Tang",
                        "structuredName": {
                            "firstName": "Jinhui",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinhui Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48043335"
                        ],
                        "name": "Richang Hong",
                        "slug": "Richang-Hong",
                        "structuredName": {
                            "firstName": "Richang",
                            "lastName": "Hong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richang Hong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145537762"
                        ],
                        "name": "Haojie Li",
                        "slug": "Haojie-Li",
                        "structuredName": {
                            "firstName": "Haojie",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haojie Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114940115"
                        ],
                        "name": "Zhiping Luo",
                        "slug": "Zhiping-Luo",
                        "structuredName": {
                            "firstName": "Zhiping",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiping Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2515436"
                        ],
                        "name": "Yantao Zheng",
                        "slug": "Yantao-Zheng",
                        "structuredName": {
                            "firstName": "Yantao",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yantao Zheng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "NUS-WIDE [ 5 ] is significantly larger with 270k images and over 5k unique tags but contains ground truth for only 81 categories."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6483070,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b80a580a6f2eca77524302acd944fd6edf0a0611",
            "isKey": false,
            "numCitedBy": 2109,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a web image dataset created by NUS's Lab for Media Search. The dataset includes: (1) 269,648 images and the associated tags from Flickr, with a total of 5,018 unique tags; (2) six types of low-level features extracted from these images, including 64-D color histogram, 144-D color correlogram, 73-D edge direction histogram, 128-D wavelet texture, 225-D block-wise color moments extracted over 5x5 fixed grid partitions, and 500-D bag of words based on SIFT descriptions; and (3) ground-truth for 81 concepts that can be used for evaluation. Based on this dataset, we highlight characteristics of Web image collections and identify four research issues on web image annotation and retrieval. We also provide the baseline results for web image annotation by learning from the tags using the traditional k-NN algorithm. The benchmark results indicate that it is possible to learn effective models from sufficiently large image dataset to facilitate general image retrieval."
            },
            "slug": "NUS-WIDE:-a-real-world-web-image-database-from-of-Chua-Tang",
            "title": {
                "fragments": [],
                "text": "NUS-WIDE: a real-world web image database from National University of Singapore"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The benchmark results indicate that it is possible to learn effective models from sufficiently large image dataset to facilitate general image retrieval and four research issues on web image annotation and retrieval are identified."
            },
            "venue": {
                "fragments": [],
                "text": "CIVR '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756979"
                        ],
                        "name": "K. V. D. Sande",
                        "slug": "K.-V.-D.-Sande",
                        "structuredName": {
                            "firstName": "Koen",
                            "lastName": "Sande",
                            "middleNames": [
                                "E.",
                                "A.",
                                "van",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. V. D. Sande"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695527"
                        ],
                        "name": "T. Gevers",
                        "slug": "T.-Gevers",
                        "structuredName": {
                            "firstName": "Theo",
                            "lastName": "Gevers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gevers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145404204"
                        ],
                        "name": "Cees G. M. Snoek",
                        "slug": "Cees-G.-M.-Snoek",
                        "structuredName": {
                            "firstName": "Cees",
                            "lastName": "Snoek",
                            "middleNames": [
                                "G.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cees G. M. Snoek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 143
                            }
                        ],
                        "text": "linear coding (LCC [30]), and the Fisher vector [19] (we adapted the implementation of [13]), in connection with the color sift variant rgSift [29] (Tab."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 828465,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1aa5a8ad5b7031ba39e1dc0537484694364a1312",
            "isKey": false,
            "numCitedBy": 2099,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Image category recognition is important to access visual information on the level of objects and scene types. So far, intensity-based descriptors have been widely used for feature extraction at salient points. To increase illumination invariance and discriminative power, color descriptors have been proposed. Because many different descriptors exist, a structured overview is required of color invariant descriptors in the context of image category recognition. Therefore, this paper studies the invariance properties and the distinctiveness of color descriptors (software to compute the color descriptors from this paper is available from http://www.colordescriptors.com) in a structured way. The analytical invariance properties of color descriptors are explored, using a taxonomy based on invariance properties with respect to photometric transformations, and tested experimentally using a data set with known illumination conditions. In addition, the distinctiveness of color descriptors is assessed experimentally using two benchmarks, one from the image domain and one from the video domain. From the theoretical and experimental results, it can be derived that invariance to light intensity changes and light color changes affects category recognition. The results further reveal that, for light intensity shifts, the usefulness of invariance is category-specific. Overall, when choosing a single descriptor and no prior knowledge about the data set and object and scene categories is available, the OpponentSIFT is recommended. Furthermore, a combined set of color descriptors outperforms intensity-based SIFT and improves category recognition by 8 percent on the PASCAL VOC 2007 and by 7 percent on the Mediamill Challenge."
            },
            "slug": "Evaluating-Color-Descriptors-for-Object-and-Scene-Sande-Gevers",
            "title": {
                "fragments": [],
                "text": "Evaluating Color Descriptors for Object and Scene Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "From the theoretical and experimental results, it can be derived that invariance to light intensity changes and light color changes affects category recognition and the usefulness of invariance is category-specific."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11691394"
                        ],
                        "name": "R. Rifkin",
                        "slug": "R.-Rifkin",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Rifkin",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rifkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753266"
                        ],
                        "name": "A. Klautau",
                        "slug": "A.-Klautau",
                        "structuredName": {
                            "firstName": "Aldebaro",
                            "lastName": "Klautau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Klautau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "Third, we contrast knowledge transfer with the traditional approach of one-versus-all classification [21], which is often neglected in previous knowledge transfer work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13391792,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c44749c8496a82512047aad0fd5e31e1b979d6a",
            "isKey": false,
            "numCitedBy": 1846,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of multiclass classification. Our main thesis is that a simple \"one-vs-all\" scheme is as accurate as any other approach, assuming that the underlying binary classifiers are well-tuned regularized classifiers such as support vector machines. This thesis is interesting in that it disagrees with a large body of recent published work on multiclass classification. We support our position by means of a critical review of the existing literature, a substantial collection of carefully controlled experimental work, and theoretical arguments."
            },
            "slug": "In-Defense-of-One-Vs-All-Classification-Rifkin-Klautau",
            "title": {
                "fragments": [],
                "text": "In Defense of One-Vs-All Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "It is argued that a simple \"one-vs-all\" scheme is as accurate as any other approach, assuming that the underlying binary classifiers are well-tuned regularized classifiers such as support vector machines."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749609"
                        ],
                        "name": "S. Lazebnik",
                        "slug": "S.-Lazebnik",
                        "structuredName": {
                            "firstName": "Svetlana",
                            "lastName": "Lazebnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lazebnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189388"
                        ],
                        "name": "J. Ponce",
                        "slug": "J.-Ponce",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Ponce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ponce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "BoW: bag of visual words, SPM: spatial pyramid matching [15], LLC: locality-constrained linear coding [30], Fisher vector [19], SVC: Super-Vector Coding [31], Lbp: local binary patterns, SGD: stochastic gradient decent [3], ASGD: averaging SGD [16]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 194
                            }
                        ],
                        "text": "In an attempt to regulate the performance-runtime tradeoff, we explore different combinations of techniques used by the best performing approaches [25, 16] such as spatial pyramid matching (SPM [15]), locality-constrained"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2421251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dbaff29d3898cf60f63f5a34cb9610ebb75220c",
            "isKey": false,
            "numCitedBy": 8328,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting \"spatial pyramid\" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba\u2019s \"gist\" and Lowe\u2019s SIFT descriptors."
            },
            "slug": "Beyond-Bags-of-Features:-Spatial-Pyramid-Matching-Lazebnik-Schmid",
            "title": {
                "fragments": [],
                "text": "Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence that exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47655614"
                        ],
                        "name": "G. Griffin",
                        "slug": "G.-Griffin",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Griffin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Griffin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 209
                            }
                        ],
                        "text": "A first line of research exploits the hierarchical structure of the object class space imposed by a general-to-specific ordering, either based on an existing hierarchy [17, 32] or learned from visual features [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 155
                            }
                        ],
                        "text": "The first direction imposes a hierarchical structure on the space of object classes, according to the general-to-specific ordering defined by the data set [12, 17, 32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12805251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49bab85f250ff4c086665691df4971c438336e0b",
            "isKey": false,
            "numCitedBy": 234,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "The computational complexity of current visual categorization algorithms scales linearly at best with the number of categories. The goal of classifying simultaneously Ncat = 104 - 105 visual categories requires sub-linear classification costs. We explore algorithms for automatically building classification trees which have, in principle, logNcat complexity. We find that a greedy algorithm that recursively splits the set of categories into the two minimally confused subsets achieves 5-20 fold speedups at a small cost in classification performance. Our approach is independent of the specific classification algorithm used. A welcome by-product of our algorithm is a very reasonable taxonomy of the Caltech-256 dataset."
            },
            "slug": "Learning-and-using-taxonomies-for-fast-visual-Griffin-Perona",
            "title": {
                "fragments": [],
                "text": "Learning and using taxonomies for fast visual categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is found that a greedy algorithm that recursively splits the set of categories into the two minimally confused subsets achieves 5-20 fold speedups at a small cost in classification performance."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109615009"
                        ],
                        "name": "Xi Zhou",
                        "slug": "Xi-Zhou",
                        "structuredName": {
                            "firstName": "Xi",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xi Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782042"
                        ],
                        "name": "Kai Yu",
                        "slug": "Kai-Yu",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49104973"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "BoW: bag of visual words, SPM: spatial pyramid matching [15], LLC: locality-constrained linear coding [30], Fisher vector [19], SVC: Super-Vector Coding [ 31 ], Lbp: local binary patterns, SGD: stochastic gradient decent [3], ASGD: averaging SGD [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7405065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e65c9f0a64b6a4333b12e2adc3861ad75aca83b",
            "isKey": false,
            "numCitedBy": 555,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a new framework for image classification using local visual descriptors. The pipeline first performs a non-linear feature transformation on descriptors, then aggregates the results together to form image-level representations, and finally applies a classification model. For all the three steps we suggest novel solutions which make our approach appealing in theory, more scalable in computation, and transparent in classification. Our experiments demonstrate that the proposed classification method achieves state-of-the-art accuracy on the well-known PASCAL benchmarks."
            },
            "slug": "Image-Classification-Using-Super-Vector-Coding-of-Zhou-Yu",
            "title": {
                "fragments": [],
                "text": "Image Classification Using Super-Vector Coding of Local Image Descriptors"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This paper introduces a new framework for image classification using local visual descriptors that first performs a non-linear feature transformation on descriptors, then aggregates the results together to form image-level representations, and finally applies a classification model."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3081065"
                        ],
                        "name": "Gy\u00f6rgy Szarvas",
                        "slug": "Gy\u00f6rgy-Szarvas",
                        "structuredName": {
                            "firstName": "Gy\u00f6rgy",
                            "lastName": "Szarvas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gy\u00f6rgy Szarvas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780779"
                        ],
                        "name": "Torsten Zesch",
                        "slug": "Torsten-Zesch",
                        "structuredName": {
                            "firstName": "Torsten",
                            "lastName": "Zesch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Torsten Zesch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730400"
                        ],
                        "name": "Iryna Gurevych",
                        "slug": "Iryna-Gurevych",
                        "structuredName": {
                            "firstName": "Iryna",
                            "lastName": "Gurevych",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iryna Gurevych"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 35
                            }
                        ],
                        "text": "On average, using single measures (Wikipedia, Yahoo Holonyms, Image, or Snippets) performs in the same order of magnitude as inner WordNet nodes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 179
                            }
                        ],
                        "text": "For mining class-attribute associations we choose the best performing measures [22, 23] which are applicable to large scale: (1) the explicit semantic analysis based on Wikipedia [26]; (2) Yahoo Holynyms which is based on hitcounts and uses specific part queries such as \u201cthe wheel of the car\u201d; (3) Yahoo Image which is based on image-search hitcounts; and (4) Yahoo Snippets which is based on web page summaries returned by the search engine."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 271
                            }
                        ],
                        "text": "While most work based on similarity between classes [1, 10] require a few training samples for new classes, we employ our unsupervised approach [22] where class similarities are mined automatically using semantic relatedness measures with linguistic knowledge bases like Wikipedia or web search."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8511986,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f85457dc413feff0c926f963d0ff57901c3c8ebd",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "The Explicit Semantic Analysis (ESA) model based on term cooccurrences in Wikipedia has been regarded as state-of-the-art semantic relatedness measure in the recent years. We provide an analysis of the important parameters of ESA using datasets in five different languages. Additionally, we propose the use of ESA with multiple lexical semantic resources thus exploiting multiple evidence of term cooccurrence to improve over the Wikipedia-based measure. Exploiting the improved robustness and coverage of the proposed combination, we report improved performance over single resources in word semantic relatedness, solving word choice problems, classification of semantic relations between nominals, and text similarity."
            },
            "slug": "Combining-Heterogeneous-Knowledge-Resources-for-Szarvas-Zesch",
            "title": {
                "fragments": [],
                "text": "Combining Heterogeneous Knowledge Resources for Improved Distributional Semantic Models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Improved performance over single resources in word semantic relatedness, solving word choice problems, classification of semantic relations between nominals, and text similarity is reported."
            },
            "venue": {
                "fragments": [],
                "text": "CICLing"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713934"
                        ],
                        "name": "Antoine Bordes",
                        "slug": "Antoine-Bordes",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Bordes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoine Bordes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741426"
                        ],
                        "name": "P. Gallinari",
                        "slug": "P.-Gallinari",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Gallinari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gallinari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 106
                            }
                        ],
                        "text": "In order to benefit from modern multi-core hardware, we further implemented a parallelized version of MeanSGD based on Bouttou\u2019s SGD [4], exploiting data parallelism."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 233
                            }
                        ],
                        "text": "Motivated by the potential of stochastic gradient-based optimization for rapid convergence, and in line with the two best performing ILSVRC10 approaches, we use linear SVM classifiers, trained using stochastic gradient descent (SGD) [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "As can be seen in Figure 3a, using MeanSGD (solid lines) instead of SGD (dashed lines) significantly speeds up convergence and improves performance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 219
                            }
                        ],
                        "text": "BoW: bag of visual words, SPM: spatial pyramid matching [15], LLC: locality-constrained linear coding [30], Fisher vector [19], SVC: Super-Vector Coding [31], Lbp: local binary patterns, SGD: stochastic gradient decent [3], ASGD: averaging SGD [16]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "We use a SVM (MeanSGD) stacked on top of the scores of all nodes and both features to learn the relative importance of the nodes, i.e. we learn one-vs-all classifiers which use the classifier scores as feature vectors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "While the score of the normal SGD after n epochs only depends on the weights and bias after the final epoch\nfSGD(x) = \u3008wn, x\u3009+ bn, (7)\nwe compute the mean over all epochs in MeanSGD:\nfMeanSGD(x) = \u2211n\ni=1\u3008wi, x\u3009+ bi n\n(8)\n(where \u3008w, x\u3009 is the scaler product of w an x)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "Similar in spirit to averaging SGD (ASGD) [20, 16], we average the SVM\u2019s weight and bias."
                    },
                    "intents": []
                }
            ],
            "corpusId": 347551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b44ff78214ccd975ce16fbbc333423ca78d99141",
            "isKey": true,
            "numCitedBy": 358,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The SGD-QN algorithm is a stochastic gradient descent algorithm that makes careful use of second-order information and splits the parameter update into independently scheduled components. Thanks to this design, SGD-QN iterates nearly as fast as a first-order stochastic gradient descent but requires less iterations to achieve the same accuracy. This algorithm won the \"Wild Track\" of the first PASCAL Large Scale Learning Challenge (Sonnenburg et al., 2008)."
            },
            "slug": "SGD-QN:-Careful-Quasi-Newton-Stochastic-Gradient-Bordes-Bottou",
            "title": {
                "fragments": [],
                "text": "SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The SGD-QN algorithm is a stochastic gradient descent algorithm that makes careful use of second-order information and splits the parameter update into independently scheduled components."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1671874,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "68c03788224000794d5491ab459be0b2a2c38677",
            "isKey": false,
            "numCitedBy": 13889,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4]."
            },
            "slug": "WordNet:-A-Lexical-Database-for-English-Miller",
            "title": {
                "fragments": [],
                "text": "WordNet: A Lexical Database for English"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "WordNet1 provides a more effective combination of traditional lexicographic information and modern computing, and is an online lexical database designed for use under program control."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703847"
                        ],
                        "name": "Boris Polyak",
                        "slug": "Boris-Polyak",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Polyak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boris Polyak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754887"
                        ],
                        "name": "A. Juditsky",
                        "slug": "A.-Juditsky",
                        "structuredName": {
                            "firstName": "Anatoli",
                            "lastName": "Juditsky",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Juditsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3548228,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dc61f37ecc552413606d8c89ffbc46ec98ed887",
            "isKey": false,
            "numCitedBy": 1536,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A new recursive algorithm of stochastic approximation type with the averaging of trajectories is investigated. Convergence with probability one is proved for a variety of classical optimization and identification problems. It is also demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence."
            },
            "slug": "Acceleration-of-stochastic-approximation-by-Polyak-Juditsky",
            "title": {
                "fragments": [],
                "text": "Acceleration of stochastic approximation by averaging"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Convergence with probability one is proved for a variety of classical optimization and identification problems and it is demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 133
                            }
                        ],
                        "text": "Motivated by the potential of stochastic gradient-based optimization for rapid convergence, and in line with the two best performing ILSVRC10 approaches, we use linear SVM classifiers, trained using stochastic gradient descent (SGD) [3]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 79
                            }
                        ],
                        "text": "Our evaluation is based on a recently proposed large-scale data set (ILSVRC10, [2]) and includes three prominent approaches to knowledge transfer1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 63
                            }
                        ],
                        "text": "In order to allow for a sufficient range of experiments on the ILSVRC10 dataset, we require an image representation that is both powerful enough to achieve good performance and reasonably sized to support efficient learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "BoW [2] Sift LibLinear 1,000 80 91 BoW Sift MeanSGD 1,000 72 86 BoW + SPM rgSift MeanSGD 8,000 59 76 LLC + SPM rgSift MeanSGD 21,000 50 69 Fisher vector rgSift MeanSGD 32,768 43 61 LLC+SPM, Fisher rgSift MeanSGD 53,768 38 57 Fisher+SPM [25] Sift, Color SGD 262,144 34 \u2013 LLC,SVC+SPM [16] Hog, Lbp ASGD 1,179,648 28 47"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 92
                            }
                        ],
                        "text": "Based on this dataset the ImageNet Large Scale Visual Recognition Challenge 2010 (ILSVRC10, [2]) has been introduced."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 102
                            }
                        ],
                        "text": "In all experiments, we further maintain the original split into training and test data defined by the ILSVRC10 data set, meaning that we train on the known (800 class) fraction of the original training set (1,005,761 images), and test on the unseen (200 class) fraction of the original test set (30,000 images)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 29
                            }
                        ],
                        "text": "In particular, we divide the ILSVRC10 data set randomly into two disjoint sets of object classes, one assumed known (800 classes), and one assumed unseen (200 classes)1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 4
                            }
                        ],
                        "text": "The ILSVRC10, however, is neither provided with a set of attributes nor with manual class-attribute associations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 186
                            }
                        ],
                        "text": "We evaluate knowledge transfer on the recently proposed ImageNet data set [7], specifically, on the associated ImageNet Large Scale Visual Recognition Competition 2010 (ILSVRC10) subset [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 55
                            }
                        ],
                        "text": "3 gives results for classifying all test images of the ILSVRC10 data set into 1000 classes, using the provided training set for training."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "ILSVRC10 [2] introduced and defined the following performance measures used throughout the paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "As suggested in [2] we report top-n errors for n = 5 and n = 1, which corresponds to 1-accuracy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 46
                            }
                        ],
                        "text": "We thus base our choice on the outcome of the ILSVRC10 competition, which we recapitulate in part in Tab."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 45
                            }
                        ],
                        "text": "We exploit the hierarchical structure of the ILSVRC10 to train two types of classifiers (see for a small sample subgraph Fig."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Large Scale Visual Recognition Challenge 2010"
            },
            "venue": {
                "fragments": [],
                "text": "www.image-net.org/challenges/LSVRC/2010/"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "We observe that the performance ranges from 80% top5 error rate for a BoW Sift baseline (Tab."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "\u2022 ImageNet Challenge [1] \u2022 Data is available for all classes \u2022 1 000 classes \u2022 1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 66
                            }
                        ],
                        "text": "Based on this dataset the ImageNet Large Scale Visual Recognition Challenge 2010 (ILSVRC10, [2]) has been introduced."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "5 Top 1 BoW[1] Sift LibLinear 1,000 80 91 BoW Sift MeanSGD 1,000 72 86 BoW+SPM rgSift MeanSGD 8,000 59 76 LLC+SPM rgSift MeanSGD 21,000 50 69 Fisher vector rgSift MeanSGD 32,768 43 61 LLC+SPM, Fisher rgSift MeanSGD 53,768 38 57 Fisher+SPM [3] Sift, Color SGD 262,144 34 LLC,SVC+SPM [2] Hog, Lbp ASGD 1,179,648 28 47"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Large Scale Visual Recognition Challenge"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "NUS-WIDE [5] is significantly larger with 270k images and over 5k unique tags but contains ground truth for only 81 categories."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nuswide: A real-world web image database from national university of singapore"
            },
            "venue": {
                "fragments": [],
                "text": "CIVR"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "In order to allow for a sufficient range of experiments on the ILSVRC10 dataset, we require an image representation that is both powerful enough to achieve good performance and reasonably sized to support efficient learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "3a and 3b, Tab."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 186
                            }
                        ],
                        "text": "We evaluate knowledge transfer on the recently proposed ImageNet data set [7], specifically, on the associated ImageNet Large Scale Visual Recognition Competition 2010 (ILSVRC10) subset [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Large Scale Visual Recognition Challenge 2010. www.image-net.org/challenges/LSVRC"
            },
            "venue": {
                "fragments": [],
                "text": "Large Scale Visual Recognition Challenge 2010. www.image-net.org/challenges/LSVRC"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 236
                            }
                        ],
                        "text": "BoW [2] Sift LibLinear 1,000 80 91 BoW Sift MeanSGD 1,000 72 86 BoW + SPM rgSift MeanSGD 8,000 59 76 LLC + SPM rgSift MeanSGD 21,000 50 69 Fisher vector rgSift MeanSGD 32,768 43 61 LLC+SPM, Fisher rgSift MeanSGD 53,768 38 57 Fisher+SPM [25] Sift, Color SGD 262,144 34 \u2013 LLC,SVC+SPM [16] Hog, Lbp ASGD 1,179,648 28 47"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 147
                            }
                        ],
                        "text": "In an attempt to regulate the performance-runtime tradeoff, we explore different combinations of techniques used by the best performing approaches [25, 16] such as spatial pyramid matching (SPM [15]), locality-constrained"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improved Fisher Vector for Large Scale Image Classification"
            },
            "venue": {
                "fragments": [],
                "text": "www.imagenet.org/challenges/LSVRC/2010/ILSVRC2010 XRCE.pdf"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 153
                            }
                        ],
                        "text": "BoW: bag of visual words, SPM: spatial pyramid matching [15], LLC: locality-constrained linear coding [30], Fisher vector [19], SVC: Super-Vector Coding [31], Lbp: local binary patterns, SGD: stochastic gradient decent [3], ASGD: averaging SGD [16]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kai Yu and T"
            },
            "venue": {
                "fragments": [],
                "text": "S. Huang. Image classification using super-vector coding of local image descriptors. In ECCV"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 239
                            }
                        ],
                        "text": "5 Top 1 BoW[1] Sift LibLinear 1,000 80 91 BoW Sift MeanSGD 1,000 72 86 BoW+SPM rgSift MeanSGD 8,000 59 76 LLC+SPM rgSift MeanSGD 21,000 50 69 Fisher vector rgSift MeanSGD 32,768 43 61 LLC+SPM, Fisher rgSift MeanSGD 53,768 38 57 Fisher+SPM [3] Sift, Color SGD 262,144 34 LLC,SVC+SPM [2] Hog, Lbp ASGD 1,179,648 28 47"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improved Fisher Vector for Large Scale"
            },
            "venue": {
                "fragments": [],
                "text": "Image Classification,"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Large - scale image classification : fast feature extraction and SVM training Semantic hierarchies for visual object recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improved Fisher Vector for Large Scale Image Classification. www.imagenet .org/challenges/LSVRC"
            },
            "venue": {
                "fragments": [],
                "text": "Improved Fisher Vector for Large Scale Image Classification. www.imagenet .org/challenges/LSVRC"
            },
            "year": 2010
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 13
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 39,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Evaluating-knowledge-transfer-and-zero-shot-in-a-Rohrbach-Stark/c30b9fb837e912ccf3919fdb64e9543fca57799e?sort=total-citations"
}