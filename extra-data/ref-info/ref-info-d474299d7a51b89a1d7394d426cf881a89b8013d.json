{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 18
                            }
                        ],
                        "text": "We adopt from the Valiant model of learining [28] the demands that learning algorithms be efficient and general in the sense that they perform well for a wide class of p-concepts and for any distribution over the domain."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 520,
                                "start": 513
                            }
                        ],
                        "text": "We say that cg is learnable with a model of probability (respectively, learnable with a decision rule) if there is an algorithm A such that for any target p-concept c ~ cg, for any target distribution D over X, for any inputs e > 0, 6 > 0, and 7 > 0, algorithm A, given access to EX, halts and with probability at least 1 - 6 outputs a p-concept h that is an (e, 7)-good model of probability (respectively, an e-good decision rule) for c with respect to D. Note that this model of learning p-concepts generalizes Valiant's model for learning deterministic concepts."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 7
                            }
                        ],
                        "text": "In the Valiant model, Blumer et al. [5] show that it suffices for learning to find a consistent hypothesis that is slightly shorter than the sample data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 14
                            }
                        ],
                        "text": "For instance, Valiant [28] describes such an algorithm for learning k-CNF (the class of Boolean formulas consisting of a conjunction of clauses, each a disjunction of at most k literals)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 139
                            }
                        ],
                        "text": "Various techniques in this regard have been developed in the Valiant model, such as those of Pitt and Valiant [22], and Kearns and Valiant [17, 15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 19
                            }
                        ],
                        "text": "This is done using Valiant's algorithm [28], here denoted V, for learning monomials from positive examples only in the distribution-free deterministic model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 170
                            }
                        ],
                        "text": "However, we show that p-concept classes of infinite quadratic loss dimension may sometimes be learned efficiently, in contrast to classes of infinite VC-dimension in the Valiant model, which are not learnable in any amount of time."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 155
                            }
                        ],
                        "text": "The primary contribution of this research is that of providing initial positive results for efficient learnability in a natural and important extension to Valiant's model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 203
                            }
                        ],
                        "text": "Here we have several motivations: first, it is of philosophical interest to investigate the most general conditions under which learning is equivalent to some form of data compression; second, as in the Valiant model, we hope that Occam's razor will help isolate and simplify the probabilistic analysis of learning algorithms; third, Occam's razor may be easier to apply than uniform-convergence methods in the case that the pseudo dimension is unknown or difficult to compute; and fourth, Occam's razor may give better sample-size bounds than direct analyses."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 36
                            }
                        ],
                        "text": "This may be significant because the Valiant model has been criticized for its strong hardness results and drought of powerful positive results, as well as for the unrealistic deterministic and noise-free view it takes of the concepts to be learned."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 18
                            }
                        ],
                        "text": "We adopt from the Valiant model for learning deterministic concepts 1-28] the emphasis on learning algorithms that are both efficient (in the sense of polynomial time) and general (in the sense of working for the largest possible p-concept classes and against any probability distribution over the domain)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 409,
                                "start": 402
                            }
                        ],
                        "text": "We then show that the quadratic loss dimension, when finite, is also a lower bound on the required sample size for learning any p-concept class with a model of probability; thus the quadratic loss dimension, when finite, characterizes the sample complexity of p-concept learning with a model of probability in the same way that the Vapnik-Chervonenkis (VC) dimension characterizes sample complexity in Valiant's model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 23
                            }
                        ],
                        "text": "In contrast, since the Valiant model tends to emphasize concept classes based on standard circuit complexity, one is quickly led to study very powerful and apparently difficult classes such as disjunctive normal form Boolean expressions."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5437238,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a54657b8de38a18f30fd154d713f9522f705166c",
            "isKey": false,
            "numCitedBy": 236,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis is a study of the computational complexity of machine learning from examples in the distribution-free model introduced by L. G. Valiant (V84). In the distribution-free model, a learning algorithm receives positive and negative examples of an unknown target set (or concept) that is chosen from some known class of sets (or concept class). These examples are generated randomly according to a fixed but unknown probability distribution representing Nature, and the goal of the learning algorithm is to infer an hypothesis concept that closely approximates the target concept with respect to the unknown distribution. This thesis is concerned with proving theorems about learning in this formal mathematical model. \nWe are interested in the phenomenon of efficient learning in the distribution-free model, in the standard polynomial-time sense. Our results include general tools for determining the polynomial-time learnability of a concept class, an extensive study of efficient learning when errors are present in the examples, and lower bounds on the number of examples required for learning in our model. A centerpiece of the thesis is a series of results demonstrating the computational difficulty of learning a number of well-studied concept classes. These results are obtained by reducing some apparently hard number-theoretic problems from cryptography to the learning problems. The hard-to-learn concept classes include the sets represented by Boolean formulae, deterministic finite automata and a simplified form of neural networks. We also give algorithms for learning powerful concept classes under the uniform distribution, and give equivalences between natural models of efficient learnability. \nThis thesis also includes detailed definitions and motivation for the distribution-free model, a chapter discussing past research in this model and related models, and a short list of important open problems."
            },
            "slug": "Computational-complexity-of-machine-learning-Kearns",
            "title": {
                "fragments": [],
                "text": "Computational complexity of machine learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A centerpiece of the thesis is a series of results demonstrating the computational difficulty of learning a number of well-studied concept classes by reducing some apparently hard number-theoretic problems from cryptography to the learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "ACM distinguished dissertations"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145629164"
                        ],
                        "name": "N. Abe",
                        "slug": "N.-Abe",
                        "structuredName": {
                            "firstName": "Naoki",
                            "lastName": "Abe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Abe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40331700"
                        ],
                        "name": "J. Takeuchi",
                        "slug": "J.-Takeuchi",
                        "structuredName": {
                            "firstName": "Jun\u2019ichi",
                            "lastName": "Takeuchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Takeuchi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42620512,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2434815eaf0c6c6745d01b6b2dd44532b82a498e",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Polynomial-learnability-of-probabilistic-concepts-Abe-Warmuth",
            "title": {
                "fragments": [],
                "text": "Polynomial learnability of probabilistic concepts with respect to the Kullback-Leibler divergence"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '91"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 18
                            }
                        ],
                        "text": "We adopt from the Valiant model of learining [28] the demands that learning algorithms be efficient and general in the sense that they perform well for a wide class of p-concepts and for any distribution over the domain."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 520,
                                "start": 513
                            }
                        ],
                        "text": "We say that cg is learnable with a model of probability (respectively, learnable with a decision rule) if there is an algorithm A such that for any target p-concept c ~ cg, for any target distribution D over X, for any inputs e > 0, 6 > 0, and 7 > 0, algorithm A, given access to EX, halts and with probability at least 1 - 6 outputs a p-concept h that is an (e, 7)-good model of probability (respectively, an e-good decision rule) for c with respect to D. Note that this model of learning p-concepts generalizes Valiant's model for learning deterministic concepts."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 7
                            }
                        ],
                        "text": "In the Valiant model, Blumer et al. [5] show that it suffices for learning to find a consistent hypothesis that is slightly shorter than the sample data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "For instance, Valiant [27] describes such an algorithm for learning k-CNF (the class of Boolean formulas consisting of a conjunction of clauses, each a disjunction of at most k literals)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "We adopt from the Valiant model for learning deterministic concepts [27] the emphasis on learning algorithms that are both e cient (in the sense of polynomial time) and general (in the sense of working for the largest possible p-concept classes and against any probability distribution over the domain)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 14
                            }
                        ],
                        "text": "For instance, Valiant [28] describes such an algorithm for learning k-CNF (the class of Boolean formulas consisting of a conjunction of clauses, each a disjunction of at most k literals)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 19
                            }
                        ],
                        "text": "This is done using Valiant's algorithm [28], here denoted V, for learning monomials from positive examples only in the distribution-free deterministic model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 170
                            }
                        ],
                        "text": "However, we show that p-concept classes of infinite quadratic loss dimension may sometimes be learned efficiently, in contrast to classes of infinite VC-dimension in the Valiant model, which are not learnable in any amount of time."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 155
                            }
                        ],
                        "text": "The primary contribution of this research is that of providing initial positive results for efficient learnability in a natural and important extension to Valiant's model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 203
                            }
                        ],
                        "text": "Here we have several motivations: first, it is of philosophical interest to investigate the most general conditions under which learning is equivalent to some form of data compression; second, as in the Valiant model, we hope that Occam's razor will help isolate and simplify the probabilistic analysis of learning algorithms; third, Occam's razor may be easier to apply than uniform-convergence methods in the case that the pseudo dimension is unknown or difficult to compute; and fourth, Occam's razor may give better sample-size bounds than direct analyses."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 36
                            }
                        ],
                        "text": "This may be significant because the Valiant model has been criticized for its strong hardness results and drought of powerful positive results, as well as for the unrealistic deterministic and noise-free view it takes of the concepts to be learned."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 18
                            }
                        ],
                        "text": "We adopt from the Valiant model for learning deterministic concepts 1-28] the emphasis on learning algorithms that are both efficient (in the sense of polynomial time) and general (in the sense of working for the largest possible p-concept classes and against any probability distribution over the domain)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 409,
                                "start": 402
                            }
                        ],
                        "text": "We then show that the quadratic loss dimension, when finite, is also a lower bound on the required sample size for learning any p-concept class with a model of probability; thus the quadratic loss dimension, when finite, characterizes the sample complexity of p-concept learning with a model of probability in the same way that the Vapnik-Chervonenkis (VC) dimension characterizes sample complexity in Valiant's model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 23
                            }
                        ],
                        "text": "In contrast, since the Valiant model tends to emphasize concept classes based on standard circuit complexity, one is quickly led to study very powerful and apparently difficult classes such as disjunctive normal form Boolean expressions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "We adopt from the Valiantmodel of learning [27] the demands that learning algorithms be e cient and general in the sense that they perform well for a wide class of p-concepts and for any distribution over the domain."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "This is done using Valiant's algorithm [27], here denoted V , for learning monomials from positive examples only in the distribu16"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10ddb646feddc12337b5a755c72e153e37088c02",
            "isKey": true,
            "numCitedBy": 4191,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems."
            },
            "slug": "A-theory-of-the-learnable-Valiant",
            "title": {
                "fragments": [],
                "text": "A theory of the learnable"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper regards learning as the phenomenon of knowledge acquisition in the absence of explicit programming, and gives a precise methodology for studying this phenomenon from a computational viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '84"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47494927"
                        ],
                        "name": "L. Pitt",
                        "slug": "L.-Pitt",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Pitt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Pitt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18940285,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93849122caf1b10c9611eddb707e0720441c73f6",
            "isKey": false,
            "numCitedBy": 541,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "The computational complexity of learning Boolean concepts from examples is investigated. It is shown for various classes of concept representations that these cannot be learned feasibly in a distribution-free sense unless R = NP. These classes include (a) disjunctions of two monomials, (b) Boolean threshold functions, and (c) Boolean formulas in which each variable occurs at most once. Relationships between learning of heuristics and finding approximate solutions to NP-hard optimization problems are given."
            },
            "slug": "Computational-limitations-on-learning-from-examples-Pitt-Valiant",
            "title": {
                "fragments": [],
                "text": "Computational limitations on learning from examples"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "It is shown for various classes of concept representations that these cannot be learned feasibly in a distribution-free sense unless R = NP, and relationships between learning of heuristics and finding approximate solutions to NP-hard optimization problems are given."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722243"
                        ],
                        "name": "N. Linial",
                        "slug": "N.-Linial",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Linial",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Linial"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144830983"
                        ],
                        "name": "Y. Mansour",
                        "slug": "Y.-Mansour",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Mansour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Mansour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113911099"
                        ],
                        "name": "R. Rivest",
                        "slug": "R.-Rivest",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rivest",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rivest"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "(Technically, this is not always true if \\dynamic\" sampling is allowed; see Linial, Mansour and Rivest's paper [19] for further details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12405514,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "907f58a77ef4909d0e96c352cd5c7379eb22d5f5",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of learning a concept from examples in a distribution-free model is considered. The notion of dynamic sampling, wherein the number of examples examined can increase with the complexity of the target concept, is introduced. This method is used to establish the learnability of various concept classes with an infinite Vapnik-Chervonenkis (VC) dimension. An important variation on the problem of learning from examples, called approximating from examples, is also discussed. The problem of computing the VC dimension of a finite concept set defined on a finite domain is considered.<<ETX>>"
            },
            "slug": "Results-on-learnability-and-the-Vapnik-Chervonenkis-Linial-Mansour",
            "title": {
                "fragments": [],
                "text": "Results on learnability and the Vapnik-Chervonenkis dimension"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The notion of dynamic sampling, wherein the number of examples examined can increase with the complexity of the target concept, is introduced and is used to establish the learnability of various concept classes with an infinite Vapnik-Chervonenkis (VC) dimension."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 36671080,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a35203c70c6ed6a95faacd4f1c71a51692af37fb",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The probably approximately correct (PAC) model of learning from examples is generalized. The problem of learning functions from a set X into a set Y is considered, assuming only that the examples are generated by independent draws according to an unknown probability measure on X*Y. The learner's goal is to find a function in a given hypothesis space of functions from X into Y that on average give Y values that are close to those observed in random examples. The discrepancy is measured by a bounded real-valued loss function. The average loss is called the error of the hypothesis. A theorem on the uniform convergence of empirical error estimates to true error rates is given for certain hypothesis spaces, and it is shown how this implies learnability. A generalized notion of VC dimension that applies to classes of real-valued functions and a notion of capacity for classes of functions that map into a bounded metric space are given. These measures are used to bound the rate of convergence of empirical error estimates to true error rates, giving bounds on the sample size needed for learning using hypotheses in these classes. As an application, a distribution-independent uniform convergence result for certain classes of functions computed by feedforward neural nets is obtained. Distribution-specific uniform convergence results for classes of functions that are uniformly continuous on average are also obtained.<<ETX>>"
            },
            "slug": "Generalizing-the-PAC-model:-sample-size-bounds-from-Haussler",
            "title": {
                "fragments": [],
                "text": "Generalizing the PAC model: sample size bounds from metric dimension-based uniform convergence results"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The probably approximately correct (PAC) model of learning from examples is generalized, and a distribution-independent uniform convergence result for certain classes of functions computed by feedforward neural nets is obtained."
            },
            "venue": {
                "fragments": [],
                "text": "30th Annual Symposium on Foundations of Computer Science"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749123"
                        ],
                        "name": "W. Aiello",
                        "slug": "W.-Aiello",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Aiello",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Aiello"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773793"
                        ],
                        "name": "M. Mihail",
                        "slug": "M.-Mihail",
                        "structuredName": {
                            "firstName": "Milena",
                            "lastName": "Mihail",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mihail"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15120986,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aac9a3bc61091886b0771ac923c93f9a2c3e7c10",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We observe that the Linial, Mansour, and Nissan method of learning boolean concepts (under uniform sampling distribution) by reconstructing their Fourier represent ation [LMN89] extends when the concepts are probabilistic in the sense of Kearns and Shapire [KS90]. We show that probabilistic decision lists, and more generally probabilistic decision trees with at most one occurrence of each literal, can be approximate ed by polynomially small Fourier represent ations, and that the non-negligible Fourier coefficients can be efficiently identified and estimated. Hence, all such concepts are learnable in polynomial time under uniform sampling distribution. This is the first instance where Fourier methods result in polynomial learning algorithms: the polynomiality of our results should be contrasted to the np\u201dlylogn complexities in the analogous cases of [LMN89] and [M90]. The new ingredient of our work that allows us to achieve this polynomiality is that via refined Fourier analysis we are able to isolate the polynomially small set of non-negligible Fourier coefficients that reside in a super-polynomially large area of the spectrum. We further observe that several more general concept classes have slightly super-polynomial (npolyk)gn ) learning algorithms. These classes include all polynomial-size probabilistic decision trees, their convex combinations, etc. A concrete special case which results in polynomial learnabil\u201cBdl ColIl]lltl[\\icalioI]s Research, Morristown NJ 07960. aidlo((!fl ash .Ixdlcorc.con]. flkll (bmmnnicat.ions Research, hlorrist.own NJ 07!w0, ]I~illail(@)fl&sll .l}cllcorc. col]). ity is the weighted arithmetization of k-DNF."
            },
            "slug": "Learning-the-Fourier-spectrum-of-probabilistic-and-Aiello-Mihail",
            "title": {
                "fragments": [],
                "text": "Learning the Fourier spectrum of probabilistic lists and trees"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The new ingredient of this work that allows us to achieve this polynomiality is that via refined Fourier analysis the authors are able to isolate the polynomially small set of non-negligible Fourier coefficients that reside in a super-polynomially large area of the spectrum."
            },
            "venue": {
                "fragments": [],
                "text": "SODA '91"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2961287"
                        ],
                        "name": "N. Littlestone",
                        "slug": "N.-Littlestone",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Littlestone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Littlestone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7165930,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e02d1ffa45de336af35ae6fde2e8f6f19d5e50ff",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Equivalence-of-models-for-polynomial-learnability-Haussler-Kearns",
            "title": {
                "fragments": [],
                "text": "Equivalence of models for polynomial learnability"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '88"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2647026"
                        ],
                        "name": "A. Blumer",
                        "slug": "A.-Blumer",
                        "structuredName": {
                            "firstName": "Anselm",
                            "lastName": "Blumer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blumer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683946"
                        ],
                        "name": "A. Ehrenfeucht",
                        "slug": "A.-Ehrenfeucht",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Ehrenfeucht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ehrenfeucht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "'s paper [6] for a full discussion of the VC-dimension."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[6] for learning deterministic concepts."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1138467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0b8fa3496283d4d808fba9ff62d5f024bcf23be",
            "isKey": false,
            "numCitedBy": 1909,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Valiant's learnability model is extended to learning classes of concepts defined by regions in Euclidean space En. The methods in this paper lead to a unified treatment of some of Valiant's results, along with previous results on distribution-free convergence of certain pattern recognition algorithms. It is shown that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned. Using this parameter, the complexity and closure properties of learnable classes are analyzed, and the necessary and sufficient conditions are provided for feasible learnability."
            },
            "slug": "Learnability-and-the-Vapnik-Chervonenkis-dimension-Blumer-Ehrenfeucht",
            "title": {
                "fragments": [],
                "text": "Learnability and the Vapnik-Chervonenkis dimension"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper shows that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6298480,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a769ec3a8fb442548beeafa9b5e0d71661b195ac",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we initiate an investigation of generalizations of the Probably Approximately Correct (PAC) learning model that attempt to signi cantly weaken the target function assumptions. The ultimate goal in this direction is informally termed agnostic learning, in which we make virtually no assumptions on the target function. The name derives from the fact that as designers of learning algorithms, we give up the belief that Nature (as represented by the target function) has a simple or succinct explanation. We give a number of positive and negative results that provide an initial outline of the possibilities for agnostic learning. Our results include hardness results for the most obvious generalization of the PAC model to an agnostic setting, an e cient and general agnostic learning method based on dynamic programming, relationships between loss functions for agnostic learning, and an algorithm for a learning problem that involves hidden variables."
            },
            "slug": "Toward-Eecient-Agnostic-Learning-Schapire",
            "title": {
                "fragments": [],
                "text": "Toward Eecient Agnostic Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Results include hardness results for the most obvious generalization of the PAC model to an agnostic setting, an e cient and general agnostic learning method based on dynamic programming, relationships between loss functions for agnosticLearning, and an algorithm for a learning problem that involves hidden variables."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150652510"
                        ],
                        "name": "Ming Li",
                        "slug": "Ming-Li",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1507349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25875a29eded2acdad72cf897df11c2df2d92ec1",
            "isKey": false,
            "numCitedBy": 396,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper an extension of the distribution-free model of learning introduced by Valiant [Comm. ACM, 27(1984), pp. 1134\u20131142] that allows the presence of malicious errors in the examples given to a learning algorithm is studied. Such errors are generated by an adversary with unbounded computational power and access to the entire history of the learning algorithm\u2019s computation. Thus, a worst-case model of errors is studied.The results of this research include general methods for bounding the rate of error tolerable by any learning algorithm, efficient algorithms tolerating nontrivial rates of malicious errors, and equivalences between problems of learning with errors and standard combinatorial optimization problems."
            },
            "slug": "Learning-in-the-presence-of-malicious-errors-Kearns-Li",
            "title": {
                "fragments": [],
                "text": "Learning in the presence of malicious errors"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "General methods for bounding the rate of error tolerable by any learning algorithm, efficient algorithms tolerating nontrivial rates of malicious errors, and equivalences between problems of learning with errors and standard combinatorial optimization problems are studied."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '88"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 43711678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "656a33c1db546da8490d6eba259e2a849d73a001",
            "isKey": false,
            "numCitedBy": 1012,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "The premise of this article is that learning procedures used to train artificial neural networks are inherently statistical techniques. It follows that statistical theory can provide considerable insight into the properties, advantages, and disadvantages of different network learning methods. We review concepts and analytical results from the literatures of mathematical statistics, econometrics, systems identification, and optimization theory relevant to the analysis of learning in artificial neural networks. Because of the considerable variety of available learning procedures and necessary limitations of space, we cannot provide a comprehensive treatment. Our focus is primarily on learning procedures for feedforward networks. However, many of the concepts and issues arising in this framework are also quite broadly relevant to other network learning paradigms. In addition to providing useful insights, the material reviewed here suggests some potentially useful new training methods for artificial neural networks."
            },
            "slug": "Learning-in-Artificial-Neural-Networks:-A-White",
            "title": {
                "fragments": [],
                "text": "Learning in Artificial Neural Networks: A Statistical Perspective"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Concepts and analytical results from the literatures of mathematical statistics, econometrics, systems identification, and optimization theory relevant to the analysis of learning in artificial neural networks are reviewed."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5222015"
                        ],
                        "name": "R. Sloan",
                        "slug": "R.-Sloan",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Sloan",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sloan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 180
                            }
                        ],
                        "text": "This structured behavior strongly distinguishes these learning scenarios from a \\noisy\" setting, such as the one considered by Angluin and Laird [3], Kearns and Li [16], and Sloan [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 174
                            }
                        ],
                        "text": "This structured behavior strongly distinguishes these learning scenarios from a \"noisy\" setting, such as the one considered by Angluin and Laird [3], Kearns and Li [17], and Sloan [27]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3610395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b9357d47d3f4da2b886baae05afdd8a35b70bee",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The algorithm for pac learning <italic>k</italic>-DNF or <italic>k</italic>-CNF in the presence of malicious attribute noise in polynomial time claimed by Sloan [Slo88] does not work. It is currently open whether such an algorithm exists."
            },
            "slug": "Corrigendum-to-types-of-noise-in-data-for-concept-Sloan",
            "title": {
                "fragments": [],
                "text": "Corrigendum to types of noise in data for concept learning"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "The algorithm for pac learning k-DNF or k-CNF in the presence of malicious attribute noise in polynomial time claimed by Sloan [Slo88] does not work."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2911738"
                        ],
                        "name": "D. Angluin",
                        "slug": "D.-Angluin",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Angluin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Angluin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 1279641,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2bab38d12c4ee82319cc89d16bca21b301a7138",
            "isKey": false,
            "numCitedBy": 666,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The main purpose of this paper is to give techniques for analysing the probabilistic performance of certain kinds of algorithms, and hence to suggest some fast algorithms with provably desirable probabilistic behaviour. The particular problems we consider are: finding Hamiltonian circuits in directed graphs (DHC), finding Hamiltonian circuits in undirected graphs (UHC), and finding perfect matchings in undirected graphs (PM). We show that for each problem there is an algorithm that is extremely fast (0(n(log n)2) for DHC and UHC, and 0(nlog n) for PM), and which with probability tending to one finds a solution in randomly chosen graphs of sufficient density. These results contrast with the known NP-completeness of the first two problems [2,12] and the best worst-case upper bound known of 0(n2.5) for the last [9]."
            },
            "slug": "Fast-probabilistic-algorithms-for-hamiltonian-and-Angluin-Valiant",
            "title": {
                "fragments": [],
                "text": "Fast Probabilistic Algorithms for Hamiltonian Circuits and Matchings"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that for each problem there is an algorithm that is extremely fast, and which with probability tending to one finds a solution in randomly chosen graphs of sufficient density, and the results contrast with the known NP-completeness of the first two problems."
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51378744"
                        ],
                        "name": "J. Lamperti",
                        "slug": "J.-Lamperti",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lamperti",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lamperti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 37016743,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f274a4cee19d93218c209aa07f47abea0598ae91",
            "isKey": false,
            "numCitedBy": 1027,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "It is clear that for given I,un } and t, the better theorem of this kind would be the one in which (2) is proved for the larger class of functions f. In this paper we shall show that certain known \"invariance principles\" can under some hypotheses be improved by considerably enlarging the class of functions for which (2) holds. This will be done by considering spaces S other than the customary ones. For example, in studying convergence to the Wiener process, it is usual to let S be the space (denoted e) of continuous functions with the uniform topology. However, this choice does not fully exploit the pleasant properties of the Wiener path-functions, which are not only continuous but also Holder continuous of any order up to 1/2. Therefore we shall attempt to use spaces Lip5 in place of e as the function-space S. When weak convergence can be established using such spaces, the class of functionals for which (2) is known to hold becomes much larger than before. To carry out the idea sketched above it is necessary to have a criterion which guarantees that the sample functions of a stochastic process are a.s."
            },
            "slug": "ON-CONVERGENCE-OF-STOCHASTIC-PROCESSES-Lamperti",
            "title": {
                "fragments": [],
                "text": "ON CONVERGENCE OF STOCHASTIC PROCESSES"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70416525"
                        ],
                        "name": "W. Hoeffding",
                        "slug": "W.-Hoeffding",
                        "structuredName": {
                            "firstName": "Wassily",
                            "lastName": "Hoeffding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hoeffding"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121341745,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c66db9b93d75c0ff52e7f84605b8389345307006",
            "isKey": false,
            "numCitedBy": 8035,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Upper bounds are derived for the probability that the sum S of n independent random variables exceeds its mean ES by a positive number nt. It is assumed that the range of each summand of S is bounded or bounded above. The bounds for Pr {S \u2013 ES \u2265 nt} depend only on the endpoints of the ranges of the summands and the mean, or the mean and the variance of S. These results are then used to obtain analogous inequalities for certain sums of dependent random variables such as U statistics and the sum of a random sample without replacement from a finite population."
            },
            "slug": "Probability-inequalities-for-sum-of-bounded-random-Hoeffding",
            "title": {
                "fragments": [],
                "text": "Probability Inequalities for sums of Bounded Random Variables"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145608940"
                        ],
                        "name": "A. Kandel",
                        "slug": "A.-Kandel",
                        "structuredName": {
                            "firstName": "Abraham",
                            "lastName": "Kandel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kandel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "(See Kandel's book [14] for a good introduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122076223,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd51ec0c67dd2c91d96ae303f78f3cae1c2523d9",
            "isKey": false,
            "numCitedBy": 332,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fuzzy-techniques-in-pattern-recognition-Kandel",
            "title": {
                "fragments": [],
                "text": "Fuzzy techniques in pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1881615"
                        ],
                        "name": "R. Dudley",
                        "slug": "R.-Dudley",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Dudley",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dudley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 133
                            }
                        ],
                        "text": "We begin with a description of the learning framework that was proposed by Haussler [12], and that extends the work of Pollard [24], Dudley [-10], Vapnik [-29], and others."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "Dudley [10] shows that a d-dimensional linear function space has pseudo dimension d."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 40
                            }
                        ],
                        "text": "Clearly h is in Yf, as is the target c.\nDudley [10] shows that a d-dimensional linear function space has pseudo dimension d. (This is reproved by Haussler [12, Theorem 4].)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 139
                            }
                        ],
                        "text": "We begin with a description of the learning framework that was proposed by Haussler [?], and that extends the work of Pollard [23], Dudley [10], Vapnik [28] and others."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121416923,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "10fd7180b2c0f14e5575b4892e74932b983af822",
            "isKey": true,
            "numCitedBy": 570,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Let $(X, \\mathscr{A}, P)$ be a probability space. Let $X_1, X_2,\\cdots,$ be independent $X$-valued random variables with distribution $P$. Let $P_n := n^{-1}(\\delta_{X_1} + \\cdots + \\delta_{X_n})$ be the empirical measure and let $\\nu_n := n^\\frac{1}{2}(P_n - P)$. Given a class $\\mathscr{C} \\subset \\mathscr{a}$, we study the convergence in law of $\\nu_n$, as a stochastic process indexed by $\\mathscr{C}$, to a certain Gaussian process indexed by $\\mathscr{C}$. If convergence holds with respect to the supremum norm $\\sup_{C \\in \\mathscr{C}}|f(C)|$, in a suitable (usually nonseparable) function space, we call $\\mathscr{C}$ a Donsker class. For measurability, $X$ may be a complete separable metric space, $\\mathscr{a} =$ Borel sets, and $\\mathscr{C}$ a suitable collection of closed sets or open sets. Then for the Donsker property it suffices that for some $m$, and every set $F \\subset X$ with $m$ elements, $\\mathscr{C}$ does not cut all subsets of $F$ (Vapnik-Cervonenkis classes). Another sufficient condition is based on metric entropy with inclusion. If $\\mathscr{C}$ is a sequence $\\{C_m\\}$ independent for $P$, then $\\mathscr{C}$ is a Donsker class if and only if for some $r, \\sigma_m(P(C_m)(1 - P(C_m)))^r < \\infty$."
            },
            "slug": "Central-Limit-Theorems-for-Empirical-Measures-Dudley",
            "title": {
                "fragments": [],
                "text": "Central Limit Theorems for Empirical Measures"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16927,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8142232,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a36b028d024bf358c4af1a5e1dc3ca0aed23b553",
            "isKey": false,
            "numCitedBy": 3710,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady. The paper was first published in Russian as \u0412\u0430\u043f\u043d\u0438\u043a \u0412. \u041d. and \u0427\u0435\u0440\u0432\u043e\u043d\u0435\u043d\u043a\u0438\u0441 \u0410. \u042f. \u041e \u0440\u0430\u0432\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0419 \u0441\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u0447\u0430\u0441\u0442\u043e\u0442 \u043f\u043e\u044f\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043e\u0431\u044b\u0442\u0438\u0419 \u043a \u0438\u0445 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044f\u043c. \u0422\u0435\u043e\u0440\u0438\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0419 \u0438 \u0435\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f 16(2), 264\u2013279 (1971)."
            },
            "slug": "Chervonenkis:-On-the-uniform-convergence-of-of-to-Vapnik",
            "title": {
                "fragments": [],
                "text": "Chervonenkis: On the uniform convergence of relative frequencies of events to their probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "3 can be applied to learn so-called \\t-transform functions\" considered by Mansour [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 74
                            }
                        ],
                        "text": "3 can be applied to learn so-called \"t-transform functions\" considered by Mansour [22]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 84
                            }
                        ],
                        "text": "(Technically, this is not always true if \"dynamic\" sampling is allowed; see Linial, Mansour, and Rivest's paper [20] for further details.)"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning via Fourier transform"
            },
            "venue": {
                "fragments": [],
                "text": "Unpublished manuscript,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 48
                            }
                        ],
                        "text": "I\nFinally, we remark that Kearns, Schapire, and Sellie [ 19] have recently extended this result beyond the class of partially visible monomials to the class of partially visible k-term DNF formulas."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "Finally, we remark that Kearns, Schapire and Sellie [18] have recently extended this result beyond the class of partially visible monomials to the class of partially visible k-term DNF formulas."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 114
                            }
                        ],
                        "text": "Specifically, if f is a k-term DNF formula over a set of hidden and visible variables, then Kearns, Schapire, and Sellie give an efficient algorithm for learning with a model of probability the p-concept induced by regarding f as a probabilistic function over only the visible variables."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Toward e cient agnostic learning"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory,"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69994460"
                        ],
                        "name": "\u698a\u539f \u5eb7\u6587",
                        "slug": "\u698a\u539f-\u5eb7\u6587",
                        "structuredName": {
                            "firstName": "\u698a\u539f",
                            "lastName": "\u5eb7\u6587",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u698a\u539f \u5eb7\u6587"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62114389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52c17eede181f779917ff29c185cd5714641c663",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Algorithmic-learning-of-formal-languages-and-trees-\u698a\u539f",
            "title": {
                "fragments": [],
                "text": "Algorithmic learning of formal languages and decision trees"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109503620"
                        ],
                        "name": "Temple F. Smith",
                        "slug": "Temple-F.-Smith",
                        "structuredName": {
                            "firstName": "Temple",
                            "lastName": "Smith",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Temple F. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[5] show that it su ces for learning to nd a consistent hypothesis that is slightly shorter than the sample data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 64
                            }
                        ],
                        "text": "In this section, we present a generalized form of Occam's Razor [5] applicable to the minimization of bounded loss functions, and in particular to learning p-concepts with a model of probability or a decision rule."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4276691,
            "fieldsOfStudy": [
                "Biology",
                "Medicine"
            ],
            "id": "0b4d43ef0051a225e07af8194e81007ebba8d787",
            "isKey": false,
            "numCitedBy": 705,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Occam's-razor-Smith",
            "title": {
                "fragments": [],
                "text": "Occam's razor"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its applications"
            },
            "venue": {
                "fragments": [],
                "text": "On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its applications"
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Valiant . A theory of the learnable"
            },
            "venue": {
                "fragments": [],
                "text": "Learning decision lists . Machine Learning"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Valiant . Fast probabilistic algorithms for Hamiltonian circuitsand matchings"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Relating data compression and learnability. Unpublished manuscript"
            },
            "venue": {
                "fragments": [],
                "text": "Relating data compression and learnability. Unpublished manuscript"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Relating data compression and learnability, unpublished manuscript"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning via Fourier transform. Unpublished manuscript"
            },
            "venue": {
                "fragments": [],
                "text": "Learning via Fourier transform. Unpublished manuscript"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 351,
                                "start": 339
                            }
                        ],
                        "text": "We then show that the quadratic loss dimension, when finite, is also a lower bound on the required sample size for learning any p-concept class with a model of probability; thus the quadratic loss dimension, when finite, characterizes the sample complexity of p-concept learning with a model of probability in the same way that the Vapnik-Chervonenkis (VC) dimension characterizes sample complexity in Valiant's model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 62
                            }
                        ],
                        "text": "Specifically, we can apply the uniform convergence results of Vapnik and Chervonenkis 1-30] to show that, with high probability, each interval /j has probability at most e7/2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "Speci cally, we can apply the uniform convergence results of Vapnik and Chervonenkis [29] to show that, with high probability, each interval Ij has probability at most =2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Chervonenkis"
            },
            "venue": {
                "fragments": [],
                "text": "On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its applications, XVI(2):264{280,"
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Warmuth . Learn - ability and the Vapnik - Chervonenkis dimension"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Association for Computing Machinery"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Relating data compression and learnability. Unpublished manuscript"
            },
            "venue": {
                "fragments": [],
                "text": "Relating data compression and learnability. Unpublished manuscript"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Central limit theorems for empirical measures. The Annals of Probability"
            },
            "venue": {
                "fragments": [],
                "text": "Central limit theorems for empirical measures. The Annals of Probability"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of Dependences"
            },
            "venue": {
                "fragments": [],
                "text": "Based on Empirical Data. Springer-Verlag,"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Warmuth . Occam \u2019 s razor"
            },
            "venue": {
                "fragments": [],
                "text": "Information Pmess - ing Letters"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalizing the PAC model for neural net and other learning applications"
            },
            "venue": {
                "fragments": [],
                "text": "The Annals of Prubability"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Warmuth . Occam ' srazor"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Computer and System Sciences"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the uniform convergence of relative frequencies of events to their probabilities, Theory Probab"
            },
            "venue": {
                "fragments": [],
                "text": "Appl"
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning decision lists, Mach"
            },
            "venue": {
                "fragments": [],
                "text": "Learning decision lists, Mach"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Warmuth . Learn - ability and the Vapnik - Chervonenkis dimension"
            },
            "venue": {
                "fragments": [],
                "text": "Information Processing Letters"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 28
                            }
                        ],
                        "text": "Finally, Abe, Takeuchi, and Warmuth [, 1 ] have shown that all of these problems are equivalent (modulo polynomial-time computation) to the problem of finding, with high probability and for given 5, a hypothesis with small Kullback-Liebler divergence, i.e., a hypothesis h for which\n[ Ex~D c(x)lg \\h - ' -~ /+ (1 - c(x))lg \\ ~ / j ~  5."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 37
                            }
                        ],
                        "text": ") Finally, Abe, Takeuchi and Warmuth [1] have shown that all of these problems are equivalent (modulo polynomial-time computation) to the problem of nding, with high probability and for given , a hypothesis with small Kullback-Liebler divergence, i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Polynomial learnability of proba-  bilistic concepts with respect to the Kullback-Liebler divergence"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the Fourth  Annual Workshop on Computational Learning Theory,"
            },
            "year": 1991
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 9,
            "methodology": 5
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 42,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Efficient-distribution-free-learning-of-concepts-Kearns-Schapire/d474299d7a51b89a1d7394d426cf881a89b8013d?sort=total-citations"
}