{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145150531"
                        ],
                        "name": "ilkay Ulusoy",
                        "slug": "ilkay-Ulusoy",
                        "structuredName": {
                            "firstName": "ilkay",
                            "lastName": "Ulusoy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "ilkay Ulusoy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792884"
                        ],
                        "name": "Charles M. Bishop",
                        "slug": "Charles-M.-Bishop",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles M. Bishop"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 47
                            }
                        ],
                        "text": "We consider the generative model introduced in [9], which we now briefly describe."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14271284,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59be0bc6397f99386dd6a87b5966735e88948b54",
            "isKey": false,
            "numCitedBy": 192,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Many approaches to object recognition are founded on probability theory, and can be broadly characterized as either generative or discriminative according to whether or not the distribution of the image features is modelled. Generative and discriminative methods have very different characteristics, as well as complementary strengths and weaknesses. In this paper we introduce new generative and discriminative models for object detection and classification based on weakly labelled training data. We use these models to illustrate the relative merits of the two approaches in the context of a data set of widely varying images of non-rigid objects (animals). Our results support the assertion that neither approach alone will be sufficient for large scale object recognition, and we discuss techniques for combining them."
            },
            "slug": "Generative-versus-discriminative-methods-for-object-Ulusoy-Bishop",
            "title": {
                "fragments": [],
                "text": "Generative versus discriminative methods for object recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The results support the assertion that neither generative or discriminative approach alone will be sufficient for large scale object recognition, and the techniques for combining them are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144160673"
                        ],
                        "name": "Alex Holub",
                        "slug": "Alex-Holub",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Holub",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Holub"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14904115,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c3cac0f568ae9261ff9c80eeda55a13e83ae7fb",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Here we explore a discriminative learning method on underlying generative models for the purpose of discriminating between object categories. Visual recognition algorithms learn models from a set of training examples. Generative models learn their representations by considering data from a single class. Generative models are popular in computer vision for many reasons, including their ability to elegantly incorporate prior knowledge and to handle correspondences between object parts and detected features. However, generative models are often inferior to discriminative models during classification tasks. We study a discriminative approach to learning object categories which maintains the representational power of generative learning, but trains the generative models in a discriminative manner. The discriminatively trained models perform better during classification tasks as a result of selecting discriminative sets of features. We conclude by proposing a multi-class object recognition system which initially trains object classes in a generative manner, identifies subsets of similar classes with high confusion, and finally trains models for these subsets in a discriminative manner to realize gains in classification performance."
            },
            "slug": "A-discriminative-framework-for-modelling-object-Holub-Perona",
            "title": {
                "fragments": [],
                "text": "A discriminative framework for modelling object classes"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A discrim inative approach to learning object categories which maintains the representational power of generative learning, but trains the generative models in a discriminative manner to realize gains in classification performance is studied."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684865"
                        ],
                        "name": "Guillaume Bouchard",
                        "slug": "Guillaume-Bouchard",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Bouchard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Bouchard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 25
                            }
                        ],
                        "text": "Recently several authors [2, 3] have proposed hybrids of the generative and discriminative approaches in which a model is trained by optimizing a convex combination of the generative and discriminative log likelihood functions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 123
                            }
                        ],
                        "text": "In an attempt to gain the benefit of both generative and discriminative approaches, heuristic procedure have been proposed [2, 3] which interpolate between these two extremes by taking a convex combination of the generative and discriminative objective functions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 71
                            }
                        ],
                        "text": "This concept of discriminative training has been taken a stage further [2, 3] by maximizing a function given by a convex combination of (9) and (10) of the form"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 94
                            }
                        ],
                        "text": "In particular, there has been much interest in \u2018discriminative training\u2019 of generative models [2, 3, 12] with a view to improving classification accuracy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9821240,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "522e300f2f51e9124057ef597e2f00c4678f287b",
            "isKey": true,
            "numCitedBy": 164,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Given any generative classifier based on an inexact density model, we can define a discriminative counterpart that reduces its asymptotic error rate. We introduce a family of classifiers that interpolate the two approaches, thus providing a new way to compare them and giving an estimation procedure whose classification performance is well balanced between the bias of generative classifiers and the variance of discriminative ones. We show that an intermediate trade-off between the two strategies is often preferable, both theoretically and in experiments on real data."
            },
            "slug": "The-Tradeoff-Between-Generative-and-Discriminative-Bouchard-Triggs",
            "title": {
                "fragments": [],
                "text": "The Tradeoff Between Generative and Discriminative Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A family of classifiers that interpolate the two approaches to classification are introduced, thus providing a new way to compare them and giving an estimation procedure whose classification performance is well balanced between the bias of generative classifiers and the variance of discriminative ones."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2406794"
                        ],
                        "name": "Oksana Yakhnenko",
                        "slug": "Oksana-Yakhnenko",
                        "structuredName": {
                            "firstName": "Oksana",
                            "lastName": "Yakhnenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oksana Yakhnenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809608"
                        ],
                        "name": "A. Silvescu",
                        "slug": "A.-Silvescu",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Silvescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Silvescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145513516"
                        ],
                        "name": "Vasant G Honavar",
                        "slug": "Vasant-G-Honavar",
                        "structuredName": {
                            "firstName": "Vasant",
                            "lastName": "Honavar",
                            "middleNames": [
                                "G"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vasant G Honavar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "In order to improve the predictive performance of generative models it has been proposed to use \u2018discriminative training\u2019 [12] which involves maximizing"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 94
                            }
                        ],
                        "text": "In particular, there has been much interest in \u2018discriminative training\u2019 of generative models [2, 3, 12] with a view to improving classification accuracy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1699609,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5fc3868cc16629e615ebc45a47891dfb109a0fc5",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a discriminative counterpart of the directed Markov Models of order k - 1, or MM(k - 1) for sequence classification. MM(k - 1) models capture dependencies among neighboring elements of a sequence. The parameters of the classifiers are initialized to based on the maximum likelihood estimates for their generative counterparts. We derive gradient based update equations for the parameters of the sequence classifiers in order to maximize the conditional likelihood function. Results of our experiments with data sets drawn from biological sequence classification (specifically protein function and subcellular localization) and text classification applications show that the discriminatively trained sequence classifiers outperform their generative counterparts, confirming the benefits of discriminative training when the primary objective is classification. Our experiments also show that the discriminatively trained MM(k - 1) sequence classifiers are competitive with the computationally much more expensive Support Vector Machines trained using k-gram representations of sequences."
            },
            "slug": "Discriminatively-trained-Markov-model-for-sequence-Yakhnenko-Silvescu",
            "title": {
                "fragments": [],
                "text": "Discriminatively trained Markov model for sequence classification"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Results of the experiments show that the discriminatively trained MM(k - 1) sequence classifiers outperform their generative counterparts, confirming the benefits of discriminative training when the primary objective is classification."
            },
            "venue": {
                "fragments": [],
                "text": "Fifth IEEE International Conference on Data Mining (ICDM'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145659093"
                        ],
                        "name": "S. Kapadia",
                        "slug": "S.-Kapadia",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Kapadia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kapadia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 76
                            }
                        ],
                        "text": "This approach has been widely used in speech recognition with great success [5] where generative hidden Markov models are trained by optimizing the predictive conditional distribution."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53860610,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "963fbb9751e087ab666533042366aad4d919f753",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "Most modern speech recognition systems are based on hidden Markov models. Yet despite their widespread use many of their properties are not well understood. This work aims to increase our understanding about the training of hidden Markov models for classi cation. We rst examine the question of what is the best measure of hidden Markov model tness. Our research shows us that the principle that motivated many of the previous studies, the incorrect-model sub-optimality of maximum likelihood, is wrong. We further show that the identity of the best hidden Markov model tness measure or objective function depends on two outside factors, the model exibility and the size of the training set. These factors control the point at which training set performance or test set generalisation become the limiting factors. We conjecture that the major e ect controlling test set generalisation is the confusion environment in which the state probability density functions are trained. Based on this idea we introduce a new class of hidden Markov model, the frame discriminative hidden Markov model. We focus on zero memory frame discriminative hidden Markov models, these having the same generalisation ability as maximum likelihood hidden Markov models but better training set performance. We also study the optimisation of the frame discrimination objective function. A comparison of traditional learning techniques with modern machine learning ones shows that the machine learning ones are considerably faster. We also show that it is possible to increase the speed of learning by incorporating extra knowledge about the hessian structure of the tness surface. Taking these ideas together we obtain a general purpose and reasonably fast training algorithm, on-line Manhattan quick-prop. We then apply our zero memory frame discriminative objective function and on-line quickprop to two alphabet recognition tasks. The experimental results provide an empirical veri cation of the training set/test set performance of frame discriminative training. The results also show that compared to maximum likelihood hidden Markov models, we can produce considerable reductions in model size with frame discriminative hidden Markov models while maintaining the same accuracy. Lastly we present some ideas for future work."
            },
            "slug": "Discriminative-Training-of-Hidden-Markov-Models-Kapadia",
            "title": {
                "fragments": [],
                "text": "Discriminative Training of Hidden Markov Models"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The research shows that the principle that motivated many of the previous studies, the incorrect-model sub-optimality of maximum likelihood, is wrong and it is shown that the identity of the best hidden Markov model measure or objective function depends on the model exibility and the size of the training set."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52626911"
                        ],
                        "name": "T. Minka",
                        "slug": "T.-Minka",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Minka",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Minka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 195
                            }
                        ],
                        "text": "In this paper we adopt a new perspective which says that there is only one correct way to train a given model, and that a \u2018discriminatively trained\u2019 generative model is fundamentally a new model [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "Following [7] we therefore propose an alternative view of discriminative training, which will lead to an elegant framework for blending generative and discriminative approaches."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "In this paper we develop a novel viewpoint [7] which says that, for a given model, there is a unique likelihood function and hence there is only one correct way to train it."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117959553,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87481ddd8b5ba67639843dd257ff3cfbf8258c65",
            "isKey": true,
            "numCitedBy": 85,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Suppose you are given a dataset of pairs (x, c) where c is a class variable and x is a vector of features. Given a new x, you want to predict its class. The generative i.i.d. approach to this problem posits a model family p(x, c | \u03b8) = p(x | c, \u03bb)p(c | \u03c0) (1) and chooses the best parameters \u03b8 = {\u03bb, \u03c0} by maximizing (or integrating over) the joint distribution (where D denotes the data): p(D, \u03b8) = p(\u03b8) \u220f"
            },
            "slug": "Discriminative-models,-not-discriminative-training-Minka",
            "title": {
                "fragments": [],
                "text": "Discriminative models, not discriminative training"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The generative i.i.d. approach to this problem posits a model family p(x, c | \u03b8) and chooses the best parameters \u03b8 = {\u03bb, \u03c0} by maximizing (or integrating over) the joint distribution (where D denotes the data)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[8] show that logistic regression (the discriminative counterpart of a Naive Bayes generative model) works better than its generative counterpart, but only for a large number of training datapoints (large depending on the complexity of the problem), which confirms the need for using unlabelled data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 296750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90929a6aa901ba958eb4960aeeb594c752e08369",
            "isKey": false,
            "numCitedBy": 2230,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widely-held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observation\u2014which is borne out in repeated experiments\u2014that while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster."
            },
            "slug": "On-Discriminative-vs.-Generative-Classifiers:-A-of-Ng-Jordan",
            "title": {
                "fragments": [],
                "text": "On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "It is shown, contrary to a widely-held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716777"
                        ],
                        "name": "A. Criminisi",
                        "slug": "A.-Criminisi",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Criminisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Criminisi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52626911"
                        ],
                        "name": "T. Minka",
                        "slug": "T.-Minka",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Minka",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Minka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "Our features are taken from [11], in which the original RGB images are first converted into the CIE (L, a, b) colour space [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[11] use a more powerful technique to reduce the number of features, but since this is a supervised method based on fully labelled training data, we"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5893207,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03a073589eaf8ce3440464d020e0d0b26df5869b",
            "isKey": false,
            "numCitedBy": 997,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new algorithm for the automatic recognition of object classes from images (categorization). Compact and yet discriminative appearance-based object class models are automatically learned from a set of training images. The method is simple and extremely fast, making it suitable for many applications such as semantic image retrieval, Web search, and interactive image editing. It classifies a region according to the proportions of different visual words (clusters in feature space). The specific visual words and the typical proportions in each object are learned from a segmented training set. The main contribution of this paper is twofold: i) an optimally compact visual dictionary is learned by pair-wise merging of visual words from an initially large dictionary. The final visual words are described by GMMs. ii) A novel statistical measure of discrimination is proposed which is optimized by each merge operation. High classification accuracy is demonstrated for nine object classes on photographs of real objects viewed under general lighting conditions, poses and viewpoints. The set of test images used for validation comprise: i) photographs acquired by us, ii) images from the Web and iii) images from the recently released Pascal dataset. The proposed algorithm performs well on both texture-rich objects (e.g. grass, sky, trees) and structure-rich ones (e.g. cars, bikes, planes)"
            },
            "slug": "Object-categorization-by-learned-universal-visual-Winn-Criminisi",
            "title": {
                "fragments": [],
                "text": "Object categorization by learned universal visual dictionary"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An optimally compact visual dictionary is learned by pair-wise merging of visual words from an initially large dictionary, and a novel statistical measure of discrimination is proposed which is optimized by each merge operation."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145859952"
                        ],
                        "name": "M. Varma",
                        "slug": "M.-Varma",
                        "structuredName": {
                            "firstName": "Manik",
                            "lastName": "Varma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Varma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 47183337,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "7a33502bddcf2775636c12ea114573431c08085b",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate texture classification from single images obtained under unknown viewpoint and illumination. A statistical approach is developed where textures are modelled by the joint probability distribution of filter responses. This distribution is represented by the frequency histogram of filter response cluster centres (textons). Recognition proceeds from single, uncalibrated images and the novelty here is that rotationally invariant filters are used and the filter response space is low dimensional.Classification performance is compared with the filter banks and methods of Leung and Malik [IJCV, 2001], Schmid [CVPR, 2001] and Cula and Dana [IJCV, 2004] and it is demonstrated that superior performance is achieved here. Classification results are presented for all 61 materials in the Columbia-Utrecht texture database.We also discuss the effects of various parameters on our classification algorithm\u2014such as the choice of filter bank and rotational invariance, the size of the texton dictionary as well as the number of training images used. Finally, we present a method of reliably measuring relative orientation co-occurrence statistics in a rotationally invariant manner, and discuss whether incorporating such information can enhance the classifier\u2019s performance."
            },
            "slug": "A-Statistical-Approach-to-Texture-Classification-Varma-Zisserman",
            "title": {
                "fragments": [],
                "text": "A Statistical Approach to Texture Classification from Single Images"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A method of reliably measuring relative orientation co-occurrence statistics in a rotationally invariant manner is presented, and whether incorporating such information can enhance the classifier\u2019s performance is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2316129"
                        ],
                        "name": "M. Meil\u0103",
                        "slug": "M.-Meil\u0103",
                        "structuredName": {
                            "firstName": "Marina",
                            "lastName": "Meil\u0103",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meil\u0103"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 189
                            }
                        ],
                        "text": "In recent years there has been growing interest in a complementary approach based on generative models, which define a joint distribution p(x, c|\u03b8) over both input vectors and class labels [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117766090,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "60373a16e4088cf6f91ddd03f617474d42cf8927",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Machine-learning:-Discriminative-and-generative-Meil\u0103",
            "title": {
                "fragments": [],
                "text": "Machine learning: Discriminative and generative"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145833095"
                        ],
                        "name": "S. Kothari",
                        "slug": "S.-Kothari",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Kothari",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kothari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681982"
                        ],
                        "name": "H. Oh",
                        "slug": "H.-Oh",
                        "structuredName": {
                            "firstName": "Heekuck",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Oh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 350,
                                "start": 347
                            }
                        ],
                        "text": "In a Bayesian setting the value of this hyper-parameter can therefore be optimized by maximizing the marginal likelihood in which the model parameters have been integrated out, thereby allowing the optimal trade-off between generative and discriminative limits to be determined entirely from the training data without recourse to cross-validation [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 190
                            }
                        ],
                        "text": "However, the correctness of the mathematical derivation of these gradients, as well as their numerical implementation, can easily be verified by comparison against numerical differentiation [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 177751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbc0a468ab103ae29717703d4aa9f682f6a2b664",
            "isKey": false,
            "numCitedBy": 15338,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-Networks-for-Pattern-Recognition-Kothari-Oh",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2415834"
                        ],
                        "name": "J. Kasson",
                        "slug": "J.-Kasson",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Kasson",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kasson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2362910"
                        ],
                        "name": "Wil Plouffe",
                        "slug": "Wil-Plouffe",
                        "structuredName": {
                            "firstName": "Wil",
                            "lastName": "Plouffe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wil Plouffe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 123
                            }
                        ],
                        "text": "Our features are taken from [11], in which the original RGB images are first converted into the CIE (L, a, b) colour space [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17901178,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebccc5ae687d127a7a031408a458a35d953f5307",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Important standards for device-independent color allow many different color encodings. This freedom obliges users of these standards to choose the color space in which to represent their data. A device-independent interchange color space must exhibit an exact mapping to a colorimetric color representation, ability to encode all visible colors, compact representation for given accuracy, and low computational cost for transforms to and from device-dependent spaces. The performance of CIE 1931 XYZ, CIELUV, CIELAB, YES, CCIR 601-2 YCbCr, and SMPTE-C RGB is measured against these requirements. With extensions, all of these spaces can meet the first two requirements. Quantizing error dominates the representational errors of the tested color spaces. Spaces that offer low quantization error also have low gain for image noise. All linear spaces are less compact than nonlinear alternatives. The choice of nonlinearity is not critical; a wide range of gammas yields acceptable results. The choice of primaries for RGB representations is not critical, except that high-chroma primaries should be avoided. Quantizing the components of the candidate spaces with varying precision yields only small improvements. Compatibility with common image data compression techniques leads to the requirement for low luminance contamination, a property that compromises several otherwise acceptable spaces. The conversion of a device-independent representation to popular device spaces by means of trilinear interpolation requires substantially fewer lookup table entries with CCIR 601-2 YCbCr and CIELAB."
            },
            "slug": "An-analysis-of-selected-computer-interchange-color-Kasson-Plouffe",
            "title": {
                "fragments": [],
                "text": "An analysis of selected computer interchange color spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The conversion of a device-independent representation to popular device spaces by means of trilinear interpolation requires substantially fewer lookup table entries with CCIR 601-2 YCbCr and CIELAB."
            },
            "venue": {
                "fragments": [],
                "text": "TOGS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768120"
                        ],
                        "name": "T. Jebara",
                        "slug": "T.-Jebara",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Jebara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jebara"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Our results show that, when the supply of labelled training data is limited, the optimum performance corresponds to a balance between the purely generative and the purely discriminative."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59816026,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "881d8df991fa3d0c6e3b780e21b207f58a20ac03",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Machine-Learning:-Discriminative-and-Generative-in-Jebara",
            "title": {
                "fragments": [],
                "text": "Machine Learning: Discriminative and Generative (Kluwer International Series in Engineering and Computer Science)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 7,
            "methodology": 7
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 13,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Principled-Hybrids-of-Generative-and-Discriminative-Lasserre-Bishop/f5d565d307a746d8bc0feb52c873995af698deca?sort=total-citations"
}