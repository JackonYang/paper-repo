{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 40
                            }
                        ],
                        "text": "Many recent detectors follow this setup [9, 10, 11, 14, 24, 28], including our own work, which uses the Fast R-CNN detector as its staring point [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 114
                            }
                        ],
                        "text": "Modern object detectors predominantly follow the paradigm established by Girshick et al. in their seminal work on Region CNNs [11]: first an object proposal algorithm [17] generates candidate regions that may contain objects, second, a CNN classifies each proposal region."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "in their seminal work on Region CNNs [11]: first an object proposal algorithm [16] generates candidate regions that may contain objects, second, a CNN classifies each proposal region."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[11] in which a CNN is used to classify regions generated by an object proposal algorithm [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 215827080,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "isKey": true,
            "numCitedBy": 17089,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn."
            },
            "slug": "Rich-Feature-Hierarchies-for-Accurate-Object-and-Girshick-Donahue",
            "title": {
                "fragments": [],
                "text": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper proposes a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2032184078"
                        ],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "Modern object detectors predominantly follow the paradigm established by Girshick et al. in their seminal work on Region CNNs [11]: first an object proposal algorithm [17] generates candidate regions that may contain objects, second, a CNN classifies each proposal region."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "More recently, large gains in proposal quality have been achieved through use of CNNs [23, 24, 25, 29]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "Object classification [18, 28, 30] and object detection [10, 27, 29] have rapidly progressed with advancements in convolutional neural networks (CNNs) [20] and the advent of large visual recognition datasets [5, 7, 21]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "*Note: Fast R-CNN and Faster R-CNN results are on test-dev as reported in [24], but results between splits tend to be quite similar."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 204
                            }
                        ],
                        "text": "Deformable part models [8] followed this framework but allowed for more object variability and thus found success across general object categories; likewise, Sermanet et al. [26, 27] showcased the use of CNNs for general object detection in a sliding window fashion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 40
                            }
                        ],
                        "text": "Many recent detectors follow this setup [9, 10, 11, 14, 24, 28], including our own work, which uses the Fast R-CNN detector as its staring point [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "Numerous ideas for exploiting context in CNNs have been proposed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 43
                            }
                        ],
                        "text": "Most recent detectors follow this paradigm [9, 10, 24] and they have achieved rapid and impressive improvements in detection performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "The field has witnessed rapid progress in CNNs in recent years."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 86
                            }
                        ],
                        "text": "More recently, large gains in proposal quality have been achieved through use of CNNs [22, 23, 24, 28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10328909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "isKey": true,
            "numCitedBy": 32562,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available"
            },
            "slug": "Faster-R-CNN:-Towards-Real-Time-Object-Detection-Ren-He",
            "title": {
                "fragments": [],
                "text": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals and further merge RPN and Fast R-CNN into a single network by sharing their convolutionAL features."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2708655"
                        ],
                        "name": "Pedro H. O. Pinheiro",
                        "slug": "Pedro-H.-O.-Pinheiro",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Pinheiro",
                            "middleNames": [
                                "H.",
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro H. O. Pinheiro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "Our implementation of Fast R-CNN [10] with DeepMask object proposals [22] achieves an overall AP of 25."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "Modern object detectors predominantly follow the paradigm established by Girshick et al. in their seminal work on Region CNNs [11]: first an object proposal algorithm [17] generates candidate regions that may contain objects, second, a CNN classifies each proposal region."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "More recently, large gains in proposal quality have been achieved through use of CNNs [23, 24, 25, 29]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 86
                            }
                        ],
                        "text": "We train our MultiPath detector using the recently proposed DeepMask object proposals [22, 23], which, like our model, are well adapted to the COCO dataset."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "Object classification [18, 28, 30] and object detection [10, 27, 29] have rapidly progressed with advancements in convolutional neural networks (CNNs) [20] and the advent of large visual recognition datasets [5, 7, 21]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 32
                            }
                        ],
                        "text": "Switching to DeepMask proposals [22, 23] increases accuracy by a further very substantial 5."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 33
                            }
                        ],
                        "text": "We use DeepMask object proposals [22, 23] and focus exclusively on the recent COCO dataset [20] which presents novel challenges for detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 62
                            }
                        ],
                        "text": "Our MultiPath network, coupled with DeepMask object proposals [22, 23], achieves major gains on COCO detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 204
                            }
                        ],
                        "text": "Deformable part models [8] followed this framework but allowed for more object variability and thus found success across general object categories; likewise, Sermanet et al. [26, 27] showcased the use of CNNs for general object detection in a sliding window fashion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "In this work we use DeepMask segmentation proposals [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "Numerous ideas for exploiting context in CNNs have been proposed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "The field has witnessed rapid progress in CNNs in recent years."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 86
                            }
                        ],
                        "text": "More recently, large gains in proposal quality have been achieved through use of CNNs [22, 23, 24, 28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 140529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b8d0df903496699e52b4daee5d1815b7b784cf7",
            "isKey": false,
            "numCitedBy": 671,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent object detection systems rely on two critical steps: (1) a set of object proposals is predicted as efficiently as possible, and (2) this set of candidate proposals is then passed to an object classifier. Such approaches have been shown they can be fast, while achieving the state of the art in detection performance. In this paper, we propose a new way to generate object proposals, introducing an approach based on a discriminative convolutional network. Our model is trained jointly with two objectives: given an image patch, the first part of the system outputs a class-agnostic segmentation mask, while the second part of the system outputs the likelihood of the patch being centered on a full object. At test time, the model is efficiently applied on the whole test image and generates a set of segmentation masks, each of them being assigned with a corresponding object likelihood score. We show that our model yields significant improvements over state-of-the-art object proposal algorithms. In particular, compared to previous approaches, our model obtains substantially higher object recall using fewer proposals. We also show that our model is able to generalize to unseen categories it has not seen during training. Unlike all previous approaches for generating object masks, we do not rely on edges, superpixels, or any other form of low-level segmentation."
            },
            "slug": "Learning-to-Segment-Object-Candidates-Pinheiro-Collobert",
            "title": {
                "fragments": [],
                "text": "Learning to Segment Object Candidates"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new way to generate object proposals is proposed, introducing an approach based on a discriminative convolutional network that obtains substantially higher object recall using fewer proposals and is able to generalize to unseen categories it has not seen during training."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365442"
                        ],
                        "name": "B. Alexe",
                        "slug": "B.-Alexe",
                        "structuredName": {
                            "firstName": "Bogdan",
                            "lastName": "Alexe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Alexe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1879646"
                        ],
                        "name": "Thomas Deselaers",
                        "slug": "Thomas-Deselaers",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Deselaers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Deselaers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 124
                            }
                        ],
                        "text": "Object Proposals: When originally introduced, object proposals were based on lowlevel grouping cues, edges, and superpixels [1, 2, 16, 31, 33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7316529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2eb6caace8296fd4dfd4947efa4fe911c8e133b2",
            "isKey": false,
            "numCitedBy": 1196,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a generic objectness measure, quantifying how likely it is for an image window to contain an object of any class. We explicitly train it to distinguish objects with a well-defined boundary in space, such as cows and telephones, from amorphous background elements, such as grass and road. The measure combines in a Bayesian framework several image cues measuring characteristics of objects, such as appearing different from their surroundings and having a closed boundary. These include an innovative cue to measure the closed boundary characteristic. In experiments on the challenging PASCAL VOC 07 dataset, we show this new cue to outperform a state-of-the-art saliency measure, and the combined objectness measure to perform better than any cue alone. We also compare to interest point operators, a HOG detector, and three recent works aiming at automatic object segmentation. Finally, we present two applications of objectness. In the first, we sample a small numberof windows according to their objectness probability and give an algorithm to employ them as location priors for modern class-specific object detectors. As we show experimentally, this greatly reduces the number of windows evaluated by the expensive class-specific model. In the second application, we use objectness as a complementary score in addition to the class-specific model, which leads to fewer false positives. As shown in several recent papers, objectness can act as a valuable focus of attention mechanism in many other applications operating on image windows, including weakly supervised learning of object categories, unsupervised pixelwise segmentation, and object tracking in video. Computing objectness is very efficient and takes only about 4 sec. per image."
            },
            "slug": "Measuring-the-Objectness-of-Image-Windows-Alexe-Deselaers",
            "title": {
                "fragments": [],
                "text": "Measuring the Objectness of Image Windows"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A generic objectness measure, quantifying how likely it is for an image window to contain an object of any class, and uses objectness as a complementary score in addition to the class-specific model, which leads to fewer false positives."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060028"
                        ],
                        "name": "D. Eigen",
                        "slug": "D.-Eigen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Eigen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Eigen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46447747"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143949035"
                        ],
                        "name": "Micha\u00ebl Mathieu",
                        "slug": "Micha\u00ebl-Mathieu",
                        "structuredName": {
                            "firstName": "Micha\u00ebl",
                            "lastName": "Mathieu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Micha\u00ebl Mathieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4071727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5507caf1210b6bfbd04fe02b2669bc14292e23a1",
            "isKey": false,
            "numCitedBy": 4353,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
            },
            "slug": "OverFeat:-Integrated-Recognition,-Localization-and-Sermanet-Eigen",
            "title": {
                "fragments": [],
                "text": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This integrated framework for using Convolutional Networks for classification, localization and detection is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 and obtained very competitive results for the detection and classifications tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2475428"
                        ],
                        "name": "Spyros Gidaris",
                        "slug": "Spyros-Gidaris",
                        "structuredName": {
                            "firstName": "Spyros",
                            "lastName": "Gidaris",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Spyros Gidaris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2505902"
                        ],
                        "name": "N. Komodakis",
                        "slug": "N.-Komodakis",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Komodakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Komodakis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "We also compare our foveal approach to the multiregion network [9] which used 10 regions (for a fair comparison, we re-implement it in our setup)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 109
                            }
                        ],
                        "text": "With the reduced number of heads, we can train the network end-to-end rather than each head separately as in [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 75
                            }
                        ],
                        "text": "Right: Our 4-region foveal setup versus the 10 regions used in multiregion [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "Our approach is most related to [9], however, we use just four contextual regions organized in a foveal structure and importantly our classifier is trained jointly end-to-end."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "More recently, [9] proposed to use ten contextual regions around each object with different crops."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 101
                            }
                        ],
                        "text": "To integrate context into our model, we looked at the promising results from the \u2018multiregion\u2019 model [9] for inspiration."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 40
                            }
                        ],
                        "text": "Many recent detectors follow this setup [9, 10, 11, 14, 24, 28], including our own work, which uses the Fast R-CNN detector as its staring point [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 135
                            }
                        ],
                        "text": "Our foveal model can be interpreted as a simplified version of the multiregion model that only uses four regions instead of the ten in [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "Surprisingly, our approach outperforms [9] despite using fewer regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 43
                            }
                        ],
                        "text": "Most recent detectors follow this paradigm [9, 10, 24] and they have achieved rapid and impressive improvements in detection performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 215824235,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e3ea1c7a3c6cc15590ab5e62a171536fe01b9b1c",
            "isKey": true,
            "numCitedBy": 611,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an object detection system that relies on a multi-region deep convolutional neural network (CNN) that also encodes semantic segmentation-aware features. The resulting CNN-based representation aims at capturing a diverse set of discriminative appearance factors and exhibits localization sensitivity that is essential for accurate object localization. We exploit the above properties of our recognition module by integrating it on an iterative localization mechanism that alternates between scoring a box proposal and refining its location with a deep CNN regression model. Thanks to the efficient use of our modules, we detect objects with very high localization accuracy. On the detection challenges of PASCAL VOC2007 and PASCAL VOC2012 we achieve mAP of 78.2% and 73.9% correspondingly, surpassing any other published work by a significant margin."
            },
            "slug": "Object-Detection-via-a-Multi-region-and-Semantic-Gidaris-Komodakis",
            "title": {
                "fragments": [],
                "text": "Object Detection via a Multi-region and Semantic Segmentation-Aware CNN Model"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "An object detection system that relies on a multi-region deep convolutional neural network that also encodes semantic segmentation-aware features that aims at capturing a diverse set of discriminative appearance factors and exhibits localization sensitivity that is essential for accurate object localization."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2708655"
                        ],
                        "name": "Pedro H. O. Pinheiro",
                        "slug": "Pedro-H.-O.-Pinheiro",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Pinheiro",
                            "middleNames": [
                                "H.",
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro H. O. Pinheiro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33493200"
                        ],
                        "name": "Tsung-Yi Lin",
                        "slug": "Tsung-Yi-Lin",
                        "structuredName": {
                            "firstName": "Tsung-Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "Modern object detectors predominantly follow the paradigm established by Girshick et al. in their seminal work on Region CNNs [11]: first an object proposal algorithm [17] generates candidate regions that may contain objects, second, a CNN classifies each proposal region."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "More recently, large gains in proposal quality have been achieved through use of CNNs [23, 24, 25, 29]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 86
                            }
                        ],
                        "text": "We train our MultiPath detector using the recently proposed DeepMask object proposals [22, 23], which, like our model, are well adapted to the COCO dataset."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "Object classification [18, 28, 30] and object detection [10, 27, 29] have rapidly progressed with advancements in convolutional neural networks (CNNs) [20] and the advent of large visual recognition datasets [5, 7, 21]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 32
                            }
                        ],
                        "text": "Switching to DeepMask proposals [22, 23] increases accuracy by a further very substantial 5."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 33
                            }
                        ],
                        "text": "We use DeepMask object proposals [22, 23] and focus exclusively on the recent COCO dataset [20] which presents novel challenges for detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 62
                            }
                        ],
                        "text": "Our MultiPath network, coupled with DeepMask object proposals [22, 23], achieves major gains on COCO detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 204
                            }
                        ],
                        "text": "Deformable part models [8] followed this framework but allowed for more object variability and thus found success across general object categories; likewise, Sermanet et al. [26, 27] showcased the use of CNNs for general object detection in a sliding window fashion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "Numerous ideas for exploiting context in CNNs have been proposed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 96
                            }
                        ],
                        "text": "Effective localization of small objects requires higher-resolution features from earlier layers [3, 13, 21, 23, 25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "The field has witnessed rapid progress in CNNs in recent years."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "Specifically, we used an early version of the improved variant of DeepMask described in [23] that includes top-down refinement but is based on the VGG-A architecture [27], not the later ResNet architecture presented in [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 86
                            }
                        ],
                        "text": "More recently, large gains in proposal quality have been achieved through use of CNNs [22, 23, 24, 28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15278025,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "361b19d2c00d086fa8ef860374f5e1d862fd2f30",
            "isKey": false,
            "numCitedBy": 727,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Object segmentation requires both object-level information and low-level pixel data. This presents a challenge for feedforward networks: lower layers in convolutional nets capture rich spatial information, while upper layers encode object-level knowledge but are invariant to factors such as pose and appearance. In this work we propose to augment feedforward nets for object segmentation with a novel top-down refinement approach. The resulting bottom-up/top-down architecture is capable of efficiently generating high-fidelity object masks. Similarly to skip connections, our approach leverages features at all layers of the net. Unlike skip connections, our approach does not attempt to output independent predictions at each layer. Instead, we first output a coarse \u2018mask encoding\u2019 in a feedforward pass, then refine this mask encoding in a top-down pass utilizing features at successively lower layers. The approach is simple, fast, and effective. Building on the recent DeepMask network for generating object proposals, we show accuracy improvements of 10\u201320% in average recall for various setups. Additionally, by optimizing the overall network architecture, our approach, which we call SharpMask, is 50 % faster than the original DeepMask network (under .8 s per image)."
            },
            "slug": "Learning-to-Refine-Object-Segments-Pinheiro-Lin",
            "title": {
                "fragments": [],
                "text": "Learning to Refine Object Segments"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes to augment feedforward nets for object segmentation with a novel top-down refinement approach that is capable of efficiently generating high-fidelity object masks and is 50 % faster than the original DeepMask network."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144828948"
                        ],
                        "name": "Scott E. Reed",
                        "slug": "Scott-E.-Reed",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Reed",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott E. Reed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1838674"
                        ],
                        "name": "Dragomir Anguelov",
                        "slug": "Dragomir-Anguelov",
                        "structuredName": {
                            "firstName": "Dragomir",
                            "lastName": "Anguelov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dragomir Anguelov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17718521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4328ec9d98eff5d7eb70997f76d81b27849f3220",
            "isKey": false,
            "numCitedBy": 314,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Current high-quality object detection approaches use the scheme of salience-based object proposal methods followed by post-classification using deep convolutional features. This spurred recent research in improving object proposal methods. However, domain agnostic proposal generation has the principal drawback that the proposals come unranked or with very weak ranking, making it hard to trade-off quality for running time. This raises the more fundamental question of whether high-quality proposal generation requires careful engineering or can be derived just from data alone. We demonstrate that learning-based proposal methods can effectively match the performance of hand-engineered methods while allowing for very efficient runtime-quality trade-offs. Using the multi-scale convolutional MultiBox (MSC-MultiBox) approach, we substantially advance the state-of-the-art on the ILSVRC 2014 detection challenge data set, with $0.5$ mAP for a single model and $0.52$ mAP for an ensemble of two models. MSC-Multibox significantly improves the proposal quality over its predecessor MultiBox~method: AP increases from $0.42$ to $0.53$ for the ILSVRC detection challenge. Finally, we demonstrate improved bounding-box recall compared to Multiscale Combinatorial Grouping with less proposals on the Microsoft-COCO data set."
            },
            "slug": "Scalable,-High-Quality-Object-Detection-Szegedy-Reed",
            "title": {
                "fragments": [],
                "text": "Scalable, High-Quality Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is demonstrated that learning-based proposal methods can effectively match the performance of hand-engineered methods while allowing for very efficient runtime-quality trade-offs."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2536361"
                        ],
                        "name": "J. Hosang",
                        "slug": "J.-Hosang",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Hosang",
                            "middleNames": [
                                "Hendrik"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hosang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798000"
                        ],
                        "name": "Rodrigo Benenson",
                        "slug": "Rodrigo-Benenson",
                        "structuredName": {
                            "firstName": "Rodrigo",
                            "lastName": "Benenson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rodrigo Benenson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14742646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c016579af5becc230fb9efc1f885f2afa65a46e",
            "isKey": false,
            "numCitedBy": 621,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "Current top performing object detectors employ detection proposals to guide the search for objects, thereby avoiding exhaustive sliding window search across images. Despite the popularity and widespread use of detection proposals, it is unclear which trade-offs are made when using them during object detection. We provide an in-depth analysis of twelve proposal methods along with four baselines regarding proposal repeatability, ground truth annotation recall on PASCAL, ImageNet, and MS COCO, and their impact on DPM, R-CNN, and Fast R-CNN detection performance. Our analysis shows that for object detection improving proposal localisation accuracy is as important as improving recall. We introduce a novel metric, the average recall (AR), which rewards both high recall and good localisation and correlates surprisingly well with detection performance. Our findings show common strengths and weaknesses of existing methods, and provide insights and metrics for selecting and tuning proposal methods."
            },
            "slug": "What-Makes-for-Effective-Detection-Proposals-Hosang-Benenson",
            "title": {
                "fragments": [],
                "text": "What Makes for Effective Detection Proposals?"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An in-depth analysis of twelve proposal methods along with four baselines regarding proposal repeatability, ground truth annotation recall on PASCAL, ImageNet, and MS COCO, and their impact on DPM, R-CNN, and Fast R- CNN detection performance shows that for object detection improving proposal localisation accuracy is as important as improving recall."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790580"
                        ],
                        "name": "Bharath Hariharan",
                        "slug": "Bharath-Hariharan",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Hariharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bharath Hariharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778133"
                        ],
                        "name": "Pablo Arbel\u00e1ez",
                        "slug": "Pablo-Arbel\u00e1ez",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Arbel\u00e1ez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pablo Arbel\u00e1ez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 81
                            }
                        ],
                        "text": "Such \u2018skip\u2019 architectures have recently become popular for semantic segmentation [13, 21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 96
                            }
                        ],
                        "text": "Effective localization of small objects requires higher-resolution features from earlier layers [3, 13, 21, 23, 25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12225766,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "428db42e86f6d51292e23fa57797e35cecd0e2ee",
            "isKey": false,
            "numCitedBy": 1355,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognition algorithms based on convolutional networks (CNNs) typically use the output of the last layer as a feature representation. However, the information in this layer may be too coarse spatially to allow precise localization. On the contrary, earlier layers may be precise in localization but will not capture semantics. To get the best of both worlds, we define the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel. Using hypercolumns as pixel descriptors, we show results on three fine-grained localization tasks: simultaneous detection and segmentation [22], where we improve state-of-the-art from 49.7 mean APr [22] to 60.0, keypoint localization, where we get a 3.3 point boost over [20], and part labeling, where we show a 6.6 point gain over a strong baseline."
            },
            "slug": "Hypercolumns-for-object-segmentation-and-Hariharan-Arbel\u00e1ez",
            "title": {
                "fragments": [],
                "text": "Hypercolumns for object segmentation and fine-grained localization"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Using hypercolumns as pixel descriptors, this work defines the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel, and shows results on three fine-grained localization tasks: simultaneous detection and segmentation, and keypoint localization."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48527910"
                        ],
                        "name": "P. Doll\u00e1r",
                        "slug": "P.-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060858809"
                        ],
                        "name": "Ron Appel",
                        "slug": "Ron-Appel",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Appel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron Appel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 57
                            }
                        ],
                        "text": "Until recently, the sliding window paradigm was dominant [6, 32], especially for face and pedestrian detection."
                    },
                    "intents": []
                }
            ],
            "corpusId": 219903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84e0d68e41788644c78cfdc3f4ac3cbea7854a5c",
            "isKey": false,
            "numCitedBy": 1707,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "Multi-resolution image features may be approximated via extrapolation from nearby scales, rather than being computed explicitly. This fundamental insight allows us to design object detection algorithms that are as accurate, and considerably faster, than the state-of-the-art. The computational bottleneck of many modern detectors is the computation of features at every scale of a finely-sampled image pyramid. Our key insight is that one may compute finely sampled feature pyramids at a fraction of the cost, without sacrificing performance: for a broad family of features we find that features computed at octave-spaced scale intervals are sufficient to approximate features on a finely-sampled pyramid. Extrapolation is inexpensive as compared to direct feature computation. As a result, our approximation yields considerable speedups with negligible loss in detection accuracy. We modify three diverse visual recognition systems to use fast feature pyramids and show results on both pedestrian detection (measured on the Caltech, INRIA, TUD-Brussels and ETH data sets) and general object detection (measured on the PASCAL VOC). The approach is general and is widely applicable to vision algorithms requiring fine-grained multi-scale analysis. Our approximation is valid for images with broad spectra (most natural images) and fails for images with narrow band-pass spectra (e.g., periodic textures)."
            },
            "slug": "Fast-Feature-Pyramids-for-Object-Detection-Doll\u00e1r-Appel",
            "title": {
                "fragments": [],
                "text": "Fast Feature Pyramids for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "For a broad family of features, this work finds that features computed at octave-spaced scale intervals are sufficient to approximate features on a finely-sampled pyramid, and this approximation yields considerable speedups with negligible loss in detection accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3304536"
                        ],
                        "name": "Jifeng Dai",
                        "slug": "Jifeng-Dai",
                        "structuredName": {
                            "firstName": "Jifeng",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jifeng Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 0
                            }
                        ],
                        "text": "[3, 4, 15]), most previous object detection work has focused on the PASCAL [7] and ImageNet [5] detection datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8510667,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e9b1f6061ef779e3ad0819c2832a29168eaeb9d",
            "isKey": false,
            "numCitedBy": 984,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic segmentation research has recently witnessed rapid progress, but many leading methods are unable to identify object instances. In this paper, we present Multitask Network Cascades for instance-aware semantic segmentation. Our model consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects. These networks form a cascaded structure, and are designed to share their convolutional features. We develop an algorithm for the nontrivial end-to-end training of this causal, cascaded structure. Our solution is a clean, single-step training framework and can be generalized to cascades that have more stages. We demonstrate state-of-the-art instance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our method takes only 360ms testing an image using VGG-16, which is two orders of magnitude faster than previous systems for this challenging problem. As a by product, our method also achieves compelling object detection results which surpass the competitive Fast/Faster R-CNN systems. The method described in this paper is the foundation of our submissions to the MS COCO 2015 segmentation competition, where we won the 1st place."
            },
            "slug": "Instance-Aware-Semantic-Segmentation-via-Multi-task-Dai-He",
            "title": {
                "fragments": [],
                "text": "Instance-Aware Semantic Segmentation via Multi-task Network Cascades"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This paper presents Multitask Network Cascades for instance-aware semantic segmentation, which consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects, and develops an algorithm for the nontrivial end-to-end training of this causal, cascaded structure."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33493200"
                        ],
                        "name": "Tsung-Yi Lin",
                        "slug": "Tsung-Yi-Lin",
                        "structuredName": {
                            "firstName": "Tsung-Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145854440"
                        ],
                        "name": "M. Maire",
                        "slug": "M.-Maire",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Maire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "The COCO dataset [20] contains objects at multiple scales, in context and among clutter, and under frequent occlusion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "1Going forward, we use the notation introduced by the COCO dataset [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Recently, the COCO dataset [20] was introduced to push object detection to more challenging settings."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 208
                            }
                        ],
                        "text": "Object classification [18, 27, 29] and object detection [10, 26, 28] have rapidly progressed with advancements in convolutional neural networks (CNNs) [19] and the advent of large visual recognition datasets [5, 7, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "We use DeepMask object proposals [22, 23] and focus exclusively on the recent COCO dataset [20] which presents novel challenges for detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "On the other hand, the COCO evaluation metric [20] averages AP across IoU thresholds between 50 and 95, awarding a higher AP for higher-overlap bounding boxes1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14113767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71b7178df5d2b112d07e45038cb5637208659ff7",
            "isKey": false,
            "numCitedBy": 19779,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model."
            },
            "slug": "Microsoft-COCO:-Common-Objects-in-Context-Lin-Maire",
            "title": {
                "fragments": [],
                "text": "Microsoft COCO: Common Objects in Context"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding by gathering images of complex everyday scenes containing common objects in their natural context."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 128
                            }
                        ],
                        "text": "Dropout & Weight Decay: Dropout helped regularize training and we keep the same dropout value of 0.5 that was used for training VGG-D."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 35
                            }
                        ],
                        "text": "In our work we use variants of the VGG network [28], specifically VGG-A for DeepMask and VGG-D for our MultiPath network."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "First, 4 images are propagated through the VGG-D network trunk, in parallel with 1 image per GPU."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "For all following experiments, Fast R-CNN [10] serves as our baseline detector (with VGG-D [28] features pre-trained on ImageNet [5])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 22
                            }
                        ],
                        "text": "Object classification [18, 27, 29] and object detection [10, 26, 28] have rapidly progressed with advancements in convolutional neural networks (CNNs) [19] and the advent of large visual recognition datasets [5, 7, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "As in Fast R-CNN, the VGG-D network [27] (pretrained on ImageNet [5]) is applied to each input image and RoI-pooling is used to extract features for each object proposal."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "The much deeper VGG [28] and GoogleNet [30] models further pushed accuracy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "Fast R-CNN performs RoI-pooling after the VGG-D conv5 layer."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 147
                            }
                        ],
                        "text": "Specifically, we used an early version of the improved variant of DeepMask described in [24] that includes top-down refinement but is based on the VGG-A architecture [28], not the later ResNet architecture presented in [15]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "The much deeper VGG [27] and GoogleNet [29] models further pushed accuracy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "As in Fast R-CNN, the VGG-D network [28] (pretrained on ImageNet [5]) is applied to each input image and RoI-pooling is used to extract features for each object proposal."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "C V\nVGG16 Trunk\nBBox Regression conv1\nconv2\nconv3\nconv4\nconv5 Classifier\n& Regression Features (4096x4)\nFoveal Region 1 VGG16 Classifier\nfc6fc5\nROI (1x)\nFoveal Region 2\nROI (1.5x) VGG16 Classifier\nfc6fc5\nFoveal Region 3\nROI (2x) VGG16 Classifier\nfc6fc5\nFovel Region 4\nROI (4x) VGG16 Classifier\nfc6fc5\nClass Scores\n(81)\nClassifier @ IoU 0.50\nClassifier @ IoU 0.55\nClassifier @ IoU 0.60\nClassifier @ IoU 0.65\nClassifier @ IoU 0.70 Classifier @ IoU 0.75 Bbox\nRegression (324)Input image\n+\nFigure 1: Proposed MultiPath architecture."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "In our work we use variants of the VGG network [27], specifically VGG-A for DeepMask and VGG-D for our MultiPath network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "For all following experiments, Fast R-CNN [10] serves as our baseline detector (with VGG-D [27] features pre-trained on ImageNet [5])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 166
                            }
                        ],
                        "text": "Specifically, we used an early version of the improved variant of DeepMask described in [23] that includes top-down refinement but is based on the VGG-A architecture [27], not the later ResNet architecture presented in [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14124313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb42cf88027de515750f230b23b1a057dc782108",
            "isKey": true,
            "numCitedBy": 62223,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."
            },
            "slug": "Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman",
            "title": {
                "fragments": [],
                "text": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work investigates the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting using an architecture with very small convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782282"
                        ],
                        "name": "Evan Shelhamer",
                        "slug": "Evan-Shelhamer",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Shelhamer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evan Shelhamer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117314646"
                        ],
                        "name": "Jonathan Long",
                        "slug": "Jonathan-Long",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Long",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 81
                            }
                        ],
                        "text": "Such \u2018skip\u2019 architectures have recently become popular for semantic segmentation [13, 21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 96
                            }
                        ],
                        "text": "Effective localization of small objects requires higher-resolution features from earlier layers [3, 13, 21, 23, 25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1629541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "317aee7fc081f2b137a85c4f20129007fd8e717e",
            "isKey": false,
            "numCitedBy": 15652,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build \u201cfully convolutional\u201d networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image."
            },
            "slug": "Fully-Convolutional-Networks-for-Semantic-Shelhamer-Long",
            "title": {
                "fragments": [],
                "text": "Fully Convolutional Networks for Semantic Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is shown that convolutional networks by themselves, trained end- to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144499674"
                        ],
                        "name": "Sean Bell",
                        "slug": "Sean-Bell",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "Bell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sean Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144374926"
                        ],
                        "name": "K. Bala",
                        "slug": "K.-Bala",
                        "structuredName": {
                            "firstName": "Kavita",
                            "lastName": "Bala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[3] proposed to revisit skip connections for general object detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 0
                            }
                        ],
                        "text": "[3, 4, 15]), most previous object detection work has focused on the PASCAL [7] and ImageNet [5] detection datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "Our own implementation of skip connections closely resembles [3]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 96
                            }
                        ],
                        "text": "Effective localization of small objects requires higher-resolution features from earlier layers [3, 13, 21, 23, 25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "from conv3, conv4, and conv5 layers in the same manner as described in [3] and provide this as input to each foveal classifier, as illustrated in Figure 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "Note that [3] used weight decay but not dropout, so perhaps it is sufficient to have just one form of regularization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7148278,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "adc4e63b58cf4092420533fd877b8c29f8e2ec1d",
            "isKey": true,
            "numCitedBy": 905,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "It is well known that contextual and multi-scale representations are important for accurate visual recognition. In this paper we present the Inside-Outside Net (ION), an object detector that exploits information both inside and outside the region of interest. Contextual information outside the region of interest is integrated using spatial recurrent neural networks. Inside, we use skip pooling to extract information at multiple scales and levels of abstraction. Through extensive experiments we evaluate the design space and provide readers with an overview of what tricks of the trade are important. ION improves state-of-the-art on PASCAL VOC 2012 object detection from 73.9% to 77.9% mAP. On the new and more challenging MS COCO dataset, we improve state-of-the-art from 19.7% to 33.1% mAP. In the 2015 MS COCO Detection Challenge, our ION model won \"Best Student Entry\" and finished 3rd place overall. As intuition suggests, our detection results provide strong evidence that context and multi-scale representations improve small object detection."
            },
            "slug": "Inside-Outside-Net:-Detecting-Objects-in-Context-Bell-Zitnick",
            "title": {
                "fragments": [],
                "text": "Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The Inside-Outside Net (ION), an object detector that exploits information both inside and outside the region of interest, provides strong evidence that context and multi-scale representations improve small object detection."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2918391"
                        ],
                        "name": "Yodsawalai Chodpathumwan",
                        "slug": "Yodsawalai-Chodpathumwan",
                        "structuredName": {
                            "firstName": "Yodsawalai",
                            "lastName": "Chodpathumwan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yodsawalai Chodpathumwan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2279233"
                        ],
                        "name": "Qieyun Dai",
                        "slug": "Qieyun-Dai",
                        "structuredName": {
                            "firstName": "Qieyun",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qieyun Dai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "75 [.706] C50 [.768] Loc [.784] Sim [.816] Oth [.944] BG [1.00] FN (i) large Figure 6: Detailed analysis of detector performance on unseen COCO validation images at select settings (plots in style of [16] generated by COCO API code). (a) Removing localization errors would lead to an AP10 of 58.8 on COCO (\u2018Loc\u2019). Removing similar and other class confusion (\u2018Sim\u2019 and \u2018Oth\u2019) would only lead to slight imp"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6650709,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f877575217f0868f496a6123954b2ea933fb21e",
            "isKey": true,
            "numCitedBy": 444,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows how to analyze the influences of object characteristics on detection performance and the frequency and impact of different types of false positives. In particular, we examine effects of occlusion, size, aspect ratio, visibility of parts, viewpoint, localization error, and confusion with semantically similar objects, other labeled objects, and background. We analyze two classes of detectors: the Vedaldi et al. multiple kernel learning detector and different versions of the Felzenszwalb et al. detector. Our study shows that sensitivity to size, localization error, and confusion with similar objects are the most impactful forms of error. Our analysis also reveals that many different kinds of improvement are necessary to achieve large gains, making more detailed analysis essential for the progress of recognition research. By making our software and annotations available, we make it effortless for future researchers to perform similar analysis."
            },
            "slug": "Diagnosing-Error-in-Object-Detectors-Hoiem-Chodpathumwan",
            "title": {
                "fragments": [],
                "text": "Diagnosing Error in Object Detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper shows how to analyze the influences of object characteristics on detection performance and the frequency and impact of different types of false positives, and shows that sensitivity to size, localization error, and confusion with similar objects are the most impactful forms of error."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 101
                            }
                        ],
                        "text": "Except for concurrent work (e.g. [3, 4, 15]), most previous object detection work has focused on the PASCAL [7] and ImageNet [5] detection datasets."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 208
                            }
                        ],
                        "text": "Object classification [18, 27, 29] and object detection [10, 26, 28] have rapidly progressed with advancements in convolutional neural networks (CNNs) [19] and the advent of large visual recognition datasets [5, 7, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 130
                            }
                        ],
                        "text": "Specifically, we use AP to denote AP averaged across IoU values from 50 to 95, and APu to denote AP at IoU threshold u (e.g., the PASCAL metric is denoted by AP50)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "In PASCAL [7] and ImageNet [5], the scoring metric only considers whether the detection bounding box has intersection over union (IoU) overlap greater than 50 with the ground truth."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 75
                            }
                        ],
                        "text": "[3, 4, 15]), most previous object detection work has focused on the PASCAL [7] and ImageNet [5] detection datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 96
                            }
                        ],
                        "text": "We set the scale to 800px which improves AP by 0.5 points over the 600px scale used by [10] for PASCAL."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4246903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82635fb63640ae95f90ee9bdc07832eb461ca881",
            "isKey": true,
            "numCitedBy": 11690,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension."
            },
            "slug": "The-Pascal-Visual-Object-Classes-(VOC)-Challenge-Everingham-Gool",
            "title": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The state-of-the-art in evaluated methods for both classification and detection are reviewed, whether the methods are statistically different, what they are learning from the images, and what the methods find easy or confuse."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 124
                            }
                        ],
                        "text": "Object Proposals: When originally introduced, object proposals were based on lowlevel grouping cues, edges, and superpixels [1, 2, 16, 31, 33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5984060,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b183947ee15718b45546eda6b01e179b9a95421f",
            "isKey": false,
            "numCitedBy": 2411,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of object proposals is an effective recent approach for increasing the computational efficiency of object detection. We propose a novel method for generating object bounding box proposals using edges. Edges provide a sparse yet informative representation of an image. Our main observation is that the number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. We propose a simple box objectness score that measures the number of edges that exist in the box minus those that are members of contours that overlap the box\u2019s boundary. Using efficient data structures, millions of candidate boxes can be evaluated in a fraction of a second, returning a ranked set of a few thousand top-scoring proposals. Using standard metrics, we show results that are significantly more accurate than the current state-of-the-art while being faster to compute. In particular, given just 1000 proposals we achieve over 96% object recall at overlap threshold of 0.5 and over 75% recall at the more challenging overlap of 0.7. Our approach runs in 0.25 seconds and we additionally demonstrate a near real-time variant with only minor loss in accuracy."
            },
            "slug": "Edge-Boxes:-Locating-Object-Proposals-from-Edges-Zitnick-Doll\u00e1r",
            "title": {
                "fragments": [],
                "text": "Edge Boxes: Locating Object Proposals from Edges"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel method for generating object bounding box proposals using edges is proposed, showing results that are significantly more accurate than the current state-of-the-art while being faster to compute."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2127604"
                        ],
                        "name": "Soumith Chintala",
                        "slug": "Soumith-Chintala",
                        "structuredName": {
                            "firstName": "Soumith",
                            "lastName": "Chintala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soumith Chintala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[25] used two contextual regions centered on each object for pedestrian detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 96
                            }
                        ],
                        "text": "Effective localization of small objects requires higher-resolution features from earlier layers [3, 13, 21, 23, 25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[25] proposed to use a \u2018multi-stage\u2019 classifier that used features at many convolutional layers for pedestrian detection, showing improved results."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[25, 26] showcased the use of CNNs for general object detection in a sliding window fashion."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1342687,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1306ce652f556fbb9e794d91084a29294298e6d",
            "isKey": false,
            "numCitedBy": 773,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-the-art and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage."
            },
            "slug": "Pedestrian-Detection-with-Unsupervised-Multi-stage-Sermanet-Kavukcuoglu",
            "title": {
                "fragments": [],
                "text": "Pedestrian Detection with Unsupervised Multi-stage Feature Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work reports state-of-the-art and competitive results on all major pedestrian datasets with a convolutional network model that uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Compared to the baseline Fast R-CNN detector [10] with Selective Search proposals [31], which achieves an AP of 19."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "The original loss L used in Fast R-CNN [10] is given by:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 56
                            }
                        ],
                        "text": "Object classification [18, 27, 29] and object detection [10, 26, 28] have rapidly progressed with advancements in convolutional neural networks (CNNs) [19] and the advent of large visual recognition datasets [5, 7, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "Our system is based on the Fast R-CNN framework [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 180
                            }
                        ],
                        "text": "Overall, we obtain substantial improvements in detection accuracy on COCO by using DeepMask in place of the Selective Search [31] proposals used in the original work on Fast R-CNN [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "In particular, we begin with the Fast R-CNN object detector [10], and test a number of intuitive modifications to explicitly address the unique challenges of this dataset, including small object detection, detection of objects in context, and improved localization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 40
                            }
                        ],
                        "text": "Many recent detectors follow this setup [9, 10, 11, 14, 24, 28], including our own work, which uses the Fast R-CNN detector as its staring point [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "For all following experiments, Fast R-CNN [10] serves as our baseline detector (with VGG-D [27] features pre-trained on ImageNet [5])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 43
                            }
                        ],
                        "text": "Most recent detectors follow this paradigm [9, 10, 24] and they have achieved rapid and impressive improvements in detection performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206770307,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "7ffdbc358b63378f07311e883dddacc9faeeaf4b",
            "isKey": true,
            "numCitedBy": 14073,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn."
            },
            "slug": "Fast-R-CNN-Girshick",
            "title": {
                "fragments": [],
                "text": "Fast R-CNN"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection that builds on previous work to efficiently classify object proposals using deep convolutional networks."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "AP AP50 AP75 APS APM APL AR1 AR10 AR100 ARS ARM ARL ResNet [15] 27."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[15] introduced the even deeper Residual Networks (ResNet) that have greatly improved the state of the art and have also proven effective for object detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "Only the deeper ResNet classifier [15] outperformed our approach."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "Only the deeper ResNet classifier [15] outperformed our approach (and potentially ResNet could be integrated as the feature extractor in our MultiPath network, leading to further gains)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 0
                            }
                        ],
                        "text": "[3, 4, 15]), most previous object detection work has focused on the PASCAL [7] and ImageNet [5] detection datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 219
                            }
                        ],
                        "text": "Specifically, we used an early version of the improved variant of DeepMask described in [23] that includes top-down refinement but is based on the VGG-A architecture [27], not the later ResNet architecture presented in [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 3
                            }
                        ],
                        "text": "AP AP50 AP75 APS APM APL AR1 AR10 AR100 ARS ARM ARL\nResNet [15] 27.9 51.2 27.6 8.6 30.2 45.3 25.4 37.1 38.0 16.6 43.3 57.8 MultiPath 25.0 45.4 24.5 7.2 28.8 39.0 23.8 36.6 38.5 17.0 46.7 53.5 1026 16.7 31.5 15.9 1.8 17.2 30.2 17.2 24.0 24.6 4.5 28.2 40.3 ResNet [15] 37.1 58.8 39.8 17.3 41.5 52.5 31.9 47.5 48.9 26.7 55.2 67.9 MultiPath 33.2 51.9 36.3 13.6 37.2 47.8 29.9 46.0 48.3 23.4 56.0 66.4 ION [3] 30.7 52.9 31.7 11.8 32.8 44.8 27.7 42.8 45.4 23.0 50.1 63.0 Fast R-CNN* [10] 19.3 39.3 19.9 3.5 18.8 34.6 21.4 29.5 29.8 7.7 32.2 50.2 Faster R-CNN* [25] 21.9 42.7 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014\nTable 3: Top: COCO test-standard segmentation results."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": true,
            "numCitedBy": 95326,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "Deformable part models [8] followed this framework but allowed for more object variability and thus found success across general object categories; likewise, Sermanet et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3198903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e79272fe3d65197100eae8be9fec6469107969ae",
            "isKey": false,
            "numCitedBy": 9374,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function."
            },
            "slug": "Object-Detection-with-Discriminatively-Trained-Part-Felzenszwalb-Girshick",
            "title": {
                "fragments": [],
                "text": "Object Detection with Discriminatively Trained Part Based Models"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An object detection system based on mixtures of multiscale deformable part models that is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 22
                            }
                        ],
                        "text": "Object classification [18, 27, 29] and object detection [10, 26, 28] have rapidly progressed with advancements in convolutional neural networks (CNNs) [19] and the advent of large visual recognition datasets [5, 7, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "The introduction of AlexNet [18] reinvigorated the use of deep learning for visual recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195908774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "isKey": false,
            "numCitedBy": 80947,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
            },
            "slug": "ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever",
            "title": {
                "fragments": [],
                "text": "ImageNet classification with deep convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called \"dropout\" that proved to be very effective."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 40
                            }
                        ],
                        "text": "Many recent detectors follow this setup [9, 10, 11, 14, 24, 28], including our own work, which uses the Fast R-CNN detector as its staring point [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "Specifically, we perform two ROI-poolings: the first follows [14] and uses the floor and ceil operations for determining the RoI region, the second uses the round operation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[14] implement context in a more implicit way by aggregating CNN features prior to classification using different sized pooling regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 436933,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cbb19236820a96038d000dc629225d36e0b6294a",
            "isKey": false,
            "numCitedBy": 4774,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224<inline-formula><tex-math>$\\times$ </tex-math><alternatives><inline-graphic xlink:type=\"simple\" xlink:href=\"he-ieq1-2389824.gif\"/></alternatives></inline-formula>224) input image. This requirement is \u201cartificial\u201d and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102 <inline-formula><tex-math>$\\times$</tex-math><alternatives><inline-graphic xlink:type=\"simple\" xlink:href=\"he-ieq2-2389824.gif\"/> </alternatives></inline-formula> faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition."
            },
            "slug": "Spatial-Pyramid-Pooling-in-Deep-Convolutional-for-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work equips the networks with another pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement, and develops a new network structure, called SPP-net, which can generate a fixed-length representation regardless of image size/scale."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111328101"
                        ],
                        "name": "Michael Jones",
                        "slug": "Michael-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 57
                            }
                        ],
                        "text": "Until recently, the sliding window paradigm was dominant [6, 32], especially for face and pedestrian detection."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2796017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b09ec0b350f8352bce46a2f5bf7ae97c83a7b9ca",
            "isKey": false,
            "numCitedBy": 11227,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second."
            },
            "slug": "Robust-Real-Time-Face-Detection-Viola-Jones",
            "title": {
                "fragments": [],
                "text": "Robust Real-Time Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new image representation called the \u201cIntegral Image\u201d is introduced which allows the features used by the detector to be computed very quickly and a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157222093"
                        ],
                        "name": "Wei Liu",
                        "slug": "Wei-Liu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144828948"
                        ],
                        "name": "Scott E. Reed",
                        "slug": "Scott-E.-Reed",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Reed",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott E. Reed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1838674"
                        ],
                        "name": "Dragomir Anguelov",
                        "slug": "Dragomir-Anguelov",
                        "structuredName": {
                            "firstName": "Dragomir",
                            "lastName": "Anguelov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dragomir Anguelov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39863668"
                        ],
                        "name": "Andrew Rabinovich",
                        "slug": "Andrew-Rabinovich",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Rabinovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Rabinovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 29
                            }
                        ],
                        "text": "The much deeper VGG [28] and GoogleNet [30] models further pushed accuracy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 22
                            }
                        ],
                        "text": "Object classification [18, 27, 29] and object detection [10, 26, 28] have rapidly progressed with advancements in convolutional neural networks (CNNs) [19] and the advent of large visual recognition datasets [5, 7, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "The much deeper VGG [27] and GoogleNet [29] models further pushed accuracy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206592484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e15cf50aa89fee8535703b9f9512fca5bfc43327",
            "isKey": false,
            "numCitedBy": 29482,
            "numCiting": 278,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection."
            },
            "slug": "Going-deeper-with-convolutions-Szegedy-Liu",
            "title": {
                "fragments": [],
                "text": "Going deeper with convolutions"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A deep convolutional neural network architecture codenamed Inception is proposed that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14)."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778133"
                        ],
                        "name": "Pablo Arbel\u00e1ez",
                        "slug": "Pablo-Arbel\u00e1ez",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Arbel\u00e1ez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pablo Arbel\u00e1ez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403171438"
                        ],
                        "name": "J. Pont-Tuset",
                        "slug": "J.-Pont-Tuset",
                        "structuredName": {
                            "firstName": "Jordi",
                            "lastName": "Pont-Tuset",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pont-Tuset"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065306202"
                        ],
                        "name": "Jonathan T. Barron",
                        "slug": "Jonathan-T.-Barron",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Barron",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan T. Barron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34854725"
                        ],
                        "name": "F. Marqu\u00e9s",
                        "slug": "F.-Marqu\u00e9s",
                        "structuredName": {
                            "firstName": "Ferran",
                            "lastName": "Marqu\u00e9s",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Marqu\u00e9s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 124
                            }
                        ],
                        "text": "Object Proposals: When originally introduced, object proposals were based on lowlevel grouping cues, edges, and superpixels [1, 2, 16, 31, 33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4517687,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83d1118c2b2995a3e0cf9b6159e4c59e85cabb7e",
            "isKey": false,
            "numCitedBy": 1039,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a unified approach for bottom-up hierarchical image segmentation and object candidate generation for recognition, called Multiscale Combinatorial Grouping (MCG). For this purpose, we first develop a fast normalized cuts algorithm. We then propose a high-performance hierarchical segmenter that makes effective use of multiscale information. Finally, we propose a grouping strategy that combines our multiscale regions into highly-accurate object candidates by exploring efficiently their combinatorial space. We conduct extensive experiments on both the BSDS500 and on the PASCAL 2012 segmentation datasets, showing that MCG produces state-of-the-art contours, hierarchical regions and object candidates."
            },
            "slug": "Multiscale-Combinatorial-Grouping-Arbel\u00e1ez-Pont-Tuset",
            "title": {
                "fragments": [],
                "text": "Multiscale Combinatorial Grouping"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work first develops a fast normalized cuts algorithm, then proposes a high-performance hierarchical segmenter that makes effective use of multiscale information, and proposes a grouping strategy that combines the authors' multiscales regions into highly-accurate object candidates by exploring efficiently their combinatorial space."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847596"
                        ],
                        "name": "Wei Dong",
                        "slug": "Wei-Dong",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94451829"
                        ],
                        "name": "K. Li",
                        "slug": "K.-Li",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 208
                            }
                        ],
                        "text": "Object classification [18, 27, 29] and object detection [10, 26, 28] have rapidly progressed with advancements in convolutional neural networks (CNNs) [19] and the advent of large visual recognition datasets [5, 7, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "As in Fast R-CNN, the VGG-D network [27] (pretrained on ImageNet [5]) is applied to each input image and RoI-pooling is used to extract features for each object proposal."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "In PASCAL [7] and ImageNet [5], the scoring metric only considers whether the detection bounding box has intersection over union (IoU) overlap greater than 50 with the ground truth."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 92
                            }
                        ],
                        "text": "[3, 4, 15]), most previous object detection work has focused on the PASCAL [7] and ImageNet [5] detection datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 129
                            }
                        ],
                        "text": "For all following experiments, Fast R-CNN [10] serves as our baseline detector (with VGG-D [27] features pre-trained on ImageNet [5])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57246310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b47265245e8db53a553049dcb27ed3e495fd625",
            "isKey": true,
            "numCitedBy": 27407,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond."
            },
            "slug": "ImageNet:-A-large-scale-hierarchical-image-database-Deng-Dong",
            "title": {
                "fragments": [],
                "text": "ImageNet: A large-scale hierarchical image database"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new database called \u201cImageNet\u201d is introduced, a large-scale ontology of images built upon the backbone of the WordNet structure, much larger in scale and diversity and much more accurate than the current image datasets."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143853801"
                        ],
                        "name": "Benjamin Graham",
                        "slug": "Benjamin-Graham",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Graham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Graham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "To maximize accuracy prior to submitting to the COCO leaderboard, we added validation data to training, employed horizontal flip and fractional max pooling [12] at inference, and ensembled 6 models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "FMP: Inspired by Fractional Max Pooling [12], we perform multiple RoI-pooling operations with perturbed pooling parameters and average the softmax outputs (note that the network trunk is computed only once)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16452744,
            "fieldsOfStudy": [
                "Computer Science",
                "Economics"
            ],
            "id": "55dda8f230566867acbfaa7bdd08fd8c7b8721ed",
            "isKey": false,
            "numCitedBy": 433,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional networks almost always incorporate some form of spatial pooling, and very often it is max-pooling with = 2. Max-pooling act on the hidden layers of the network, reducing their size by an integer multiplicative factor . The amazing by product of discarding 75% of your data is that you build into the network a degree of invariance with respect to translations and elastic distortions. However, if you simply alternate convolutional layers with max-pooling layers, performance is limited due to the rapid reduction in spatial size, and the disjoint nature of the pooling regions. We have formulated a fractional version of max-pooling where is allowed to take non-integer values. Our version of maxpooling is stochastic as there are lots of different ways of constructing suitable pooling regions. We find that our form of fractional max-pooling reduces overfitting on a variety of datasets: for instance, we improve on the state of the art for CIFAR-100 without even using dropout."
            },
            "slug": "Fractional-Max-Pooling-Graham",
            "title": {
                "fragments": [],
                "text": "Fractional Max-Pooling"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The form of fractional max-pooling formulated is found to reduce overfitting on a variety of datasets: for instance, it improves on the state of the art for CIFAR-100 without even using dropout."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "However, as discussed, context is known to play an important role in object recognition [30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "Context: Context is known to play an important role in visual recognition [30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1073705,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c99f2391b956dc189855541e49e53c21ae5ec603",
            "isKey": false,
            "numCitedBy": 888,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "There is general consensus that context can be a rich source of information about an object's identity, location and scale. In fact, the structure of many real-world scenes is governed by strong configurational rules akin to those that apply to a single object. Here we introduce a simple framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains. The resulting scheme serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes."
            },
            "slug": "Contextual-Priming-for-Object-Detection-Torralba",
            "title": {
                "fragments": [],
                "text": "Contextual Priming for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A simple framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "Object classification [18, 27, 29] and object detection [10, 26, 28] have rapidly progressed with advancements in convolutional neural networks (CNNs) [19] and the advent of large visual recognition datasets [5, 7, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35262,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "Both data and model parallelism are used in training [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5556470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80d800dfadbe2e6c7b2367d9229cc82912d55889",
            "isKey": false,
            "numCitedBy": 883,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks."
            },
            "slug": "One-weird-trick-for-parallelizing-convolutional-Krizhevsky",
            "title": {
                "fragments": [],
                "text": "One weird trick for parallelizing convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A new way to parallelize the training of convolutional neural networks across multiple GPUs is presented, which scales significantly better than all alternatives when applied to modern convolutionAL neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079216774"
                        ],
                        "name": "Aaas News",
                        "slug": "Aaas-News",
                        "structuredName": {
                            "firstName": "Aaas",
                            "lastName": "News",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaas News"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49572302"
                        ],
                        "name": "E. Lu",
                        "slug": "E.-Lu",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152177137"
                        ],
                        "name": "Min-Min Zhou",
                        "slug": "Min-Min-Zhou",
                        "structuredName": {
                            "firstName": "Min-Min",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min-Min Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2089616909"
                        ],
                        "name": "Rong Mocsai",
                        "slug": "Rong-Mocsai",
                        "structuredName": {
                            "firstName": "Rong",
                            "lastName": "Mocsai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rong Mocsai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145950435"
                        ],
                        "name": "A. Myers",
                        "slug": "A.-Myers",
                        "structuredName": {
                            "firstName": "Attila",
                            "lastName": "Myers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Myers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153511341"
                        ],
                        "name": "E. Huang",
                        "slug": "E.-Huang",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069414766"
                        ],
                        "name": "B. Jackson",
                        "slug": "B.-Jackson",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Jackson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Jackson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1974998016"
                        ],
                        "name": "Davide Ferrari",
                        "slug": "Davide-Ferrari",
                        "structuredName": {
                            "firstName": "Davide",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Davide Ferrari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2062865615"
                        ],
                        "name": "V. Tybulewicz",
                        "slug": "V.-Tybulewicz",
                        "structuredName": {
                            "firstName": "V",
                            "lastName": "Tybulewicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Tybulewicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1820879863"
                        ],
                        "name": "V. Lowell",
                        "slug": "V.-Lowell",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lowell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lowell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081138031"
                        ],
                        "name": "Clifford A Lepore",
                        "slug": "Clifford-A-Lepore",
                        "structuredName": {
                            "firstName": "Clifford",
                            "lastName": "Lepore",
                            "middleNames": [
                                "A"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Clifford A Lepore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116515431"
                        ],
                        "name": "J. Koretzky",
                        "slug": "J.-Koretzky",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Koretzky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Koretzky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46292705"
                        ],
                        "name": "Gary Kahn",
                        "slug": "Gary-Kahn",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Kahn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gary Kahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2127593366"
                        ],
                        "name": "M. L.",
                        "slug": "M.-L.",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "L.",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. L."
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143807167"
                        ],
                        "name": "F. Achard",
                        "slug": "F.-Achard",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Achard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Achard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1491458517"
                        ],
                        "name": "H. D. Eva",
                        "slug": "H.-D.-Eva",
                        "structuredName": {
                            "firstName": "Hugh",
                            "lastName": "Eva",
                            "middleNames": [
                                "Douglas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. D. Eva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083204118"
                        ],
                        "name": "Ernst-Detlef See Also Schulze",
                        "slug": "Ernst-Detlef-See-Also-Schulze",
                        "structuredName": {
                            "firstName": "Ernst-Detlef",
                            "lastName": "See Also Schulze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ernst-Detlef See Also Schulze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5329613"
                        ],
                        "name": "J. Acharya",
                        "slug": "J.-Acharya",
                        "structuredName": {
                            "firstName": "Jairaj",
                            "lastName": "Acharya",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Acharya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49185789"
                        ],
                        "name": "U. Acharya",
                        "slug": "U.-Acharya",
                        "structuredName": {
                            "firstName": "Usha",
                            "lastName": "Acharya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Acharya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49185789"
                        ],
                        "name": "U. Acharya",
                        "slug": "U.-Acharya",
                        "structuredName": {
                            "firstName": "Usha",
                            "lastName": "Acharya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Acharya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2125129322"
                        ],
                        "name": "Shetal Patel",
                        "slug": "Shetal-Patel",
                        "structuredName": {
                            "firstName": "Shetal",
                            "lastName": "Patel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shetal Patel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6398536"
                        ],
                        "name": "E. Koundakjian",
                        "slug": "E.-Koundakjian",
                        "structuredName": {
                            "firstName": "E",
                            "lastName": "Koundakjian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Koundakjian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4943028"
                        ],
                        "name": "K. Nagashima",
                        "slug": "K.-Nagashima",
                        "structuredName": {
                            "firstName": "Kunio",
                            "lastName": "Nagashima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nagashima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144714836"
                        ],
                        "name": "Xianlin Han",
                        "slug": "Xianlin-Han",
                        "structuredName": {
                            "firstName": "Xianlin",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xianlin Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5329613"
                        ],
                        "name": "J. Acharya",
                        "slug": "J.-Acharya",
                        "structuredName": {
                            "firstName": "Jairaj",
                            "lastName": "Acharya",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Acharya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118745841"
                        ],
                        "name": "D. L. Adams",
                        "slug": "D.-L.-Adams",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Adams",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. L. Adams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2086038579"
                        ],
                        "name": "Jonathan C And Horton",
                        "slug": "Jonathan-C-And-Horton",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "And Horton",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan C And Horton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88229803"
                        ],
                        "name": "Blood",
                        "slug": "Blood",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Blood",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Blood"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49185181"
                        ],
                        "name": "M. Adams",
                        "slug": "M.-Adams",
                        "structuredName": {
                            "firstName": "Melissa",
                            "lastName": "Adams",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Adams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4557983"
                        ],
                        "name": "M. McVey",
                        "slug": "M.-McVey",
                        "structuredName": {
                            "firstName": "Mitch",
                            "lastName": "McVey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McVey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5909720"
                        ],
                        "name": "J. Sekelsky",
                        "slug": "J.-Sekelsky",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Sekelsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sekelsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20399621"
                        ],
                        "name": "J. Adamson",
                        "slug": "J.-Adamson",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Adamson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Adamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3752234"
                        ],
                        "name": "G. Kochendoerfer",
                        "slug": "G.-Kochendoerfer",
                        "structuredName": {
                            "firstName": "Gerd",
                            "lastName": "Kochendoerfer",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kochendoerfer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7745713"
                        ],
                        "name": "A. W. Adeleke",
                        "slug": "A.-W.-Adeleke",
                        "structuredName": {
                            "firstName": "A",
                            "lastName": "Adeleke",
                            "middleNames": [
                                "W"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. W. Adeleke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079495253"
                        ],
                        "name": "A. See Kamdem-Toham",
                        "slug": "A.-See-Kamdem-Toham",
                        "structuredName": {
                            "firstName": "A",
                            "lastName": "See Kamdem-Toham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. See Kamdem-Toham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076929873"
                        ],
                        "name": "Alan Aderem",
                        "slug": "Alan-Aderem",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Aderem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alan Aderem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152676657"
                        ],
                        "name": "C. Picard",
                        "slug": "C.-Picard",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Picard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Picard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115759360"
                        ],
                        "name": "Aeschlimann",
                        "slug": "Aeschlimann",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Aeschlimann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aeschlimann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4858956"
                        ],
                        "name": "G. Haug",
                        "slug": "G.-Haug",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Haug",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Haug"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065008773"
                        ],
                        "name": "G. Agarwal",
                        "slug": "G.-Agarwal",
                        "structuredName": {
                            "firstName": "Girish",
                            "lastName": "Agarwal",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Agarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33260240"
                        ],
                        "name": "M. Scully",
                        "slug": "M.-Scully",
                        "structuredName": {
                            "firstName": "Marlan",
                            "lastName": "Scully",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Scully"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144699300"
                        ],
                        "name": "H. Aguilaniu",
                        "slug": "H.-Aguilaniu",
                        "structuredName": {
                            "firstName": "Hugo",
                            "lastName": "Aguilaniu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Aguilaniu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46832542"
                        ],
                        "name": "L. Gustafsson",
                        "slug": "L.-Gustafsson",
                        "structuredName": {
                            "firstName": "Lena",
                            "lastName": "Gustafsson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gustafsson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5667177"
                        ],
                        "name": "M. Rigoulet",
                        "slug": "M.-Rigoulet",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Rigoulet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rigoulet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145009924"
                        ],
                        "name": "T. Nystr\u00f6m",
                        "slug": "T.-Nystr\u00f6m",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Nystr\u00f6m",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nystr\u00f6m"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079593476"
                        ],
                        "name": "Asymmetric Inheri",
                        "slug": "Asymmetric-Inheri",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Asymmetric Inheri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Asymmetric Inheri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46954724"
                        ],
                        "name": "Ferhaan Ahmad",
                        "slug": "Ferhaan-Ahmad",
                        "structuredName": {
                            "firstName": "Ferhaan",
                            "lastName": "Ahmad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ferhaan Ahmad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6434784"
                        ],
                        "name": "J. Schmitt",
                        "slug": "J.-Schmitt",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Schmitt",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmitt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144434668"
                        ],
                        "name": "M. Aida",
                        "slug": "M.-Aida",
                        "structuredName": {
                            "firstName": "Misako",
                            "lastName": "Aida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7206169"
                        ],
                        "name": "S. C. Ammal",
                        "slug": "S.-C.-Ammal",
                        "structuredName": {
                            "firstName": "Salai",
                            "lastName": "Ammal",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. C. Ammal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2602073"
                        ],
                        "name": "J. Aizenberg",
                        "slug": "J.-Aizenberg",
                        "structuredName": {
                            "firstName": "Joanna",
                            "lastName": "Aizenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Aizenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3052861"
                        ],
                        "name": "D. Muller",
                        "slug": "D.-Muller",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Muller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Muller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4458474"
                        ],
                        "name": "J. Grazul",
                        "slug": "J.-Grazul",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Grazul",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Grazul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48858545"
                        ],
                        "name": "D. Hamann",
                        "slug": "D.-Hamann",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "Hamann",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hamann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2086018723"
                        ],
                        "name": "J. Ajioka",
                        "slug": "J.-Ajioka",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Ajioka",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ajioka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "91271984"
                        ],
                        "name": "C. Su",
                        "slug": "C.-Su",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Su",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1580391782"
                        ],
                        "name": "A. Akella",
                        "slug": "A.-Akella",
                        "structuredName": {
                            "firstName": "Aravind",
                            "lastName": "Akella",
                            "middleNames": [
                                "B"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Akella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "151103050"
                        ],
                        "name": "M. Alam",
                        "slug": "M.-Alam",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Alam",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Alam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112249315"
                        ],
                        "name": "F. Gao",
                        "slug": "F.-Gao",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4464233"
                        ],
                        "name": "A. Alatas",
                        "slug": "A.-Alatas",
                        "structuredName": {
                            "firstName": "Ahmet",
                            "lastName": "Alatas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Alatas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50169977"
                        ],
                        "name": "H. Sinn",
                        "slug": "H.-Sinn",
                        "structuredName": {
                            "firstName": "Harald",
                            "lastName": "Sinn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sinn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7480073"
                        ],
                        "name": "Titus V. Albu",
                        "slug": "Titus-V.-Albu",
                        "structuredName": {
                            "firstName": "Titus",
                            "lastName": "Albu",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Titus V. Albu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8204857"
                        ],
                        "name": "P. S. Zuev",
                        "slug": "P.-S.-Zuev",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Zuev",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. S. Zuev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402325831"
                        ],
                        "name": "M. Al-Dayeh",
                        "slug": "M.-Al-Dayeh",
                        "structuredName": {
                            "firstName": "Maher",
                            "lastName": "Al-Dayeh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Al-Dayeh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117860354"
                        ],
                        "name": "J. Dwyer",
                        "slug": "J.-Dwyer",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Dwyer",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dwyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114606050"
                        ],
                        "name": "A. Al-ghonaium",
                        "slug": "A.-Al-ghonaium",
                        "structuredName": {
                            "firstName": "Abdulaziz",
                            "lastName": "Al-ghonaium",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Al-ghonaium"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088336541"
                        ],
                        "name": "Sami Al-Hajjar",
                        "slug": "Sami-Al-Hajjar",
                        "structuredName": {
                            "firstName": "Sami",
                            "lastName": "Al-Hajjar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sami Al-Hajjar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1421934622"
                        ],
                        "name": "S. Al-Jumaah",
                        "slug": "S.-Al-Jumaah",
                        "structuredName": {
                            "firstName": "Sulaiman",
                            "lastName": "Al-Jumaah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Al-Jumaah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46591015"
                        ],
                        "name": "A. Allakhverdov",
                        "slug": "A.-Allakhverdov",
                        "structuredName": {
                            "firstName": "Andrei",
                            "lastName": "Allakhverdov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Allakhverdov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144263358"
                        ],
                        "name": "V. Pokrovsky",
                        "slug": "V.-Pokrovsky",
                        "structuredName": {
                            "firstName": "V.",
                            "lastName": "Pokrovsky",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Pokrovsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2074561950"
                        ],
                        "name": "Allen",
                        "slug": "Allen",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Allen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Allen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111025088"
                        ],
                        "name": "A. P. Brown",
                        "slug": "A.-P.-Brown",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Brown",
                            "middleNames": [
                                "P",
                                "See"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115215460"
                        ],
                        "name": "James H. Allen",
                        "slug": "James-H.-Allen",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Allen",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James H. Allen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111025088"
                        ],
                        "name": "A. P. Brown",
                        "slug": "A.-P.-Brown",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Brown",
                            "middleNames": [
                                "P",
                                "See"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2086384291"
                        ],
                        "name": "James H Gillooly",
                        "slug": "James-H-Gillooly",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Gillooly",
                            "middleNames": [
                                "H"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James H Gillooly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056512668"
                        ],
                        "name": "James",
                        "slug": "James",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "James",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 124
                            }
                        ],
                        "text": "Object Proposals: When originally introduced, object proposals were based on lowlevel grouping cues, edges, and superpixels [1, 2, 16, 31, 33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "in their seminal work on Region CNNs [11]: first an object proposal algorithm [16] generates candidate regions that may contain objects, second, a CNN classifies each proposal region."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "[11] in which a CNN is used to classify regions generated by an object proposal algorithm [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15642412,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "3e0e337b477e0c226da00eae03fd29882275a469",
            "isKey": false,
            "numCitedBy": 68901,
            "numCiting": 259,
            "paperAbstract": {
                "fragments": [],
                "text": "Direct fabrication of large micropatterned single crystals. p1205 21 Feb 2003. (news): Academy plucks best biophysicists from a sea of mediocrity. p994 14 Feb 2003."
            },
            "slug": "\u201cA-and-B\u201d:-News-Lu",
            "title": {
                "fragments": [],
                "text": "\u201cA and B\u201d:"
            },
            "venue": {
                "fragments": [],
                "text": "Sophonisba Breckinridge"
            },
            "year": 2019
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "Compared to the baseline Fast R-CNN detector [10] with Selective Search proposals [31], which achieves an AP of 19."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 124
                            }
                        ],
                        "text": "Object Proposals: When originally introduced, object proposals were based on lowlevel grouping cues, edges, and superpixels [1, 2, 16, 31, 33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 33
                            }
                        ],
                        "text": "With the original Fast R-CNN and Selective Search proposals, box regression increases AP by 3.5 points, but with our MultiPath model and DeepMask proposals, box regression only increases AP by 1.1 points."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 83
                            }
                        ],
                        "text": "This is already a large improvement over the original Fast R-CNN results that used Selective Search proposals [32], we will return to this shortly."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 426,
                                "start": 410
                            }
                        ],
                        "text": "AP50 AP base +bb \u2206 base +bb \u2206\nSS + Fast R-CNN 38.2 39.8 +1.6 18.1 21.6 +3.5 SS + MultiPath 38.0 38.5 +0.5 20.9 22.8 +1.9 DM + Fast R-CNN 42.5 43.4 +0.9 23.5 25.2 +1.7 DM + MultiPath 44.5 44.8 +0.3 26.8 27.9 +1.1\nAP50 \u2206 AP \u2206 baseline 44.8 27.9 + trainval 47.5 +2.7 30.2 +2.3 + hflip 48.3 +0.8 30.8 +0.6 + FMP 49.6 +1.3 31.5 +0.7 + ensembling 51.9 +2.3 33.2 +1.7\nTable 2: Left: Bounding box regression is key when using Selective Search (SS) proposals and the Fast R-CNN classifier (our implementation)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 55
                            }
                        ],
                        "text": "Compared to the baseline Fast R-CNN detector [10] with Selective Search proposals [32], which achieves an AP of 19.3, this represents a 66% improvement in performance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 108
                            }
                        ],
                        "text": "Overall, we obtain substantial improvements in detection accuracy on COCO by using DeepMask in place of the Selective Search [32] proposals used in the original work on Fast R-CNN [10]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "Overall, we obtain substantial improvements in detection accuracy on COCO by using DeepMask in place of the Selective Search [31] proposals used in the original work on Fast R-CNN [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 103
                            }
                        ],
                        "text": "Accuracy saturates using 400 DeepMask proposals per image and using \u223c50 DeepMask proposals matches 2000 Selective Search proposals."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 110
                            }
                        ],
                        "text": "This is already a large improvement over the original Fast R-CNN results that used Selective Search proposals [31], we will return to this shortly."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 135
                            }
                        ],
                        "text": "We coupled our resulting MultiPath classifier with DeepMask proposals and achieved a 66% improvement over the baseline Fast R-CNN with Selective Search."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 47
                            }
                        ],
                        "text": "The original implementation of Fast R-CNN with Selective Search proposals [32] has an AP of 19.3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 130
                            }
                        ],
                        "text": "Our results saturate with around 400 DeepMask proposals per image and using just 50 DeepMask proposals matches accuracy with 2000 Selective Search proposals."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "The original implementation of Fast R-CNN with Selective Search proposals [31] has an AP of 19."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Selective search for object recog"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "Classifier: The CNN used for classification forms an integral part of the detection pipeline and is key in determining final detector accuracy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We expect that integration of ResNet into our system could further boost accuracy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "Object classification [18, 28, 30] and object detection [10, 27, 29] have rapidly progressed with advancements in convolutional neural networks (CNNs) [20] and the advent of large visual recognition datasets [5, 7, 21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 134
                            }
                        ],
                        "text": "These three modifications allow information to flow along multiple paths in our network, including through features from multiple network layers and from multiple object views, see Figure 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "Many recent detectors follow this setup [9, 10, 11, 14, 25, 29], including our own work, which uses the Fast R-CNN detector as its staring point [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "Our MultiPath network, coupled with DeepMask object proposals [23, 24], achieves major gains on COCO detection.\nscales, including a high percentage of small objects, (2) objects are less iconic, often in nonstandard configurations and amid clutter or heavy occlusion, and (3) the evaluation metric\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ICCV"
            },
            "venue": {
                "fragments": [],
                "text": "ICCV"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "Modern object detectors predominantly follow the paradigm established by Girshick et al. in their seminal work on Region CNNs [11]: first an object proposal algorithm [17] generates candidate regions that may contain objects, second, a CNN classifies each proposal region."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "More recently, large gains in proposal quality have been achieved through use of CNNs [23, 24, 25, 29]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [28], in addition to region specific features, features from the entire image are used to improve region classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 56
                            }
                        ],
                        "text": "Object classification [18, 27, 29] and object detection [10, 26, 28] have rapidly progressed with advancements in convolutional neural networks (CNNs) [19] and the advent of large visual recognition datasets [5, 7, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "Object classification [18, 28, 30] and object detection [10, 27, 29] have rapidly progressed with advancements in convolutional neural networks (CNNs) [20] and the advent of large visual recognition datasets [5, 7, 21]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 204
                            }
                        ],
                        "text": "Deformable part models [8] followed this framework but allowed for more object variability and thus found success across general object categories; likewise, Sermanet et al. [26, 27] showcased the use of CNNs for general object detection in a sliding window fashion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 40
                            }
                        ],
                        "text": "Many recent detectors follow this setup [9, 10, 11, 14, 24, 28], including our own work, which uses the Fast R-CNN detector as its staring point [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "Numerous ideas for exploiting context in CNNs have been proposed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "The field has witnessed rapid progress in CNNs in recent years."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 86
                            }
                        ],
                        "text": "More recently, large gains in proposal quality have been achieved through use of CNNs [22, 23, 24, 28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scalable"
            },
            "venue": {
                "fragments": [],
                "text": "high-quality object detection. arXiv:1412.1441"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Going deeper with convolutions Contextual priming for object detection"
            },
            "venue": {
                "fragments": [],
                "text": "IJCV"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Going deeper with convolutions Contextual priming for object detection"
            },
            "venue": {
                "fragments": [],
                "text": "IJCV"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 56
                            }
                        ],
                        "text": "Object classification [18, 27, 29] and object detection [10, 26, 28] have rapidly progressed with advancements in convolutional neural networks (CNNs) [19] and the advent of large visual recognition datasets [5, 7, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[25, 26] showcased the use of CNNs for general object detection in a sliding window fashion."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Overfeat: Integrated recognition"
            },
            "venue": {
                "fragments": [],
                "text": "localization and detection using convolutional networks. In ICLR"
            },
            "year": 2014
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 29,
            "methodology": 20,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 40,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/A-MultiPath-Network-for-Object-Detection-Zagoruyko-Lerer/0aeae528df555192420bd799ca5f31899b64ed59?sort=total-citations"
}