{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748501"
                        ],
                        "name": "Claire Cardie",
                        "slug": "Claire-Cardie",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cardie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Cardie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 774849,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cf6f1209d29c151b693861e083850f1b385c595",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a case-based approach to knowledge acquisition for natural language systems that simultaneously learns part of speech, word sense, and concept activation knowledge for all open class words in a corpus. The parser begins with a lexicon of function words and creates a case base of context-sensitive word definitions during a humansupervised training phase. Then, given an unknown word and the context in which it occurs, the parser retrieves definitions from the case base to infer the word's syntactic and semantic features. By encoding context as part of a definition, the meaning of a word can change dynamically in response to surrounding phrases without the need for explicit lexical disambiguation heuristics. Moreover, the approach acquires all three classes of knowledge using the same case representation and requires relatively little training and no hand-coded knowledge acquisition heuristics. We evaluate it in experiments that explore two of many practical applications of the technique and conclude that the case-based method provides a promising approach to automated dictionary construction and knowledge acquisition for sentence analysis in limited domains. In addition, we present a novel case retrieval algorithm that uses decision trees to improve the performance of a k-nearest neighbor similarity metric."
            },
            "slug": "A-Case-Based-Approach-to-Knowledge-Acquisition-for-Cardie",
            "title": {
                "fragments": [],
                "text": "A Case-Based Approach to Knowledge Acquisition for Domain-Specific Sentence Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is concluded that the case-based method provides a promising approach to automated dictionary construction and knowledge acquisition for sentence analysis in limited domains and a novel case retrieval algorithm that uses decision trees to improve the performance of a k-nearest neighbor similarity metric."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770316"
                        ],
                        "name": "P. Jacobs",
                        "slug": "P.-Jacobs",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Jacobs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2797754"
                        ],
                        "name": "U. Zernik",
                        "slug": "U.-Zernik",
                        "structuredName": {
                            "firstName": "Uri",
                            "lastName": "Zernik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Zernik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1084686,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "0c3264d6f5c501922c28aa122c49e90982267acd",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Language acquisition addresses two important text processing issues. The immediate problem is understanding a text in spite of the existence of lexical gaps. The long term issue is that the understander must incorporate new words into its lexicon for future use. This paper describes an approach to constructing new lexical entries in a gradual process by analyzing a sequence of example texts. This approach permits the graceful tolerance of new words while enabling the automated extension of the lexicon. Each new acquired lexeme starts as a set of assumptions derived from the analysis of each word in a textual context. A variety of knowledge sources, including morphological, syntactic, semantic, and contextual knowledge, determine the assumptions. These assumptions, along with justifications and dependencies, are interpreted and refined by a learning program that ultimately updates the system's lexicon. This approach uses existing linguistic knowledge, and generalization of multiple occurrences, to create new operational lexical entries."
            },
            "slug": "Acquiring-Lexical-Knowledge-from-Text:-A-Case-Study-Jacobs-Zernik",
            "title": {
                "fragments": [],
                "text": "Acquiring Lexical Knowledge from Text: A Case Study"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper describes an approach to constructing new lexical entries in a gradual process by analyzing a sequence of example texts to permit the graceful tolerance of new words while enabling the automated extension of the lexicon."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1693468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d922631a6bf8361d7602e12cafb9e15d421c827",
            "isKey": false,
            "numCitedBy": 836,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a program that disambiguates English word senses in unrestricted text using statistical models of the major Roget's Thesaurus categories. Roget's categories serve as approximations of conceptual classes. The categories listed for a word in Roget's index tend to correspond to sense distinctions; thus selecting the most likely category provides a useful level of sense disambiguation. The selection of categories is accomplished by identifying and weighting words that are indicative of each category when seen in context, using a Bayesian theoretical framework.Other statistical approaches have required special corpora or hand-labeled training examples for much of the lexicon. Our use of class models overcomes this knowledge acquisition bottleneck, enabling training on unrestricted monolingual text without human intervention. Applied to the 10 million word Grolier's Encyclopedia, the system correctly disambiguated 92% of the instances of 12 polysemous words that have been previously studied in the literature."
            },
            "slug": "Word-Sense-Disambiguation-Using-Statistical-Models-Yarowsky",
            "title": {
                "fragments": [],
                "text": "Word-Sense Disambiguation Using Statistical Models of Roget\u2019s Categories Trained on Large Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A program that disambiguates English word senses in unrestricted text using statistical models of the major Roget's Thesaurus categories, enabling training on unrestricted monolingual text without human intervention."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66511259"
                        ],
                        "name": "R. Beckwith",
                        "slug": "R.-Beckwith",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Beckwith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Beckwith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721801"
                        ],
                        "name": "C. Fellbaum",
                        "slug": "C.-Fellbaum",
                        "structuredName": {
                            "firstName": "Christiane",
                            "lastName": "Fellbaum",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fellbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145386345"
                        ],
                        "name": "Derek Gross",
                        "slug": "Derek-Gross",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Gross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Gross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113623689"
                        ],
                        "name": "K. Miller",
                        "slug": "K.-Miller",
                        "structuredName": {
                            "firstName": "Katherine",
                            "lastName": "Miller",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 101
                            }
                        ],
                        "text": "There have been a few large-scale efforts to create broad semantic knowledge bases, such as WordNet (Miller, 1990) and Cyc (Lenat, Prakash, and Shepherd, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2146137,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "4bd970a37c59c97804ff93cbb2c108e081de3a37",
            "isKey": false,
            "numCitedBy": 5335,
            "numCiting": 133,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard alphabetical procedures for organizing lexical information put together words that are spelled alike and scatter words with similar or related meanings haphazardly through the list. Unfortunately, there is no obvious alternative, no other simple way for lexicographers to keep track of what has been done or for readers to find the word they are looking for. But a frequent objection to this solution is that finding things on an alphabetical list can be tedious and time-consuming. Many people who would like to refer to a dictionary decide not to bother with it because finding the information would interrupt their work and break their train of thought."
            },
            "slug": "Introduction-to-WordNet:-An-On-line-Lexical-Miller-Beckwith",
            "title": {
                "fragments": [],
                "text": "Introduction to WordNet: An On-line Lexical Database"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Standard alphabetical procedures for organizing lexical information put together words that are spelled alike and scatter words with similar or related meanings haphazardly through the list."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3086368"
                        ],
                        "name": "R. Berwick",
                        "slug": "R.-Berwick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Berwick",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Berwick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16337517,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "5e6fdf2dfcefe043394298c9e437f962aad0f81e",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes work in progress on a computer program that uses syntactic constraints to derive the meanings of verbs from an analysis of simple English example stories. The central idea is an extension of Winston's (Winston 1975) program that learned the structural descriptions of blocks world scenes. In the new research, English verbs take the place of blocks world objects like ARCH and TOWER, with frame-based descriptions of causal relationships serving as the structural descriptions. Syntactic constraints derived from the parsing of story plots are used to drive an analogical matching procedure. Analogical matching gives a way to compare descriptions of known words to unknown words. The \"meaning\" of a new verb is learned by matching pan of the causal network description of a story precis containing the unknown word to a set of such descriptions derived from similar stories that contain only known words. The best match forges an assignment between objects and relations such that the unknown verb is matched to a known verb, with the assignment being guided by syntactic constraints. The causal network surrounding the unknown item is then used as a scaffolding to construct a network representing the use of the novel word in a particular context. Words (and their associated stories) that are \"best matches\" are grouped together into a similarity network, according to the match score."
            },
            "slug": "Learning-Word-Meanings-From-Examples-Berwick",
            "title": {
                "fragments": [],
                "text": "Learning Word Meanings From Examples"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A computer program that uses syntactic constraints to derive the meanings of verbs from an analysis of simple English example stories is described, an extension of Winston's (Winston 1975) program that learned the structural descriptions of blocks world scenes."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732071"
                        ],
                        "name": "R. Weischedel",
                        "slug": "R.-Weischedel",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Weischedel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Weischedel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3227843"
                        ],
                        "name": "M. Meteer",
                        "slug": "M.-Meteer",
                        "structuredName": {
                            "firstName": "Marie",
                            "lastName": "Meteer",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meteer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744313"
                        ],
                        "name": "L. Ramshaw",
                        "slug": "L.-Ramshaw",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Ramshaw",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ramshaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2452408"
                        ],
                        "name": "Jeff Palmucci",
                        "slug": "Jeff-Palmucci",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Palmucci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Palmucci"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 174
                            }
                        ],
                        "text": "Although we used a hand-crafted part-of-speech dictionary for these experiments, statistical and corpusbased taggers are readily available (e.g., (Brill, 1994; Church, 1989; Weischedel et al., 1993))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6838726,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a94da952fb8ffc77881028081e90efb494f1c5d",
            "isKey": false,
            "numCitedBy": 329,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "From spring 1990 through fall 1991, we performed a battery of small experiments to test the effectiveness of supplementing knowledge-based techniques with probabilistic models. This paper reports our experiments in predicting parts of speech of highly ambiguous words, predicting the intended interpretation of an utterance when more than one interpretation satisfies all known syntactic and semantic constraints, and learning caseframe informationfor verbsfrom example uses.From these experiments, we are convinced that probabilistic models based on annotated corpora can effectively reduce the ambiguity in processing text and can be used to acquire lexical informationfrom a corpus, by supplementing knowledge-based techniques.Based on the results of those experiments, we have constructed a new natural language system (PLUM) for extracting data from text, e.g., newswire text."
            },
            "slug": "Coping-with-Ambiguity-and-Unknown-Words-through-Weischedel-Meteer",
            "title": {
                "fragments": [],
                "text": "Coping with Ambiguity and Unknown Words through Probabilistic Models"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A new natural language system (PLUM) is constructed for extracting data from text, e.g., newswire text, based on results of experiments in predicting parts of speech of highly ambiguous words, predicting the intended interpretation of an utterance when more than one interpretation satisfies all known syntactic and semantic constraints."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400286782"
                        ],
                        "name": "P. Wiemer-Hastings",
                        "slug": "P.-Wiemer-Hastings",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Wiemer-Hastings",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Wiemer-Hastings"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779477"
                        ],
                        "name": "S. Lytinen",
                        "slug": "S.-Lytinen",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Lytinen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lytinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8529717,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "0dbef0679ae6cb5725ae6e1b5b071f36e9514469",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We have implemented an incremental lexical acquisition mechanism that learns the meanings of previously unknown words from the context in which they appear, as a part of the process of parsing and semantically interpreting sentences. Implementation of this algorithm brought to light a fundamental difference between learning verbs and learning nouns. Specifically, because verbs typically play the predicate role in English sentences, whereas nouns typically function as arguments, we found that different mechanisms were required to learn verbs and nouns. Because of this difference in usage, our learning algorithm formulates the most specific hypotheses possible, consistent with the data, for verb meanings, but the most general hypotheses possible for nouns. Subsequent examples may falsify a current hypothesis, causing verb meanings to be generalized and noun meanings to be made more specific. This paper describes the two approaches used to learn verbs and nouns in the system, and reports on the system's performance in substantial empirical testing."
            },
            "slug": "The-Ups-and-Downs-of-Lexical-Acquisition-Wiemer-Hastings-Lytinen",
            "title": {
                "fragments": [],
                "text": "The Ups and Downs of Lexical Acquisition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The two approaches used to learn verbs and nouns in the system are described, and the system's performance in substantial empirical testing is reported on."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 160
                            }
                        ],
                        "text": "Although we used a hand-crafted part-of-speech dictionary for these experiments, statistical and corpusbased taggers are readily available (e.g., (Brill, 1994; Church, 1989; Weischedel et al., 1993))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3166885,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10",
            "isKey": false,
            "numCitedBy": 1058,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.<<ETX>>"
            },
            "slug": "A-Stochastic-Parts-Program-and-Noun-Phrase-Parser-Church",
            "title": {
                "fragments": [],
                "text": "A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A program that tags each word in an input sentence with the most likely part of speech has been written and performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143712374"
                        ],
                        "name": "J. Carbonell",
                        "slug": "J.-Carbonell",
                        "structuredName": {
                            "firstName": "Jaime",
                            "lastName": "Carbonell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Carbonell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16742497,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3f09d77332979d8315c775c1e6654323ff661cd",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses an approach to incremental learning in natural language processing. The technique of projecting and integrating semantic constraints to learn word definitions is analyzed as implemented in the POLITICS system. Extensions and improvements of this technique are developed. The problem of generalizing existing word meanings and understanding metaphorical uses of words is addressed in terms of semantic constraint integration."
            },
            "slug": "Towards-a-Self-Extending-Parser-Carbonell",
            "title": {
                "fragments": [],
                "text": "Towards a Self-Extending Parser"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "This paper discusses an approach to incremental learning in natural language processing by projecting and integrating semantic constraints to learn word definitions as implemented in the POLITICS system."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144297714"
                        ],
                        "name": "R. Granger",
                        "slug": "R.-Granger",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Granger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Granger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 139
                            }
                        ],
                        "text": "Some systems learn the meanings of unknown words using expectations derived from other word definitions in the surrounding context (e.g., (Granger, 1977; Carbonell, 1979; Jacobs and Zernik, 1988; Hastings and Lytinen, 1994))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "in the surrounding context (e.g., ( Granger, 1977;"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9255668,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5213e864b82c4dde46ce9f6b82c403729426d3a2",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The inferencing task of figuring out words from context is implemented in the presence of a large database of world knowledge. The program does not require interaction with the user, but rather uses internal parser expectations and knowledge embodied in scripts to figure out likely definitions for unknown words, and to create context-specific definitions for such words."
            },
            "slug": "FOUL-UP:-A-Program-that-Figures-Out-Meanings-of-Granger",
            "title": {
                "fragments": [],
                "text": "FOUL-UP: A Program that Figures Out Meanings of Words from Context"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The inferencing task of figuring out words from context is implemented in the presence of a large database of world knowledge to figure out likely definitions for unknown words, and to create context-specific definitions for such words."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1925215"
                        ],
                        "name": "W. Lehnert",
                        "slug": "W.-Lehnert",
                        "structuredName": {
                            "firstName": "Wendy",
                            "lastName": "Lehnert",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Lehnert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748501"
                        ],
                        "name": "Claire Cardie",
                        "slug": "Claire-Cardie",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cardie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Cardie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145262164"
                        ],
                        "name": "David Fisher",
                        "slug": "David-Fisher",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fisher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Fisher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144051750"
                        ],
                        "name": "J. McCarthy",
                        "slug": "J.-McCarthy",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "McCarthy",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. McCarthy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691993"
                        ],
                        "name": "E. Riloff",
                        "slug": "E.-Riloff",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Riloff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riloff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "weapon. In the UMass/MUC-4 information extraction system ( Lehnert et al., 1992 ), the words ammu-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 50
                            }
                        ],
                        "text": "In the UMass/MUC-4 information extraction system (Lehnert et al., 1992), the words ammunition and bullets were defined as weapons, mainly for the purpose of selectional restrictions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5934104,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72c8167620e213b1c28ad34096594e758825d91a",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "CIRCUS is a conceptual analyzer that produces semantic case frame representations for input sentences. Although space does not permit us to give a full technical description of CIRCUS, we will attempt to convey some sense of sentence analysis via CIRCUS. For more details, please consult [2] and [1]."
            },
            "slug": "University-of-Massachusetts:-description-of-the-as-Lehnert-Cardie",
            "title": {
                "fragments": [],
                "text": "University of Massachusetts: description of the CIRCUS system as used for MUC-4"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work will attempt to convey some sense of sentence analysis via CIRCUS, a conceptual analyzer that produces semantic case frame representations for input sentences."
            },
            "venue": {
                "fragments": [],
                "text": "MUC"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704635"
                        ],
                        "name": "D. Lenat",
                        "slug": "D.-Lenat",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Lenat",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lenat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144516995"
                        ],
                        "name": "M. Prakash",
                        "slug": "M.-Prakash",
                        "structuredName": {
                            "firstName": "Mayank",
                            "lastName": "Prakash",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Prakash"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052381298"
                        ],
                        "name": "M. Shepherd",
                        "slug": "M.-Shepherd",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Shepherd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shepherd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9013752,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d756c70a3cb6df2fdf52f0027942f10712ebde56",
            "isKey": false,
            "numCitedBy": 351,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The major limitations in building large software have always been (a) its brittleness when confronted by problems that were not foreseen by its builders, and (by the amount of manpower required. The recent history of expert systems, for example highlights how constricting the brittleness and knowledge acquisition bottlenecks are. Moreover, standard software methodology (e.g., working from a detailed \"spec\") has proven of little use in AI, a field which by definition tackles ill- structured problems. How can these bottlenecks be widened? Attractive, elegant answers have included machine learning, automatic programming, and natural language understanding. But decades of work on such systems have convinced us that each of these approaches has difficulty \"scaling up\" for want a substantial base of real world knowledge."
            },
            "slug": "CYC:-Using-Common-Sense-Knowledge-to-Overcome-and-Lenat-Prakash",
            "title": {
                "fragments": [],
                "text": "CYC: Using Common Sense Knowledge to Overcome Brittleness and Knowledge Acquisition Bottlenecks"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "The major limitations in building large software have always been its brittleness when confronted by problems that were not foreseen by its builders, and its bottlenecks can be widened."
            },
            "venue": {
                "fragments": [],
                "text": "AI Mag."
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "MUC-4 Proceedings. 1992. Proceedings of the Fourth Message Understanding Conference"
            },
            "venue": {
                "fragments": [],
                "text": "MUC-4 Proceedings. 1992. Proceedings of the Fourth Message Understanding Conference"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 147
                            }
                        ],
                        "text": "Although we used a hand-crafted part-of-speech dictionary for these experiments, statistical and corpusbased taggers are readily available (e.g., (Brill, 1994; Church, 1989; Weischedel et al., 1993))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some Advances in Rule-based Part of Speech Tagging"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Twelfth National Conference on Artiicial Intelligence"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 143
                            }
                        ],
                        "text": "Other approaches use example or case-based methods to match unknown word contexts against previously seen word contexts (e.g., (Berwick, 1989; Cardie, 1993))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Case-Based Approach to Knowledge Acquisition for DomainSpecific Sentence Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the Eleventh National Conference on Artificial Intelligence,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proceedings of the Fourth Message Understanding Conference (MUC-4)"
            },
            "venue": {
                "fragments": [],
                "text": "Morgan Kaufmann, San Mateo, CA."
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 139
                            }
                        ],
                        "text": "Some systems learn the meanings of unknown words using expectations derived from other word definitions in the surrounding context (e.g., (Granger, 1977; Carbonell, 1979; Jacobs and Zernik, 1988; Hastings and Lytinen, 1994))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "FOUL-UP: A Program"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 147
                            }
                        ],
                        "text": "Although we used a hand-crafted part-of-speech dictionary for these experiments, statistical and corpusbased taggers are readily available (e.g., (Brill, 1994; Church, 1989; Weischedel et al., 1993))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some Advances in Rule-based Part"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Coping with Ambiguity and"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proceedings1992] MUC-4 Proceedings . 1992. Proceedings of the Fourth Message Understanding Conference (MUC-4)"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings1992] MUC-4 Proceedings . 1992. Proceedings of the Fourth Message Understanding Conference (MUC-4)"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 99
                            }
                        ],
                        "text": "There have been a few large-scale e orts to create broad semantic knowledge bases, such as WordNet (Miller, 1990) and Cyc (Lenat, Prakash, and Shepherd, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 101
                            }
                        ],
                        "text": "There have been a few large-scale efforts to create broad semantic knowledge bases, such as WordNet (Miller, 1990) and Cyc (Lenat, Prakash, and Shepherd, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Wordnet: An On-line Lexical"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 160
                            }
                        ],
                        "text": "Although we used a hand-crafted part-of-speech dictionary for these experiments, statistical and corpusbased taggers are readily available (e.g., (Brill, 1994; Church, 1989; Weischedel et al., 1993))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Stochastic Parts Program and"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Word Mean"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 147
                            }
                        ],
                        "text": "Although we used a hand-crafted part-of-speech dictionary for these experiments, statistical and corpusbased taggers are readily available (e.g., (Brill, 1994; Church, 1989; Weischedel et al., 1993))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some Advances in Rulebased Part of Speech Tagging"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the Twelfth National Conference on Artificial Intelligence,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some Advances in Rule - based Part of Speech Tagging"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Twelfth National Conference on Artificial Intelligence"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 196
                            }
                        ],
                        "text": "Some systems learn the meanings of unknown words using expectations derived from other word definitions in the surrounding context (e.g., (Granger, 1977; Carbonell, 1979; Jacobs and Zernik, 1988; Hastings and Lytinen, 1994))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Ups and"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 154
                            }
                        ],
                        "text": "Some systems learn the meanings of unknown words using expectations derived from other word definitions in the surrounding context (e.g., (Granger, 1977; Carbonell, 1979; Jacobs and Zernik, 1988; Hastings and Lytinen, 1994))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Towards a Self-Extending"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 143
                            }
                        ],
                        "text": "Other approaches use example or case-based methods to match unknown word contexts against previously seen word contexts (e.g., (Berwick, 1989; Cardie, 1993))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Case-Based Approach to Knowledge Acquisition for Domain-Speciic Sentence Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Eleventh National Conference on Artiicial Intelligence"
            },
            "year": 1993
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 7,
            "methodology": 9
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 28,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/A-Corpus-Based-Approach-for-Building-Semantic-Riloff-Shepherd/b3e9130ecab419f8267fccadf80c1ee2489be793?sort=total-citations"
}