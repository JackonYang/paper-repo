{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794409"
                        ],
                        "name": "K. Grauman",
                        "slug": "K.-Grauman",
                        "structuredName": {
                            "firstName": "Kristen",
                            "lastName": "Grauman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Grauman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "Previously, an EMD embedding was developed for global color histogram matching [7] and sets of local shape features [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 227
                            }
                        ],
                        "text": "Our implementation is modeled on the video data mining method in [18]; we omit the \u201cstop-list\u201d and temporal feature tracking steps since we are matching static, non-sequential images in addition to video frames in these experiments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2096702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed95a0639e622d46cc391546952252fe3d6b0af4",
            "isKey": false,
            "numCitedBy": 217,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Weighted graph matching is a good way to align a pair of shapes represented by a set of descriptive local features; the set of correspondences produced by the minimum cost matching between two shapes' features often reveals how similar the shapes are. However due to the complexity of computing the exact minimum cost matching, previous algorithms could only run efficiently when using a limited number of features per shape, and could not scale to perform retrievals from large databases. We present a contour matching algorithm that quickly computes the minimum weight matching between sets of descriptive local features using a recently introduced low-distortion embedding of the earth mover's distance (EMD) into a normed space. Given a novel embedded contour, the nearest neighbors in a database of embedded contours are retrieved in sublinear time via approximate nearest neighbors search with locality-sensitive hashing (LSH). We demonstrate our shape matching method on a database of 136,500 images of human figures. Our method achieves a speedup of four orders of magnitude over the exact method, at the cost of only a 4% reduction in accuracy."
            },
            "slug": "Fast-contour-matching-using-approximate-earth-Grauman-Darrell",
            "title": {
                "fragments": [],
                "text": "Fast contour matching using approximate earth mover's distance"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work presents a contour matching algorithm that quickly computes the minimum weight matching between sets of descriptive local features using a recently introduced low-distortion embedding of the earth mover's distance (EMD) into a normed space."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794409"
                        ],
                        "name": "K. Grauman",
                        "slug": "K.-Grauman",
                        "structuredName": {
                            "firstName": "Kristen",
                            "lastName": "Grauman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Grauman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 142
                            }
                        ],
                        "text": "We normalize the weight given to each feature in a set to produce equal-mass sets, in order to allow each set to vary in cardinality (but see [6] for an extension that allows unequal-mass sets and partial matchings)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 211
                            }
                        ],
                        "text": "In recent work we have extended the method to allow partial matchings over unequal-mass sets using multi-resolution histogram intersection, and we show its capacity as a kernel for discriminative classification [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7549619,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc3098cff5469c55c3e81dc127563afe6dbadf22",
            "isKey": false,
            "numCitedBy": 177,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Discriminative learning is challenging when examples are sets of features, and the sets vary in cardinality and lack any sort of meaningful ordering. Kernel-based classification methods can learn complex decision boundaries, but a kernel over unordered set inputs must somehow solve for correspondences \u2013 generally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space. This \u201cpyramid match\u201d computation is linear in the number of features, and it implicitly finds correspondences based on the finest resolution histogram cell where a matched pair first appears. Since the kernel does not penalize the presence of extra features, it is robust to clutter. We show the kernel function is positive-definite, making it valid for use in learning algorithms whose optimal solutions are guaranteed only for Mercer kernels. We demonstrate our algorithm on object recognition tasks and show it to be accurate and dramatically faster than current approaches."
            },
            "slug": "Pyramid-Match-Kernels:-Discriminative-with-Sets-of-Grauman-Darrell",
            "title": {
                "fragments": [],
                "text": "Pyramid Match Kernels: Discriminative Classification with Sets of Image Features (version 2)"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new fast kernel function is presented which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space and is shown to be positive-definite, making it valid for use in learning algorithms whose optimal solutions are guaranteed only for Mercer kernels."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794409"
                        ],
                        "name": "K. Grauman",
                        "slug": "K.-Grauman",
                        "structuredName": {
                            "firstName": "Kristen",
                            "lastName": "Grauman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Grauman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026of the PCA-SIFT features as input to all methods, and on the order of 102 prototypes for the prototypicalfeature method (100, 400, and 700 clusters for the scenes, objects, and textures, respectively); these parameters were optimized for recognition performance on a held out set of examples."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13036203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "625bce34ec80d29242340400d916e799d2975430",
            "isKey": false,
            "numCitedBy": 1593,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Discriminative learning is challenging when examples are sets of features, and the sets vary in cardinality and lack any sort of meaningful ordering. Kernel-based classification methods can learn complex decision boundaries, but a kernel over unordered set inputs must somehow solve for correspondences epsivnerally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space. This \"pyramid match\" computation is linear in the number of features, and it implicitly finds correspondences based on the finest resolution histogram cell where a matched pair first appears. Since the kernel does not penalize the presence of extra features, it is robust to clutter. We show the kernel function is positive-definite, making it valid for use in learning algorithms whose optimal solutions are guaranteed only for Mercer kernels. We demonstrate our algorithm on object recognition tasks and show it to be accurate and dramatically faster than current approaches"
            },
            "slug": "The-pyramid-match-kernel:-discriminative-with-sets-Grauman-Darrell",
            "title": {
                "fragments": [],
                "text": "The pyramid match kernel: discriminative classification with sets of image features"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new fast kernel function is presented which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space and is shown to be positive-definite, making it valid for use in learning algorithms whose optimal solutions are guaranteed only for Mercer kernels."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719780"
                        ],
                        "name": "Yan Ke",
                        "slug": "Yan-Ke",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Ke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Ke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694199"
                        ],
                        "name": "R. Sukthankar",
                        "slug": "R.-Sukthankar",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Sukthankar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sukthankar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 242
                            }
                        ],
                        "text": "The normalized rank is 0 for perfect performance (i.e., when all relevant images in the database are retrieved as a query\u2019s nearest neighbors), and it approaches 1 as performance worsens; a random retrieval results in a normalized rank of 0.5 [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 288876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a46f093f587eaa29c4d87c755b4b1fa3eabecdb",
            "isKey": true,
            "numCitedBy": 1998,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Stable local feature detection and representation is a fundamental component of many image registration and object recognition algorithms. Mikolajczyk and Schmid (June 2003) recently evaluated a variety of approaches and identified the SIFT [D. G. Lowe, 1999] algorithm as being the most resistant to common image deformations. This paper examines (and improves upon) the local image descriptor used by SIFT. Like SIFT, our descriptors encode the salient aspects of the image gradient in the feature point's neighborhood; however, instead of using SIFT's smoothed weighted histograms, we apply principal components analysis (PCA) to the normalized gradient patch. Our experiments demonstrate that the PCA-based local descriptors are more distinctive, more robust to image deformations, and more compact than the standard SIFT representation. We also present results showing that using these descriptors in an image retrieval application results in increased accuracy and faster matching."
            },
            "slug": "PCA-SIFT:-a-more-distinctive-representation-for-Ke-Sukthankar",
            "title": {
                "fragments": [],
                "text": "PCA-SIFT: a more distinctive representation for local image descriptors"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper examines (and improves upon) the local image descriptor used by SIFT, and demonstrates that the PCA-based local descriptors are more distinctive, more robust to image deformations, and more compact than the standard SIFT representation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1829696"
                        ],
                        "name": "Y. Rubner",
                        "slug": "Y.-Rubner",
                        "structuredName": {
                            "firstName": "Yossi",
                            "lastName": "Rubner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Rubner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145086151"
                        ],
                        "name": "Carlo Tomasi",
                        "slug": "Carlo-Tomasi",
                        "structuredName": {
                            "firstName": "Carlo",
                            "lastName": "Tomasi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlo Tomasi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744254"
                        ],
                        "name": "L. Guibas",
                        "slug": "L.-Guibas",
                        "structuredName": {
                            "firstName": "Leonidas",
                            "lastName": "Guibas",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Guibas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 143
                            }
                        ],
                        "text": "Previously, researchers applying EMD have mapped raw image features to prototypes or cluster centers in order to get around EMD\u2019s computational burden; the input to EMD is then a set of prototypes weighted by their frequency in the image [16, 10]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 128
                            }
                        ],
                        "text": "The EMD embedding also enables the use of approximate nearest neighbor (NN) search techniques that guarantee query times that are sublinear in the number of examples to be searched [7, 4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 178
                            }
                        ],
                        "text": "For the prototypical feature scheme, vector quantization is used to map all descriptors to a discrete set of prototypes, which are found by running k-means on a set of examples containing images from each class."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14106275,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d13a04844e4a781e5180987118f732d93aa9f398",
            "isKey": false,
            "numCitedBy": 4139,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the properties of a metric between two distributions, the Earth Mover's Distance (EMD), for content-based image retrieval. The EMD is based on the minimal cost that must be paid to transform one distribution into the other, in a precise sense, and was first proposed for certain vision problems by Peleg, Werman, and Rom. For image retrieval, we combine this idea with a representation scheme for distributions that is based on vector quantization. This combination leads to an image comparison framework that often accounts for perceptual similarity better than other previously proposed methods. The EMD is based on a solution to the transportation problem from linear optimization, for which efficient algorithms are available, and also allows naturally for partial matching. It is more robust than histogram matching techniques, in that it can operate on variable-length representations of the distributions that avoid quantization and other binning problems typical of histograms. When used to compare distributions with the same overall mass, the EMD is a true metric. In this paper we focus on applications to color and texture, and we compare the retrieval performance of the EMD with that of other distances."
            },
            "slug": "The-Earth-Mover's-Distance-as-a-Metric-for-Image-Rubner-Tomasi",
            "title": {
                "fragments": [],
                "text": "The Earth Mover's Distance as a Metric for Image Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper investigates the properties of a metric between two distributions, the Earth Mover's Distance (EMD), for content-based image retrieval, and compares the retrieval performance of the EMD with that of other distances."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 176
                            }
                        ],
                        "text": "\u2026which detects points that are invariant to scale and affine transformations, and the Scale Invariant Feature Transform (SIFT) interest operator of [11], which detects points that\nare invariant to scaling and rotation and has been shown in practice to be resistant to common image transformations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "We also show how spatial neighborhood constraints may be incorporated directly into the matching process by augmenting features with invariant descriptions of their geometric relationship with other features in the image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 123
                            }
                        ],
                        "text": "However, the complexity of finding the optimal correspondences between two equal-mass sets under exact EMD is cubic in the number of features per set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14457153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "642e328cae81c5adb30069b680cf60ba6b475153",
            "isKey": false,
            "numCitedBy": 6761,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieved is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching in two full length feature films."
            },
            "slug": "Video-Google:-a-text-retrieval-approach-to-object-Sivic-Zisserman",
            "title": {
                "fragments": [],
                "text": "Video Google: a text retrieval approach to object matching in videos"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "An approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video, represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749609"
                        ],
                        "name": "S. Lazebnik",
                        "slug": "S.-Lazebnik",
                        "structuredName": {
                            "firstName": "Svetlana",
                            "lastName": "Lazebnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lazebnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189388"
                        ],
                        "name": "J. Ponce",
                        "slug": "J.-Ponce",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Ponce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ponce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 115
                            }
                        ],
                        "text": "(1)\nComparing bags of local features with EMD is essentially measuring how much effort would be required to transform one bag into the other."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15208439,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "400736e84b3b04ffa41542cf62b6546e57f590f7",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a framework for texture recognition based on local affine-invariant descriptors and their spatial layout. At modelling time, a generative model of local descriptors is learned from sample images using the EM algorithm. The EM framework allows the incorporation of unsegmented multitexture images into the training set. The second modelling step consists of gathering co-occurrence statistics of neighboring descriptors. At recognition time, initial probabilities computed from the generative model are refined using a relaxation step that incorporates co-occurrence statistics. Performance is evaluated on images of an indoor scene and pictures of wild animals."
            },
            "slug": "Affine-invariant-local-descriptors-and-neighborhood-Lazebnik-Schmid",
            "title": {
                "fragments": [],
                "text": "Affine-invariant local descriptors and neighborhood statistics for texture recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A framework for texture recognition based on local affine-invariant descriptors and their spatial layout is presented and initial probabilities computed from the generative model are refined using a relaxation step that incorporates co-occurrence statistics."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704728"
                        ],
                        "name": "T. Tuytelaars",
                        "slug": "T.-Tuytelaars",
                        "structuredName": {
                            "firstName": "Tinne",
                            "lastName": "Tuytelaars",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tuytelaars"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 186
                            }
                        ],
                        "text": "An interest operator is generally applied to the image to detect stable or distinctive points, and then a local descriptor is extracted from the patch or ellipse around each interest point."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 163
                            }
                        ],
                        "text": "\u2026ignores useful information that is captured by the co-occurrence of a set of distinctive features \u2013 information that is especially important when categorization of objects or textures is the goal \u2013 and it fails to distinguish between images having varying numbers of similar features (see Figure 1)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 241
                            }
                        ],
                        "text": "The measure of this effort is based on establishing the correspondence between two images\u2019 unordered descriptive local features that results in the lowest possible overall matching cost, where matching cost is defined by a ground distance D between two local features (e.g., the L2 norm)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 22188121,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "979089260419884b43cfeb3b23df23b6a7734f9f",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This contribution develops a new technique for content-based image retrieval. Where most existing image retrieval systems mainly focus on color and color distribution or texture, we classify the images based on local invariants. These features represent the image in a very compact way and allow fast comparison and feature matching with images in the database. Using local features makes the system robust to occlusions and changes in the background. Using invariants makes it robust to changes in viewpoint and illumination."
            },
            "slug": "Content-Based-Image-Retrieval-Based-on-Local-Tuytelaars-Gool",
            "title": {
                "fragments": [],
                "text": "Content-Based Image Retrieval Based on Local Affinely Invariant Regions"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This contribution develops a new technique for content-based image retrieval that classify the images based on local invariants that represent the image in a very compact way and allow fast comparison and feature matching with images in the database."
            },
            "venue": {
                "fragments": [],
                "text": "VISUAL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712041"
                        ],
                        "name": "K. Mikolajczyk",
                        "slug": "K.-Mikolajczyk",
                        "structuredName": {
                            "firstName": "Krystian",
                            "lastName": "Mikolajczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mikolajczyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 153
                            }
                        ],
                        "text": "\u2026ignores useful information that is captured by the co-occurrence of a set of distinctive features \u2013 information that is especially important when categorization of objects or textures is the goal \u2013 and it fails to distinguish between images having varying numbers of similar features (see Figure 1)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 246
                            }
                        ],
                        "text": "The measure of this effort is based on establishing the correspondence between two images\u2019 unordered descriptive local features that results in the lowest possible overall matching cost, where matching cost is defined by a ground distance D between two local features (e.g., the L2 norm)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2326264,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6647f56413722e812cb084fbb3597ba18ceada36",
            "isKey": false,
            "numCitedBy": 759,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new method for detecting scale invariant interest points. The method is based on two recent results on scale space: (1) Interest points can be adapted to scale and give repeatable results (geometrically stable). (2) Local extrema over scale of normalized derivatives indicate the presence of characteristic local structures. Our method first computes a multi-scale representation for the Harris interest point detector. We then select points at which a local measure (the Laplacian) is maximal over scales. This allows a selection of distinctive points for which the characteristic scale is known. These points are invariant to scale, rotation and translation as well as robust to illumination changes and limited changes of viewpoint. For indexing, the image is characterized by a set of scale invariant points; the scale associated with each point allows the computation of a scale invariant descriptor. Our descriptors are, in addition, invariant to image rotation, of affine illumination changes and robust to small perspective deformations. Experimental results for indexing show an excellent performance up to a scale factor of 4 for a database with more than 5000 images."
            },
            "slug": "Indexing-based-on-scale-invariant-interest-points-Mikolajczyk-Schmid",
            "title": {
                "fragments": [],
                "text": "Indexing Based on Scale Invariant Interest Points"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This paper presents a new method for detecting scale invariant interest points based on two recent results on scale space: 1) Interest points can be adapted to scale and give repeatable results (geometrically stable); 2) local extrema over scale of normalized derivatives indicate the presence of characteristic local structures."
            },
            "venue": {
                "fragments": [],
                "text": "ICCV"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808423"
                        ],
                        "name": "G. Csurka",
                        "slug": "G.-Csurka",
                        "structuredName": {
                            "firstName": "Gabriela",
                            "lastName": "Csurka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Csurka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 185
                            }
                        ],
                        "text": "\u2026which detects points that are invariant to scale and affine transformations, and the Scale Invariant Feature Transform (SIFT) interest operator of [11], which detects points that\nare invariant to scaling and rotation and has been shown in practice to be resistant to common image transformations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 77
                            }
                        ],
                        "text": "We also show how spatial neighborhood constraints may be incorporated directly into the matching process by augmenting features with invariant descriptions of their geometric relationship with other features in the image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 128
                            }
                        ],
                        "text": "However, the complexity of finding the optimal correspondences between two equal-mass sets under exact EMD is cubic in the number of features per set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17606900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b91180d8853d00e8f2df7ee3532e07d3d0cce2af",
            "isKey": false,
            "numCitedBy": 5008,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel method for generic visual categorization: the problem of identifying the object content of natural images while generalizing across variations inherent to the object class. This bag of keypoints method is based on vector quantization of affine invariant descriptors of image patches. We propose and compare two alternative implementations using different classifiers: Naive Bayes and SVM. The main advantages of the method are that it is simple, computationally efficient and intrinsically invariant. We present results for simultaneously classifying seven semantic visual categories. These results clearly demonstrate that the method is robust to background clutter and produces good categorization accuracy even without exploiting geometric information."
            },
            "slug": "Visual-categorization-with-bags-of-keypoints-Csurka",
            "title": {
                "fragments": [],
                "text": "Visual categorization with bags of keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This bag of keypoints method is based on vector quantization of affine invariant descriptors of image patches and shows that it is simple, computationally efficient and intrinsically invariant."
            },
            "venue": {
                "fragments": [],
                "text": "eccv 2004"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749609"
                        ],
                        "name": "S. Lazebnik",
                        "slug": "S.-Lazebnik",
                        "structuredName": {
                            "firstName": "Svetlana",
                            "lastName": "Lazebnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lazebnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189388"
                        ],
                        "name": "J. Ponce",
                        "slug": "J.-Ponce",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Ponce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ponce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "We demonstrate our method in three very different con-\n1We use the words \u201cbag\u201d or \u201cdiscrete distribution\u201d interchangeably to refer to an unordered collection of features that may contain duplications.\ntexts: recognition of scenes, objects, and textures."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 183
                            }
                        ],
                        "text": "For the prototypical feature scheme, vector quantization is used to map all descriptors to a discrete set of prototypes, which are found by running k-means on a set of examples containing images from each class."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 132
                            }
                        ],
                        "text": "For a metric space (X ,D) and two equal-mass sets Bp,Bq \u2282 X , the EMD is the minimum possible cost of \u03c0, a matching between Bp and Bq:\nEMD(Bp,Bq) = min \u03c0:Bp\u2192Bq \u2211 s\u2208Bp D(s, \u03c0(s))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 137
                            }
                        ],
                        "text": "However, the complexity of finding the optimal correspondences between two equal-mass sets under exact EMD is cubic in the number of features per set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 256365,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb342b4d65a79120c6ddc3782f330c829bffd000",
            "isKey": true,
            "numCitedBy": 195,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a texture representation suitable for recognizing images of textured surfaces under a wide range of transformations, including viewpoint changes and nonrigid deformations. At the feature extraction stage, a sparse set of affine-invariant local patches is extracted from the image. This spatial selection process permits the computation of characteristic scale and neighborhood shape for every texture element. The proposed texture representation is evaluated in retrieval and classification tasks using the entire Brodatz database and a collection of photographs of textured surfaces taken from different viewpoints."
            },
            "slug": "A-sparse-texture-representation-using-regions-Lazebnik-Schmid",
            "title": {
                "fragments": [],
                "text": "A sparse texture representation using affine-invariant regions"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The proposed texture representation is evaluated in retrieval and classification tasks using the entire Brodatz database and a collection of photographs of textured surfaces taken from different viewpoints."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682878"
                        ],
                        "name": "A. Gionis",
                        "slug": "A.-Gionis",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Gionis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gionis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688317"
                        ],
                        "name": "P. Indyk",
                        "slug": "P.-Indyk",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Indyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Indyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "84095744"
                        ],
                        "name": "R. Motwani",
                        "slug": "R.-Motwani",
                        "structuredName": {
                            "firstName": "Rajeev",
                            "lastName": "Motwani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Motwani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026of the PCA-SIFT features as input to all methods, and on the order of 102 prototypes for the prototypicalfeature method (100, 400, and 700 clusters for the scenes, objects, and textures, respectively); these parameters were optimized for recognition performance on a held out set of examples."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We show the advantage of using the joint statistics when matching with local features as opposed to matching each feature independently under a voting scheme, and we investigate the benefits of matching the actual detected features as opposed to vector-quantized versions of them."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1578969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e74388f55f2cc704c4de410578887a53a9433b0",
            "isKey": false,
            "numCitedBy": 3454,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "The nearestor near-neighbor query problems arise in a large variety of database applications, usually in the context of similarity searching. Of late, there has been increasing interest in building search/index structures for performing similarity search over high-dimensional data, e.g., image databases, document collections, time-series databases, and genome databases. Unfortunately, all known techniques for solving this problem fall prey to the \\curse of dimensionality.\" That is, the data structures scale poorly with data dimensionality; in fact, if the number of dimensions exceeds 10 to 20, searching in k-d trees and related structures involves the inspection of a large fraction of the database, thereby doing no better than brute-force linear search. It has been suggested that since the selection of features and the choice of a distance metric in typical applications is rather heuristic, determining an approximate nearest neighbor should su ce for most practical purposes. In this paper, we examine a novel scheme for approximate similarity search based on hashing. The basic idea is to hash the points Supported by NAVY N00014-96-1-1221 grant and NSF Grant IIS-9811904. Supported by Stanford Graduate Fellowship and NSF NYI Award CCR-9357849. Supported by ARO MURI Grant DAAH04-96-1-0007, NSF Grant IIS-9811904, and NSF Young Investigator Award CCR9357849, with matching funds from IBM, Mitsubishi, Schlumberger Foundation, Shell Foundation, and Xerox Corporation. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 25th VLDB Conference, Edinburgh, Scotland, 1999. from the database so as to ensure that the probability of collision is much higher for objects that are close to each other than for those that are far apart. We provide experimental evidence that our method gives signi cant improvement in running time over other methods for searching in highdimensional spaces based on hierarchical tree decomposition. Experimental results also indicate that our scheme scales well even for a relatively large number of dimensions (more than 50)."
            },
            "slug": "Similarity-Search-in-High-Dimensions-via-Hashing-Gionis-Indyk",
            "title": {
                "fragments": [],
                "text": "Similarity Search in High Dimensions via Hashing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experimental results indicate that the novel scheme for approximate similarity search based on hashing scales well even for a relatively large number of dimensions, and provides experimental evidence that the method gives improvement in running time over other methods for searching in highdimensional spaces based on hierarchical tree decomposition."
            },
            "venue": {
                "fragments": [],
                "text": "VLDB"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151194032"
                        ],
                        "name": "H. M\u00fcller",
                        "slug": "H.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Henning",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145866011"
                        ],
                        "name": "W. M\u00fcller",
                        "slug": "W.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760329"
                        ],
                        "name": "D. Squire",
                        "slug": "D.-Squire",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Squire",
                            "middleNames": [
                                "McG."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Squire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398659765"
                        ],
                        "name": "S. Marchand-Maillet",
                        "slug": "S.-Marchand-Maillet",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Marchand-Maillet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Marchand-Maillet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809085"
                        ],
                        "name": "T. Pun",
                        "slug": "T.-Pun",
                        "structuredName": {
                            "firstName": "Thierry",
                            "lastName": "Pun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13096664,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90c7798c0a1168ff96e68f19e53ec733da74e2ba",
            "isKey": false,
            "numCitedBy": 634,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Performance-evaluation-in-content-based-image-and-M\u00fcller-M\u00fcller",
            "title": {
                "fragments": [],
                "text": "Performance evaluation in content-based image retrieval: overview and proposals"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775242"
                        ],
                        "name": "Frederik Schaffalitzky",
                        "slug": "Frederik-Schaffalitzky",
                        "structuredName": {
                            "firstName": "Frederik",
                            "lastName": "Schaffalitzky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frederik Schaffalitzky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The method for matching scenes given in [ 17 ] uses voting to identify candidate matches, then applies a series of steps to verify geometric consistency within larger neighborhoods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Our work differs from the voting-based techniques [19, 12, 11,  17 ] in that we do not match features within an image independently, but instead consider the joint statistics of the invariant features as a whole when matching."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A number of recent matching techniques extract invariant local features for all images, and then use voting to rank the database images in similarity: the query image\u2019s features vote independently for features from the database images (where votes go to the most similar feature under some distance, e.g., L2), possibly followed by a verification step to account for spatial or geometric relationships between the features [12, 11, 19,  17 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14920652,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f091b49d6a695a1f47087cf4c7c28f9eb219167e",
            "isKey": true,
            "numCitedBy": 77,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe progress in matching shots which are images of the same 3D scene in a film. The problem is hard because the camera viewpoint may change substantially between shots, with consequent changes in the imaged appearance of the scene due to foreshortening, scale changes and partial occlusion.We demonstrate that wide baseline matching techniques can be successfully employed for this task by matching key frames between shots. The wide baseline method represents each frame by a set of viewpoint invariant local feature vectors. The local spatial support of the features means that segmentation of the frame (e.g. into foreground/background) is not required, and partial occlusion is tolerated.Results of matching shots for a number of different scene types are illustrated on a commercial film."
            },
            "slug": "Automated-Scene-Matching-in-Movies-Schaffalitzky-Zisserman",
            "title": {
                "fragments": [],
                "text": "Automated Scene Matching in Movies"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is demonstrated that wide baseline matching techniques can be successfully employed for this task by matching key frames between shots by representing each frame by a set of viewpoint invariant local feature vectors."
            },
            "venue": {
                "fragments": [],
                "text": "CIVR"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712041"
                        ],
                        "name": "K. Mikolajczyk",
                        "slug": "K.-Mikolajczyk",
                        "structuredName": {
                            "firstName": "Krystian",
                            "lastName": "Mikolajczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mikolajczyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In this work, we experiment with two types of interest operators: the Harris-Affine detector described in [ 13 ], which detects points that are invariant to scale and affine transformations, and the Scale Invariant Feature Transform (SIFT) interest operator of [11], which detects points that"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "To extract the SIFT, PCA-SIFT, and Harris-Affine features in these experiments, we used the code that the authors of [11, 8,  13 ] have provided online."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1704741,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8b440596b28dc6683caa2b5f6fbca70963e5909e",
            "isKey": false,
            "numCitedBy": 4161,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose a novel approach for detecting interest points invariant to scale and affine transformations. Our scale and affine invariant detectors are based on the following recent results: (1) Interest points extracted with the Harris detector can be adapted to affine transformations and give repeatable results (geometrically stable). (2) The characteristic scale of a local structure is indicated by a local extremum over scale of normalized derivatives (the Laplacian). (3) The affine shape of a point neighborhood is estimated based on the second moment matrix.Our scale invariant detector computes a multi-scale representation for the Harris interest point detector and then selects points at which a local measure (the Laplacian) is maximal over scales. This provides a set of distinctive points which are invariant to scale, rotation and translation as well as robust to illumination changes and limited changes of viewpoint. The characteristic scale determines a scale invariant region for each point. We extend the scale invariant detector to affine invariance by estimating the affine shape of a point neighborhood. An iterative algorithm modifies location, scale and neighborhood of each point and converges to affine invariant points. This method can deal with significant affine transformations including large scale changes. The characteristic scale and the affine shape of neighborhood determine an affine invariant region for each point.We present a comparative evaluation of different detectors and show that our approach provides better results than existing methods. The performance of our detector is also confirmed by excellent matching results; the image is described by a set of scale/affine invariant descriptors computed on the regions associated with our points."
            },
            "slug": "Scale-&-Affine-Invariant-Interest-Point-Detectors-Mikolajczyk-Schmid",
            "title": {
                "fragments": [],
                "text": "Scale & Affine Invariant Interest Point Detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A comparative evaluation of different detectors is presented and it is shown that the proposed approach for detecting interest points invariant to scale and affine transformations provides better results than existing methods."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644050191"
                        ],
                        "name": "G. LoweDavid",
                        "slug": "G.-LoweDavid",
                        "structuredName": {
                            "firstName": "G",
                            "lastName": "LoweDavid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. LoweDavid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We have applied our method in three domains where efficient image matching is useful: scene recognition, object categorization, and texture classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "Some objects from different classes are similar enough in terms of shape and local corner-based interest points that the PCA-SIFT feature cannot discriminate between them."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "The SIFT interest operator was used to detect keypoints, and PCA-SIFT descriptors formed the feature sets."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "In this work, we experiment with two types of interest operators: the Harris-Affine detector described in [13], which detects points that are invariant to scale and affine transformations, and the Scale Invariant Feature Transform (SIFT) interest operator of [11], which detects points that\nare\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 232
                            }
                        ],
                        "text": "In this work, we experiment with two types of interest operators: the Harris-Affine detector described in [13], which detects points that are invariant to scale and affine transformations, and the Scale Invariant Feature Transform (SIFT) interest operator of [11], which detects points that\nare invariant to scaling and rotation and has been shown in practice to be resistant to common image transformations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "To extract the SIFT, PCA-SIFT, and Harris-Affine features in these experiments, we used the code that the authors of [11, 8, 13] have provided online."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "The Harris-Affine detector was used to detect interest points, and PCA-SIFT descriptors were again used to compose the feature sets."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "We used the first eight dimensions of the PCA-SIFT features as input to all methods, and on the order of 102 prototypes for the prototypicalfeature method (100, 400, and 700 clusters for the scenes, objects, and textures, respectively); these parameters were optimized for recognition performance on a held out set of examples."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 147
                            }
                        ],
                        "text": "Instead, researchers have recently turned to representations based on local features that can be reliably detected (for example, using a Harris or SIFT [11] interest\noperator) and are invariant to the transformations likely to occur across images, such as photometric or various geometric transformations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "PCA-SIFT: A More Distinctive Representation for Local Image Descriptors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "We employ the low-dimensional gradient-based descriptor called PCA-SIFT [8] as a descriptor for patches extracted at these interest points."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 144
                            }
                        ],
                        "text": "\u2026researchers have recently turned to representations based on local features that can be reliably detected (for example, using a Harris or SIFT [11] interest\noperator) and are invariant to the transformations likely to occur across images, such as photometric or various geometric\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 251
                            }
                        ],
                        "text": "The measure of this effort is based on establishing the correspondence between two images\u2019 unordered descriptive local features that results in the lowest possible overall matching cost, where matching cost is defined by a ground distance D between two local features (e.g., the L2 norm)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "This experiment indicates that the salient SIFT features were reliably extracted in each instance of a scene, making it possible for voting to be very successful."
                    },
                    "intents": []
                }
            ],
            "corpusId": 174065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4cab9c4b571761203ed4c3a4c5a07dd615f57a91",
            "isKey": true,
            "numCitedBy": 25505,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ..."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-LoweDavid",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144406261"
                        ],
                        "name": "Shmuel Peleg",
                        "slug": "Shmuel-Peleg",
                        "structuredName": {
                            "firstName": "Shmuel",
                            "lastName": "Peleg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shmuel Peleg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27379268"
                        ],
                        "name": "M. Werman",
                        "slug": "M.-Werman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Werman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Werman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416601"
                        ],
                        "name": "H. Rom",
                        "slug": "H.-Rom",
                        "structuredName": {
                            "firstName": "Hillel",
                            "lastName": "Rom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "Previously, researchers applying EMD have mapped raw image features to prototypes or cluster centers in order to get around EMD\u2019s computational burden; the input to EMD is then a set of prototypes weighted by their frequency in the image [16, 10]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18415340,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "f20e58409d59528ffff6613eddd83b176f5e404e",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown that by defining a suitable measure for the comparison of images, changes in resolution can be treated with the same tool as changes in color resolution. A gray-tone image, for example, can be compared to a half-tone image having only two colors (black and white), but of higher spatial resolution. A graph-theoretical definition of the basic measure used is introduced. This is followed by application to spatial resampling and gray-level requantization. This results in a hybrid treatment of resolution, and the possibility of trading spatial for gray-level resolution and vice versa. >"
            },
            "slug": "A-Unified-Approach-to-the-Change-of-Resolution:-and-Peleg-Werman",
            "title": {
                "fragments": [],
                "text": "A Unified Approach to the Change of Resolution: Space and Gray-Level"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "By defining a suitable measure for the comparison of images, changes in resolution can be treated with the same tool as changes in color resolution, and the possibility of trading spatial for gray-level resolution and vice versa is explored."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398659765"
                        ],
                        "name": "S. Marchand-Maillet",
                        "slug": "S.-Marchand-Maillet",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Marchand-Maillet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Marchand-Maillet"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18204586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e86b2fb7131649899ab4b399ab3b0d8f99118793",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article, we give an overview of the main tasks involved in designing a platform for the evaluation of content-based image retrieval systems. A number of issues should be addressed from the construction of an image collection to the definition of standard performance measures. The aim here is to introduce the Benchathlon network that has been created with the aim of setting up such a platform. Sub tasks engendered by this context are listed and the organization of our collaboration detailed. While participants are allocated to specific subtasks, everyone may submit a solution to another parallel task. The last part of this article gives a list of the contribution that our Viper team has submitted to the Benchathlon network. Perspectives for these benchmarking efforts are finally given."
            },
            "slug": "Performance-Evaluation-in-Content-based-Image-The-Marchand-Maillet",
            "title": {
                "fragments": [],
                "text": "Performance Evaluation in Content-based Image Retrieval: The Benchathlon Network"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "An overview of the main tasks involved in designing a platform for the evaluation of content-based image retrieval systems is given and the Benchathlon network that has been created with the aim of setting up such a platform is introduced."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 222
                            }
                        ],
                        "text": "\u2026of the PCA-SIFT features as input to all methods, and on the order of 102 prototypes for the prototypicalfeature method (100, 400, and 700 clusters for the scenes, objects, and textures, respectively); these parameters were optimized for recognition performance on a held out set of examples."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 81
                            }
                        ],
                        "text": "Instead, we can use a low-distortion EMD embedding to reduce the problem of correspondence between sets of local features to an L1 distance (see Figure 3)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We show the advantage of using the joint statistics when matching with local features as opposed to matching each feature independently under a voting scheme, and we investigate the benefits of matching the actual detected features as opposed to vector-quantized versions of them."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 190
                            }
                        ],
                        "text": "Our implementation is modeled on the video data mining method in [18]; we omit the \u201cstop-list\u201d and temporal feature tracking steps since we are matching static, non-sequential images in addition to video frames in these experiments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast Image Retrieval via Embeddings"
            },
            "venue": {
                "fragments": [],
                "text": "3rd Intl Workshop on Statistical and Computational Theories of Vision"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1] http://www.vision.ethz.ch/projects/categorization"
            },
            "venue": {
                "fragments": [],
                "text": "1] http://www.vision.ethz.ch/projects/categorization"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 183
                            }
                        ],
                        "text": "Both result in a similarity-invariant descriptor, since the length ratios of two lines and the angle between two lines are invariant binary relations under similarity transformations [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computer Vision: A Modern Approach, chapter 18"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Vision: A Modern Approach, chapter 18"
            },
            "year": 2003
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 12,
            "methodology": 10
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 21,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Efficient-image-matching-with-distributions-of-Grauman-Darrell/d582df7250124b0523e07f61766cfa422eb5cde2?sort=total-citations"
}