{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 74
                            }
                        ],
                        "text": "Our implementation is based on the open-source CSLM toolkit described in (Schwenk, 2010; Schwenk et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1274371,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4a258df43cc14e46988de9a4a7b2f0ea817529b",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical machine translation systems are based on one or more translation models and a language model of the target language. While many different translation models and phrase extraction algorithms have been proposed, a standard word n-gram back-off language model is used in most systems. \n \nIn this work, we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary. A neural network is used to perform the projection and the probability estimation. We consider the translation of European Parliament Speeches. This task is part of an international evaluation organized by the TC-STAR project in 2006. The proposed method achieves consistent improvements in the BLEU score on the development and test data. \n \nWe also present algorithms to improve the estimation of the language model probabilities when splitting long sentences into shorter chunks."
            },
            "slug": "Continuous-Space-Language-Models-for-Statistical-Schwenk",
            "title": {
                "fragments": [],
                "text": "Continuous Space Language Models for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes to use a new statistical language model that is based on a continuous representation of the words in the vocabulary, which achieves consistent improvements in the BLEU score on the development and test data."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680233"
                        ],
                        "name": "Marta R. Costa-juss\u00e0",
                        "slug": "Marta-R.-Costa-juss\u00e0",
                        "structuredName": {
                            "firstName": "Marta",
                            "lastName": "Costa-juss\u00e0",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marta R. Costa-juss\u00e0"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779548"
                        ],
                        "name": "Jos\u00e9 A. R. Fonollosa",
                        "slug": "Jos\u00e9-A.-R.-Fonollosa",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Fonollosa",
                            "middleNames": [
                                "A.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jos\u00e9 A. R. Fonollosa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "As far as we know, there is only one attempt to integrate the CSLM directly into the translation process (Zamora-Mart\u00ednez et al., 2010)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "This is in fact tricky since many LM probabilities are requested and it is not straight forward to delay a bunch of requests so that we can use the CSLM more efficiently."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "The implementation of the continuous space translation model is based on an extension of the CSLM toolkit and it will be freely available."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 156
                            }
                        ],
                        "text": "To the best of our knowledge, previous research to apply continuous space methods to the translation model, were limited to tuple-based translation models (Schwenk et al., 2007; Le et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 311,
                                "start": 307
                            }
                        ],
                        "text": "Our neural network has the following architecture: a 320 dimensional projection layer for each input word, one common hidden layer of dimension 768, one 512 dimensional additional hidden layer for each output, and seven output layers of dimension 16384 (we use the mechanism of a short list provided by the CSLM toolkit)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "The CSLM has a much higher complexity than a back-off LM, in particular because of the high dimension of the output layer."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "Let us first the recall the principles of the CSLM, using the same notion as (Schwenk, 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 122
                            }
                        ],
                        "text": "This allows to to see the translations model like a standard n-gram LM task and it is straight forward to apply the CSLM (Schwenk et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "The continuous space language model (CSLM) was very successfully applied to large vocabulary speech recognition, and more recently to SMT, e.g. (Schwenk et al., 2006; Le et al., 2010; Zamora-Mart\u00ednez et al., 2010; Schwenk et al., 2012)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "Both architectures are trained by the same back-propagation algorithm than the CSLM \u2013 we just have a target vector for each output layer."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 104
                            }
                        ],
                        "text": "Previous works on continuous space translation models in an bilingual tuple system only used rescoring (Schwenk et al., 2007; Le et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "Our implementation is based on the open-source CSLM toolkit described in (Schwenk, 2010; Schwenk et al., 2012)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "We use the option proposed by the CSLM toolkit to limit the size of the output layer to the most frequent words (short list)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3130183,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "057cb927b59c7cc16141fca2c825da1e3e3ef81a",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of smoothing translation probabilities in a bilingual N-grambased statistical machine translation system. It is proposed to project the bilingual tuples onto a continuous space and to estimate the translation probabilities in this representation. A neural network is used to perform the projection and the probability estimation. Smoothing probabilities is most important for tasks with a limited amount of training material. We consider here the BTEC task of the 2006 IWSLT evaluation. Improvements in all official automatic measures are reported when translating from Italian to English. Using a continuous space model for the translation model and the target language model, an improvement of 1.5 BLEU on the test data is observed."
            },
            "slug": "Smooth-Bilingual-N-Gram-Translation-Schwenk-Costa-juss\u00e0",
            "title": {
                "fragments": [],
                "text": "Smooth Bilingual N-Gram Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "It is proposed to project the bilingual tuples onto a continuous space and to estimate the translation probabilities in this representation and a neural network is used to perform the projection and the probability estimation."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784529"
                        ],
                        "name": "H. Le",
                        "slug": "H.-Le",
                        "structuredName": {
                            "firstName": "Hai",
                            "lastName": "Le",
                            "middleNames": [
                                "Son"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2311059"
                        ],
                        "name": "A. Allauzen",
                        "slug": "A.-Allauzen",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Allauzen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Allauzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846431"
                        ],
                        "name": "Fran\u00e7ois Yvon",
                        "slug": "Fran\u00e7ois-Yvon",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Yvon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fran\u00e7ois Yvon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14810278,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1401b98a6eb0b0ddaf5bdfd559ed1766434446b8",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of conventional maximum likelihood estimates hinders the performance of existing phrase-based translation models. For lack of sufficient training data, most models only consider a small amount of context. As a partial remedy, we explore here several continuous space translation models, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations. In order to handle a large set of translation units, these representations and the associated estimates are jointly computed using a multi-layer neural network with a SOUL architecture. In small scale and large scale English to French experiments, we show that the resulting models can effectively be trained and used on top of a n-gram translation system, delivering significant improvements in performance."
            },
            "slug": "Continuous-Space-Translation-Models-with-Neural-Le-Allauzen",
            "title": {
                "fragments": [],
                "text": "Continuous Space Translation Models with Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Several continuous space translation models are explored, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations, jointly computed using a multi-layer neural network with a SOUL architecture."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398809775"
                        ],
                        "name": "Francisco Zamora-Mart\u00ednez",
                        "slug": "Francisco-Zamora-Mart\u00ednez",
                        "structuredName": {
                            "firstName": "Francisco",
                            "lastName": "Zamora-Mart\u00ednez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francisco Zamora-Mart\u00ednez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145816817"
                        ],
                        "name": "M. J. Bleda",
                        "slug": "M.-J.-Bleda",
                        "structuredName": {
                            "firstName": "Mar\u00eda",
                            "lastName": "Bleda",
                            "middleNames": [
                                "Jos\u00e9",
                                "Castro"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. J. Bleda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 184
                            }
                        ],
                        "text": "The continuous space language model (CSLM) was very successfully applied to large vocabulary speech recognition, and more recently to SMT, e.g. (Schwenk et al., 2006; Le et al., 2010; Zamora-Mart\u00ednez et al., 2010; Schwenk et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "KEYWORDS: Statistical machine translation, phrase probability estimation, continuous space models, neural network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 106
                            }
                        ],
                        "text": "As far as we know, there is only one attempt to integrate the CSLM directly into the translation process (Zamora-Mart\u00ednez et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 27709198,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f37e4bf5dc51c4176b84415e5237d421d81a34e",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural Network Language Models (NNLMs) have been applied to Statistical Machine Translation (SMT) outperforming the translation quality. N -best list rescoring is the most popular approach to deal with the computational problems that appear when using huge NNLMs. But the question of \u201chow much improvement could be achieved in a coupled system\u201d remains unanswered. This open question motivated some previous work of us in order to speed the evaluation of NNLMs. Now, this work integrates the NNLM evaluation in the core of the SMT decoder. NNLMs are used in combination with statistical standard N -gram language models under the maximum entropy framework in an N -gram-based SMT system. A reordering decoder builds a reordering graph coupled during a Viterbi decoding. ThisN -gram-based SMT system enhanced with NNLMs for the French-English BTEC task of the IWSLT\u201910 evaluation campaign is described in detail. An improvement between1.8 and2.4 BLEU points was obtained from the baseline system to the official primary system. This system has been positioned as second in the automatic evaluation of the IWSLT\u201910 official results."
            },
            "slug": "N-gram-based-machine-translation-enhanced-with-Zamora-Mart\u00ednez-Bleda",
            "title": {
                "fragments": [],
                "text": "N-gram-based machine translation enhanced with neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work integrates the NNLM evaluation in the core of the SMT decoder, which has been positioned as second in the automatic evaluation of the IWSLT\u201910 official results."
            },
            "venue": {
                "fragments": [],
                "text": "IWSLT"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2002316"
                        ],
                        "name": "F. Och",
                        "slug": "F.-Och",
                        "structuredName": {
                            "firstName": "Franz",
                            "lastName": "Och",
                            "middleNames": [
                                "Josef"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Och"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "KEYWORDS: Statistical machine translation, phrase probability estimation, continuous space models, neural network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 77
                            }
                        ],
                        "text": "The log-linear approach is commonly used to consider more feature functions (Och, 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5474833,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f12451245667a85d0ee225a80880fc93c71cc8b",
            "isKey": false,
            "numCitedBy": 3304,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria. A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text. In this paper, we analyze various training criteria which directly optimize translation quality. These training criteria make use of recently proposed automatic evaluation metrics. We describe a new algorithm for efficient training an unsmoothed error count. We show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure."
            },
            "slug": "Minimum-Error-Rate-Training-in-Statistical-Machine-Och",
            "title": {
                "fragments": [],
                "text": "Minimum Error Rate Training in Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784529"
                        ],
                        "name": "H. Le",
                        "slug": "H.-Le",
                        "structuredName": {
                            "firstName": "Hai",
                            "lastName": "Le",
                            "middleNames": [
                                "Son"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2311059"
                        ],
                        "name": "A. Allauzen",
                        "slug": "A.-Allauzen",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Allauzen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Allauzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696542"
                        ],
                        "name": "Guillaume Wisniewski",
                        "slug": "Guillaume-Wisniewski",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Wisniewski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Wisniewski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846431"
                        ],
                        "name": "Fran\u00e7ois Yvon",
                        "slug": "Fran\u00e7ois-Yvon",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Yvon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fran\u00e7ois Yvon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 167
                            }
                        ],
                        "text": "The continuous space language model (CSLM) was very successfully applied to large vocabulary speech recognition, and more recently to SMT, e.g. (Schwenk et al., 2006; Le et al., 2010; Zamora-Mart\u00ednez et al., 2010; Schwenk et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "KEYWORDS: Statistical machine translation, phrase probability estimation, continuous space models, neural network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8176549,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47e3d8a1f8e92923e739ca34bea17004a40514e9",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Using multi-layer neural networks to estimate the probabilities of word sequences is a promising research area in statistical language modeling, with applications in speech recognition and statistical machine translation. However, training such models for large vocabulary tasks is computationally challenging which does not scale easily to the huge corpora that are nowadays available. In this work, we study the performance and behavior of two neural statistical language models so as to highlight some important caveats of the classical training algorithms. The induced word embeddings for extreme cases are also analysed, thus providing insight into the convergence issues. A new initialization scheme and new training techniques are then introduced. These methods are shown to greatly reduce the training time and to significantly improve performance, both in terms of perplexity and on a large-scale translation task."
            },
            "slug": "Training-Continuous-Space-Language-Models:-Some-Le-Allauzen",
            "title": {
                "fragments": [],
                "text": "Training Continuous Space Language Models: Some Practical Issues"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work studies the performance and behavior of two neural statistical language models so as to highlight some important caveats of the classical training algorithms, and introduces a new initialization scheme and new training techniques to greatly reduce the training time and to significantly improve performance."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207041403,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0fcc184b3b90405ec3ceafd6a4007c749df7c363",
            "isKey": false,
            "numCitedBy": 554,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Continuous-space-language-models-Schwenk",
            "title": {
                "fragments": [],
                "text": "Continuous space language models"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34812618"
                        ],
                        "name": "A. Rousseau",
                        "slug": "A.-Rousseau",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Rousseau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rousseau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1992242"
                        ],
                        "name": "M. Attik",
                        "slug": "M.-Attik",
                        "structuredName": {
                            "firstName": "Mohammed",
                            "lastName": "Attik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Attik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 214
                            }
                        ],
                        "text": "The continuous space language model (CSLM) was very successfully applied to large vocabulary speech recognition, and more recently to SMT, e.g. (Schwenk et al., 2006; Le et al., 2010; Zamora-Mart\u00ednez et al., 2010; Schwenk et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "KEYWORDS: Statistical machine translation, phrase probability estimation, continuous space models, neural network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 89
                            }
                        ],
                        "text": "Our implementation is based on the open-source CSLM toolkit described in (Schwenk, 2010; Schwenk et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7801390,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1f27c00ead87b2bde0e1a4ef732677dd9b989ff",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Language models play an important role in large vocabulary speech recognition and statistical machine translation systems. The dominant approach since several decades are back-off language models. Some years ago, there was a clear tendency to build huge language models trained on hundreds of billions of words. Lately, this tendency has changed and recent works concentrate on data selection. Continuous space methods are a very competitive approach, but they have a high computational complexity and are not yet in widespread use. This paper presents an experimental comparison of all these approaches on a large statistical machine translation task. We also describe an open-source implementation to train and use continuous space language models (CSLM) for such large tasks. We describe an efficient implementation of the CSLM using graphical processing units from Nvidia. By these means, we are able to train an CSLM on more than 500 million words in 20 hours. This CSLM provides an improvement of up to 1.8 BLEU points with respect to the best back-off language model that we were able to build."
            },
            "slug": "Large,-Pruned-or-Continuous-Space-Language-Models-a-Schwenk-Rousseau",
            "title": {
                "fragments": [],
                "text": "Large, Pruned or Continuous Space Language Models on a GPU for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An experimental comparison of all the approaches to train and use continuous space language models (CSLM) on a large statistical machine translation task and an efficient implementation of the CSLM using graphical processing units from Nvidia is described."
            },
            "venue": {
                "fragments": [],
                "text": "WLM@NAACL-HLT"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36037226"
                        ],
                        "name": "R\u00e9jean Ducharme",
                        "slug": "R\u00e9jean-Ducharme",
                        "structuredName": {
                            "firstName": "R\u00e9jean",
                            "lastName": "Ducharme",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9jean Ducharme"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120247189"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909943744"
                        ],
                        "name": "Christian Janvin",
                        "slug": "Christian-Janvin",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Janvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Janvin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 41
                            }
                        ],
                        "text": "An alternative approach was proposed by (Bengio and Ducharme, 2001; Bengio et al., 2003)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 221275765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "isKey": false,
            "numCitedBy": 6011,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts."
            },
            "slug": "A-Neural-Probabilistic-Language-Model-Bengio-Ducharme",
            "title": {
                "fragments": [],
                "text": "A Neural Probabilistic Language Model"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784037"
                        ],
                        "name": "T. Brants",
                        "slug": "T.-Brants",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Brants",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brants"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054252"
                        ],
                        "name": "Ashok Popat",
                        "slug": "Ashok-Popat",
                        "structuredName": {
                            "firstName": "Ashok",
                            "lastName": "Popat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashok Popat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2092025743"
                        ],
                        "name": "P. Xu",
                        "slug": "P.-Xu",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2002316"
                        ],
                        "name": "F. Och",
                        "slug": "F.-Och",
                        "structuredName": {
                            "firstName": "Franz",
                            "lastName": "Och",
                            "middleNames": [
                                "Josef"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Och"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 87
                            }
                        ],
                        "text": "A possible implementation could be based on the work on distributed LMs, for instance (Brants et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "4.2 \u2013 287688) and the DARPA BOLT project."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 633992,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba786c46373892554b98df42df7af6f5da343c9d",
            "isKey": true,
            "numCitedBy": 533,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Systems, methods, and computer program products for machine translation are provided. In some implementations a system is provided. The system includes a language model including a collection of n-grams from a corpus, each n-gram having a corresponding relative frequency in the corpus and an order n corresponding to a number of tokens in the n-gram, each n-gram corresponding to a backoff n-gram having an order of n-1 and a collection of backoff scores, each backoff score associated with an n-gram, the backoff score determined as a function of a backoff factor and a relative frequency of a corresponding backoff n-gram in the corpus."
            },
            "slug": "Large-Language-Models-in-Machine-Translation-Brants-Popat",
            "title": {
                "fragments": [],
                "text": "Large Language Models in Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "Systems, methods, and computer program products for machine translation are provided for backoff score determination as a function of a backoff factor and a relative frequency of a corresponding backoff n-gram in the corpus."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784529"
                        ],
                        "name": "H. Le",
                        "slug": "H.-Le",
                        "structuredName": {
                            "firstName": "Hai",
                            "lastName": "Le",
                            "middleNames": [
                                "Son"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3135839"
                        ],
                        "name": "I. Oparin",
                        "slug": "I.-Oparin",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Oparin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Oparin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2311059"
                        ],
                        "name": "A. Allauzen",
                        "slug": "A.-Allauzen",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Allauzen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Allauzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685010"
                        ],
                        "name": "J. Gauvain",
                        "slug": "J.-Gauvain",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Gauvain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauvain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846431"
                        ],
                        "name": "Fran\u00e7ois Yvon",
                        "slug": "Fran\u00e7ois-Yvon",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Yvon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fran\u00e7ois Yvon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 31
                            }
                        ],
                        "text": "Other options are explored in (Le et al., 2011)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 57
                            }
                        ],
                        "text": "An alternative approach was proposed by (Bengio and Ducharme, 2001; Bengio et al., 2003)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14828669,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3aaa1e4974800767fcbd2c24c2f2af42bf412f97",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a new neural network language model (NNLM) based on word clustering to structure the output vocabulary: Structured Output Layer NNLM. This model is able to handle vocabularies of arbitrary size, hence dispensing with the design of short-lists that are commonly used in NNLMs. Several softmax layers replace the standard output layer in this model. The output structure depends on the word clustering which uses the continuous word representation induced by a NNLM. The GALE Mandarin data was used to carry out the speech-to-text experiments and evaluate the NNLMs. On this data the well tuned baseline system has a character error rate under 10%. Our model achieves consistent improvements over the combination of an n-gram model and classical short-list NNLMs both in terms of perplexity and recognition accuracy."
            },
            "slug": "Structured-Output-Layer-neural-network-language-Le-Oparin",
            "title": {
                "fragments": [],
                "text": "Structured Output Layer neural network language model"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A new neural network language model (NNLM) based on word clustering to structure the output vocabulary: Structured Output Layer NNLM, able to handle vocabularies of arbitrary size, hence dispensing with the design of short-lists that are commonly used in NNLMs."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2002316"
                        ],
                        "name": "F. Och",
                        "slug": "F.-Och",
                        "structuredName": {
                            "firstName": "Franz",
                            "lastName": "Och",
                            "middleNames": [
                                "Josef"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Och"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 114
                            }
                        ],
                        "text": "A popular approach are phrase-based models which translate short sequences of words together (Koehn et al., 2003; Och and Ney, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5219389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de2df29b0a0312de7270c3f5a0af6af5645cf91a",
            "isKey": false,
            "numCitedBy": 4470,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented."
            },
            "slug": "A-Systematic-Comparison-of-Various-Statistical-Och-Ney",
            "title": {
                "fragments": [],
                "text": "A Systematic Comparison of Various Statistical Alignment Models"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2458308"
                        ],
                        "name": "George F. Foster",
                        "slug": "George-F.-Foster",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Foster",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George F. Foster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143937779"
                        ],
                        "name": "R. Kuhn",
                        "slug": "R.-Kuhn",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Kuhn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kuhn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065539629"
                        ],
                        "name": "Howard Johnson",
                        "slug": "Howard-Johnson",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Howard Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "KEYWORDS: Statistical machine translation, phrase probability estimation, continuous space models, neural network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 97
                            }
                        ],
                        "text": "We are only aware of few works to perform more sophisticated smoothing techniques, for instance (Foster et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5984042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c67397ac4c61fd1b702d52a7bb6d42a5bf8d7aa",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss different strategies for smoothing the phrasetable in Statistical MT, and give results over a range of translation settings. We show that any type of smoothing is a better idea than the relative-frequency estimates that are often used. The best smoothing techniques yield consistent gains of approximately 1% (absolute) according to the BLEU metric."
            },
            "slug": "Phrasetable-Smoothing-for-Statistical-Machine-Foster-Kuhn",
            "title": {
                "fragments": [],
                "text": "Phrasetable Smoothing for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that any type of smoothing is a better idea than the relative-frequency estimates that are often used and the best smoothing techniques yield consistent gains of approximately 1% (absolute) according to the BLEU metric."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34812618"
                        ],
                        "name": "A. Rousseau",
                        "slug": "A.-Rousseau",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Rousseau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rousseau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076086"
                        ],
                        "name": "Fethi Bougares",
                        "slug": "Fethi-Bougares",
                        "structuredName": {
                            "firstName": "Fethi",
                            "lastName": "Bougares",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fethi Bougares"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682046"
                        ],
                        "name": "P. Del\u00e9glise",
                        "slug": "P.-Del\u00e9glise",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Del\u00e9glise",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Del\u00e9glise"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736665"
                        ],
                        "name": "Y. Est\u00e8ve",
                        "slug": "Y.-Est\u00e8ve",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Est\u00e8ve",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Est\u00e8ve"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 67
                            }
                        ],
                        "text": "According to the system description of the best performing system (Rousseau et al., 2011), adding more (out-of domain) parallel training data yields only small improvements."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 147
                            }
                        ],
                        "text": "\u2026. . . , tp, s1, . . . , sq)\u00d7 P(t2|t3, . . . , tp, s1, . . . , sq)\u00d7 P(t3, . . . , tp|s1, . . . , sq) (3)\n= p\u220f\nk=1\nP(tk|tk+1, . . . , tp, s1, . . . , sq)\u2248 p\u220f\nk=1\nP(tk|s1, . . . , sq) = p\u220f\nk=1\nP(tk|s\u0304) (4)\nAt a first look, our model seems to be based on the approximation in the last line of the\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 10633717,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81bfd99022651988d55bf5750164018714535f1e",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the three systems developed by the LIUM for the IWSLT 2011 evaluation campaign. We participated in three of the proposed tasks, namely the Automatic Speech Recognition task (ASR), the ASR system combination task (ASR_SC) and the Spoken Language Translation task (SLT), since these tasks are all related to speech translation. We present the approaches and specificities we developed on each task."
            },
            "slug": "LIUM\u2019s-systems-for-the-IWSLT-2011-speech-tasks-Rousseau-Bougares",
            "title": {
                "fragments": [],
                "text": "LIUM\u2019s systems for the IWSLT 2011 speech translation tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This paper describes the three systems developed by the LIUM for the IWSLT 2011 evaluation campaign, namely the Automatic Speech Recognition task, the ASR system combination task and the Spoken Language Translation task."
            },
            "venue": {
                "fragments": [],
                "text": "IWSLT"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50624849"
                        ],
                        "name": "P. Schlesinger",
                        "slug": "P.-Schlesinger",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Schlesinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Schlesinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707466"
                        ],
                        "name": "N. Calzolari",
                        "slug": "N.-Calzolari",
                        "structuredName": {
                            "firstName": "Nicoletta",
                            "lastName": "Calzolari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Calzolari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136069"
                        ],
                        "name": "W. Hahn",
                        "slug": "W.-Hahn",
                        "structuredName": {
                            "firstName": "Walther",
                            "lastName": "Hahn",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122932096"
                        ],
                        "name": "T\u00fcbingen Aravind Joshi",
                        "slug": "T\u00fcbingen-Aravind-Joshi",
                        "structuredName": {
                            "firstName": "T\u00fcbingen",
                            "lastName": "Joshi",
                            "middleNames": [
                                "Aravind"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T\u00fcbingen Aravind Joshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121503395"
                        ],
                        "name": "Philadelphia Jaroslav Peregrin",
                        "slug": "Philadelphia-Jaroslav-Peregrin",
                        "structuredName": {
                            "firstName": "Philadelphia",
                            "lastName": "Peregrin",
                            "middleNames": [
                                "Jaroslav"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philadelphia Jaroslav Peregrin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153912145"
                        ],
                        "name": "P. A. Rosen",
                        "slug": "P.-A.-Rosen",
                        "structuredName": {
                            "firstName": "Paris",
                            "lastName": "Rosen",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. A. Rosen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120551239"
                        ],
                        "name": "Prague Petr Sgall",
                        "slug": "Prague-Petr-Sgall",
                        "structuredName": {
                            "firstName": "Prague",
                            "lastName": "Sgall",
                            "middleNames": [
                                "Petr"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prague Petr Sgall"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 162925095,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "3ed8b91459f95191f524412e40fb759857855a69",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Prague-Bulletin-of-Mathematical-Linguistics-Schlesinger-Calzolari",
            "title": {
                "fragments": [],
                "text": "The Prague Bulletin of Mathematical Linguistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 94
                            }
                        ],
                        "text": "A popular approach are phrase-based models which translate short sequences of words together (Koehn et al., 2003; Och and Ney, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical phrase-based machine translation"
            },
            "venue": {
                "fragments": [],
                "text": "HLT/NACL"
            },
            "year": 2003
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 10,
            "methodology": 7
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 16,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Continuous-Space-Translation-Models-for-Statistical-Schwenk/5f08df805f14baa826dbddcb002277b15d3f1556?sort=total-citations"
}