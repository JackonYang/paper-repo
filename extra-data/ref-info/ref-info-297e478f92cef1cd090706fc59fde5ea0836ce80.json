{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144856857"
                        ],
                        "name": "P. D. Souza",
                        "slug": "P.-D.-Souza",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Souza",
                            "middleNames": [
                                "V.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Souza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3853032"
                        ],
                        "name": "J. Lai",
                        "slug": "J.-Lai",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Lai",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 25
                            }
                        ],
                        "text": "[Bahl et al., 1989] and [Brown et al., 1990] is their difficulty in classifying a word not encountered in the training data; in addition, such methods have no remedy if a word's distribution in the training data differs substantially from its distribution in the test data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 1
                            }
                        ],
                        "text": "[Brown et al., 1990] and [Brill et al., 1990] propose algorithms for building word class hierarchies using ngram distributions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 1
                            }
                        ],
                        "text": "[Brown et al., 1990] suggests a hierarchical partitioning of the vocabulary into equivalence classes on the basis of similar bigram distributions: trigram models can then be estimated by defining f(x~ -1) = c,_~cn_l, where each cj is the equivalence class of xj."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10986188,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3de5d40b60742e3dfa86b19e7f660962298492af",
            "isKey": false,
            "numCitedBy": 3318,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics."
            },
            "slug": "Class-Based-n-gram-Models-of-Natural-Language-Brown-Pietra",
            "title": {
                "fragments": [],
                "text": "Class-Based n-gram Models of Natural Language"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work addresses the problem of predicting a word from previous words in a sample of text and discusses n-gram models based on classes of words, finding that these models are able to extract classes that have the flavor of either syntactically based groupings or semanticallybased groupings, depending on the nature of the underlying statistics."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746017"
                        ],
                        "name": "G. Grefenstette",
                        "slug": "G.-Grefenstette",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grefenstette"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15475212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30ef09a2a371ac80044617000de47e71906b93d0",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Knowledge-poor corpus-based approaches to natural language processing are attractive in that they do not incur the diiculties associated with complex knowledge bases and real-world inferences. However, these kinds of language processing techniques in isolation often do not suuce for a particular task; for this reason we are interested in nding ways to combine various techniques and improve their results. Accordingly, we conducted experiments to reene the results of an automatic lexical discovery technique by making use of a statistically-based syntactic similarity measure. The discovery program uses lexico-syntactic patterns to nd instances of the hyponymy relation in large text bases. Once relations of this sort are found, they should be inserted into an existing lexicon or thesaurus. However, the terms in the relation may have multiple senses, thus hampering automatic placement. In order to address this problem we tried to make a term-similarity determination technique choose where, in an existing thesaurus, to install a lexical relation. The union of these two corpus-based methods is promising , although only partially successful in the experiments run so far. Here we report some preliminary results, and make suggestions for how to improve the technique in future."
            },
            "slug": "A-Method-for-Re-ning-Automatically-Discovered-Weak-Grefenstette-Hearst",
            "title": {
                "fragments": [],
                "text": "A Method for Re ning Automatically-Discovered Lexical Relations: Combining Weak Techniques for Stronger Results"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "A term-similarity determination technique choose where, in an existing thesaurus, to install a lexical relation, and the union of these two corpus-based methods is promising, although only partially successful in the experiments run so far."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21169546"
                        ],
                        "name": "Donald Hindle",
                        "slug": "Donald-Hindle",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Hindle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald Hindle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 15
                            }
                        ],
                        "text": "For example , [Hindle, 1990] uses an estimate of mutual information to calculate what nouns a verb can take as its subjects and objects, based on distributions found within a large corpus of naturally occurring text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 56
                            }
                        ],
                        "text": "An ex- ZThe experiment differed from that described in [Hindle, 1990] primarily in the way the direct object of the verb was identified; see Section 4 for details. ample is the automatic discovery of verb selectional preferences for natural language systems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 71
                            }
                        ],
                        "text": "The object of the verb was identified in a fashion similar to that of [Hindle, 1990], except that a set of heuristics was used to extract surface objects only from surface strings, rather than using a fragmentary parse to extract both deep and surface objects."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 74
                            }
                        ],
                        "text": "Previous efforts using word/word associations (e.g. [Church et al., 1989; Hindle, 1990]) appear to have relied on the strength of large numbers to wash out inappropriate senses."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 1
                            }
                        ],
                        "text": "[Hindle, 1990] describes one distributional approach to classifying nouns, based upon the verbs for which they tend to appear as arguments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 273
                            }
                        ],
                        "text": "It has become common in statistical studies of natural language data to use measures of lexical association , such as the information-theoretic measure of mutual information, to extract useful relationships between words (e.g. [Church et al., 1989; Church and Hanks, 1989; Hindle, 1990])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15862538,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "f3f3dcfcaa960ec201e0381f4d026e57e64bea76",
            "isKey": true,
            "numCitedBy": 689,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described. The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification."
            },
            "slug": "Noun-Classification-from-Predicate-Argument-Hindle",
            "title": {
                "fragments": [],
                "text": "Noun Classification from Predicate-Argument Structures"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746017"
                        ],
                        "name": "G. Grefenstette",
                        "slug": "G.-Grefenstette",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grefenstette"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17086579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4b66a159672c613f9d4c7cdb8e6c6f85d871bf5",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Knowledge-poor corpus-based approaches to natural language processing are attractive in that they do not incur the difficulties associated with complex knowledge bases and real-world inferences. However, these kinds of language processing techniques in isolation often do not suffice for a particular task; for this reason we are interested in finding ways to combine various techniques and improve their results. Accordingly, we conducted experiments to refine the results of an automatic lexical discovery technique by making use of a statistically-based syntactic similarity measure. The discovery program uses lexico-syntactic patterns to find instances of the hyponymy relation in large text bases. Once relations of this sort are found, they should be inserted into an existing lexicon or thesaurus. However, the terms in the relation may have multiple senses, thus hampering automatic placement. In order to address this problem we applied a termsimilarity determination technique to the problem of choosing where, in an existing lexical hierarchy, to install a lexical relation. The union of these two corpus-based methods is promising, although only partially successful in the experiments run so far. Here we report some preliminary results, and make suggestions for how to improve the technique in future."
            },
            "slug": "Refining-Automatically-Discovered-Lexical-Combining-Hearst-Grefenstette",
            "title": {
                "fragments": [],
                "text": "Refining Automatically-Discovered Lexical Relations: Combining Weak Techniques for Stronger Results"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A termsimilarity determination technique is applied to the problem of choosing where, in an existing lexical hierarchy, to install a lexical relation, and the union of these two corpus-based methods is promising, although only partially successful in the experiments run so far."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 6
                            }
                        ],
                        "text": "Work [Yarowsky, 1992] has independently investigated an approach similar to the one described here and applied it to the problem of word sense disambiguation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 81
                            }
                        ],
                        "text": "The most significant difference between the two approaches, as was the case for [Yarowsky, 1992], is in how nouns are grouped into classes."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1693468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d922631a6bf8361d7602e12cafb9e15d421c827",
            "isKey": false,
            "numCitedBy": 836,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a program that disambiguates English word senses in unrestricted text using statistical models of the major Roget's Thesaurus categories. Roget's categories serve as approximations of conceptual classes. The categories listed for a word in Roget's index tend to correspond to sense distinctions; thus selecting the most likely category provides a useful level of sense disambiguation. The selection of categories is accomplished by identifying and weighting words that are indicative of each category when seen in context, using a Bayesian theoretical framework.Other statistical approaches have required special corpora or hand-labeled training examples for much of the lexicon. Our use of class models overcomes this knowledge acquisition bottleneck, enabling training on unrestricted monolingual text without human intervention. Applied to the 10 million word Grolier's Encyclopedia, the system correctly disambiguated 92% of the instances of 12 polysemous words that have been previously studied in the literature."
            },
            "slug": "Word-Sense-Disambiguation-Using-Statistical-Models-Yarowsky",
            "title": {
                "fragments": [],
                "text": "Word-Sense Disambiguation Using Statistical Models of Roget\u2019s Categories Trained on Large Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A program that disambiguates English word senses in unrestricted text using statistical models of the major Roget's Thesaurus categories, enabling training on unrestricted monolingual text without human intervention."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145022783"
                        ],
                        "name": "E. Brill",
                        "slug": "E.-Brill",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 26
                            }
                        ],
                        "text": "[Brown et al., 1990] and [Brill et al., 1990] propose algorithms for building word class hierarchies using ngram distributions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 96
                            }
                        ],
                        "text": "9The version of the Brown corpus used was the tagged corpus found as part of the Penn Treebank [Brill et al., 1990]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2769726,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "098de23f08e080bed8a224bf1ad2e504688d3db3",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Two experiments that strongly suggest that largely distributional techniques might be developed to automatically provide both a set of part of speech tags for English and a skeletal parsing of free English text are described. In one experiment the authors have developed a constituent boundary parsing algorithm that derives an (unlabeled) bracketing, given text annotated for part of speech as input. In other experiment the authors have investigated whether a distributional analysis can discover a part of speech tag set which might prove adequate to support experiments. The state of a tagged natural language corpus to aid such experiments is summarized.<<ETX>>"
            },
            "slug": "Deducing-Linguistic-Structure-from-the-Statistics-Brill-Magerman",
            "title": {
                "fragments": [],
                "text": "Deducing Linguistic Structure from the Statistics of Large Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Two experiments that strongly suggest that largely distributional techniques might be developed to automatically provide both a set of part of speech tags for English and a skeletal parsing of free English text are described."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145165877"
                        ],
                        "name": "P. Hanks",
                        "slug": "P.-Hanks",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Hanks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hanks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 249
                            }
                        ],
                        "text": "It has become common in statistical studies of natural language data to use measures of lexical association , such as the information-theoretic measure of mutual information, to extract useful relationships between words (e.g. [Church et al., 1989; Church and Hanks, 1989; Hindle, 1990])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 1
                            }
                        ],
                        "text": "[Church and Hanks, 1989] define the association ratio between two words as a variation on mutual information in which word y must follow word z within a window of w words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 97
                            }
                        ],
                        "text": "For the purposes of lexicography, the work presented here can be considered a generalization of [Church and Hanks, 1989], which describes the use of a lexical association score in the identification of semantic classes and the analysis of concordances."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9558665,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "9e2caa39ac534744a180972a30a320ad0ae41ea3",
            "isKey": false,
            "numCitedBy": 4363,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor. ) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words."
            },
            "slug": "Word-Association-Norms,-Mutual-Information-and-Church-Hanks",
            "title": {
                "fragments": [],
                "text": "Word Association Norms, Mutual Information and Lexicography"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34938639"
                        ],
                        "name": "W. Gale",
                        "slug": "W.-Gale",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gale",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 123
                            }
                        ],
                        "text": "3 aScaling the pointwise mutual information score in this fashion is relatively common in practice; cf. the score used by [Gale et al., 1992] and the \"expected utility\" of [Rosenfeld and Huang, 1992]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17567112,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214",
            "isKey": false,
            "numCitedBy": 657,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Word sense disambiguation has been recognized as a major problem in natural language processing research for over forty years. Both quantitive and qualitative methods have been tried, but much of this work has been stymied by difficulties in acquiring appropriate lexical resources. The availability of this testing and training material has enabled us to develop quantitative disambiguation methods that achieve 92% accuracy in discriminating between two very distinct senses of a noun. In the training phase, we collect a number of instances of each sense of the polysemous noun. Then in the testing phase, we are given a new instance of the noun, and are asked to assign the instance to one of the senses. We attempt to answer this question by comparing the context of the unknown instance with contexts of known instances using a Bayesian argument that has been applied successfully in related tasks such as author identification and information retrieval. The proposed method is probably most appropriate for those aspects of sense disambiguation that are closest to the information retrieval task. In particular, the proposed method was designed to disambiguate senses that are usually associated with different topics."
            },
            "slug": "A-method-for-disambiguating-word-senses-in-a-large-Gale-Church",
            "title": {
                "fragments": [],
                "text": "A method for disambiguating word senses in a large corpus"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The proposed method was designed to disambiguate senses that are usually associated with different topics using a Bayesian argument that has been applied successfully in related tasks such as author identification and information retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Humanit."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144654543"
                        ],
                        "name": "Roberto Basili",
                        "slug": "Roberto-Basili",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Basili",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roberto Basili"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802339"
                        ],
                        "name": "M. Pazienza",
                        "slug": "M.-Pazienza",
                        "structuredName": {
                            "firstName": "Maria",
                            "lastName": "Pazienza",
                            "middleNames": [
                                "Teresa"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pazienza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782522"
                        ],
                        "name": "P. Velardi",
                        "slug": "P.-Velardi",
                        "structuredName": {
                            "firstName": "Paola",
                            "lastName": "Velardi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Velardi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7121547,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09a3882e627373100090f52f1f6ae8542e782afa",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "When implementing computational lexicons it is important to keep in mind the texts that a NLP system must deal with. Words relate to each other in many different, often queer, ways: this information is rarely found in dictionaries, and it is quite hard to be invented a priori, despite the imagination that linguists exhibit at inventing esoteric examples.In this paper we present the results of an experiment in learning from corpora the frequent selectional restrictions holding between content words. The method is based on the analysis of word associations augmented with syntactic markers and semantic tags. Word pairs are extracted by a morphosyntactic analyzer and clustered according to their semantic tags. A statistical measure is applied to the data to evaluate the significance of a detected relation. Clustered association data render the study of word associations more interesting with several respects: data are more reliable even for smaller corpora, more easy to interpret, and have many practical applications in NLP."
            },
            "slug": "Computational-Lexicons:-the-Neat-Examples-and-the-Basili-Pazienza",
            "title": {
                "fragments": [],
                "text": "Computational Lexicons: the Neat Examples and the Odd Exemplars"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents the results of an experiment in learning from corpora the frequent selectional restrictions holding between content words, based on the analysis of word associations augmented with syntactic markers and semantic tags."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66511259"
                        ],
                        "name": "R. Beckwith",
                        "slug": "R.-Beckwith",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Beckwith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Beckwith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721801"
                        ],
                        "name": "C. Fellbaum",
                        "slug": "C.-Fellbaum",
                        "structuredName": {
                            "firstName": "Christiane",
                            "lastName": "Fellbaum",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fellbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145386345"
                        ],
                        "name": "Derek Gross",
                        "slug": "Derek-Gross",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Gross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Gross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113623689"
                        ],
                        "name": "K. Miller",
                        "slug": "K.-Miller",
                        "structuredName": {
                            "firstName": "Katherine",
                            "lastName": "Miller",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 143
                            }
                        ],
                        "text": "Rather than considering all possible classes, I propose constraining the set of possible word classes by using WordNet [Beckwith et al., 1991; Miller, 1990], a broad-coverage lexical/conceptual hierarchy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 141
                            }
                        ],
                        "text": "For largely practical reasons, I have chosen to adopt a knowledge-based approach, using the WordNet ]exical database [Beckwith et al., 1991; Miller, 1990]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2146137,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "4bd970a37c59c97804ff93cbb2c108e081de3a37",
            "isKey": false,
            "numCitedBy": 5335,
            "numCiting": 133,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard alphabetical procedures for organizing lexical information put together words that are spelled alike and scatter words with similar or related meanings haphazardly through the list. Unfortunately, there is no obvious alternative, no other simple way for lexicographers to keep track of what has been done or for readers to find the word they are looking for. But a frequent objection to this solution is that finding things on an alphabetical list can be tedious and time-consuming. Many people who would like to refer to a dictionary decide not to bother with it because finding the information would interrupt their work and break their train of thought."
            },
            "slug": "Introduction-to-WordNet:-An-On-line-Lexical-Miller-Beckwith",
            "title": {
                "fragments": [],
                "text": "Introduction to WordNet: An On-line Lexical Database"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Standard alphabetical procedures for organizing lexical information put together words that are spelled alike and scatter words with similar or related meanings haphazardly through the list."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2696176"
                        ],
                        "name": "L. Bahl",
                        "slug": "L.-Bahl",
                        "structuredName": {
                            "firstName": "Lalit",
                            "lastName": "Bahl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144856857"
                        ],
                        "name": "P. D. Souza",
                        "slug": "P.-D.-Souza",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Souza",
                            "middleNames": [
                                "V.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Souza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 1
                            }
                        ],
                        "text": "[Bahl et al., 1989] and [Brown et al., 1990] is their difficulty in classifying a word not encountered in the training data; in addition, such methods have no remedy if a word's distribution in the training data differs substantially from its distribution in the test data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 40552549,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af386a4e0f2615ed929fdc64a86df8e383bd6121",
            "isKey": false,
            "numCitedBy": 293,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of predicting the next word a speaker will say, given the words already spoken; is discussed. Specifically, the problem is to estimate the probability that a given word will be the next word uttered. Algorithms are presented for automatically constructing a binary decision tree designed to estimate these probabilities. At each node of the tree there is a yes/no question relating to the words already spoken, and at each leaf there is a probability distribution over the allowable vocabulary. Ideally, these nodal questions can take the form of arbitrarily complex Boolean expressions, but computationally cheaper alternatives are also discussed. Some results obtained on a 5000-word vocabulary with a tree designed to predict the next word spoken from the preceding 20 words are included. The tree is compared to an equivalent trigram model and shown to be superior. >"
            },
            "slug": "A-tree-based-statistical-language-model-for-natural-Bahl-Brown",
            "title": {
                "fragments": [],
                "text": "A tree-based statistical language model for natural language speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "Algorithms are presented for automatically constructing a binary decision tree designed to estimate the probability that a given word will be the next word uttered, which is compared to an equivalent trigram model and shown to be superior."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144531812"
                        ],
                        "name": "Xuedong Huang",
                        "slug": "Xuedong-Huang",
                        "structuredName": {
                            "firstName": "Xuedong",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuedong Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 173
                            }
                        ],
                        "text": "3 aScaling the pointwise mutual information score in this fashion is relatively common in practice; cf. the score used by [Gale et al., 1992] and the \"expected utility\" of [Rosenfeld and Huang, 1992]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8058984,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "914b06746d7305bcb5a38b6b4234e1b08f30a94b",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe two attempt to improve our stochastic language models. In the first, we identify a systematic overestimation in the traditional backoff model, and use statistical reasoning to correct it. Our modification results in up to 6% reduction in the perplexity of various tasks. Although the improvement is modest, it is achieved with hardly any increase in the complexity of the model. Both analysis and empirical data suggest that the modification is most suitable when training data is sparse.In the second attempt, we propose a new type of adaptive language model. Existing adaptive models use a dynamic cache, based on the history of the document seen up to that point. But another source of information in the history, within-document word sequence correlations, has not yet been tapped. We describe a model that attempts to capture this information, using a framework where one word sequence triggers another, causing its estimated probability to be raised. We discuss various issues in the design of such a model, and describe our first attempt at building one. Our preliminary results include a perplexity reduction of between 10% and 32%, depending on the test set."
            },
            "slug": "Improvements-in-Stochastic-Language-Modeling-Rosenfeld-Huang",
            "title": {
                "fragments": [],
                "text": "Improvements in Stochastic Language Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Two attempt to improve stochastic language models are described, and a new type of adaptive language model is proposed, using a framework where one word sequence triggers another, causing its estimated probability to be raised."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884373"
                        ],
                        "name": "J. Elman",
                        "slug": "J.-Elman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Elman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 171
                            }
                        ],
                        "text": "\u2026there is no foolproof way to choose between the knowledge-free and knowledge-based ap- 4 Noun classification has even been investigated within a connectionist framework [Elman, 1990], although to my knowledge no one has attempted a practical demonstration using a non-trivial corpus. proaches."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2763403,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "668087f0ae7ce1de6e0bd0965dbb480c08103260",
            "isKey": false,
            "numCitedBy": 9863,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
            },
            "slug": "Finding-Structure-in-Time-Elman",
            "title": {
                "fragments": [],
                "text": "Finding Structure in Time"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory and suggests a method for representing lexical categories and the type/token distinction is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2414040"
                        ],
                        "name": "B. MacWhinney",
                        "slug": "B.-MacWhinney",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "MacWhinney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. MacWhinney"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 91
                            }
                        ],
                        "text": "The experiment has recently been replicated using parental speech from the CHILDES corpus [MacWhinney, 1991], without excluding any classes o priori, with comparable results."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 144168661,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "35b3b5e7c6c15c833645b8bb5a3d7fef0d8524dc",
            "isKey": false,
            "numCitedBy": 4369,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Language research thrives on data collected from spontaneous interactions in naturally occurring situations. However, the process of collecting, transcribing, and analyzing naturalistic data can be extremely time-consuming and often unreliable. This book describes three basic tools for language analysis of transcript data by computer that have been developed in the context of the \"Child Language Data Exchange System (CHILDES)\" project. These are: the \"CHAT\" transcription and coding format, the \"CLAN\" package of analysis programs, and the \"CHILDES\" database. These tools have brought about significant changes in the way research is conducted in the child language field. They are being used with great success by researchers working with second language learning, adult conversational interactions, sociological content analyses, and language recovery in aphasia, as well as by students of child language development. The tools are widely applicable, although this book concentrates on their use in the child language field, believing that researchers from other areas can make the necessary analogies to their own topics. This thoroughly revised 2nd edition includes documentation on a dozen new computer programs that have been added to the basic system for transcript analysis. The most important of these new programs is the \"CHILDES\" Text Editor (CED) which can be used for a wide variety of purposes, including editing non-Roman orthographies, systematically adding codes to transcripts, checking the files for correct use of \"CHAT,\" and linking the files to digitized audio and videotape. In addition to information on the new computer programs, the manual documents changed the shape of the \"CHILDES/BIB\" system--given a major update in 1994--which now uses a new computer database system. The documentation for the \"CHILDES\" transcript database has been updated to include new information on old corpora and information on more than a dozen new corpora from many different languages. Finally, the system of \"CHAT\" notations for file transcript have been clarified to emphasize the ways in which the codes are used by particular \"CLAN\" programs. The new edition concludes with a discussion of new directions in transcript analysis and links between the \"CHILDES\" database and other developments in multimedia computing and global networking. It also includes complete references organized by research topic area for the more than 300 published articles that have made use of the \"CHILDES\" database and/or the \"CLAN\" programs. LEA also distributes the \"CLAN\" programs and the complete \"CHILDES\" Database--including corpora from several languages and discourse situations--described in \"The CHILDES Project.\" Be sure to choose the correct platform (IBM or Macintosh) for the \"CLAN\" programs; the \"CHILDES\" Database CD-ROM runs on both platforms."
            },
            "slug": "The-CHILDES-project:-tools-for-analyzing-talk-MacWhinney",
            "title": {
                "fragments": [],
                "text": "The Childes Project: Tools for Analyzing Talk"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This book describes three basic tools for language analysis of transcript data by computer that have been developed in the context of the \"Child Language Data Exchange System (CHILDES)\" project, and focuses on their use in the child language field, believing that researchers from other areas can make the necessary analogies to their own topics."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2696176"
                        ],
                        "name": "L. Bahl",
                        "slug": "L.-Bahl",
                        "structuredName": {
                            "firstName": "Lalit",
                            "lastName": "Bahl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 150
                            }
                        ],
                        "text": "\u2026from several different equivalence classes either by \"backing off\" or by smoothing; for example, the smoothing technique of interpolated estimation [Bahl et al., 1983; Jelinek and Mercer., 1980] where each fi determines a different equivalence class, and the Ai's weight the equivalence classes'\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14789841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58",
            "isKey": false,
            "numCitedBy": 1403,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Speech recognition is formulated as a problem of maximum likelihood decoding. This formulation requires statistical models of the speech production process. In this paper, we describe a number of statistical models for use in speech recognition. We give special attention to determining the parameters for such models from sparse data. We also describe two decoding methods, one appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks. To illustrate the usefulness of the methods described, we review a number of decoding results that have been obtained with them."
            },
            "slug": "A-Maximum-Likelihood-Approach-to-Continuous-Speech-Bahl-Jelinek",
            "title": {
                "fragments": [],
                "text": "A Maximum Likelihood Approach to Continuous Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper describes a number of statistical models for use in speech recognition, with special attention to determining the parameters for such models from sparse data, and describes two decoding methods appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 148
                            }
                        ],
                        "text": "Church and Hanks write: 1The definition of mutual information is also related to other information-theoretic notions such as relative entropy; see [Cover and Thomas, 1991] for a clear discussion of these and related topics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": false,
            "numCitedBy": 42796,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92771838"
                        ],
                        "name": "S. Abbey",
                        "slug": "S.-Abbey",
                        "structuredName": {
                            "firstName": "Sydney.",
                            "lastName": "Abbey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Abbey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 98717726,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "bdf78e7912a2ca83571328c513d2f166aa3e8be1",
            "isKey": false,
            "numCitedBy": 311,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Using a recently published compilation of analytical data on reference samples SY-2, SY-3 and MRG-1 as an example, a case is presented in favor of more precise terminology in describing analytical methods used in reporting data in collaborative programs. The technique of final measurement in a particular method is often less important than earlier operations, such as sample attack or technique of separation."
            },
            "slug": "What-is-A-\u201cMethod\u201d-Abbey",
            "title": {
                "fragments": [],
                "text": "What is A \u201cMethod\u201d?"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A case is presented in favor of more precise terminology in describing analytical methods used in reporting data in collaborative programs because the technique of final measurement in a particular method is often less important than earlier operations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 15
                            }
                        ],
                        "text": "For example , [Hindle, 1990] uses an estimate of mutual information to calculate what nouns a verb can take as its subjects and objects, based on distributions found within a large corpus of naturally occurring text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 56
                            }
                        ],
                        "text": "An ex- ZThe experiment differed from that described in [Hindle, 1990] primarily in the way the direct object of the verb was identified; see Section 4 for details. ample is the automatic discovery of verb selectional preferences for natural language systems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 71
                            }
                        ],
                        "text": "The object of the verb was identified in a fashion similar to that of [Hindle, 1990], except that a set of heuristics was used to extract surface objects only from surface strings, rather than using a fragmentary parse to extract both deep and surface objects."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 74
                            }
                        ],
                        "text": "Previous efforts using word/word associations (e.g. [Church et al., 1989; Hindle, 1990]) appear to have relied on the strength of large numbers to wash out inappropriate senses."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 1
                            }
                        ],
                        "text": "[Hindle, 1990] describes one distributional approach to classifying nouns, based upon the verbs for which they tend to appear as arguments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 273
                            }
                        ],
                        "text": "It has become common in statistical studies of natural language data to use measures of lexical association , such as the information-theoretic measure of mutual information, to extract useful relationships between words (e.g. [Church et al., 1989; Church and Hanks, 1989; Hindle, 1990])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Noun classification from predicateargument structures"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 28th Annual Meeting of the ACL"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 1
                            }
                        ],
                        "text": "[Miller et ai., 1990], p. 2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 143
                            }
                        ],
                        "text": "Rather than considering all possible classes, I propose constraining the set of possible word classes by using WordNet [Beckwith et al., 1991; Miller, 1990], a broad-coverage lexical/conceptual hierarchy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 141
                            }
                        ],
                        "text": "For largely practical reasons, I have chosen to adopt a knowledge-based approach, using the WordNet ]exical database [Beckwith et al., 1991; Miller, 1990]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 74
                            }
                        ],
                        "text": "See [Beckwith et al., 1991]. sStrictly speaking, WordNet as described by [Miller et ai., 1990] ). all three elements: the WordNet noun hierarchy has no unique top element, so even the most general WordNet class that includes water and tea (the class (thing, [entity, thing])) fails to include\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Five papers on WordNet. CSL Report"
            },
            "venue": {
                "fragments": [],
                "text": "Five papers on WordNet. CSL Report"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 147
                            }
                        ],
                        "text": "\u2026two parts: a (straightforward) application of word-based mutual information to sets of words, and a proposal 13The indefinite object alternation [Levin, 1989] and the specified object alternation [Cote, 1992] --roughly, verbs for which the direct object is understood when omitted. for\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 74
                            }
                        ],
                        "text": "and example (3) shows an instance of the causative/inchoative alternation [Levin, 1989]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 36
                            }
                        ],
                        "text": "13The indefinite object alternation [Levin, 1989] and the specified object alternation [Cote, 1992] -- roughly, verbs for which the direct object is understood when omitted."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Towards a lexical organization of English verbs"
            },
            "venue": {
                "fragments": [],
                "text": "Towards a lexical organization of English verbs"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11148235"
                        ],
                        "name": "R. Burchfield",
                        "slug": "R.-Burchfield",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Burchfield",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Burchfield"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 143660780,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "73ca80f87a5509a0f4cc62471b8f088f66facd0b",
            "isKey": false,
            "numCitedBy": 1257,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Frequency-Analysis-of-English-Usage:-Lexicon-and-By-Burchfield",
            "title": {
                "fragments": [],
                "text": "Frequency Analysis of English Usage: Lexicon and Grammar. By W. Nelson Francis and Henry Ku\u010dera with the assistance of Andrew W. Mackie. Boston: Houghton Mifflin. 1982. x + 561"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "estimation [Bahl et al., 1983; Jelinek and Mercer., 1980] estimates"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61012010,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a923c9f89ed53b6e835b3807c0c1bd8d532687b",
            "isKey": false,
            "numCitedBy": 1037,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Interpolated-estimation-of-Markov-source-parameters-Jelinek",
            "title": {
                "fragments": [],
                "text": "Interpolated estimation of Markov source parameters from sparse data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34938639"
                        ],
                        "name": "W. Gale",
                        "slug": "W.-Gale",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gale",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145165877"
                        ],
                        "name": "P. Hanks",
                        "slug": "P.-Hanks",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Hanks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hanks"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21169546"
                        ],
                        "name": "Donald Hindle",
                        "slug": "Donald-Hindle",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Hindle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald Hindle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5923203,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "3b4ce43e22b441931ad3afcb15d943e0a50c09bb",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "There are a number of collocational constraints in natural languages that ought to play a more important role in natural language parsers. Thus, for example, it is hard for most parsers to take advantage of the fact that wine is typically drunk, produced, and sold, but (probably) not pruned. So too, it is hard for a parser to know which verbs go with which prepositions (e.g., set up) and which nouns fit together to form compound noun phrases (e.g., computer programmer). This paper will attempt to show that many of these types of concerns can be addressed with syntactic methods (symbol pushing), and need not require explicit semantic interpretation. We have found that it is possible to identify many of these interesting co-occurrence relations by computing simple summary statistics over millions of words of text. This paper will summarize a number of experiments carried out by various subsets of the authors over the last few years. The term collocation will be used quite broadly to include constraints on SVO (subject verb object) triples, phrasal verbs, compound noun phrases, and psycholinguistic notions of word association (e.g., doctorinurse)."
            },
            "slug": "Parsing,-Word-Associations-and-Typical-Relations-Church-Gale",
            "title": {
                "fragments": [],
                "text": "Parsing, Word Associations and Typical Predicate-Argument Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper will attempt to show that many of these types of concerns can be addressed with syntactic methods (symbol pushing, and need not require explicit semantic interpretation), and it is found that it is possible to identify many ofThese interesting co-occurrence relations by computing simple summary statistics over millions of words of text."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 141
                            }
                        ],
                        "text": "\u2026mutual information to sets of words, and a proposal 13The indefinite object alternation [Levin, 1989] and the specified object alternation [Cote, 1992] --roughly, verbs for which the direct object is understood when omitted. for organizing those sets according to a knowledgebased\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discourse functions of two types of null objects in English"
            },
            "venue": {
                "fragments": [],
                "text": "Presented at the 66th Annual Meeting of the Linguistic Society of America"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discourse functions of two types of null objects in English, January 1992"
            },
            "venue": {
                "fragments": [],
                "text": "Build a Lexicon,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 31
                            }
                        ],
                        "text": "For example, the Brown Corpus [Francis and Kucera, 1982] has the attractive (and for some tasks, necessary) property of providing a sample that is balanced across many genres."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Frequency Analysis of English Usage"
            },
            "venue": {
                "fragments": [],
                "text": "Frequency Analysis of English Usage"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Roberto Basili, Teresa Pazienza, and Paola Velardi"
            },
            "venue": {
                "fragments": [],
                "text": "Computational lexicons: the neat examples and the odd exemplars. In Third Conference on Applied Natural Language"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mercer . A treebased statistical language model for natural language speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Acoustics"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 6
                            }
                        ],
                        "text": "Work [Yarowsky, 1992] has independently investigated an approach similar to the one described here and applied it to the problem of word sense disambiguation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 81
                            }
                        ],
                        "text": "The most significant difference between the two approaches, as was the case for [Yarowsky, 1992], is in how nouns are grouped into classes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "[Yarowsky, 1992] has independently investigated an approach similar to the one described here and applied it to the problem of word sense disambiguation."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Word-sense disambiguation using statistical models of Roget\u2019s categories trained on large"
            },
            "venue": {
                "fragments": [],
                "text": "Workshop on Speech and Natural Language, Arden House Conference Center, Harriman N\u00a5,"
            },
            "year": 1992
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 11,
            "methodology": 12,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 29,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/WordNet-and-Distributional-Analysis:-A-Class-based-Resnik/297e478f92cef1cd090706fc59fde5ea0836ce80?sort=total-citations"
}