{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145374927"
                        ],
                        "name": "C. Amma",
                        "slug": "C.-Amma",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Amma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Amma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40167486"
                        ],
                        "name": "Marcus Georgi",
                        "slug": "Marcus-Georgi",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Georgi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Georgi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145618636"
                        ],
                        "name": "Tanja Schultz",
                        "slug": "Tanja-Schultz",
                        "structuredName": {
                            "firstName": "Tanja",
                            "lastName": "Schultz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tanja Schultz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "The Airwriting system [1] also achieves handwriting recognition using wearable IMU sensors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14444901,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "357ad53bb7a270fb21b58c75af681482ef711850",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a wearable input system which enables interaction through 3D handwriting recognition. Users can write text in the air as if they were using an imaginary blackboard. The handwriting gestures are captured wirelessly by motion sensors applying accelerometers and gyroscopes which are attached to the back of the hand. We propose a two-stage approach for spotting and recognition of handwriting gestures. The spotting stage uses a support vector machine to identify those data segments which contain handwriting. The recognition stage uses hidden Markov models (HMMs) to generate a text representation from the motion sensor data. Individual characters are modeled by HMMs and concatenated to word models. Our system can continuously recognize arbitrary sentences, based on a freely definable vocabulary. A statistical language model is used to enhance recognition performance and to restrict the search space. We show that continuous gesture recognition with inertial sensors is feasible for gesture vocabularies that are several orders of magnitude larger than traditional vocabularies for known systems. In a first experiment, we evaluate the spotting algorithm on a realistic data set including everyday activities. In a second experiment, we report the results from a nine-user experiment on handwritten sentence recognition. Finally, we evaluate the end-to-end system on a small but realistic data set."
            },
            "slug": "Airwriting:-a-wearable-handwriting-recognition-Amma-Georgi",
            "title": {
                "fragments": [],
                "text": "Airwriting: a wearable handwriting recognition system"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that continuous gesture recognition with inertial sensors is feasible for gesture vocabularies that are several orders of magnitude larger than traditional vocABularies for known systems."
            },
            "venue": {
                "fragments": [],
                "text": "Personal and Ubiquitous Computing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3202308"
                        ],
                        "name": "L. Porzi",
                        "slug": "L.-Porzi",
                        "structuredName": {
                            "firstName": "Lorenzo",
                            "lastName": "Porzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Porzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472153"
                        ],
                        "name": "S. Messelodi",
                        "slug": "S.-Messelodi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Messelodi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Messelodi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2389886"
                        ],
                        "name": "C. M. Modena",
                        "slug": "C.-M.-Modena",
                        "structuredName": {
                            "firstName": "Carla",
                            "lastName": "Modena",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. M. Modena"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40811261"
                        ],
                        "name": "E. Ricci",
                        "slug": "E.-Ricci",
                        "structuredName": {
                            "firstName": "Elisa",
                            "lastName": "Ricci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ricci"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 165
                            }
                        ],
                        "text": "In fact, using wearable sensors to track and recognize the user\u2019s gestures is not new, and many efforts have been made to recognize the finger, hand or arm gestures [23, 26, 28, 33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4393340,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b188764d4a537568667715735a0b9db75f17c477",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Modern mobile devices provide several functionalities and new ones are being added at a breakneck pace. Unfortunately browsing the menu and accessing the functions of a mobile phone is not a trivial task for visual impaired users. Low vision people typically rely on screen readers and voice commands. However, depending on the situations, screen readers are not ideal because blind people may need their hearing for safety, and automatic recognition of voice commands is challenging in noisy environments. Novel smart watches technologies provides an interesting opportunity to design new forms of user interaction with mobile phones. We present our first works towards the realization of a system, based on the combination of a mobile phone and a smart watch for gesture control, for assisting low vision people during daily life activities. More specifically we propose a novel approach for gesture recognition which is based on global alignment kernels and is shown to be effective in the challenging scenario of user independent recognition. This method is used to build a gesture-based user interaction module and is embedded into a system targeted to visually impaired which will also integrate several other modules. We present two of them: one for identifying wet floor signs, the other for automatic recognition of predefined logos."
            },
            "slug": "A-smart-watch-based-gesture-recognition-system-for-Porzi-Messelodi",
            "title": {
                "fragments": [],
                "text": "A smart watch-based gesture recognition system for assisting people with visual impairments"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel approach for gesture recognition which is based on global alignment kernels is shown to be effective in the challenging scenario of user independent recognition and is embedded into a system targeted to visually impaired which will also integrate several other modules."
            },
            "venue": {
                "fragments": [],
                "text": "IMMPD '13"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082057310"
                        ],
                        "name": "Danial Moazen",
                        "slug": "Danial-Moazen",
                        "structuredName": {
                            "firstName": "Danial",
                            "lastName": "Moazen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danial Moazen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113002914"
                        ],
                        "name": "Seyed Sajjadi",
                        "slug": "Seyed-Sajjadi",
                        "structuredName": {
                            "firstName": "Seyed",
                            "lastName": "Sajjadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seyed Sajjadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144416264"
                        ],
                        "name": "A. Nahapetian",
                        "slug": "A.-Nahapetian",
                        "structuredName": {
                            "firstName": "Ani",
                            "lastName": "Nahapetian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nahapetian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 165
                            }
                        ],
                        "text": "In fact, using wearable sensors to track and recognize the user\u2019s gestures is not new, and many efforts have been made to recognize the finger, hand or arm gestures [23, 26, 28, 33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11835715,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0048236fb4a2cb10fc0640cbbf0fe2a87349ce72",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Wearable computing is one of the fastest growing technology markets today, with smart watches poised to take over at least of half the wearable device market. Approaches to text entry on smart watches and other wrist worn systems, independent of the small screen, is of importance to the further growth of wearable systems. The consistent user interaction and hands-free, heads-up operation of smart watches paves the way for gesture recognition methods for text entry. This paper proposes a new text input method for smart watches, which utilizes motion sensor data and machine learning approaches to detect letters written in the air by a user. This method is less computationally intensive, less expensive, and unaffected by lighting factors, when compared to computer vision approaches. The AirDraw system prototype developed to test this approach is presented, along with experimental results with close to 71% accuracy in letter recognition."
            },
            "slug": "AirDraw:-Leveraging-smart-watch-motion-sensors-for-Moazen-Sajjadi",
            "title": {
                "fragments": [],
                "text": "AirDraw: Leveraging smart watch motion sensors for mobile human computer interactions"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new text input method for smart watches is proposed, which utilizes motion sensor data and machine learning approaches to detect letters written in the air by a user, which is less computationally intensive, less expensive, and unaffected by lighting factors, when compared to computer vision approaches."
            },
            "venue": {
                "fragments": [],
                "text": "2016 13th IEEE Annual Consumer Communications & Networking Conference (CCNC)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114011273"
                        ],
                        "name": "Sheng Shen",
                        "slug": "Sheng-Shen",
                        "structuredName": {
                            "firstName": "Sheng",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sheng Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113727802"
                        ],
                        "name": "He Wang",
                        "slug": "He-Wang",
                        "structuredName": {
                            "firstName": "He",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "He Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694368"
                        ],
                        "name": "Romit Roy Choudhury",
                        "slug": "Romit-Roy-Choudhury",
                        "structuredName": {
                            "firstName": "Romit",
                            "lastName": "Choudhury",
                            "middleNames": [
                                "Roy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Romit Roy Choudhury"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 616233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0bda3bfd56fba2bf57c771c1a0406f5c9c3a3d78",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper aims to track the 3D posture of the entire arm - both wrist and elbow - using the motion and magnetic sensors on smartwatches. We do not intend to employ machine learning to train the system on a specific set of gestures. Instead, we aim to trace the geometric motion of the arm, which can then be used as a generic platform for gesture-based applications. The problem is challenging because the arm posture is a function of both elbow and shoulder motions, whereas the watch is only a single point of (noisy) measurement from the wrist. Moreover, while other tracking systems (like indoor/outdoor localization) often benefit from maps or landmarks to occasionally reset their estimates, such opportunities are almost absent here. While this appears to be an under-constrained problem, we find that the pointing direction of the forearm is strongly coupled to the arm's posture. If the gyroscope and compass on the watch can be made to estimate this direction, the 3D search space can become smaller; the IMU sensors can then be applied to mitigate the remaining uncertainty. We leverage this observation to design ArmTrak, a system that fuses the IMU sensors and the anatomy of arm joints into a modified hidden Markov model (HMM) to continuously estimate state variables. Using Kinect 2.0 as ground truth, we achieve around 9.2 cm of median error for free-form postures; the errors increase to 13.3 cm for a real time version. We believe this is a step forward in posture tracking, and with some additional work, could become a generic underlay to various practical applications."
            },
            "slug": "I-am-a-Smartwatch-and-I-can-Track-my-User's-Arm-Shen-Wang",
            "title": {
                "fragments": [],
                "text": "I am a Smartwatch and I can Track my User's Arm"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "ArmTrak is a system that fuses the IMU sensors and the anatomy of arm joints into a modified hidden Markov model (HMM) to continuously estimate state variables, which could become a generic underlay to various practical applications."
            },
            "venue": {
                "fragments": [],
                "text": "MobiSys"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2652308"
                        ],
                        "name": "Andreas Komninos",
                        "slug": "Andreas-Komninos",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Komninos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Komninos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145680789"
                        ],
                        "name": "Mark D. Dunlop",
                        "slug": "Mark-D.-Dunlop",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Dunlop",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark D. Dunlop"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 181
                            }
                        ],
                        "text": "Its inconsitency with the keyboard on other devices imposes an unfriendly learning curve and causes difficulties for the users who frequently switch between the phone and the watch [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 35
                            }
                        ],
                        "text": "(b,c) Redesigned virtual keyboards [17, 29], which are inconsistent with the one on smart phones."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2596282,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf7fe74d7c8ae5ec3beca6afc1c507ff55c81d33",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Smart watches provide users with access to many applications directly from their wrists, without the need to touch their smartphones. While applications such as email, messaging, calendar, and social networking provide views on the watch, there is normally no text-entry method, so users cannot reply on the same device. Here, the authors introduce their interaction design and an optimized alphabetic layout for smart-watch text entry, and present a lab evaluation of an implemented prototype using the OpenAdaptxt engine on a Sony SmartWatch 2. While raising some problems, the feedback from study participants indicates that reasonable quality and speed is achievable on a smart watch and encourages future work in this area. This article is part of a special issue on wearable computing."
            },
            "slug": "Text-Input-on-a-Smart-Watch-Komninos-Dunlop",
            "title": {
                "fragments": [],
                "text": "Text Input on a Smart Watch"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The authors introduce their interaction design and an optimized alphabetic layout for smart-watch text entry, and present a lab evaluation of an implemented prototype using the OpenAdaptxt engine on a Sony SmartWatch 2."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Pervasive Computing"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115472593"
                        ],
                        "name": "Chao Xu",
                        "slug": "Chao-Xu",
                        "structuredName": {
                            "firstName": "Chao",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chao Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2304251"
                        ],
                        "name": "P. Pathak",
                        "slug": "P.-Pathak",
                        "structuredName": {
                            "firstName": "Parth",
                            "lastName": "Pathak",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pathak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144752813"
                        ],
                        "name": "P. Mohapatra",
                        "slug": "P.-Mohapatra",
                        "structuredName": {
                            "firstName": "Prasant",
                            "lastName": "Mohapatra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Mohapatra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 212
                            }
                        ],
                        "text": "For example, ArmTrak [28] requires movements of the entire arm, which will easily exhaust the user if used as a text input method, and it can not be applied in space limited places such as a vehicle; The work in [33] employs classification algorithms which heavily rely on the actual data samples but suffer from the difficulties of data collection at a large scale."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "al detects index finger writings on a flat surface using the smartwatch [33]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "show that three sets are particularly effective for classifying arm and hand related gestures [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "In this condition, we have shown that the recognition results using the feature set provided in [33] are not quite good."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 165
                            }
                        ],
                        "text": "In fact, using wearable sensors to track and recognize the user\u2019s gestures is not new, and many efforts have been made to recognize the finger, hand or arm gestures [23, 26, 28, 33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207222216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba8ab0779c1b66436511b119c7fc7b1435ad8ead",
            "isKey": true,
            "numCitedBy": 212,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Smartwatch is becoming one of the most popular wearable device with many major smartphone manufacturers such as Samsung and Apple releasing their smartwatches recently. Apart from the fitness applications, the smartwatch provides a rich user interface that has enabled many applications like instant messaging and email. Since the smartwatch is worn on the wrist, it introduces a unique opportunity to understand user's arm, hand and possibly finger movements using its accelerometer and gyroscope sensors. Although user's arm and hand gestures are likely to be identified with ease using the smartwatch sensors, it is not clear how much of user's finger gestures can be recognized. In this paper, we show that motion energy measured at the smartwatch is sufficient to uniquely identify user's hand and finger gestures. We identify essential features of accelerometer and gyroscope data that reflect the movements of tendons (passing through the wrist) when performing a finger or a hand gesture. With these features, we build a classifier that can uniquely identify 37 (13 finger, 14 hand and 10 arm) gestures with an accuracy of 98\\%. We further extend our gesture recognition to identify the characters written by the user with her index finger on a surface, and show that such finger-writing can also be accurately recognized with nearly 95% accuracy. Our presented results will enable many novel applications like remote control and finger-writing-based input to devices using smartwatch."
            },
            "slug": "Finger-writing-with-Smartwatch:-A-Case-for-Finger-Xu-Pathak",
            "title": {
                "fragments": [],
                "text": "Finger-writing with Smartwatch: A Case for Finger and Hand Gesture Recognition using Smartwatch"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that motion energy measured at the smartwatch is sufficient to uniquely identify user's hand and finger gestures and will enable many novel applications like remote control and finger-writing-based input to devices using smartwatch."
            },
            "venue": {
                "fragments": [],
                "text": "HotMobile"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144586498"
                        ],
                        "name": "R. Plamondon",
                        "slug": "R.-Plamondon",
                        "structuredName": {
                            "firstName": "R\u00e9jean",
                            "lastName": "Plamondon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Plamondon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696384"
                        ],
                        "name": "S. Srihari",
                        "slug": "S.-Srihari",
                        "structuredName": {
                            "firstName": "Sargur",
                            "lastName": "Srihari",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Srihari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "This kind of work is also known as off-line handwriting recognition in literature [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15782139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d12864a8acbab1830be755bfb9cb177e31ca5e20",
            "isKey": false,
            "numCitedBy": 2744,
            "numCiting": 719,
            "paperAbstract": {
                "fragments": [],
                "text": "Handwriting has continued to persist as a means of communication and recording information in day-to-day life even with the introduction of new technologies. Given its ubiquity in human transactions, machine recognition of handwriting has practical significance, as in reading handwritten notes in a PDA, in postal addresses on envelopes, in amounts in bank checks, in handwritten fields in forms, etc. This overview describes the nature of handwritten language, how it is transduced into electronic data, and the basic concepts behind written language recognition algorithms. Both the online case (which pertains to the availability of trajectory data during writing) and the off-line case (which pertains to scanned images) are considered. Algorithms for preprocessing, character and word recognition, and performance with practical systems are indicated. Other fields of application, like signature verification, writer authentification, handwriting learning tools are also considered."
            },
            "slug": "On-Line-and-Off-Line-Handwriting-Recognition:-A-Plamondon-Srihari",
            "title": {
                "fragments": [],
                "text": "On-Line and Off-Line Handwriting Recognition: A Comprehensive Survey"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The nature of handwritten language, how it is transduced into electronic data, and the basic concepts behind written language recognition algorithms are described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153683293"
                        ],
                        "name": "Yi Li",
                        "slug": "Yi-Li",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Li"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 76
                            }
                        ],
                        "text": "Another way of recognizing handwriting relies on computer vision techniques [5, 21, 27, 30, 34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3670338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f011f98f947960d5f14f0f4420dfc1b83f798f15",
            "isKey": false,
            "numCitedBy": 270,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Hand gesture recognition (HGR) is an important research topic because some situations require silent communication with sign languages. Computational HGR systems assist silent communication, and help people learn a sign language. In this article, a novel method for contact-less HGR using Microsoft Kinect for Xbox is described, and a real-time HGR system is implemented. The system is able to detect the presence of gestures, to identify fingers, and to recognize the meanings of nine gestures in a pre-defined Popular Gesture scenario. The accuracy of the HGR system is from 84% to 99% with single-hand gestures, and from 90% to 100% if both hands perform the same gesture at the same time. Because the depth sensor of Kinect is an infrared camera, the lighting conditions, signers' skin colors and clothing, and background have little impact on the performance of this system. The accuracy and the robustness make this system a versatile component that can be integrated in a variety of applications in daily life."
            },
            "slug": "Hand-gesture-recognition-using-Kinect-Li",
            "title": {
                "fragments": [],
                "text": "Hand gesture recognition using Kinect"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel method for contact-less HGR using Microsoft Kinect for Xbox is described, and a real-time HGR system is implemented that is able to detect the presence of gestures, to identify fingers, and to recognize the meanings of nine gestures in a pre-defined Popular Gesture scenario."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE International Conference on Computer Science and Automation Engineering"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149173395"
                        ],
                        "name": "Xin Zhang",
                        "slug": "Xin-Zhang",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114133970"
                        ],
                        "name": "Zhichao Ye",
                        "slug": "Zhichao-Ye",
                        "structuredName": {
                            "firstName": "Zhichao",
                            "lastName": "Ye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhichao Ye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144838978"
                        ],
                        "name": "Lianwen Jin",
                        "slug": "Lianwen-Jin",
                        "structuredName": {
                            "firstName": "Lianwen",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lianwen Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2790939"
                        ],
                        "name": "Ziyong Feng",
                        "slug": "Ziyong-Feng",
                        "structuredName": {
                            "firstName": "Ziyong",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ziyong Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544629"
                        ],
                        "name": "Shaojie Xu",
                        "slug": "Shaojie-Xu",
                        "structuredName": {
                            "firstName": "Shaojie",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaojie Xu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 76
                            }
                        ],
                        "text": "Another way of recognizing handwriting relies on computer vision techniques [5, 21, 27, 30, 34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8425969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49f3f5a8cad7775b08a1caa65dadff0b3a26b7fb",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "With the introduction of Microsoft Kinect, there has been considerable interest in creating various attractive and feasible applications in related research fields. Kinect simultaneously captures the depth and color information and provides real-time reliable 3D full-body human-pose reconstruction that essentially turns the human body into a controller. This article presents a finger-writing system that recognizes characters written in the air without the need for an extra handheld device. This application adaptively merges depth, skin, and background models for the hand segmentation to overcome the limitations of the individual models, such as hand-face overlapping problems and the depth-color nonsynchronization. The writing fingertip is detected by a new real-time dual-mode switching method. The recognition accuracy rate is greater than 90 percent for the first five candidates of Chinese characters, English characters, and numbers."
            },
            "slug": "A-New-Writing-Experience:-Finger-Writing-in-the-Air-Zhang-Ye",
            "title": {
                "fragments": [],
                "text": "A New Writing Experience: Finger Writing in the Air Using a Kinect Sensor"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A finger-writing system that recognizes characters written in the air without the need for an extra handheld device is presented, which adaptively merges depth, skin, and background models for the hand segmentation to overcome the limitations of the individual models."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE MultiMedia"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145888238"
                        ],
                        "name": "Zhou Ren",
                        "slug": "Zhou-Ren",
                        "structuredName": {
                            "firstName": "Zhou",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhou Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691016"
                        ],
                        "name": "Jingjing Meng",
                        "slug": "Jingjing-Meng",
                        "structuredName": {
                            "firstName": "Jingjing",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jingjing Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34316743"
                        ],
                        "name": "Junsong Yuan",
                        "slug": "Junsong-Yuan",
                        "structuredName": {
                            "firstName": "Junsong",
                            "lastName": "Yuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junsong Yuan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51064498"
                        ],
                        "name": "Zhengyou Zhang",
                        "slug": "Zhengyou-Zhang",
                        "structuredName": {
                            "firstName": "Zhengyou",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhengyou Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 76
                            }
                        ],
                        "text": "Another way of recognizing handwriting relies on computer vision techniques [5, 21, 27, 30, 34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11767651,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5391193c341365535597f9070fd8d6ae642a731d",
            "isKey": false,
            "numCitedBy": 295,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Hand gesture based Human-Computer-Interaction (HCI) is one of the most natural and intuitive ways to communicate between people and machines, since it closely mimics how human interact with each other. In this demo, we present a hand gesture recognition system with Kinect sensor, which operates robustly in uncontrolled environments and is insensitive to hand variations and distortions. Our system consists of two major modules, namely, hand detection and gesture recognition. Different from traditional vision-based hand gesture recognition methods that use color-markers for hand detection, our system uses both the depth and color information from Kinect sensor to detect the hand shape, which ensures the robustness in cluttered environments. Besides, to guarantee its robustness to input variations or the distortions caused by the low resolution of Kinect sensor, we apply a novel shape distance metric called Finger-Earth Mover's Distance (FEMD) for hand gesture recognition. Consequently, our system operates accurately and efficiently. In this demo, we demonstrate the performance of our system in two real-life applications, arithmetic computation and rock-paper-scissors game."
            },
            "slug": "Robust-hand-gesture-recognition-with-kinect-sensor-Ren-Meng",
            "title": {
                "fragments": [],
                "text": "Robust hand gesture recognition with kinect sensor"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents a hand gesture recognition system with Kinect sensor, which operates robustly in uncontrolled environments and is insensitive to hand variations and distortions, and applies a novel shape distance metric called Finger-Earth Mover's Distance (FEMD) forHand gesture recognition."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2011
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 0,
        "totalPages": 0
    },
    "page_url": "https://www.semanticscholar.org/paper/SHOW-Lin-Chen/0da353e79f666a3ae7dd0a5d28c75b852a7f60bf?sort=total-citations"
}