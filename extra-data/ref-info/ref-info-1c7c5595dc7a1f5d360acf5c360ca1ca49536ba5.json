{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53400400,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d97d7f1109206fe6925badd934b4c8dc9a3b380d",
            "isKey": false,
            "numCitedBy": 268,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract When a regression problem contains many predictor variables, it is rarely wise to try to fit the data by means of a least squares regression on all of the predictor variables. Usually, a regression equation based on a few variables will be more accurate and certainly simpler. There are various methods for picking \u201cgood\u201d subsets of variables, and programs that do such procedures are part of every widely used statistical package. The most common methods are based on stepwise addition or deletion of variables and on \u201cbest subsets.\u201d The latter refers to a search method that, given the number of variables to be in the equation (say, five), locates that regression equation based on five variables that has the lowest residual sum of squares among all five variable equations. All of these procedures generate a sequence of regression equations, the first based on one variable, the next based on two variables, and so on. Each member of this sequence is called a submodel and the number of variables in the e..."
            },
            "slug": "The-Little-Bootstrap-and-other-Methods-for-in-Error-Breiman",
            "title": {
                "fragments": [],
                "text": "The Little Bootstrap and other Methods for Dimensionality Selection in Regression: X-Fixed Prediction Error"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145924374"
                        ],
                        "name": "L. Birge",
                        "slug": "L.-Birge",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Birge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Birge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3226094"
                        ],
                        "name": "P. Massart",
                        "slug": "P.-Massart",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Massart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Massart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 54611071,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "91f758bb644d1d1f93d75c33265582d27eccd33e",
            "isKey": false,
            "numCitedBy": 483,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.Our purpose in this paper is to provide a general approach to model selection via penalization for Gaussian regression and to develop our point of view about this subject. The advantage and importance of model selection come from the fact that it provides a suitable approach to many different types of problems, starting from model selection per se (among a family of parametric models, which one is more suitable for the data at hand), which includes for instance variable selection in regression models, to nonparametric estimation, for which it provides a very powerful tool that allows adaptation under quite general circumstances. Our approach to model selection also provides a natural connection between the parametric and nonparametric points of view and copes naturally with the fact that a model is not necessarily true. The method is based on the penalization of a least squares criterion which can be viewed as a generalization of Mallows\u2019Cp. A large part of our efforts will be put on choosing properly the list of models and the penalty function for various estimation problems like classical variable selection or adaptive estimation for various types of lp-bodies."
            },
            "slug": "Gaussian-model-selection-Birge-Massart",
            "title": {
                "fragments": [],
                "text": "Gaussian model selection"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "The purpose in this paper is to provide a general approach to model selection via penalization for Gaussian regression and to develop the point of view about this subject."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "It then happens that this same geometry applies to another, seemingly quite different selection method called the Lasso (Tibshirani 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "5 of Weisberg (1980): given a collection of possible predictors, we select the one having largest absolute correlation with the response y, say xj1 , and perform simple linear regression of y on xj1 ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16162039,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b365b8e45b7d81f081de44ac8f9eadf9144f3ca5",
            "isKey": false,
            "numCitedBy": 36493,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described."
            },
            "slug": "Regression-Shrinkage-and-Selection-via-the-Lasso-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Regression Shrinkage and Selection via the Lasso"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A new method for estimation in linear models called the lasso, which minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant, is proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34913208"
                        ],
                        "name": "M. R. Osborne",
                        "slug": "M.-R.-Osborne",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Osborne",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. R. Osborne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3447771"
                        ],
                        "name": "B. Presnell",
                        "slug": "B.-Presnell",
                        "structuredName": {
                            "firstName": "Brett",
                            "lastName": "Presnell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Presnell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2638770"
                        ],
                        "name": "B. Turlach",
                        "slug": "B.-Turlach",
                        "structuredName": {
                            "firstName": "Berwin",
                            "lastName": "Turlach",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Turlach"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Our main result is described here and verified in Section 5. It closely parallels the homotopy method in the papers by  Osborne, Presnell, and Turlach (2000a, 2000b) , though the LARS approach is somewhat more direct."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Quadratic programming techniques can be used to solve (1.5) though we will present an easier method here, closely related to the \u201chomotopy method\u201d of  Osborne, Presnell & Turlach (2000a) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14553495,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "db31973871f5648cffb02e7590582376e16ad0bb",
            "isKey": true,
            "numCitedBy": 915,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The title Lasso has been suggested by Tibshirani (1996) as a colourful name for a technique of variable selection which requires the minimization of a sum of squares subject to an l\n 1 bound \u03ba on the solution. This forces zero components in the minimizing solution for small values of \u03ba. Thus this bound can function as a selection parameter. This paper makes two contributions to computational problems associated with implementing the Lasso: (1) a compact descent method for solving the constrained problem for a particular value of \u03ba is formulated, and (2) a homotopy method, in which the constraint bound \u03ba becomes the homotopy parameter, is developed to completely describe the possible selection regimes. Both algorithms have a finite termination property. It is suggested that modified Gram-Schmidt orthogonalization applied to an augmented design matrix provides an effective basis for implementing the algorithms."
            },
            "slug": "A-new-approach-to-variable-selection-in-least-Osborne-Presnell",
            "title": {
                "fragments": [],
                "text": "A new approach to variable selection in least squares problems"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A compact descent method for solving the constrained problem for a particular value of \u03ba is formulated, and a homotopy method, in which the constraint bound \u03ba becomes the Homotopy parameter, is developed to completely describe the possible selection regimes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34913208"
                        ],
                        "name": "M. R. Osborne",
                        "slug": "M.-R.-Osborne",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Osborne",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. R. Osborne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3447771"
                        ],
                        "name": "B. Presnell",
                        "slug": "B.-Presnell",
                        "structuredName": {
                            "firstName": "Brett",
                            "lastName": "Presnell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Presnell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2638770"
                        ],
                        "name": "B. Turlach",
                        "slug": "B.-Turlach",
                        "structuredName": {
                            "firstName": "Berwin",
                            "lastName": "Turlach",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Turlach"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Our main result is described here and verified in Section 5. It closely parallels the homotopy method in the papers by  Osborne, Presnell, and Turlach (2000a, 2000b) , though the LARS approach is somewhat more direct."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 14422381,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "7aa39f7f3b69473705e247dd2b3a9689f10fbbc3",
            "isKey": false,
            "numCitedBy": 722,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Proposed by Tibshirani, the least absolute shrinkage and selection operator (LASSO) estimates a vector of regression coefficients by minimizing the residual sum of squares subject to a constraint on the l 1-norm of the coefficient vector. The LASSO estimator typically has one or more zero elements and thus shares characteristics of both shrinkage estimation and variable selection. In this article we treat the LASSO as a convex programming problem and derive its dual. Consideration of the primal and dual problems together leads to important new insights into the characteristics of the LASSO estimator and to an improved method for estimating its covariance matrix. Using these results we also develop an efficient algorithm for computing LASSO estimates which is usable even in cases where the number of regressors exceeds the number of observations. An S-Plus library based on this algorithm is available from StatLib."
            },
            "slug": "On-the-LASSO-and-its-Dual-Osborne-Presnell",
            "title": {
                "fragments": [],
                "text": "On the LASSO and its Dual"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Consideration of the primal and dual problems together leads to important new insights into the characteristics of the LASSO estimator and to an improved method for estimating its covariance matrix."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49895764"
                        ],
                        "name": "J. Shao",
                        "slug": "J.-Shao",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Shao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example,  Shao (1993)  shows that one needs to let the proportion used for validation grow large in order to get cross validation to find the right model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16073176,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e0149f4926373368a0d88f7fb57c5549500fd83a",
            "isKey": false,
            "numCitedBy": 1597,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We consider the problem of selecting a model having the best predictive ability among a class of linear models. The popular leave-one-out cross-validation method, which is asymptotically equivalent to many other model selection methods such as the Akaike information criterion (AIC), the C p , and the bootstrap, is asymptotically inconsistent in the sense that the probability of selecting the model with the best predictive ability does not converge to 1 as the total number of observations n \u2192 \u221e. We show that the inconsistency of the leave-one-out cross-validation can be rectified by using a leave-n v -out cross-validation with n v , the number of observations reserved for validation, satisfying n v /n \u2192 1 as n \u2192 \u221e. This is a somewhat shocking discovery, because nv/n \u2192 1 is totally opposite to the popular leave-one-out recipe in cross-validation. Motivations, justifications, and discussions of some practical aspects of the use of the leave-n v -out cross-validation method are provided, and results ..."
            },
            "slug": "Linear-Model-Selection-by-Cross-validation-Shao",
            "title": {
                "fragments": [],
                "text": "Linear Model Selection by Cross-validation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Friedman, Hastie & Tibshirani (2000) and  Friedman (2001)  studied boosting and proposed a number of procedures, the most relevant to this discussion being least-squares boosting."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u201cSmall\u201d is important here: the \u201cbig\u201d choice \ufffd = |\ufffdc\u02c6 j| leads to the classic Forward Selection technique, which can be overly greedy, impulsively eliminating covariates which are correlated with x\u02c6 j. The Stagewise procedure is related to boosting and also to Friedman\u2019s MART algorithm ( Friedman 2001 ); see Section 8, as well as Chapter 10 and Algorithm 10.4 of Hastie et al. (2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 39450643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1679beddda3a183714d380e944fe6bf586c083cd",
            "isKey": true,
            "numCitedBy": 13780,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent boosting paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such TreeBoost models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed."
            },
            "slug": "Greedy-function-approximation:-A-gradient-boosting-Friedman",
            "title": {
                "fragments": [],
                "text": "Greedy function approximation: A gradient boosting machine."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A general gradient descent boosting paradigm is developed for additive expansions based on any fitting criterion, and specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145346320"
                        ],
                        "name": "Dean P. Foster",
                        "slug": "Dean-P.-Foster",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Foster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dean P. Foster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2347035"
                        ],
                        "name": "E. George",
                        "slug": "E.-George",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "George",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. George"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120901749,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "f9f15d0c475479c5afa6e53691cd8e0b16752ffc",
            "isKey": false,
            "numCitedBy": 598,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A new criterion is proposed for the evaluation of variable selection procedures in multiple regression. This criterion, which we call the risk inflation, is based on an adjustment to the risk. Essentially, the risk inflation is the maximum increase in risk due to selecting rather than knowing the \u00abcorrect\u00bb predictors. A new variable selection procedure is obtained which, in the case of orthogonal predictors, substantially improves on AIC, C p and BIC and is close to optimal. In contrast to AIC, C p and BIC which use dimensionality penalties of 2, 2 and log n, respectively, this new procedure uses a penalty 2 log p, where p is the number of available predictors. For the case of nonorthogonal predictors, bounds for the optimal penalty are obtained"
            },
            "slug": "The-risk-inflation-criterion-for-multiple-Foster-George",
            "title": {
                "fragments": [],
                "text": "The risk inflation criterion for multiple regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721891"
                        ],
                        "name": "Jianming Ye",
                        "slug": "Jianming-Ye",
                        "structuredName": {
                            "firstName": "Jianming",
                            "lastName": "Ye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianming Ye"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 42810613,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e94dbacf2fedaeaab614dfad3b1b40ebb7f4537",
            "isKey": false,
            "numCitedBy": 510,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In the theory of linear models, the concept of degrees of freedom plays an important role. This concept is often used for measurement of model complexity, for obtaining an unbiased estimate of the error variance, and for comparison of different models. I have developed a concept of generalized degrees of freedom (GDF) that is applicable to complex modeling procedures. The definition is based on the sum of the sensitivity of each fitted value to perturbation in the corresponding observed value. The concept is nonasymptotic in nature and does not require analytic knowledge of the modeling procedures. The concept of GDF offers a unified framework under which complex and highly irregular modeling procedures can be analyzed in the same way as classical linear models. By using this framework, many difficult problems can be solved easily. For example, one can now measure the number of observations used in a variable selection process. Different modeling procedures, such as a tree-based regression and a ..."
            },
            "slug": "On-Measuring-and-Correcting-the-Effects-of-Data-and-Ye",
            "title": {
                "fragments": [],
                "text": "On Measuring and Correcting the Effects of Data Mining and Model Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The concept of GDF offers a unified framework under which complex and highly irregular modeling procedures can be analyzed in the same way as classical linear models and many difficult problems can be solved easily."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712211"
                        ],
                        "name": "G. Golub",
                        "slug": "G.-Golub",
                        "structuredName": {
                            "firstName": "Gene",
                            "lastName": "Golub",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Golub"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 126299280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "225ca57add3b3fb12ef01cc97c4683350dc93fe4",
            "isKey": false,
            "numCitedBy": 27000,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Matrix-computations-Golub",
            "title": {
                "fragments": [],
                "text": "Matrix computations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3615612"
                        ],
                        "name": "Mary C. Meyer",
                        "slug": "Mary-C.-Meyer",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Meyer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mary C. Meyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144085136"
                        ],
                        "name": "M. Woodroofe",
                        "slug": "M.-Woodroofe",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Woodroofe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Woodroofe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " general design matrices X, it can happen that \u00b5\u02c6k fails to be almost di\ufb00erentiable, we will see that the divergence formula (4.19) \u2207\u00b7\u00b5\u02c6k(y) =k doeshold almost everywhere.Indeed,certain authors[e.g., Meyer and Woodroofe (2000)] have argued that the divergence \u2207\u00b7\u00b5b of an estimator provides itself a useful measure of the e\ufb00ective dimension of a model. Turning to LARS, we shall say that \u00b5\u02c6(y) is locally linear at a data point"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 34227879,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "61f53054eecddcca3f78915e70c62aa80af1ec8f",
            "isKey": true,
            "numCitedBy": 140,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "For the problem of estimating a regression function, \u03bc say, subject to shape constraints, like monotonicity or convexity, it is argued that the divergence of the maximum likelihood estimator provides a useful measure of the effective dimension of the model. Inequalities are derived for the expected mean squared error of the maximum likelihood estimator and the expected residual sum of squares. These generalize equalities from the case of linear regression. As an application, it is shown that the maximum likelihood estimator of the error variance \u03c3 2 is asymptotically normal with mean \u03c3 2 and variance 2\u03c3 2 /n. For monotone regression, it is shown that the maximum likelihood estimator of \u03bc attains the optimal rate of convergence, and a bias correction to the maximum likelihood estimator of \u03c3 2 is derived."
            },
            "slug": "ON-THE-DEGREES-OF-FREEDOM-IN-SHAPE-RESTRICTED-Meyer-Woodroofe",
            "title": {
                "fragments": [],
                "text": "ON THE DEGREES OF FREEDOM IN SHAPE-RESTRICTED REGRESSION"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145346320"
                        ],
                        "name": "Dean P. Foster",
                        "slug": "Dean-P.-Foster",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Foster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dean P. Foster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3071037"
                        ],
                        "name": "R. Stine",
                        "slug": "R.-Stine",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Stine",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Stine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14906848,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba89e12ab370458aadff86ba38b4eea40c36100e",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Information theory offers a coherent perspective on model selection. As in Rissanen's original application of information theory to model selection, our perspective arises from viewing a model as a component of a compressed representation of data in a two-part code. The first part of such a code is an explicit representation of the model used to compress the data. Simpler models have shorter representations. The second part is the encoded data itself. Models which fit better compress the data into shorter sequences. The objective is to choose the model which produces the shortest total message length, requiring an explicit trade-off of model complexity (length of the first part) versus goodness-of-fit (length of second part). In adidtion to Rissanen's MDL criterion, this perspective illuminates the properties of numerous model selection criteria, including AIC, Cp, BIC, RIC, and EBIC. We show that each corresponds to a specific way of coding the model parameters. By selecting the model that minimizes the total message length, our representations of these criteria reproduce their more familiar definitions. Examples from wavelets illustrate the use of these methods."
            },
            "slug": "An-Information-Theoretic-Comparison-of-Model-Foster-Stine",
            "title": {
                "fragments": [],
                "text": "An Information Theoretic Comparison of Model Selection Criteria"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "By selecting the model that minimizes the total message length, the representations of numerous model selection criteria reproduce their more familiar definitions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2550392"
                        ],
                        "name": "B. Efron",
                        "slug": "B.-Efron",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Efron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Efron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Section 6 of Efron & Tibshirani (1997) and Section 7 of Efron (1986) discuss formulas (4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 31129752,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a501aaf2c52210eea45e1786b41233d6b62c774",
            "isKey": false,
            "numCitedBy": 588,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract A regression model is fitted to an observed set of data. How accurate is the model for predicting future observations? The apparent error rate tends to underestimate the true error rate because the data have been used twice, both to fit the model and to check its accuracy. We provide simple estimates for the downward bias of the apparent error rate. The theory applies to general exponential family linear models and general measures of prediction error. Special attention is given to the case of logistic regression on binary data, with error rates measured by the proportion of misclassified cases. Several connected ideas are compared: Mallows's Cp , cross-validation, generalized cross-validation, the bootstrap, and Akaike's information criterion."
            },
            "slug": "How-Biased-is-the-Apparent-Error-Rate-of-a-Rule-Efron",
            "title": {
                "fragments": [],
                "text": "How Biased is the Apparent Error Rate of a Prediction Rule"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work provides simple estimates for the downward bias of the apparent error rate of logistic regression on binary data, with error rates measured by the proportion of misclassified cases."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123655029,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "99eb3005e85f1528269aa09b5417d6ce51a33c87",
            "isKey": false,
            "numCitedBy": 1002,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method, called the nonnegative (nn) garrote, is proposed for doing subset regression. It both shrinks and zeroes coefficients. In tests on real and simulated data, it produces lower prediction error than ordinary subset selection. It is also compared to ridge regression. If the regression equations generated by a procedure do not change drastically with small changes in the data, the procedure is called stable. Subset selection is unstable, ridge is very stable, and the nn-garrote is intermediate. Simulation results illustrate the effects of instability on prediction error."
            },
            "slug": "Better-subset-regression-using-the-nonnegative-Breiman",
            "title": {
                "fragments": [],
                "text": "Better subset regression using the nonnegative garrote"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1816048"
                        ],
                        "name": "S. Rosset",
                        "slug": "S.-Rosset",
                        "structuredName": {
                            "firstName": "Saharon",
                            "lastName": "Rosset",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rosset"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115816494"
                        ],
                        "name": "Ji Zhu",
                        "slug": "Ji-Zhu",
                        "structuredName": {
                            "firstName": "Ji",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ji Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14037360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5616db9e1e377ef580bbe5c05b6e969b16dc222",
            "isKey": false,
            "numCitedBy": 287,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we study boosting methods from a new perspective. We build on recent work by Efron et al. to show that boosting approximately (and in some cases exactly) minimizes its loss criterion with an l1 constraint on the coefficient vector. This helps understand the success of boosting with early stopping as regularized fitting of the loss criterion. For the two most commonly used criteria (exponential and binomial log-likelihood), we further show that as the constraint is relaxed---or equivalently as the boosting iterations proceed---the solution converges (in the separable case) to an \"l1-optimal\" separating hyper-plane. We prove that this l1-optimal separating hyper-plane has the property of maximizing the minimal l1-margin of the training data, as defined in the boosting literature. An interesting fundamental similarity between boosting and kernel support vector machines emerges, as both can be described as methods for regularized optimization in high-dimensional predictor space, using a computational trick to make the calculation practical, and converging to margin-maximizing solutions. While this statement describes SVMs exactly, it applies to boosting only approximately."
            },
            "slug": "Boosting-as-a-Regularized-Path-to-a-Maximum-Margin-Rosset-Zhu",
            "title": {
                "fragments": [],
                "text": "Boosting as a Regularized Path to a Maximum Margin Classifier"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "It is built on recent work by Efron et al. to show that boosting approximately (and in some cases exactly) minimizes its loss criterion with an l1 constraint on the coefficient vector, and shows that as the constraint is relaxed the solution converges (in the separable case) to an \"l1-optimal\" separating hyper-plane."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The approximate regularized optimization view which emerges from our results allows us to better understand boosting and its great empirical success [ Breiman (1999) ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14488820,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a92684c164b0c46020a371ae5116df74bb37a412",
            "isKey": false,
            "numCitedBy": 551,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The theory behind the success of adaptive reweighting and combining algorithms (arcing) such as Adaboost (Freund & Schapire, 1996a, 1997) and others in reducing generalization error has not been well understood. By formulating prediction as a game where one player makes a selection from instances in the training set and the other a convex linear combination of predictors from a finite set, existing arcing algorithms are shown to be algorithms for finding good game strategies. The minimax theorem is an essential ingredient of the convergence proofs. An arcing algorithm is described that converges to the optimal strategy. A bound on the generalization error for the combined predictors in terms of their maximum error is proven that is sharper than bounds to date. Schapire, Freund, Bartlett, and Lee (1997) offered an explanation of why Adaboost works in terms of its ability to produce generally high margins. The empirical comparison of Adaboost to the optimal arcing algorithm shows that their explanation is not complete."
            },
            "slug": "Prediction-Games-and-Arcing-Algorithms-Breiman",
            "title": {
                "fragments": [],
                "text": "Prediction Games and Arcing Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The theory behind the success of adaptive reweighting and combining algorithms (arcing) such as Adaboost and others in reducing generalization error has not been well understood, and an explanation of whyAdaboost works in terms of its ability to produce generally high margins is offered."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145064923"
                        ],
                        "name": "C. Lawson",
                        "slug": "C.-Lawson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Lawson",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lawson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144308260"
                        ],
                        "name": "R. Hanson",
                        "slug": "R.-Hanson",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Hanson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hanson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122862057,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b35e036e0226d923d03c06ae393084b40c9dec7e",
            "isKey": false,
            "numCitedBy": 4447,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Since the lm function provides a lot of features it is rather complicated. So we are going to instead use the function lsfit as a model. It computes only the coefficient estimates and the residuals. Now would be a good time to read the help file for lsfit. Note that lsfit supports the fitting of multiple least squares models and weighted least squares. Our function will not, hence we can omit the arguments wt, weights and yname. Also, changing tolerances is a little advanced so we will trust the default values and omit the argument tolerance as well."
            },
            "slug": "Solving-least-squares-problems-Lawson-Hanson",
            "title": {
                "fragments": [],
                "text": "Solving least squares problems"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "Since the lm function provides a lot of features it is rather complicated so it is going to instead use the function lsfit as a model, which computes only the coefficient estimates and the residuals."
            },
            "venue": {
                "fragments": [],
                "text": "Classics in applied mathematics"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2741460"
                        ],
                        "name": "H. Ishwaran",
                        "slug": "H.-Ishwaran",
                        "structuredName": {
                            "firstName": "Hemant",
                            "lastName": "Ishwaran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ishwaran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144319330"
                        ],
                        "name": "J. S. Rao",
                        "slug": "J.-S.-Rao",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Rao",
                            "middleNames": [
                                "Sunil"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. S. Rao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Details for SVS can be found in  Ishwaran and Rao (2000, 2003) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5874933,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "4d6022aa45c5289f9e1cc378c8348f97e1625be4",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "DNA microarrays open up a broad new horizon for investigators interested in studying the genetic determinants of disease. The high throughput nature of these arrays, where differential expression for thousands of genes can be measured simultaneously, creates an enormous wealth of information, but also poses a challenge for data analysis because of the large multiple testing problem involved. The solution has generally been to focus on optimizing false-discovery rates while sacrificing power. The drawback of this approach is that more subtle expression differences will be missed that might give investigators more insight into the genetic environment necessary for a disease process to take hold. We introduce a new method for detecting differentially expressed genes based on a high-dimensional model selection technique, Bayesian ANOVA for microarrays (BAM), which strikes a balance between false rejections and false nonrejections. The basis of the new approach involves a weighted average of generalized ridge regression estimates that provides the benefits of using shrinkage estimation combined with model averaging. A simple graphical tool based on the amount of shrinkage is developed to visualize the trade-off between low false-discovery rates and finding more genes. Simulations are used to illustrate BAM's performance, and the method is applied to a large database of colon cancer gene expression data. Our working hypothesis in the colon cancer analysis is that large differential expressions may not be the only ones contributing to metastasis\u2014in fact, moderate changes in expression of genes may be involved in modifying the genetic environment to a sufficient extent for metastasis to occur. A functional biological analysis of gene effects found by BAM, but not other false-discovery-based approaches, lends support to this hypothesis."
            },
            "slug": "Detecting-Differentially-Expressed-Genes-in-Using-Ishwaran-Rao",
            "title": {
                "fragments": [],
                "text": "Detecting Differentially Expressed Genes in Microarrays Using Bayesian Model Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new method for detecting differentially expressed genes based on a high-dimensional model selection technique, Bayesian ANOVA for microarrays (BAM), which strikes a balance between false rejections and false nonrejections."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712211"
                        ],
                        "name": "G. Golub",
                        "slug": "G.-Golub",
                        "structuredName": {
                            "firstName": "Gene",
                            "lastName": "Golub",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Golub"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 126299280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "225ca57add3b3fb12ef01cc97c4683350dc93fe4",
            "isKey": false,
            "numCitedBy": 27000,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Matrix-computations-Golub",
            "title": {
                "fragments": [],
                "text": "Matrix computations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144532254"
                        ],
                        "name": "Calyampudi R. Rao",
                        "slug": "Calyampudi-R.-Rao",
                        "structuredName": {
                            "firstName": "Calyampudi",
                            "lastName": "Rao",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Calyampudi R. Rao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124210263,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1a2edbdea7b7b8daec04f39effa5674b451dc0b0",
            "isKey": false,
            "numCitedBy": 1723,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Algebra of Vectors and Matrices. Probability Theory, Tools and Techniques. Continuous Probability Models. The Theory of Least Squares and Analysis of Variance. Criteria and Methods of Estimation. Large Sample Theory and Methods. Theory of Statistical Inference. Multivariate Analysis. Publications of the Author. Author Index. Subject Index."
            },
            "slug": "Linear-statistical-inference-and-its-applications-Rao",
            "title": {
                "fragments": [],
                "text": "Linear statistical inference and its applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2025123"
                        ],
                        "name": "F. Abramovich",
                        "slug": "F.-Abramovich",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Abramovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Abramovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808156"
                        ],
                        "name": "Y. Benjamini",
                        "slug": "Y.-Benjamini",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Benjamini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Benjamini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2364792"
                        ],
                        "name": "I. Johnstone",
                        "slug": "I.-Johnstone",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Johnstone",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Johnstone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[note that the FDR penalty proposed in  Abramovich, Benjamini, Donoho and Johnstone (2000)  corresponds to the case C = 1]. It is proved in Birg\u00b4e and Massart (2001a) that if the penalty pen(k) is heavy enough (i.e., C > 1 and C \u2032 is"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "efforts would be required to complete this analysis and provide rigorous oracle inequalities in the spirit of those given in Birg\u00b4e and Massart (2001a, b ) or Abramovich, Benjamini, Donoho and Johnstone ( 2000) and also some simulations to check whether our proposal to estimate \ufffd 2 is valid or not."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Indeed, there has been an enormous range of contribution in model selection proposals, including work by Akaike (1973), Mallows (1973), Foster and George (1994), Birg\u00b4e and Massart (2001a)  and Abramovich, Benjamini, Donoho and Johnstone ( 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7581060,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1aacd77be39403ca1dbabc050ce89bea3162a00c",
            "isKey": false,
            "numCitedBy": 434,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "We attempt to recover an n-dimensional vector observed in white noise, where n is large and the vector is known to be sparse, but the degree of sparsity is unknown. We consider three different ways of defining sparsity of a vector: using the fraction of nonzero terms; imposing power-law decay bounds on the ordered entries; and controlling the lp norm for p small. We obtain a procedure which is asymptotically minimax for l r loss, simultaneously throughout a range of such sparsity classes. The optimal procedure is a data-adaptive thresholding scheme, driven by control of the False Discovery Rate (FDR). FDR control is a relatively recent innovation in simultaneous testing, ensuring that at most a certain fraction of the rejected null hypotheses will correspond to false rejections. In our treatment, the FDR control parameter qn also plays a determining role in asymptotic minimaxity. If q = lim qn \u2208 [0,1/2] and also qn > \u03b3/log(n) we get sharp asymptotic minimaxity, simultaneously, over a wide range of sparse parameter spaces and loss functions. On the other hand, q = lim qn \u2208 (1/2,1], forces the risk to exceed the minimax risk by a factor growing with q. To our knowledge, this relation between ideas in simultaneous inference and asymptotic decision theory is new. Our work provides a new perspective on a class of model selection rules which has been introduced recently by several authors. These new rules impose complexity penalization of the form 2 \ufffd log( potential model size / actual model size ). We exhibit a close connection with FDR-controlling procedures under stringent control of the false discovery rate."
            },
            "slug": "Adapting-to-unknown-sparsity-by-controlling-the-Abramovich-Benjamini",
            "title": {
                "fragments": [],
                "text": "Adapting to unknown sparsity by controlling the false discovery rate"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work provides a new perspective on a class of model selection rules which has been introduced recently by several authors, and exhibits a close connection with FDR-controlling procedures under stringent control of the false discovery rate."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207738357,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "046b7f6b48e4d9fcf173dea0a0802d7e87b383e1",
            "isKey": false,
            "numCitedBy": 5491,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy."
            },
            "slug": "Bagging-Predictors-Breiman",
            "title": {
                "fragments": [],
                "text": "Bagging Predictors"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737945"
                        ],
                        "name": "H. Akaike",
                        "slug": "H.-Akaike",
                        "structuredName": {
                            "firstName": "Hirotugu",
                            "lastName": "Akaike",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Akaike"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 119829779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "836512ed5226eaad454c48a81fcea21d48a36ef3",
            "isKey": false,
            "numCitedBy": 962,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY Closed form representations of the gradients and an approximation to the Hessian are given for an asymptotic approximation to the log likelihood function of a multidimensional autoregressive moving average Gaussian process. Their use for the numerical maximization of the likelihood function is discussed. It is shown that the procedure described by Hannan (1969) for the estimation of the parameters of one-dimensional autoregressive moving average processes is equivalent to a three-stage realization of one step of the NewtonRaphson procedure for the numerical maximization of the likelihood function, using the gradient and the approximate Hessian. This makes it straightforward to extend the procedure to the multidimensional case. The use of the block Toeplitz type characteristic of the approximate Hessian is pointed out."
            },
            "slug": "Maximum-likelihood-identification-of-Gaussian-Akaike",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood identification of Gaussian autoregressive moving average models"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "It is shown that the procedure described by Hannan (1969) for the estimation of the parameters of one-dimensional autoregressive moving average processes is equivalent to a three-stage realization of one step of the NewtonRaphson procedure for the numerical maximization of the likelihood function, using the gradient and the approximate Hessian."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808156"
                        ],
                        "name": "Y. Benjamini",
                        "slug": "Y.-Benjamini",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Benjamini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Benjamini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48775815"
                        ],
                        "name": "Y. Hochberg",
                        "slug": "Y.-Hochberg",
                        "structuredName": {
                            "firstName": "Yosef",
                            "lastName": "Hochberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Hochberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The form of Sp relies on approximations for normal order statistics commonly used in variable selection, particularly adaptive methods [ Benjamini and Hochberg (1995)  and Foster and Stine (1996)]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 45174121,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "fcef2258a963f3d3984a486185ddc4349c43aa35",
            "isKey": false,
            "numCitedBy": 75505,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses -the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferronitype procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples."
            },
            "slug": "Controlling-the-false-discovery-rate:-a-practical-Benjamini-Hochberg",
            "title": {
                "fragments": [],
                "text": "Controlling the false discovery rate: a practical and powerful approach to multiple testing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16243226"
                        ],
                        "name": "S. Weisberg",
                        "slug": "S.-Weisberg",
                        "structuredName": {
                            "firstName": "Sanford",
                            "lastName": "Weisberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Weisberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121185262,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "81ce5dcdcf7b3852b8d4be739559192730dccc3b",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A subdivision of the C p , statistic into individual components for each case is developed, and the properties of this statistic are outlined. This subdivision depends on both the change in fitted value or residual from the full model to the subset model and the change in leverage, but it does not depend directly on the values of the residuals. It is useful both as an aid in understanding C p and as a diagnostic procedure."
            },
            "slug": "A-Statistic-for-Allocating-C-p-to-Individual-Cases-Weisberg",
            "title": {
                "fragments": [],
                "text": "A Statistic for Allocating C p to Individual Cases"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712211"
                        ],
                        "name": "G. Golub",
                        "slug": "G.-Golub",
                        "structuredName": {
                            "firstName": "Gene",
                            "lastName": "Golub",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Golub"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 126299280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "225ca57add3b3fb12ef01cc97c4683350dc93fe4",
            "isKey": false,
            "numCitedBy": 27000,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Matrix-computations-Golub",
            "title": {
                "fragments": [],
                "text": "Matrix computations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144532254"
                        ],
                        "name": "Calyampudi R. Rao",
                        "slug": "Calyampudi-R.-Rao",
                        "structuredName": {
                            "firstName": "Calyampudi",
                            "lastName": "Rao",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Calyampudi R. Rao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124210263,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1a2edbdea7b7b8daec04f39effa5674b451dc0b0",
            "isKey": false,
            "numCitedBy": 1723,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Algebra of Vectors and Matrices. Probability Theory, Tools and Techniques. Continuous Probability Models. The Theory of Least Squares and Analysis of Variance. Criteria and Methods of Estimation. Large Sample Theory and Methods. Theory of Statistical Inference. Multivariate Analysis. Publications of the Author. Author Index. Subject Index."
            },
            "slug": "Linear-statistical-inference-and-its-applications-Rao",
            "title": {
                "fragments": [],
                "text": "Linear statistical inference and its applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16243226"
                        ],
                        "name": "S. Weisberg",
                        "slug": "S.-Weisberg",
                        "structuredName": {
                            "firstName": "Sanford",
                            "lastName": "Weisberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Weisberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121301894,
            "fieldsOfStudy": [
                "Mathematics",
                "Business"
            ],
            "id": "618288ca83856598cb047e5b053a24191225fb64",
            "isKey": false,
            "numCitedBy": 2466,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface.1 Scatterplots and Regression.1.1 Scatterplots.1.2 Mean Functions.1.3 Variance Functions.1.4 Summary Graph.1.5 Tools for Looking at Scatterplots.1.5.1 Size.1.5.2 Transformations.1.5.3 Smoothers for the Mean Function.1.6 Scatterplot Matrices.Problems.2 Simple Linear Regression.2.1 Ordinary Least Squares Estimation.2.2 Least Squares Criterion.2.3 Estimating sigma 2.2.4 Properties of Least Squares Estimates.2.5 Estimated Variances.2.6 Comparing Models: The Analysis of Variance.2.6.1 The F-Test for Regression.2.6.2 Interpreting p-values.2.6.3 Power of Tests.2.7 The Coefficient of Determination, R2.2.8 Confidence Intervals and Tests.2.8.1 The Intercept.2.8.2 Slope.2.8.3 Prediction.2.8.4 Fitted Values.2.9 The Residuals.Problems.3 Multiple Regression.3.1 Adding a Term to a Simple Linear Regression Model.3.1.1 Explaining Variability.3.1.2 Added-Variable Plots.3.2 The Multiple Linear Regression Model.3.3 Terms and Predictors.3.4 Ordinary Least Squares.3.4.1 Data and Matrix Notation.3.4.2 Variance-Covariance Matrix of e.3.4.3 Ordinary Least Squares Estimators.3.4.4 Properties of the Estimates.3.4.5 Simple Regression in Matrix Terms.3.5 The Analysis of Variance.3.5.1 The Coefficient of Determination.3.5.2 Hypotheses Concerning One of the Terms.3.5.3 Relationship to the t -Statistic.3.5.4 t-Tests and Added-Variable Plots.3.5.5 Other Tests of Hypotheses.3.5.6 Sequential Analysis of Variance Tables.3.6 Predictions and Fitted Values.Problems.4 Drawing Conclusions.4.1 Understanding Parameter Estimates.4.1.1 Rate of Change.4.1.2 Signs of Estimates.4.1.3 Interpretation Depends on Other Terms in the Mean Function.4.1.4 Rank Deficient and Over-Parameterized Mean Functions.4.1.5 Tests.4.1.6 Dropping Terms.4.1.7 Logarithms.4.2 Experimentation Versus Observation.4.3 Sampling from a Normal Population.4.4 More on R2.4.4.1 Simple Linear Regression and R2.4.4.2 Multiple Linear Regression.4.4.3 Regression through the Origin.4.5 Missing Data.4.5.1 Missing at Random.4.5.2 Alternatives.4.6 Computationally Intensive Methods.4.6.1 Regression Inference without Normality.4.6.2 Nonlinear Functions of Parameters.4.6.3 Predictors Measured with Error.Problems.5 Weights, Lack of Fit, and More.5.1 Weighted Least Squares.5.1.1 Applications of Weighted Least Squares.5.1.2 Additional Comments.5.2 Testing for Lack of Fit, Variance Known.5.3 Testing for Lack of Fit, Variance Unknown.5.4 General F Testing.5.4.1 Non-null Distributions.5.4.2 Additional Comments.5.5 Joint Confidence Regions.Problems.6 Polynomials and Factors.6.1 Polynomial Regression.6.1.1 Polynomials with Several Predictors.6.1.2 Using the Delta Method to Estimate a Minimum or a Maximum.6.1.3 Fractional Polynomials.6.2 Factors.6.2.1 No Other Predictors.6.2.2 Adding a Predictor: Comparing Regression Lines.6.2.3 Additional Comments.6.3 Many Factors.6.4 Partial One-Dimensional Mean Functions.6.5 Random Coefficient Models.Problems.7 Transformations.7.1 Transformations and Scatterplots.7.1.1 Power Transformations.7.1.2 Transforming Only the Predictor Variable.7.1.3 Transforming the Response Only.7.1.4 The Box and Cox Method.7.2 Transformations and Scatterplot Matrices.7.2.1 The 1D Estimation Result and Linearly Related Predictors.7.2.2 Automatic Choice of Transformation of Predictors.7.3 Transforming the Response.7.4 Transformations of Nonpositive Variables.Problems.8 Regression Diagnostics: Residuals.8.1 The Residuals.8.1.1 Difference Between e and e.8.1.2 The Hat Matrix.8.1.3 Residuals and the Hat Matrix with Weights.8.1.4 The Residuals When the Model Is Correct.8.1.5 The Residuals When the Model Is Not Correct.8.1.6 Fuel Consumption Data.8.2 Testing for Curvature.8.3 Nonconstant Variance.8.3.1 Variance Stabilizing Transformations.8.3.2 A Diagnostic for Nonconstant Variance.8.3.3 Additional Comments.8.4 Graphs for Model Assessment.8.4.1 Checking Mean Functions.8.4.2 Checking Variance Functions.Problems.9 Outliers and Influence.9.1 Outliers.9.1.1 An Outlier Test.9.1.2 Weighted Least Squares.9.1.3 Significance Levels for the Outlier Test.9.1.4 Additional Comments.9.2 Influence of Cases.9.2.1 Cook's Distance.9.2.2 Magnitude of Di .9.2.3 Computing Di .9.2.4 Other Measures of Influence.9.3 Normality Assumption.Problems.10 Variable Selection.10.1 The Active Terms.10.1.1 Collinearity.10.1.2 Collinearity and Variances.10.2 Variable Selection.10.2.1 Information Criteria.10.2.2 Computationally Intensive Criteria.10.2.3 Using Subject-Matter Knowledge.10.3 Computational Methods.10.3.1 Subset Selection Overstates Significance.10.4 Windmills.10.4.1 Six Mean Functions.10.4.2 A Computationally Intensive Approach.Problems.11 Nonlinear Regression.11.1 Estimation for Nonlinear Mean Functions.11.2 Inference Assuming Large Samples.11.3 Bootstrap Inference.11.4 References.Problems.12 Logistic Regression.12.1 Binomial Regression.12.1.1 Mean Functions for Binomial Regression.12.2 Fitting Logistic Regression.12.2.1 One-Predictor Example.12.2.2 Many Terms.12.2.3 Deviance.12.2.4 Goodness-of-Fit Tests.12.3 Binomial Random Variables.12.3.1 Maximum Likelihood Estimation.12.3.2 The Log-Likelihood for Logistic Regression.12.4 Generalized Linear Models.Problems.Appendix.A.1 Web Site.A.2 Means and Variances of Random Variables.A.2.1 E Notation.A.2.2 Var Notation.A.2.3 Cov Notation.A.2.4 Conditional Moments.A.3 Least Squares for Simple Regression.A.4 Means and Variances of Least Squares Estimates.A.5 Estimating E(Y |X) Using a Smoother.A.6 A Brief Introduction to Matrices and Vectors.A.6.1 Addition and Subtraction.A.6.2 Multiplication by a Scalar.A.6.3 Matrix Multiplication.A.6.4 Transpose of a Matrix.A.6.5 Inverse of a Matrix.A.6.6 Orthogonality.A.6.7 Linear Dependence and Rank of a Matrix.A.7 Random Vectors.A.8 Least Squares Using Matrices.A.8.1 Properties of Estimates.A.8.2 The Residual Sum of Squares.A.8.3 Estimate of Variance.A.9 The QR Factorization.A.10 Maximum Likelihood Estimates.A.11 The Box-Cox Method for Transformations.A.11.1 Univariate Case.A.11.2 Multivariate Case.A.12 Case Deletion in Linear Regression.References.Author Index.Subject Index."
            },
            "slug": "Applied-Linear-Regression-Weisberg",
            "title": {
                "fragments": [],
                "text": "Applied Linear Regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2655181"
                        ],
                        "name": "C. Mallows",
                        "slug": "C.-Mallows",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Mallows",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mallows"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 30342955,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f603e9476b6613e4f36c5848902a17b3af19e1cb",
            "isKey": false,
            "numCitedBy": 1070,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss the interpretation of Cp -plots and show how they can be calibrated in several ways. We comment on the practice of using the display as a basis for formal selection of a subset-regression model, and extend the range of application of the device to encompass arbitrary linear estimates of the regression coefficients, for example Ridge estimates."
            },
            "slug": "Some-Comments-on-Cp-Mallows",
            "title": {
                "fragments": [],
                "text": "Some Comments on Cp"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The practice of using the display as a basis for formal selection of a subset-regression model is commented on, and the range of application of the device is extended to encompass arbitrary linear estimates of the regression coefficients."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143802728"
                        ],
                        "name": "R. Cook",
                        "slug": "R.-Cook",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Cook",
                            "middleNames": [
                                "Dennis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cook"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16243226"
                        ],
                        "name": "S. Weisberg",
                        "slug": "S.-Weisberg",
                        "structuredName": {
                            "firstName": "Sanford",
                            "lastName": "Weisberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Weisberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123029478,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2f4c7253709d16cc50a9dbe671a843c816e25b1c",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In an influential article in the February 1973 issue of The American Statistician Frank Anscombe remarked that \u201c[m]ost textbooks on statistical methods, and most statistical computer programs, pay too little attention to graphs.\u201d This general observation no longer holds. Graphs of data are everywhere. Our children learn to draw and use bar charts in kindergarten and boxplots in the elementary grades; most newspapers and magazines regularly use graphical representation of data, and nearly all books on statistical methods use graphs. Graphs can be drawn in statistical packages, in spreadsheet programs, and in stand-alone graphics packages. Statisticians often bemoan the poor construction of the graphs produced, but even when the construction is adequate, graphs can be of little value. In this article, we argue that useful graphs must have a context induced by associated theory, and that a graph without the well-understood statistical context is hardly worth drawing."
            },
            "slug": "Graphs-in-Statistical-Analysis:-Is-the-Medium-the-Cook-Weisberg",
            "title": {
                "fragments": [],
                "text": "Graphs in Statistical Analysis: Is the Medium the Message?"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2655181"
                        ],
                        "name": "C. Mallows",
                        "slug": "C.-Mallows",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Mallows",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mallows"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 30342955,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f603e9476b6613e4f36c5848902a17b3af19e1cb",
            "isKey": false,
            "numCitedBy": 1070,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss the interpretation of Cp -plots and show how they can be calibrated in several ways. We comment on the practice of using the display as a basis for formal selection of a subset-regression model, and extend the range of application of the device to encompass arbitrary linear estimates of the regression coefficients, for example Ridge estimates."
            },
            "slug": "Some-Comments-on-Cp-Mallows",
            "title": {
                "fragments": [],
                "text": "Some Comments on Cp"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The practice of using the display as a basis for formal selection of a subset-regression model is commented on, and the range of application of the device is extended to encompass arbitrary linear estimates of the regression coefficients."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49243178"
                        ],
                        "name": "Ker-Chau Li",
                        "slug": "Ker-Chau-Li",
                        "structuredName": {
                            "firstName": "Ker-Chau",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ker-Chau Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30158078,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9a42098359624a4a62da7103a49e3ace2bcd1249",
            "isKey": false,
            "numCitedBy": 1944,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Modern advances in computing power have greatly widened scientists' scope in gathering and investigating information from many variables, information which might have been ignored in the past. Yet to effectively scan a large pool of variables is not an easy task, although our ability to interact with data has been much enhanced by recent innovations in dynamic graphics. In this article, we propose a novel data-analytic tool, sliced inverse regression (SIR), for reducing the dimension of the input variable x without going through any parametric or nonparametric model-fitting process. This method explores the simplicity of the inverse view of regression; that is, instead of regressing the univariate output variable y against the multivariate x, we regress x against y. Forward regression and inverse regression are connected by a theorem that motivates this method. The theoretical properties of SIR are investigated under a model of the form, y = f(\u03b2 1 x, \u2026, \u03b2 K x, e), where the \u03b2 k 's are the unknown..."
            },
            "slug": "Sliced-Inverse-Regression-for-Dimension-Reduction-Li",
            "title": {
                "fragments": [],
                "text": "Sliced Inverse Regression for Dimension Reduction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144294100"
                        ],
                        "name": "C. Stein",
                        "slug": "C.-Stein",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Stein",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Stein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The proof, which appears later in this Section, is an application of Stein\u2019s unbiased risk estimate (SURE), (Stein 1981)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121087237,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dc423c0563ac6c80ae8af840c8560438644a0e59",
            "isKey": false,
            "numCitedBy": 2575,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimation-of-the-Mean-of-a-Multivariate-Normal-Stein",
            "title": {
                "fragments": [],
                "text": "Estimation of the Mean of a Multivariate Normal Distribution"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " Hastie, Tibshirani and Friedman (2001)  noted the the striking similarity between Forward Stagewise regression and the Lasso, and conjectured that this may help explain the success of the Forward Stagewise process used in least squares boosting."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We see that the Lasso tends to shrink the OLS coefficients toward 0, more sofor small values of t. Shrinkage often improves prediction accuracy, trading off decreased variance for increased bias as discussed in  Hastie, Tibshirani and Friedman (2001) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 40654213,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bee570503aaa0ed5bc5dd4cf6aa742df0b5cef87",
            "isKey": false,
            "numCitedBy": 13814,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book.\n\nThis major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates.\n\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting."
            },
            "slug": "The-Elements-of-Statistical-Learning:-Data-Mining,-Hastie-Tibshirani",
            "title": {
                "fragments": [],
                "text": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd Edition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering."
            },
            "venue": {
                "fragments": [],
                "text": "Springer Series in Statistics"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2347035"
                        ],
                        "name": "E. George",
                        "slug": "E.-George",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "George",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. George"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145346320"
                        ],
                        "name": "Dean P. Foster",
                        "slug": "Dean-P.-Foster",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Foster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dean P. Foster"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15024291,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "69b865c41e7e263b32854488ad09a63e1e7465d2",
            "isKey": false,
            "numCitedBy": 273,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "For the problem of variable selection for the normal linear model, selection criteria such as AIC, C p , BIC and RIC have fixed dimensionality penalties. Such criteria are shown to correspond to selection of maximum posterior models under implicit hyperparameter choices for a particular hierarchical Bayes formulation. Based on this calibration, we propose empirical Bayes selection criteria that use hyperparameter estimates instead of fixed choices. For obtaining these estimates, both marginal and conditional maximum likelihood methods are considered. As opposed to traditional fixed penalty criteria, these empirical Bayes criteria have dimensionality penalties that depend on the data. Their performance is seen to approximate adaptively the performance of the best fixed-penalty criterion across a variety of orthogonal and nonorthogonal set-ups, including wavelet regression. Empirical Bayes shrinkage estimators of the selected coefficients are also proposed."
            },
            "slug": "Calibration-and-empirical-Bayes-variable-selection-George-Foster",
            "title": {
                "fragments": [],
                "text": "Calibration and empirical Bayes variable selection"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, Forward Stagewise ideas are used in \u201cBoosting\u201d, an important class of fitting methods for data mining introduced by  Freund & Schapire (1997) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6644398,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ba566223e426677d12a9a18418c023a4deec77e",
            "isKey": false,
            "numCitedBy": 13129,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line."
            },
            "slug": "A-decision-theoretic-generalization-of-on-line-and-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "A decision-theoretic generalization of on-line learning and an application to boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The model studied can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting, and it is shown that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "EuroCOLT"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2378652"
                        ],
                        "name": "R. Olshen",
                        "slug": "R.-Olshen",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Olshen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Olshen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103556459"
                        ],
                        "name": "C. J. Stone",
                        "slug": "C.-J.-Stone",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Stone",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. J. Stone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Boosting can use any adaptive fitting procedure as its \u201cbase learner\u201d (model fitter): trees are a popular choice, as implemented in CART [ Breiman, Friedman, Olshen and Stone (1984) ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29458883,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8017699564136f93af21575810d557dba1ee6fc6",
            "isKey": false,
            "numCitedBy": 16307,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Background. Introduction to Tree Classification. Right Sized Trees and Honest Estimates. Splitting Rules. Strengthening and Interpreting. Medical Diagnosis and Prognosis. Mass Spectra Classification. Regression Trees. Bayes Rules and Partitions. Optimal Pruning. Construction of Trees from a Learning Sample. Consistency. Bibliography. Notation Index. Subject Index."
            },
            "slug": "Classification-and-Regression-Trees-Breiman-Friedman",
            "title": {
                "fragments": [],
                "text": "Classification and Regression Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This chapter discusses tree classification in the context of medicine, where right Sized Trees and Honest Estimates are considered and Bayes Rules and Partitions are used as guides to optimal pruning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298005"
                        ],
                        "name": "Catherine Blake",
                        "slug": "Catherine-Blake",
                        "structuredName": {
                            "firstName": "Catherine",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Catherine Blake"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Statistics would be well served by having a repository of test problems comparable to those held at UC Irvine for judging machine learning algorithms [ Blake and Merz (1998) ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62622768,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e068be31ded63600aea068eacd12931efd2a1029",
            "isKey": false,
            "numCitedBy": 13446,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "UCI-Repository-of-machine-learning-databases-Blake",
            "title": {
                "fragments": [],
                "text": "UCI Repository of machine learning databases"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 10,
            "methodology": 10,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 38,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Least-angle-regression-Efron-Hastie/1c7c5595dc7a1f5d360acf5c360ca1ca49536ba5?sort=total-citations"
}