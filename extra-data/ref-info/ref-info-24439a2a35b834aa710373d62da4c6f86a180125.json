{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2086214008"
                        ],
                        "name": "Jacques Courtin",
                        "slug": "Jacques-Courtin",
                        "structuredName": {
                            "firstName": "Jacques",
                            "lastName": "Courtin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacques Courtin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3324938"
                        ],
                        "name": "Damien Genthial",
                        "slug": "Damien-Genthial",
                        "structuredName": {
                            "firstName": "Damien",
                            "lastName": "Genthial",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Damien Genthial"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "[2], Courtin and Genthial [8], Samuelsson [25])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 216
                            }
                        ],
                        "text": "The most common approach is probably to use some version of the dynamic programming algorithms familiar from context-free parsing, with or without statistical disambiguation (Eisner [15, 16, 17], Barbero et al. [2], Courtin and Genthial [8], Samuelsson [25])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 103
                            }
                        ],
                        "text": "Directed D-rules are also similar to but simpler than the dependency relations of Courtin and Genthial [8]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 12960994,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d7fe5d04e0af68f453e48239d4f6ef1029442fb",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "After a short recall of our view of dependency grammars, we present two dependency parsers. The first uses dependency relations to have a more concise expression of dependency rules and to get efficiency in parsing. The second uses typed feature structures to add some semantic knowledge on dependency trees and parses in a more robust left to right manner."
            },
            "slug": "Parsing-with-Dependency-Relations-and-Robust-Courtin-Genthial",
            "title": {
                "fragments": [],
                "text": "Parsing with Dependency Relations and Robust Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "Two dependency parsers are presented: the first uses dependency relations to have a more concise expression of dependency rules and to get efficiency in parsing and the second uses typed feature structures to add some semantic knowledge on dependency trees and parses in a more robust left to right manner."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49770410"
                        ],
                        "name": "H. Maruyama",
                        "slug": "H.-Maruyama",
                        "structuredName": {
                            "firstName": "Hiroshi",
                            "lastName": "Maruyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Maruyama"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "Another school proposes that dependency parsing should be cast as a constraint satisfaction problem and solved using constraint programming (Maruyama [22], Menzel and Schr\u00f6der [24], Duchier [12, 13, 14])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2167791,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "532eecf44ee2912a87cd8ec5756ba5b03c427eaa",
            "isKey": false,
            "numCitedBy": 200,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new grammatical formalism called Constraint Dependency Grammar (CDG) in which every grammatical rule is given as a constraint on word-to-word modifications. CDG parsing is formalized as a constraint satisfaction problem over a finite domain so that efficient constraint-propagation algorithms can be employed to reduce structural ambiguity without generating individual parse trees. The weak generative capacity and the computational complexity of CDG parsing are also discussed."
            },
            "slug": "Structural-Disambiguation-with-Constraint-Maruyama",
            "title": {
                "fragments": [],
                "text": "Structural Disambiguation with Constraint Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A new grammatical formalism called Constraint Dependency Grammar (CDG) is presented, in which every grammatical rule is given as a constraint on word-to-word modifications."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145416266"
                        ],
                        "name": "Glenn Carroll",
                        "slug": "Glenn-Carroll",
                        "structuredName": {
                            "firstName": "Glenn",
                            "lastName": "Carroll",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Glenn Carroll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 179
                            }
                        ],
                        "text": "Most formalizations of dependency grammar use rules that specify whole con gurations of dependents for a given head, using some notion of valence frames (Hays [19], Gaifman [18], Carroll and Charniak [3], Sleator and Temperley [27, 28], Barbero et al. [2], Eisner [17], Debusmann [11])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 200
                            }
                        ],
                        "text": "Most formalizations of dependency grammar use rules that specify whole con gurations of dependents for a given head, using some notion of valence frames (Hays [19], Gaifman [18], Carroll and Charniak [3], Sleator and Temperley [27, 28], Barbero et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9840566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb34c9981c50bde33d165a7f5faeb72018aa4d09",
            "isKey": false,
            "numCitedBy": 186,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a scheme for learning probabilistic dependency grammars from positive training examples plus constraints on rules. In particular, we present the results of two experiments. The first, in which the constraints were minimal, was unsuccessful. The second, with significant constraints, was successful within the bounds of the task we had set."
            },
            "slug": "Two-Experiments-on-Learning-Probabilistic-Grammars-Carroll-Charniak",
            "title": {
                "fragments": [],
                "text": "Two Experiments on Learning Probabilistic Dependency Grammars from Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "This work presents a scheme for learning probabilistic dependency grammars from positive training examples plus constraints on rules plus results of two experiments, in which the constraints were minimal and the first experiment was unsuccessful."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2741927"
                        ],
                        "name": "M. Covington",
                        "slug": "M.-Covington",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Covington",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Covington"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 156
                            }
                        ],
                        "text": ", Collins [4, 5, 6], Eisner [15, 16, 17], Samuelsson [25]) and they also appear well suited for languages with less rigid word order constraints (Covington [9, 10], Collins et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17239542,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77557bdd1c0209dea4a2114df83b581d01cfc9da",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "The dependency\u2013based free\u2013word\u2013order parsing algorithm of Covington (1987, 1990, 1992) can be extended to handle partly or completely fixed word order, while preserving its psychologically realistic preference for near attachment. By adding predictivity, this algorithm can be adapted to parse left\u2013branching and right\u2013branching structures in less stack space than center\u2013embedded structures, just as the human parser appears to do."
            },
            "slug": "Discontinuous-dependency-parsing-of-free-and-fixed-Covington",
            "title": {
                "fragments": [],
                "text": "Discontinuous dependency parsing of free and fixed word order: Work in progress"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The dependency\u2013based free\u2013word\u2013order parsing algorithm of Covington can be extended to handle partly or completely fixed word order, while preserving its psychologically realistic preference for near attachment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145043214"
                        ],
                        "name": "Jason Eisner",
                        "slug": "Jason-Eisner",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Eisner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Eisner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 158
                            }
                        ],
                        "text": "However, dependency-based representations have turned out to be useful in statistical approaches to parsing and disambiguation (see, e.g., Collins [4, 5, 6], Eisner [15, 16, 17], Samuelsson [25]) and they also appear well suited for languages with less rigid word order constraints (Covington [9, 10], Collins et al. [7])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 28
                            }
                        ],
                        "text": ", Collins [4, 5, 6], Eisner [15, 16, 17], Samuelsson [25]) and they also appear well suited for languages with less rigid word order constraints (Covington [9, 10], Collins et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 63
                            }
                        ],
                        "text": "There are no directly comparable results for Swedish text, but Eisner [15] reports an accuracy of 90% for probabilistic dependency parsing of English text, sampled from the Wall Street Journal section of the Penn Treebank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 276
                            }
                        ],
                        "text": "Given that Swedish is intermediate between English and Czech with regard to in ectional richness and freedom of word order, the results seem rather promising, even with the 3% drop in accuracy that can be expected when a part of speech tagger is used to preprocess the input\n(Eisner [15])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 257
                            }
                        ],
                        "text": "Most formalizations of dependency grammar use rules that specify whole con gurations of dependents for a given head, using some notion of valence frames (Hays [19], Gaifman [18], Carroll and Charniak [3], Sleator and Temperley [27, 28], Barbero et al. [2], Eisner [17], Debusmann [11])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 85
                            }
                        ],
                        "text": "Collins et al. [7] report an accuracy of 91% for English text (the same corpus as in Eisner [15]) and 80% accuracy for Czech text."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 62
                            }
                        ],
                        "text": "Parsing accuracy was measured by the attachment score used by Eisner [15] and Collins et al. [7], which is computed as the proportion of words in a sentence that is assigned the correct head (or no head if the word is a root)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 175
                            }
                        ],
                        "text": "The most common approach is probably to use some version of the dynamic programming algorithms familiar from context-free parsing, with or without statistical disambiguation (Eisner [15, 16, 17], Barbero et al. [2], Courtin and Genthial [8], Samuelsson [25])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 182
                            }
                        ],
                        "text": "The most common approach is probably to use some version of the dynamic programming algorithms familiar from context-free parsing, with or without statistical disambiguation (Eisner [15, 16, 17], Barbero et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5897173,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "c356cdc3f3293938a82662c727439be82b97cbc8",
            "isKey": true,
            "numCitedBy": 153,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter introduces weighted bilexical grammars, a formalism in which individual lexical items, such as verbs and their arguments, can have idiosyncratic selectional influences on each other. Such \u2018bilexicalism\u2019 has been a theme of much current work in parsing. The new formalism can be used to describe bilexical approaches to both dependency and phrase-structure grammars, and a slight modification yields link grammars. Its scoring approach is compatible with a wide variety of probability models."
            },
            "slug": "Bilexical-Grammars-and-their-Cubic-Time-Parsing-Eisner",
            "title": {
                "fragments": [],
                "text": "Bilexical Grammars and their Cubic-Time Parsing Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter introduces weighted bilexical grammars, a formalism in which individual lexical items, such as verbs and their arguments, can have idiosyncratic selectional influences on each other."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 10
                            }
                        ],
                        "text": ", Collins [4, 5, 6], Eisner [15, 16, 17], Samuelsson [25]) and they also appear well suited for languages with less rigid word order constraints (Covington [9, 10], Collins et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Collins et al. [7] report an accuracy of 91% for English text (the same corpus as in Eisner [15]) and 80% accuracy for Czech text."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 78
                            }
                        ],
                        "text": "Parsing accuracy was measured by the attachment score used by Eisner [15] and Collins et al. [7], which is computed as the proportion of words in a sentence that is assigned the correct head (or no head if the word is a root)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 139
                            }
                        ],
                        "text": "However, dependency-based representations have turned out to be useful in statistical approaches to parsing and disambiguation (see, e.g., Collins [4, 5, 6], Eisner [15, 16, 17], Samuelsson [25]) and they also appear well suited for languages with less rigid word order constraints (Covington [9, 10], Collins et al. [7])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ffa423a5283396c88ff3d4033d541796bd039cc",
            "isKey": true,
            "numCitedBy": 873,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96)."
            },
            "slug": "Three-Generative,-Lexicalised-Models-for-Parsing-Collins",
            "title": {
                "fragments": [],
                "text": "Three Generative, Lexicalised Models for Statistical Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A new statistical parsing model is proposed, which is a generative model of lexicalised context-free grammar and extended to include a probabilistic treatment of both subcategorisation and wh-movement."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144002335"
                        ],
                        "name": "Jan Hajic",
                        "slug": "Jan-Hajic",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Hajic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Hajic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744313"
                        ],
                        "name": "L. Ramshaw",
                        "slug": "L.-Ramshaw",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Ramshaw",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ramshaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2324070"
                        ],
                        "name": "C. Tillmann",
                        "slug": "C.-Tillmann",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Tillmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tillmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[7], which is computed as the proportion of words in a sentence that is assigned the correct head (or no head if the word is a root)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[7] report an accuracy of 91% for English text (the same corpus as in Eisner [15]) and 80% accuracy for Czech text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1269169,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cb912b4a208b217c45d57e28fc0f59599f92330",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper considers statistical parsing of Czech, which differs radically from English in at least two respects: (1) it is a highly inflected language, and (2) it has relatively free word order. These differences are likely to pose new problems for techniques that have been developed on English. We describe our experience in building on the parsing model of (Collins 97). Our final results- 80% dependency accuracy - represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text."
            },
            "slug": "A-Statistical-Parser-for-Czech-Collins-Hajic",
            "title": {
                "fragments": [],
                "text": "A Statistical Parser for Czech"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper considers statistical parsing of Czech, which differs radically from English in at least two respects: (1) it is a highly inflected language, and (2) it has relatively free word order."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 158
                            }
                        ],
                        "text": "In the past, deterministic parsing of natural language has mostly been motivated by psycholinguistic concerns, as in the well-known Parsifal system of Marcus [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6616065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd459cc59b09e612eeec5327d0690d1508ffe362",
            "isKey": false,
            "numCitedBy": 876,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Assume that the syntax of natural language can be parsed by a left-to-right deterministic mechanism without facilities for parallelism or backup. It will be shown that this 'determinism' hypothesis, explored within the context of the grammar of English, leads to a simple mechanism, a grammar interpreter. (Author)"
            },
            "slug": "A-theory-of-syntactic-recognition-for-natural-Marcus",
            "title": {
                "fragments": [],
                "text": "A theory of syntactic recognition for natural language"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It will be shown that this 'determinism' hypothesis, explored within the context of the grammar of English, leads to a simple mechanism, a grammar interpreter."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721040"
                        ],
                        "name": "D. Sleator",
                        "slug": "D.-Sleator",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Sleator",
                            "middleNames": [
                                "Dominic"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sleator"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3335864"
                        ],
                        "name": "D. Temperley",
                        "slug": "D.-Temperley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Temperley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Temperley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 205
                            }
                        ],
                        "text": "Most formalizations of dependency grammar use rules that specify whole con gurations of dependents for a given head, using some notion of valence frames (Hays [19], Gaifman [18], Carroll and Charniak [3], Sleator and Temperley [27, 28], Barbero et al. [2], Eisner [17], Debusmann [11])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 149
                            }
                        ],
                        "text": "Several di erent de nitions of projectivity can be found in the literature, but they are all roughly equivalent (see, e.g., Mel\u00a3uk [23], Hudson [20], Sleator and Temperley [27, 28])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 50
                            }
                        ],
                        "text": ", Mel\u00a3uk [23], Hudson [20], Sleator and Temperley [27, 28])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 227
                            }
                        ],
                        "text": "Most formalizations of dependency grammar use rules that specify whole con gurations of dependents for a given head, using some notion of valence frames (Hays [19], Gaifman [18], Carroll and Charniak [3], Sleator and Temperley [27, 28], Barbero et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5118729,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "5752b8dcec5856b7ad6289bbe1177acce535fba4",
            "isKey": true,
            "numCitedBy": 1030,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We define a new formal grammatical system called a link grammar. A sequence of words is in the language of a link grammar if there is a way to draw links between words in such a way that (1) the local requirements of each word are satisfied, (2) the links do not cross, and (3) the words form a connected graph. We have encoded English grammar into such a system, and written a program (based on new algorithms) for efficiently parsing with a link grammar. The formalism is lexical and makes no explicit use of constituents and categories. The breadth of English phenomena that our system handles is quite large. A number of sophisticated and new techniques were used to allow efficient parsing of this very complex grammar. Our program is written in C, and the entire system may be obtained via anonymous ftp. Several other researchers have begun to use link grammars in their own research."
            },
            "slug": "Parsing-English-with-a-Link-Grammar-Sleator-Temperley",
            "title": {
                "fragments": [],
                "text": "Parsing English with a Link Grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This work has encoded English grammar into a new formal grammatical system called a link grammar, and written a program (based on new algorithms) for efficiently parsing with this very complex grammar."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2383337"
                        ],
                        "name": "D. Duchier",
                        "slug": "D.-Duchier",
                        "structuredName": {
                            "firstName": "Denys",
                            "lastName": "Duchier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Duchier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 190
                            }
                        ],
                        "text": "Another school proposes that dependency parsing should be cast as a constraint satisfaction problem and solved using constraint programming (Maruyama [22], Menzel and Schr\u00f6der [24], Duchier [12, 13, 14])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20571698,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9099137375805e16040d51c8f3b60ac3354099b6",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Lexicalized-Syntax-and-Topology-for-Non-projective-Duchier",
            "title": {
                "fragments": [],
                "text": "Lexicalized Syntax and Topology for Non-projective Dependency Grammar"
            },
            "venue": {
                "fragments": [],
                "text": "Electron. Notes Theor. Comput. Sci."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2383337"
                        ],
                        "name": "D. Duchier",
                        "slug": "D.-Duchier",
                        "structuredName": {
                            "firstName": "Denys",
                            "lastName": "Duchier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Duchier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 190
                            }
                        ],
                        "text": "Another school proposes that dependency parsing should be cast as a constraint satisfaction problem and solved using constraint programming (Maruyama [22], Menzel and Schr\u00f6der [24], Duchier [12, 13, 14])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7939318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d29982e4fe6a8d6164051c0b42ee54efd333fd76",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Trees with labeled edges have widespread applicability, for examplefor the representation of dependency syntax trees. Given a fixednumber of nodes and constraints on how edges may be drawn betweenthem, the task of finding solution trees is known as a configurationproblem. In this paper, we formalize the configuration problem oflabeled trees and argue that it can be regarded as a constraintsatisfaction problem which can be solved directly and efficiently byconstraint propagation. In particular, we derive and prove correcta formulation of dependency parsing as a constraint satisfactionproblem.Our approach, based on constraints on finite sets and a new family of`selection' constraints, is especially well-suited for the compactrepresentation and efficient processing of ambiguity. We addressvarious issues of interest to the computational linguist such aslexical ambiguity, structural ambiguity, valency constraints,grammatical principles, and linear precedence. Finally we turn to thechallenge of efficient processing and characterize the servicesexpected of a constraint programming system: we define a formalconstraint language and specify its operational semantics withinference rules of propagation and distribution.This framework generalizes our presentation of immediate syntacticdependence for dependency parsing (Duchier, 1999a)and extends naturally to our corresponding treatment of linearprecedence (Duchier and Debusmann, 2001) based on a notion of topological ratherthan syntactic dependencies."
            },
            "slug": "Configuration-of-Labeled-Trees-under-Lexicalized-Duchier",
            "title": {
                "fragments": [],
                "text": "Configuration of Labeled Trees under Lexicalized Constraints and Principles"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper formalizes the configuration problem of labeled trees and argues that it can be regarded as a constraints satisfaction problem which can be solved directly and efficiently by constraint propagation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 10
                            }
                        ],
                        "text": ", Collins [4, 5, 6], Eisner [15, 16, 17], Samuelsson [25]) and they also appear well suited for languages with less rigid word order constraints (Covington [9, 10], Collins et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Collins et al. [7] report an accuracy of 91% for English text (the same corpus as in Eisner [15]) and 80% accuracy for Czech text."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 78
                            }
                        ],
                        "text": "Parsing accuracy was measured by the attachment score used by Eisner [15] and Collins et al. [7], which is computed as the proportion of words in a sentence that is assigned the correct head (or no head if the word is a root)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 139
                            }
                        ],
                        "text": "However, dependency-based representations have turned out to be useful in statistical approaches to parsing and disambiguation (see, e.g., Collins [4, 5, 6], Eisner [15, 16, 17], Samuelsson [25]) and they also appear well suited for languages with less rigid word order constraints (Covington [9, 10], Collins et al. [7])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12615602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3764baa7465201f054083d02b58fa75f883c4461",
            "isKey": true,
            "numCitedBy": 736,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy."
            },
            "slug": "A-New-Statistical-Parser-Based-on-Bigram-Lexical-Collins",
            "title": {
                "fragments": [],
                "text": "A New Statistical Parser Based on Bigram Lexical Dependencies"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A new statistical parser which is based on probabilities of dependencies between head-words in the parse tree, which trains on 40,000 sentences in under 15 minutes and can be improved to over 200 sentences a minute with negligible loss in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153502408"
                        ],
                        "name": "W. Menzel",
                        "slug": "W.-Menzel",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Menzel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Menzel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32472993"
                        ],
                        "name": "Ingo Schroder",
                        "slug": "Ingo-Schroder",
                        "structuredName": {
                            "firstName": "Ingo",
                            "lastName": "Schroder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ingo Schroder"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 176
                            }
                        ],
                        "text": "Another school proposes that dependency parsing should be cast as a constraint satisfaction problem and solved using constraint programming (Maruyama [22], Menzel and Schr\u00f6der [24], Duchier [12, 13, 14])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9039438,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42dd61a33ace98307f3a9215f2bac295ee7a3c0c",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an approach to the parsing of dependency structures which brings together the notion of parsing as candidate elimination, the use of graded constraints, and the parallel disambiguation of related structural representations. The approach aims at an increased level of robustness by accepting constraint violations in a controlled way, combining redundant and possibly conflicting information on different representational levels, and facilitating partial parsing as a natural mode of behavior. 1 I n t r o d u c t i o n Language understanding is based on a variety of contributions from different representational levels. From this perspective, one of the most attractive features of dependency based grammar models seems to be their relational nature which allows to accommodate various kinds of relationships in a very similar fashion. Since the basic representational framework is a rather general one it can be (re-)interpreted in many different ways. Thus, dependency relations lend themselves to model the surface syntactic structure of an utterance (with labels like subject-of, direct-object-of, determiner-of, etc.), its thematic structure (with labels like agent-of, theme-of, etc.) and even the referential structure (with labels like referential-identity, partof, possessor-of, etc.). This representational similarity obviates the necessity to integrate too many disparate informational contributions into a single tree-like representation. Instead, representational levels can be separated from each other in a clean manner with appropriate mappings being defined to relate the different components to each other. Another less obvious advantage of dependency formalisms is their suitability for the apSyn"
            },
            "slug": "Decision-Procedures-for-Dependency-Parsing-Using-Menzel-Schroder",
            "title": {
                "fragments": [],
                "text": "Decision Procedures for Dependency Parsing Using Graded Constraints"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This work presents an approach to the parsing of dependency structures which brings together the notion of parsing as candidate elimination, the use of graded constraints, and the parallel disambiguation of related structural representations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403128"
                        ],
                        "name": "C. Samuelsson",
                        "slug": "C.-Samuelsson",
                        "structuredName": {
                            "firstName": "Christer",
                            "lastName": "Samuelsson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Samuelsson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": ", Collins [4, 5, 6], Eisner [15, 16, 17], Samuelsson [25]) and they also appear well suited for languages with less rigid word order constraints (Covington [9, 10], Collins et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "[2], Courtin and Genthial [8], Samuelsson [25])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 179
                            }
                        ],
                        "text": "However, dependency-based representations have turned out to be useful in statistical approaches to parsing and disambiguation (see, e.g., Collins [4, 5, 6], Eisner [15, 16, 17], Samuelsson [25]) and they also appear well suited for languages with less rigid word order constraints (Covington [9, 10], Collins et al. [7])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 242
                            }
                        ],
                        "text": "The most common approach is probably to use some version of the dynamic programming algorithms familiar from context-free parsing, with or without statistical disambiguation (Eisner [15, 16, 17], Barbero et al. [2], Courtin and Genthial [8], Samuelsson [25])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2281795,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2881ef9f14d38c20bafe807d95c2a2d06a12280",
            "isKey": true,
            "numCitedBy": 22,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "A generative statistical model of dependency syntax is proposed based on Tesniere's classical theory. It provides a stochastic formalization of the original model of syntactic structure and augments it with a model of the string realization process, the latter which is lacking in Tesniere's original work. The resulting theory models crossing dependency links, discontinuous nuclei and string merging, and it has been given an efficient computational rendering."
            },
            "slug": "A-Statistical-Theory-of-Dependency-Syntax-Samuelsson",
            "title": {
                "fragments": [],
                "text": "A Statistical Theory of Dependency Syntax"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "A generative statistical model of dependency syntax is proposed based on Tesniere's classical theory, which provides a stochastic formalization of the original model of syntactic structure and augments it with a model of the string realization process."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2631920"
                        ],
                        "name": "Ralph Debusmann",
                        "slug": "Ralph-Debusmann",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Debusmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ralph Debusmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122512395,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "ac06f72725643df0890e6c53036b6d7cac6d80d5",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 86,
            "paperAbstract": {
                "fragments": [],
                "text": "Beginning with the groundbreaking work of Chomsky in the 1950s, syntactians have concentrated mostly on the English language. But English is not a typical natural language: in particular, its word order is very rigid, as opposed to most other languages which exhibit freer word order. The phrase structure-based approach employed for the analysis of English runs into severe problems when confronted with freer word order languages. Aside from the mainstream, linguists in Eastern Europe and Japan have pursued an approach to syntax which seemed better suited for the analysis of freer word order languages: dependency grammar. The key asset of dependency grammar is that it allows for a clean separation of syntactic dependency and surface word order. Unfortunately, none of the frameworks for dependency grammar has really caught on. We suggest two reasons for their failure: a) many of the dependencybased frameworks lack proper formalization and, perhaps surprisingly, b) most of them lack a realistic and workable account of word order. In this thesis, we try to remedy these problems in the setting of a constraint-based approach to dependency grammar based on (Duchier 1999). We present a new account of word order for dependency grammar couched in a declarative grammatical formalism called Topological Dependency Grammar (TDG). TDG allows to cleanly separate the two levels of syntactic dependency and surface word order, which greatly facilitates the conception of grammars for freer word order languages. In addition, we can efficiently parse with TDG grammars: using a reduction described in (Duchier 2000), we achieved an efficient parser implementation using modern constraint programming techniques."
            },
            "slug": "A-declarative-grammar-formalism-for-dependency-Debusmann",
            "title": {
                "fragments": [],
                "text": "A declarative grammar formalism for dependency grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This thesis presents a new account of word order for dependency grammar couched in a declarative grammatical formalism called Topological Dependency Grammar (TDG), which allows to cleanly separate the two levels of syntactic dependency and surface word order, which greatly facilitates the conception of grammars for freer word order languages."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3027974"
                        ],
                        "name": "C. Barbero",
                        "slug": "C.-Barbero",
                        "structuredName": {
                            "firstName": "Cristina",
                            "lastName": "Barbero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Barbero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2338933"
                        ],
                        "name": "L. Lesmo",
                        "slug": "L.-Lesmo",
                        "structuredName": {
                            "firstName": "Leonardo",
                            "lastName": "Lesmo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lesmo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152862473"
                        ],
                        "name": "V. Lombardo",
                        "slug": "V.-Lombardo",
                        "structuredName": {
                            "firstName": "Vincenzo",
                            "lastName": "Lombardo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lombardo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[2], Courtin and Genthial [8], Samuelsson [25])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10206811,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "89d96c55ad82f682e53fafcc0b692a0ae2be43cb",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose to introduce syntactic classes in a lexicalized dependency formalism. Subcategories of words are organized hierarchically from a general, abstract level (syntactic categories) to a word-specific level (single lexical items). The formalism is parsimonious, and useful for processing. We also sketch a parsing model that uses the hierarchical mixed-grain representation to make predictions on the structure of the input. 1 I n t r o d u c t i o n Much recent work in linguistics and computational linguistics emphasizes the role of lexical information in syntactic representation and processing. This emphasis given to the lexicon is the result of a gradual process. The original trend in linguistics has been to individuate categories of words having related characteristics the traditional syntactic categories like verb, noun, adjective, etc. and to express the structure of a sentence in terms of constituents, or phrases, built around these categories. Subsequent considerations lead to a lexicalization of grammar. Linguistically, the constraints expressed on syntactic categories are too general to explain facts about words e.g. the relation between a verb and its nominalization, \"destroy the city\" and \"destruction of the city\" or to account uniformly for a number of phenomena across languages e.g. passivization. In parsing, the use of individual item information reduces the search space of the possible structures of a sentence. From a mathematical point of view, lexicalized grammars exhibit properties like finite ambiguity (Schabes, 1990) that are of a practical interest (especially in writing realistic grammars). Dependency grammar is naturally suitable for a lexicalization, as the binary relations representing the structure of a sentence are defined with respect to the head (that is a word). Pure lexicalized formalisms, however, have also several disadvantages. Linguistically, the abstract level provided by syntactic rules is necessary to avoid the loss of generalization which would arise if classlevel information were repeated in all lexical items. In parsing, a predictive component is required to guarantee the valid prefiz property, namely the capabifity of detecting as soon as possible whether a substring is a valid prefix for the language defined by the grammar. Knowledge of syntactic categories, which does not depend on the input, is needed for a parser to be predictive. In this paper we address the problem of the interaction between syntactic and lexical information in dependency grammar. We introduce many intermediate levels between lexical items and syntactic categories, by organizing the grammar around the notion of subcategorizetion. Intuitively, a subcategorization frame for a lexical item L is a specification of the number and type of elements that L requires in order, for ml utterance that contains L, to be well-formed. For example, within the syntactic category VERB, different verbs require different numbers of nominal dependents for a well-formed sentence. In Italian (our case study), an intransitive verb such as dormirv, \"sleep\", subcategorizes for only one nominal element (the subject), while a transitive verb such as baciare, \"kiss\", subcategorizes for two nominal elements (the subject and the object) 1. Grammatical relations such as subject and object are primitive concepts in a dependency paradigm, i.e. they directly define the structure of the sentence. Consequently, the dependency paradigm is particularly suitable to define the grammar in terms of constraints on subcategorization frames. Our proposal is to use subcategories organized in a hierarchy: the upper level of the hierarchy corresponds to the syntactic categories, the other levels correspond to subcategories that are more and more 1We include the subject relation in the subcategorization, or valency, of a verb cf. (Hudson, 1990) (Mel'cuk, 1988). In most constituency theories, on the contrary, the subject is not part of the valency of a verb."
            },
            "slug": "Integration-of-syntactic-and-lexical-information-in-Barbero-Lesmo",
            "title": {
                "fragments": [],
                "text": "Integration of syntactic and lexical information in a hierarchical dependency grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The problem of the interaction between syntactic and lexical information in dependency grammar is addressed, and many intermediate levels between lexical items and syntactic categories are introduced, by organizing the grammar around the notion of subcategorizetion."
            },
            "venue": {
                "fragments": [],
                "text": "Workshop On Processing Of Dependency-Based Grammars"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145043214"
                        ],
                        "name": "Jason Eisner",
                        "slug": "Jason-Eisner",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Eisner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Eisner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 158
                            }
                        ],
                        "text": "However, dependency-based representations have turned out to be useful in statistical approaches to parsing and disambiguation (see, e.g., Collins [4, 5, 6], Eisner [15, 16, 17], Samuelsson [25]) and they also appear well suited for languages with less rigid word order constraints (Covington [9, 10], Collins et al. [7])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 28
                            }
                        ],
                        "text": ", Collins [4, 5, 6], Eisner [15, 16, 17], Samuelsson [25]) and they also appear well suited for languages with less rigid word order constraints (Covington [9, 10], Collins et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 63
                            }
                        ],
                        "text": "There are no directly comparable results for Swedish text, but Eisner [15] reports an accuracy of 90% for probabilistic dependency parsing of English text, sampled from the Wall Street Journal section of the Penn Treebank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 276
                            }
                        ],
                        "text": "Given that Swedish is intermediate between English and Czech with regard to in ectional richness and freedom of word order, the results seem rather promising, even with the 3% drop in accuracy that can be expected when a part of speech tagger is used to preprocess the input\n(Eisner [15])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 257
                            }
                        ],
                        "text": "Most formalizations of dependency grammar use rules that specify whole con gurations of dependents for a given head, using some notion of valence frames (Hays [19], Gaifman [18], Carroll and Charniak [3], Sleator and Temperley [27, 28], Barbero et al. [2], Eisner [17], Debusmann [11])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 85
                            }
                        ],
                        "text": "Collins et al. [7] report an accuracy of 91% for English text (the same corpus as in Eisner [15]) and 80% accuracy for Czech text."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 62
                            }
                        ],
                        "text": "Parsing accuracy was measured by the attachment score used by Eisner [15] and Collins et al. [7], which is computed as the proportion of words in a sentence that is assigned the correct head (or no head if the word is a root)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 175
                            }
                        ],
                        "text": "The most common approach is probably to use some version of the dynamic programming algorithms familiar from context-free parsing, with or without statistical disambiguation (Eisner [15, 16, 17], Barbero et al. [2], Courtin and Genthial [8], Samuelsson [25])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 182
                            }
                        ],
                        "text": "The most common approach is probably to use some version of the dynamic programming algorithms familiar from context-free parsing, with or without statistical disambiguation (Eisner [15, 16, 17], Barbero et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3262717,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "adfef97814b292a09520d8c78a141e7a4baf8726",
            "isKey": true,
            "numCitedBy": 728,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "After presenting a novel O(n3) parsing algorithm for dependency grammar, we develop three contrasting ways to stochasticize it. We propose (a) a lexical affinity model where words struggle to modify each other, (b) a sense tagging model where words fluctuate randomly in their selectional preferences, and (c) a generative model where the speaker fleshes out each word's syntactic and conceptual structure without regard to the implications for the hearer. We also give preliminary empirical results from evaluating the three models' parsing performance on annotated Wall Street Journal training text (derived from the Penn Treebank). In these results, the generative model performs significantly better than the others, and does about equally well at assigning part-of-speech tags."
            },
            "slug": "Three-New-Probabilistic-Models-for-Dependency-An-Eisner",
            "title": {
                "fragments": [],
                "text": "Three New Probabilistic Models for Dependency Parsing: An Exploration"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Preliminary empirical results from evaluating the three models' parsing performance on annotated Wall Street Journal training text (derived from the Penn Treebank) suggest the generative model performs significantly better than the others, and does about equally well at assigning part-of-speech tags."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 10
                            }
                        ],
                        "text": ", Collins [4, 5, 6], Eisner [15, 16, 17], Samuelsson [25]) and they also appear well suited for languages with less rigid word order constraints (Covington [9, 10], Collins et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Collins et al. [7] report an accuracy of 91% for English text (the same corpus as in Eisner [15]) and 80% accuracy for Czech text."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 78
                            }
                        ],
                        "text": "Parsing accuracy was measured by the attachment score used by Eisner [15] and Collins et al. [7], which is computed as the proportion of words in a sentence that is assigned the correct head (or no head if the word is a root)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 139
                            }
                        ],
                        "text": "However, dependency-based representations have turned out to be useful in statistical approaches to parsing and disambiguation (see, e.g., Collins [4, 5, 6], Eisner [15, 16, 17], Samuelsson [25]) and they also appear well suited for languages with less rigid word order constraints (Covington [9, 10], Collins et al. [7])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7901127,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3fc44ff7f37ec5585310666c183c65e0a0bb2446",
            "isKey": true,
            "numCitedBy": 2062,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models."
            },
            "slug": "Head-Driven-Statistical-Models-for-Natural-Language-Collins",
            "title": {
                "fragments": [],
                "text": "Head-Driven Statistical Models for Natural Language Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "Three statistical models for natural language parsing are described, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145043214"
                        ],
                        "name": "Jason Eisner",
                        "slug": "Jason-Eisner",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Eisner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Eisner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "Parsing accuracy was measured by the attachment score used by Eisner [15] and Collins et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 28
                            }
                        ],
                        "text": ", Collins [4, 5, 6], Eisner [15, 16, 17], Samuelsson [25]) and they also appear well suited for languages with less rigid word order constraints (Covington [9, 10], Collins et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "There are no directly comparable results for Swedish text, but Eisner [15] reports an accuracy of 90% for probabilistic dependency parsing of English text, sampled from the Wall Street Journal section of the Penn Treebank."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "[7] report an accuracy of 91% for English text (the same corpus as in Eisner [15]) and 80% accuracy for Czech text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 182
                            }
                        ],
                        "text": "The most common approach is probably to use some version of the dynamic programming algorithms familiar from context-free parsing, with or without statistical disambiguation (Eisner [15, 16, 17], Barbero et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3265631,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4c0e02d99de82149efd719260e5a5549a13854a",
            "isKey": true,
            "numCitedBy": 75,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This technical report is an appendix to Eisner (1996): it gives superior experimental results that were reported only in the talk version of that paper, with details of how the results were obtained. Eisner (1996) trained three probability models on a small set of about 4,000 conjunction-free, dependencygrammar parses derived from the Wall Street Journal section of the Penn Treebank, and then evaluated the models on a held-out test set, using a novel O(n 3 ) parsing algorithm. The present paper describes some details of the experiments and repeats them with a larger training set of 25,000 sentences. As reported at the talk, the more extensive training yields greatly improved performance, cutting in half the error rate of Eisner (1996). Nearly half the sentences are parsed with no misattachments; two-thirds of sentences are parsed with at most one misattachment. Of the models described in the original paper, the best score is obtained with the generative \u201cmodel C,\u201d which attaches 87\u201388% of all words to the correct parent. However, better models are also explored, in particular, two simple variants on the comprehension \u201cmodel B.\u201d The better of these has an attachment accuracy of 90%, and (unlike model C) tags words more accurately than the comparable trigram tagger. If tags are roughly known in advance, search error is all but eliminated and the new model attains an attachment accuracy of 93%. We find that the parser of Collins (1996), when combined with a highlytrained tagger, also achieves 93% when trained and tested on the same sentences. We briefly discuss the similarities and differences between Collins\u2019s model and ours, pointing out the strengths of each and noting that these strengths could be combined for either dependency parsing or phrase-structure parsing."
            },
            "slug": "An-Empirical-Comparison-of-Probability-Models-for-Eisner",
            "title": {
                "fragments": [],
                "text": "An Empirical Comparison of Probability Models for Dependency Grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The present paper describes some details of the experiments and repeats them with a larger training set of 25,000 sentences, finding that the parser of Collins (1996), when combined with a highlytrained tagger, also achieves 93% when trained and tested on the same sentences."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2740556"
                        ],
                        "name": "H. Gaifman",
                        "slug": "H.-Gaifman",
                        "structuredName": {
                            "firstName": "Haim",
                            "lastName": "Gaifman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Gaifman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 165
                            }
                        ],
                        "text": "Most formalizations of dependency grammar use rules that specify whole con gurations of dependents for a given head, using some notion of valence frames (Hays [19], Gaifman [18], Carroll and Charniak [3], Sleator and Temperley [27, 28], Barbero et al. [2], Eisner [17], Debusmann [11])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 173
                            }
                        ],
                        "text": "Most formalizations of dependency grammar use rules that specify whole con gurations of dependents for a given head, using some notion of valence frames (Hays [19], Gaifman [18], Carroll and Charniak [3], Sleator and Temperley [27, 28], Barbero et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 34462505,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "f23fe2377237a03917cef4f146f0dd4fc6a3f9bb",
            "isKey": false,
            "numCitedBy": 304,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dependency-Systems-and-Phrase-Structure-Systems-Gaifman",
            "title": {
                "fragments": [],
                "text": "Dependency Systems and Phrase-Structure Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Control."
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715952"
                        ],
                        "name": "A. Aho",
                        "slug": "A.-Aho",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Aho",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144281449"
                        ],
                        "name": "R. Sethi",
                        "slug": "R.-Sethi",
                        "structuredName": {
                            "firstName": "Ravi",
                            "lastName": "Sethi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sethi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742391"
                        ],
                        "name": "J. Ullman",
                        "slug": "J.-Ullman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Ullman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ullman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[1]), although the parse actions are di erent given that we are using directed D-rules instead of context-free grammar rules."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42981739,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7f33d55d94e75a554251fe7dc07f1d7b4db8e1a",
            "isKey": false,
            "numCitedBy": 9130,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1 Introduction 1.1 Language Processors 1.2 The Structure of a Compiler 1.3 The Evolution of Programming Languages 1.4 The Science of Building a Compiler 1.5 Applications of Compiler Technology 1.6 Programming Language Basics 1.7 Summary of Chapter 1 1.8 References for Chapter 1 2 A Simple Syntax-Directed Translator 2.1 Introduction 2.2 Syntax Definition 2.3 Syntax-Directed Translation 2.4 Parsing 2.5 A Translator for Simple Expressions 2.6 Lexical Analysis 2.7 Symbol Tables 2.8 Intermediate Code Generation 2.9 Summary of Chapter 2 3 Lexical Analysis 3.1 The Role of the Lexical Analyzer 3.2 Input Buffering 3.3 Specification of Tokens 3.4 Recognition of Tokens 3.5 The Lexical-Analyzer Generator Lex 3.6 Finite Automata 3.7 From Regular Expressions to Automata 3.8 Design of a Lexical-Analyzer Generator 3.9 Optimization of DFA-Based Pattern Matchers 3.10 Summary of Chapter 3 3.11 References for Chapter 3 4 Syntax Analysis 4.1 Introduction 4.2 Context-Free Grammars 4.3 Writing a Grammar 4.4 Top-Down Parsing 4.5 Bottom-Up Parsing 4.6 Introduction to LR Parsing: Simple LR 4.7 More Powerful LR Parsers 4.8 Using Ambiguous Grammars 4.9 Parser Generators 4.10 Summary of Chapter 4 4.11 References for Chapter 4 5 Syntax-Directed Translation 5.1 Syntax-Directed Definitions 5.2 Evaluation Orders for SDD's 5.3 Applications of Syntax-Directed Translation 5.4 Syntax-Directed Translation Schemes 5.5 Implementing L-Attributed SDD's 5.6 Summary of Chapter 5 5.7 References for Chapter 5 6 Intermediate-Code Generation 6.1 Variants of Syntax Trees 6.2 Three-Address Code 6.3 Types and Declarations 6.4 Translation of Expressions 6.5 Type Checking 6.6 Control Flow 6.7 Backpatching 6.8 Switch-Statements 6.9 Intermediate Code for Procedures 6.10 Summary of Chapter 6 6.11 References for Chapter 6 7 Run-Time Environments 7.1 Storage Organization 7.2 Stack Allocation of Space 7.3 Access to Nonlocal Data on the Stack 7.4 Heap Management 7.5 Introduction to Garbage Collection 7.6 Introduction to Trace-Based Collection 7.7 Short-Pause Garbage Collection 7.8 Advanced Topics in Garbage Collection 7.9 Summary of Chapter 7 7.10 References for Chapter 7 8 Code Generation 8.1 Issues in the Design of a Code Generator 8.2 The Target Language 8.3 Addresses in the Target Code 8.4 Basic Blocks and Flow Graphs 8.5 Optimization of Basic Blocks 8.6 A Simple Code Generator 8.7 Peephole Optimization 8.8 Register Allocation and Assignment 8.9 Instruction Selection by Tree Rewriting 8.10 Optimal Code Generation for Expressions 8.11 Dynamic Programming Code-Generation 8.12 Summary of Chapter 8 8.13 References for Chapter 8 9 Machine-Independent Optimizations 9.1 The Principal Sources of Optimization 9.2 Introduction to Data-Flow Analysis 9.3 Foundations of Data-Flow Analysis 9.4 Constant Propagation 9.5 Partial-Redundancy Elimination 9.6 Loops in Flow Graphs 9.7 Region-Based Analysis 9.8 Symbolic Analysis 9.9 Summary of Chapter 9 9.10 References for Chapter 9 10 Instruction-Level Parallelism 10.1 Processor Architectures 10.2 Code-Scheduling Constraints 10.3 Basic-Block Scheduling 10.4 Global Code Scheduling 10.5 Software Pipelining 10.6 Summary of Chapter 10 10.7 References for Chapter 10 11 Optimizing for Parallelism and Locality 11.1 Basic Concepts 11.2 Matrix Multiply: An In-Depth Example 11.3 Iteration Spaces 11.4 Affine Array Indexes 11.5 Data Reuse 11.6 Array Data-Dependence Analysis 11.7 Finding Synchronization-Free Parallelism 11.8 Synchronization Between Parallel Loops 11.9 Pipelining 11.10 Locality Optimizations 11.11 Other Uses of Affine Transforms 11.12 Summary of Chapter 11 11.13 References for Chapter 11 12 Interprocedural Analysis 12.1 Basic Concepts 12.2 Why Interprocedural Analysis? 12.3 A Logical Representation of Data Flow 12.4 A Simple Pointer-Analysis Algorithm 12.5 Context-Insensitive Interprocedural Analysis 12.6 Context-Sensitive Pointer Analysis 12.7 Datalog Implementation by BDD's 12.8 Summary of Chapter 12 12.9 References for Chapter 12 A A Complete Front End A.1 The Source Language A.2 Main A.3 Lexical Analyzer A.4 Symbol Tables and Types A.5 Intermediate Code for Expressions A.6 Jumping Code for Boolean Expressions A.7 Intermediate Code for Statements A.8 Parser A.9 Creating the Front End B Finding Linearly Independent Solutions Index"
            },
            "slug": "Compilers:-Principles,-Techniques,-and-Tools-Aho-Sethi",
            "title": {
                "fragments": [],
                "text": "Compilers: Principles, Techniques, and Tools"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "This book discusses the design of a Code Generator, the role of the Lexical Analyzer, and other topics related to code generation and optimization."
            },
            "venue": {
                "fragments": [],
                "text": "Addison-Wesley series in computer science / World student series edition"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246473"
                        ],
                        "name": "D. G. Hays",
                        "slug": "D.-G.-Hays",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hays",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. G. Hays"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "For example, given a grammar in the formalism of Hays [19], we can construct a set of directed D-rules in the following way:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 154
                            }
                        ],
                        "text": "Most formalizations of dependency grammar use rules that specify whole con gurations of dependents for a given head, using some notion of valence frames (Hays [19], Gaifman [18], Carroll and Charniak [3], Sleator and Temperley [27, 28], Barbero et al. [2], Eisner [17], Debusmann [11])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "For example, given a grammar in the formalism of Hays [19], we can construct a set of directed D-rules in the following way:\nFor every rule X(X m X 1 X1 Xn) in the original grammar, introduce rules wl w and w ! wr for every w 2 X;wl 2 X m [ [X 1; wr 2 X1 [ [Xn."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 159
                            }
                        ],
                        "text": "Most formalizations of dependency grammar use rules that specify whole con gurations of dependents for a given head, using some notion of valence frames (Hays [19], Gaifman [18], Carroll and Charniak [3], Sleator and Temperley [27, 28], Barbero et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 145050024,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "17cb4f318dc53b3c09dab637bd46897039d88046",
            "isKey": true,
            "numCitedBy": 462,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dependency-Theory:-A-Formalism-and-Some-Hays",
            "title": {
                "fragments": [],
                "text": "Dependency Theory: A Formalism and Some Observations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405337742"
                        ],
                        "name": "I. Mel'cuk",
                        "slug": "I.-Mel'cuk",
                        "structuredName": {
                            "firstName": "Igor",
                            "lastName": "Mel'cuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Mel'cuk"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 124
                            }
                        ],
                        "text": "Several di erent de nitions of projectivity can be found in the literature, but they are all roughly equivalent (see, e.g., Mel\u00a3uk [23], Hudson [20], Sleator and Temperley [27, 28])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": ", Mel\u00a3uk [23], Hudson [20], Sleator and Temperley [27, 28])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 203672231,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "1215415ac4e5abb82d7596538bc81e6247d4f020",
            "isKey": false,
            "numCitedBy": 1326,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dependency-Syntax:-Theory-and-Practice-Mel'cuk",
            "title": {
                "fragments": [],
                "text": "Dependency Syntax: Theory and Practice"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2741927"
                        ],
                        "name": "M. Covington",
                        "slug": "M.-Covington",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Covington",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Covington"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 156
                            }
                        ],
                        "text": ", Collins [4, 5, 6], Eisner [15, 16, 17], Samuelsson [25]) and they also appear well suited for languages with less rigid word order constraints (Covington [9, 10], Collins et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "The grammar rules used here are very similar to Covington's [9] notion of D-rules, except that the latter are undirected, and I will therefore call them directed D-rules."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60489042,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "5c190dbdc58618b0181b90afe8558451bbf22dd3",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 97,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-dependency-parser-for-variable-word-order-Covington",
            "title": {
                "fragments": [],
                "text": "A dependency parser for variable-word-order languages"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 190
                            }
                        ],
                        "text": "Another school proposes that dependency parsing should be cast as a constraint satisfaction problem and solved using constraint programming (Maruyama [22], Menzel and Schr\u00f6der [24], Duchier [12, 13, 14])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Axiomatizing dependency parsing using set constraints"
            },
            "venue": {
                "fragments": [],
                "text": "In Sixth Meeting on Mathematics of Language,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Meaning of the Sentence in Its Pragmatic Aspects"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 262
                            }
                        ],
                        "text": "In order to estimate the parsing accuracy that can be expected with the algorithm described in the preceding section and a grammar consisting of directed D-rules, a small experiment was performed using data from the Stockholm-Ume\u00e5 Corpus of written Swedish (SUC [29]), which is a balanced corpus consisting of ctional and non- ctional texts, organized in the same way as the Brown corpus of American English."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Version 1.0. Produced by Department of Linguistics, Ume\u00e5 University and Department of Linguistics, Stockholm University"
            },
            "venue": {
                "fragments": [],
                "text": "ISBN 91-7191-348-3.,"
            },
            "year": 1997
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 17,
            "methodology": 9,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 27,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/An-Efficient-Algorithm-for-Projective-Dependency-Nivre/24439a2a35b834aa710373d62da4c6f86a180125?sort=total-citations"
}