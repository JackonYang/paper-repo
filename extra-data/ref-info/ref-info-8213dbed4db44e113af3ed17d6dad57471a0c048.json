{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122507079,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c7d8e81f0d6f04215c08a22a9c300567e0e81e3c",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Three-fundamental-concepts-of-the-capacity-of-Vapnik",
            "title": {
                "fragments": [],
                "text": "Three fundamental concepts of the capacity of learning machines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": false,
            "numCitedBy": 26320,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39201543"
                        ],
                        "name": "J. Berger",
                        "slug": "J.-Berger",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Berger",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Berger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 145
                            }
                        ],
                        "text": "1 The Bayesian Approach in Learning Theory In the classical paradigm of function estimation, an important place belongs to the Bayesian approach (Berger, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120366929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9dd05b69d6906fff6ea6c4ba3609a6d97c9b8a3",
            "isKey": false,
            "numCitedBy": 7325,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory. The text assumes a knowledge of basic probability theory and some advanced calculus is also required."
            },
            "slug": "Statistical-Decision-Theory-and-Bayesian-Analysis-Berger",
            "title": {
                "fragments": [],
                "text": "Statistical Decision Theory and Bayesian Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31585702"
                        ],
                        "name": "A. Atkinson",
                        "slug": "A.-Atkinson",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Atkinson",
                            "middleNames": [
                                "Barnes"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Atkinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 145
                            }
                        ],
                        "text": "The idea then appeared in methods for regression estimation: (i) Ridge regression (Hoerl and Kennard, 1970), (ii) model selection (see review in (Miller, 1990))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 196
                            }
                        ],
                        "text": ", am are unknown scalars (Friedman and Stuetzle, 1981), (Breiman, Friedman, Olshen, and Stone, 1984) (in contrast to approaches developed in the 1970s for estimating linear in pammeters functions (Miller, 1990))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 151
                            }
                        ],
                        "text": "Therefore, several ideas for estimating the degree of the approximating polynomial were suggested, including (Akaike, 1970), and (Schwartz, 1978) (see (Miller, 1990))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120321381,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "25cb7e7804f29da4892dd2ccc2acea508cc8bf38",
            "isKey": false,
            "numCitedBy": 1309,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "8. Subset Selection in Regression (Monographs on Statistics and Applied Probability, no. 40). By A. J. Miller. ISBN 0 412 35380 6. Chapman and Hall, London, 1990. 240 pp. \u00a325.00."
            },
            "slug": "Subset-Selection-in-Regression-Atkinson",
            "title": {
                "fragments": [],
                "text": "Subset Selection in Regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737945"
                        ],
                        "name": "H. Akaike",
                        "slug": "H.-Akaike",
                        "structuredName": {
                            "firstName": "Hirotugu",
                            "lastName": "Akaike",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Akaike"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 109
                            }
                        ],
                        "text": "Therefore, several ideas for estimating the degree of the approximating polynomial were suggested, including (Akaike, 1970), and (Schwartz, 1978) (see (Miller, 1990))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120510150,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "22cef227af3c3700693a053655ca82f01244167b",
            "isKey": false,
            "numCitedBy": 1168,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "In a recent paper by the present author [1] a simple practical procedure of predictor identification has been proposed. It is the purpose of this paper to provide a theoretical and empirical basis of the procedure."
            },
            "slug": "Statistical-predictor-identification-Akaike",
            "title": {
                "fragments": [],
                "text": "Statistical predictor identification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145632342"
                        ],
                        "name": "F. Yates",
                        "slug": "F.-Yates",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Yates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Yates"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 97
                            }
                        ],
                        "text": "In the 1920s Fisher developed the ML method for estimating the unknown parameters of the density (Fisher, 1952)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 194
                            }
                        ],
                        "text": "It is remarkable that Fisher suggested to use the linear discriminant function even if the two covariance matrices were different and proposed a heuristic method for constructing such functions (Fisher, 1952)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 37893146,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e8b1fa820a02f25b36f858c9253b6f96bb043966",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Contributions to Mathematical StatisticsBy Prof. R. A. Fisher. (Wiley Publications in Statistics.) Pp. xvii + 658. (New York: John Wiley and Sons, Inc.; London: Chapman and Hall, Ltd., 1950.) 60s. net."
            },
            "slug": "Contributions-to-Mathematical-Statistics-Yates",
            "title": {
                "fragments": [],
                "text": "Contributions to Mathematical Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1951
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144395224"
                        ],
                        "name": "J. Parrondo",
                        "slug": "J.-Parrondo",
                        "structuredName": {
                            "firstName": "Juan",
                            "lastName": "Parrondo",
                            "middleNames": [
                                "M.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Parrondo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120489430"
                        ],
                        "name": "C. Broeck",
                        "slug": "C.-Broeck",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Broeck",
                            "middleNames": [
                                "van",
                                "den"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Broeck"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119545821,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1d67679963708621ed313e119310c19ed730d1af",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors review the Vapnik and Chervonenkis theorem as applied to the problem of generalization. By combining some of the technical modifications proposed in the literature they derive tighter bounds and a new version of the theorem bounding the accuracy in the estimation of generalization probabilities from finite samples. A critical discussion and comparison with the results from statistical mechanics is given."
            },
            "slug": "Vapnik-Chervonenkis-bounds-for-generalization-Parrondo-Broeck",
            "title": {
                "fragments": [],
                "text": "Vapnik-Chervonenkis bounds for generalization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684300"
                        ],
                        "name": "W. Stuetzle",
                        "slug": "W.-Stuetzle",
                        "structuredName": {
                            "firstName": "Werner",
                            "lastName": "Stuetzle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Stuetzle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 25
                            }
                        ],
                        "text": ", am are unknown scalars (Friedman and Stuetzle, 1981), (Breiman, Friedman, Olshen, and Stone, 1984) (in contrast to approaches developed in the 1970s for estimating linear in pammeters functions (Miller, 1990))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14183758,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "589b8659007e1124f765a5d1bd940b2bf4d79054",
            "isKey": false,
            "numCitedBy": 2177,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract A new method for nonparametric multiple regression is presented. The procedure models the regression surface as a sum of general smooth functions of linear combinations of the predictor variables in an iterative manner. It is more general than standard stepwise and stagewise regression procedures, does not require the definition of a metric in the predictor space, and lends itself to graphical interpretation."
            },
            "slug": "Projection-Pursuit-Regression-Friedman-Stuetzle",
            "title": {
                "fragments": [],
                "text": "Projection Pursuit Regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69577099"
                        ],
                        "name": "T. W. Anderson",
                        "slug": "T.-W.-Anderson",
                        "structuredName": {
                            "firstName": "Theodore",
                            "lastName": "Anderson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. W. Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2548770"
                        ],
                        "name": "R. R. Bahadur",
                        "slug": "R.-R.-Bahadur",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Bahadur",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. R. Bahadur"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 119803229,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4e7ab7027cdf6aeeee8bfe326b0a36b370e003b6",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Linear procedures for classifying an observation as coming from one of two multivariate normal distributions are studied in the case that the two distributions differ both in mean vectors and covariance matrices. We find the class of admissible linear procedures, which is the minimal complete class of linear procedures. It is shown how to construct the linear procedure which minimizes one probability of misclassification given the other and how to obtain the minimax linear procedure; Bayes linear procedures are also discussed."
            },
            "slug": "Classification-into-two-Multivariate-Normal-with-Anderson-Bahadur",
            "title": {
                "fragments": [],
                "text": "Classification into two Multivariate Normal Distributions with Different Covariance Matrices"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756533"
                        ],
                        "name": "G. Chaitin",
                        "slug": "G.-Chaitin",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Chaitin",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Chaitin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 214
                            }
                        ],
                        "text": "The Idea of Algorithmic Complexity Finally, in the 1960s one of the greatest ideas of statistics and information theory was suggested: the idea of algorithmic complexity (Solomonoff, 1960), (Kolmogorov, 1965), and (Chaitin, 1966)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207698337,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b85ae9f4367548a7dac7eae1a2a8d5866d27813a",
            "isKey": false,
            "numCitedBy": 1107,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of Turing machines for calculating finite binary sequences is studied from the point of view of information theory and the theory of recursive functions. Various results are obtained concerning the number of instructions in programs. A modified form of Turing machine is studied from the same point of view. An application to the problem of defining a patternless sequence is proposed in terms of the concepts here developed."
            },
            "slug": "On-the-Length-of-Programs-for-Computing-Finite-Chaitin",
            "title": {
                "fragments": [],
                "text": "On the Length of Programs for Computing Finite Binary Sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An application to the problem of defining a patternless sequence is proposed in terms of the concepts here developed to study the use of Turing machines for calculating finite binary sequences."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729473"
                        ],
                        "name": "M. Karpinski",
                        "slug": "M.-Karpinski",
                        "structuredName": {
                            "firstName": "Marek",
                            "lastName": "Karpinski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Karpinski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27103859"
                        ],
                        "name": "T. Werther",
                        "slug": "T.-Werther",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Werther",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Werther"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 122
                            }
                        ],
                        "text": "Karpinski and Werther showed that the VC dimension h* of this set of indicators is bounded as follows: 3m:::; h*:::; 4m+3 (Karpinski and Werther, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30416628,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4738b8f6a81750ce81924dcabf29475482d882f",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors prove upper and lower bounds on the VC dimension of sparse univariate polynomials over reals and apply these results to prove uniform learnability of sparse polynomials and rational functions. As an application the solution to the open problem of Vapnik [in Estimation of Dependences Based on Empirical Data, Springer-Verlag, Berlin, 1982] on computational approximation of the regression in a class of polynomials used in the theory of empirical data dependences is given."
            },
            "slug": "VC-Dimension-and-Uniform-Learnability-of-Sparse-and-Karpinski-Werther",
            "title": {
                "fragments": [],
                "text": "VC Dimension and Uniform Learnability of Sparse Polynomials and Rational Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The authors prove upper and lower bounds on the VC dimension of sparse univariate polynomials over reals and apply these results to prove uniform learnability of sparse polynmials and rational functions."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Comput."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 116
                            }
                        ],
                        "text": "Finally, it appeared in regularization techniques for both pattern recognition and regression estimation algorithms (Poggio and Girosi, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14892653,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "089a76dbc62a06ad30ae1925530e8733e850268e",
            "isKey": false,
            "numCitedBy": 3701,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of the approximation of nonlinear mapping, (especially continuous mappings) is considered. Regularization theory and a theoretical framework for approximation (based on regularization techniques) that leads to a class of three-layer networks called regularization networks are discussed. Regularization networks are mathematically related to the radial basis functions, mainly used for strict interpolation tasks. Learning as approximation and learning as hypersurface reconstruction are discussed. Two extensions of the regularization approach are presented, along with the approach's corrections to splines, regularization, Bayes formulation, and clustering. The theory of regularization networks is generalized to a formulation that includes task-dependent clustering and dimensionality reduction. Applications of regularization networks are discussed. >"
            },
            "slug": "Networks-for-approximation-and-learning-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "Networks for approximation and learning"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31220579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1339348aeef592802288d9d929a085cb3ae61c4b",
            "isKey": false,
            "numCitedBy": 451,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes error-correction adjustment procedures for determining the weight vector of linear pattern classifiers under general pattern distribution. It is mainly aimed at clarifying theoretically the performance of adaptive pattern classifiers. In the case where the loss depends on the distance between a pattern vector and a decision boundary and where the average risk function is unimodal, it is proved that, by the procedures proposed here, the weight vector converges to the optimal one even under nonseparable pattern distributions. The speed and the accuracy of convergence are analyzed, and it is shown that there is an important tradeoff between speed and accuracy of convergence. Dynamical behaviors, when the probability distributions of patterns are changing, are also shown. The theory is generalized and made applicable to the case with general discriminant functions, including piecewise-linear discriminant functions."
            },
            "slug": "A-Theory-of-Adaptive-Pattern-Classifiers-Amari",
            "title": {
                "fragments": [],
                "text": "A Theory of Adaptive Pattern Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "It is proved that, by the procedures proposed here, the weight vector converges to the optimal one even under nonseparable pattern distributions, and there is an important tradeoff between speed and accuracy of convergence."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Electron. Comput."
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708279"
                        ],
                        "name": "C. Micchelli",
                        "slug": "C.-Micchelli",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Micchelli",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Micchelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 35
                            }
                        ],
                        "text": "For the theory of RBF machines see (Micchelli, 1986), (Powell, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14461054,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9d700e611ee7ffdf54873684a9e8883d3da0bcd7",
            "isKey": false,
            "numCitedBy": 1188,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Among other things, we prove that multiquadric surface interpolation is always solvable, thereby settling a conjecture of R. Franke."
            },
            "slug": "Interpolation-of-scattered-data:-Distance-matrices-Micchelli",
            "title": {
                "fragments": [],
                "text": "Interpolation of scattered data: Distance matrices and conditionally positive definite functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 171
                            }
                        ],
                        "text": "holds true, where hE is the VC dimension of the set of functions L(y,f(x,a))K(x,xo;(3), a E A, (3 E (0,00) and h(3 is the VC dimension of the set offunctions K(x, Xo, (3) (Vapnik and Bottou, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2327934,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "447c6c09cc0051d56ff7020941fce6734a0fd3e7",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "In previous publications (Bottou and Vapnik 1992; Vapnik 1992) we described local learning algorithms, which result in performance improvements for real problems. We present here the theoretical framework on which these algorithms are based. First, we present a new statement of certain learning problems, namely the local risk minimization. We review the basic results of the uniform convergence theory of learning, and extend these results to local risk minimization. We also extend the structural risk minimization principle for both pattern recognition problems and regression problems. This extended induction principle is the basis for a new class of algorithms."
            },
            "slug": "Local-Algorithms-for-Pattern-Recognition-and-Vapnik-Bottou",
            "title": {
                "fragments": [],
                "text": "Local Algorithms for Pattern Recognition and Dependencies Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "The theoretical framework on which local learning algorithms, which result in performance improvements for real problems, are based are presented, and a new statement of certain learning problems, namely the local risk minimization is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477489"
                        ],
                        "name": "L. Devroye",
                        "slug": "L.-Devroye",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Devroye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Devroye"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 84
                            }
                        ],
                        "text": "In 1988 Devroye found a way to obtain a nonasymptotic bound with the constant a = 2 (Devroye, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6686370,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e85a68602abf92fcc1efb8b7aa90d27d141a80c2",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 149,
            "paperAbstract": {
                "fragments": [],
                "text": "A test sequence is used to select the best rule from a class of discrimination rules defined in terms of the training sequence. The Vapnik-Chervonenkis and related inequalities are used to obtain distribution-free bounds on the difference between the probability of error of the selected rule and the probability of error of the best rule in the given class. The bounds are used to prove the consistency and asymptotic optimality for several popular classes, including linear discriminators, nearest-neighbor rules, kernel-based rules, histogram rules, binary tree classifiers, and Fourier series classifiers. In particular, the method can be used to choose the smoothing parameter in kernel-based rules, to choose k in the k-nearest neighbor rule, and to choose between parametric and nonparametric rules. >"
            },
            "slug": "Automatic-Pattern-Recognition:-A-Study-of-the-of-Devroye",
            "title": {
                "fragments": [],
                "text": "Automatic Pattern Recognition: A Study of the Probability of Error"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The Vapnik-Chervonenkis method can be used to choose the smoothing parameter in kernel-based rules, to choose k in the k-nearest neighbor rule, and to choose between parametric and nonparametric rules."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 339,
                                "start": 308
                            }
                        ],
                        "text": "Using these concepts, the law of large numbers in functional space (necessary and sufficient conditions for uniform convergence of the frequencies to their probabilities) was found, its relation to learning processes was described, and the main nonasymptotic bounds for the rate of convergence were obtained (Vapnik and Chervonenkis, 1968); complete proofs were published by 1971 (Vapnik and Chervonenkis, 1971)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8142232,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a36b028d024bf358c4af1a5e1dc3ca0aed23b553",
            "isKey": false,
            "numCitedBy": 3709,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady. The paper was first published in Russian as \u0412\u0430\u043f\u043d\u0438\u043a \u0412. \u041d. and \u0427\u0435\u0440\u0432\u043e\u043d\u0435\u043d\u043a\u0438\u0441 \u0410. \u042f. \u041e \u0440\u0430\u0432\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0419 \u0441\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u0447\u0430\u0441\u0442\u043e\u0442 \u043f\u043e\u044f\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043e\u0431\u044b\u0442\u0438\u0419 \u043a \u0438\u0445 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044f\u043c. \u0422\u0435\u043e\u0440\u0438\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0419 \u0438 \u0435\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f 16(2), 264\u2013279 (1971)."
            },
            "slug": "Chervonenkis:-On-the-uniform-convergence-of-of-to-Vapnik",
            "title": {
                "fragments": [],
                "text": "Chervonenkis: On the uniform convergence of relative frequencies of events to their probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7035291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ca97e1668e305fb719845f84a05a62dfb946a5d",
            "isKey": false,
            "numCitedBy": 578,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Very rarely are training data evenly distributed in the input space. Local learning algorithms attempt to locally adjust the capacity of the training system to the properties of the training set in each area of the input space. The family of local learning algorithms contains known methods, like the k-nearest neighbors method (kNN) or the radial basis function networks (RBF), as well as new algorithms. A single analysis models some aspects of these algorithms. In particular, it suggests that neither kNN or RBF, nor nonlocal classifiers, achieve the best compromise between locality and capacity. A careful control of these parameters in a simple local learning algorithm has provided a performance breakthrough for an optical character recognition problem. Both the error rate and the rejection performance have been significantly improved."
            },
            "slug": "Local-Learning-Algorithms-Bottou-Vapnik",
            "title": {
                "fragments": [],
                "text": "Local Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A single analysis suggests that neither kNN or RBF, nor nonlocal classifiers, achieve the best compromise between locality and capacity."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": false,
            "numCitedBy": 10833,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 74
                            }
                        ],
                        "text": "14) i=l where ai and Vi are arbitrary values and Wi are arbitrary vectors (Breiman, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 166
                            }
                        ],
                        "text": "In 1992-1993 Jones, Barron, and Breiman described a structure on different sets of functions that has a fast rate of approximation (Jones, 1992), (Barron, 1993), and (Breiman, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12319558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81a1e67bb9a2cc7a94acbfa4c9174ddbd22ce705",
            "isKey": false,
            "numCitedBy": 484,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A hinge function y=h(x) consists of two hyperplanes continuously joined together at a hinge. In regression (prediction), classification (pattern recognition), and noiseless function approximation, use of sums of hinge functions gives a powerful and efficient alternative to neural networks with computation times several orders of magnitude less than is obtained by fitting neural networks with a comparable number of parameters. A simple and effective method for finding good hinges is presented. >"
            },
            "slug": "Hinging-hyperplanes-for-regression,-classification,-Breiman",
            "title": {
                "fragments": [],
                "text": "Hinging hyperplanes for regression, classification, and function approximation"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A simple and effective method for finding good hinges is presented and it is shown that use of sums of hinge functions gives a powerful and efficient alternative to neural networks with computation times several orders of magnitude less than is obtained by fitting neural Networks with a comparable number of parameters."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20658113"
                        ],
                        "name": "A. Barron",
                        "slug": "A.-Barron",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barron",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 194
                            }
                        ],
                        "text": "13) i=l where ai and Vi are arbitrary values, Wi are arbitrary vectors, and S(u) is a sigmoid function (a monotonically increasing function such that limu --+-oo S(u) = -1, limu --+oo S(u) = 1) (Barron, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 146
                            }
                        ],
                        "text": "In 1992-1993 Jones, Barron, and Breiman described a structure on different sets of functions that has a fast rate of approximation (Jones, 1992), (Barron, 1993), and (Breiman, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15383918,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04113e8974341f97258800126d05fd8df2751b7e",
            "isKey": false,
            "numCitedBy": 2590,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Approximation properties of a class of artificial neural networks are established. It is shown that feedforward networks with one layer of sigmoidal nonlinearities achieve integrated squared error of order O(1/n), where n is the number of nodes. The approximated function is assumed to have a bound on the first moment of the magnitude distribution of the Fourier transform. The nonlinear parameters associated with the sigmoidal nodes, as well as the parameters of linear combination, are adjusted in the approximation. In contrast, it is shown that for series expansions with n terms, in which only the parameters of linear combination are adjusted, the integrated squared approximation error cannot be made smaller than order 1/n/sup 2/d/ uniformly for functions satisfying the same smoothness assumption, where d is the dimension of the input to the function. For the class of functions examined, the approximation rate and the parsimony of the parameterization of the networks are shown to be advantageous in high-dimensional settings. >"
            },
            "slug": "Universal-approximation-bounds-for-superpositions-a-Barron",
            "title": {
                "fragments": [],
                "text": "Universal approximation bounds for superpositions of a sigmoidal function"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The approximation rate and the parsimony of the parameterization of the networks are shown to be advantageous in high-dimensional settings and the integrated squared approximation error cannot be made smaller than order 1/n/sup 2/d/ uniformly for functions satisfying the same smoothness assumption."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49128898"
                        ],
                        "name": "U. A. M\u00fcller",
                        "slug": "U.-A.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Urs",
                            "lastName": "M\u00fcller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. A. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1776424"
                        ],
                        "name": "Eduard S\u00e4ckinger",
                        "slug": "Eduard-S\u00e4ckinger",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "S\u00e4ckinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eduard S\u00e4ckinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46946958,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55f58ee028e8d86ddf80f68b7538bfb5d6005dc8",
            "isKey": false,
            "numCitedBy": 581,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper compares the performance of several classifier algorithms on a standard database of handwritten digits. We consider not only raw accuracy, but also training time, recognition time, and memory requirements. When available, we report measurements of the fraction of patterns that must be rejected so that the remaining patterns have misclassification rates less than a given threshold."
            },
            "slug": "Comparison-of-classifier-methods:-a-case-study-in-Bottou-Cortes",
            "title": {
                "fragments": [],
                "text": "Comparison of classifier methods: a case study in handwritten digit recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper compares the performance of several classifier algorithms on a standard database of handwritten digits by considering not only raw accuracy, but also training time, recognition time, and memory requirements."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 12th IAPR International Conference on Pattern Recognition, Vol. 3 - Conference C: Signal Processing (Cat. No.94CH3440-5)"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080632378"
                        ],
                        "name": "B. Victorri",
                        "slug": "B.-Victorri",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Victorri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Victorri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16067356,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "153f64ab7c1c24b1b136d8da2f36c6333b8dbfdd",
            "isKey": false,
            "numCitedBy": 384,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "In pattern recognition, statistical modeling, or regression, the amount of data is the most critical factor affecting the performance. If the amount of data and computational resources are near infinite, many algorithmes will probably converge to the optimal solution. When this is not the case, one has to introduce regularizers and a-priori knowledge to supplement the available data in order to boost the performance. Invariance (or known dependance) with respect to transformation of the input is a frequent occurrence of such an a-priori knowledge. In this chapter, we introduce the concept of tangent vectors, which compactly represent the essence of these transformation invariances, and two classes of algorithms, \u201cTangent distance\u201d and \u2018Tangent propagation\u201d, which make use of these invariances to improve performance."
            },
            "slug": "Transformation-Invariance-in-Pattern-Distance-and-Simard-LeCun",
            "title": {
                "fragments": [],
                "text": "Transformation Invariance in Pattern Recognition-Tangent Distance and Tangent Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This chapter introduces the concept of tangent vectors, which compactly represent the essence of these transformation invariances, and two classes of algorithms, \u201cTangent distance\u201d and \u2018Tangent propagation\u201d, which make use of theseinvariances to improve performance."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks: Tricks of the Trade"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32183271"
                        ],
                        "name": "A. E. Hoerl",
                        "slug": "A.-E.-Hoerl",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Hoerl",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. E. Hoerl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94158926"
                        ],
                        "name": "R. Kennard",
                        "slug": "R.-Kennard",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Kennard",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kennard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 28142999,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1473110f6c33b483251ade10b79416d3efee2da4",
            "isKey": false,
            "numCitedBy": 4980,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X\u2032X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X\u2032X to obtain biased estimates with smaller mean square error."
            },
            "slug": "Ridge-Regression:-Biased-Estimation-for-Problems-Hoerl-Kennard",
            "title": {
                "fragments": [],
                "text": "Ridge Regression: Biased Estimation for Nonorthogonal Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The ridge trace is introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality, and how to augment X\u2032X to obtain biased estimates with smaller mean square error."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727567"
                        ],
                        "name": "R. Solomonoff",
                        "slug": "R.-Solomonoff",
                        "structuredName": {
                            "firstName": "Ray",
                            "lastName": "Solomonoff",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Solomonoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 170
                            }
                        ],
                        "text": "The Idea of Algorithmic Complexity Finally, in the 1960s one of the greatest ideas of statistics and information theory was suggested: the idea of algorithmic complexity (Solomonoff, 1960), (Kolmogorov, 1965), and (Chaitin, 1966)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17118014,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a510b16a624e512de69e4ede947d5989b44823b3",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Some preliminary work is presented on a very general new theory of inductive inference. The extrapolation of an ordered sequence of symbols is implemented by computing the a priori probabilities of various sequences of symbols. The a priori probability of a sequence is obtained by considering a universal Turing machine whose output is the sequence in question. An approximation to the a priori probability is given by the shortest input to the machine that will give the desired output. A more exact formulation is given, and it is made somewhat plausible that extrapolation probabilities obtained will be largely independent of just which universal Turing machine was used, providing that the sequence to be extrapolated has an adequate amount of information in it. Some examples are worked out to show the application of the method to specific problems. Applications of the method to curve fitting and other continuous problems are discussed to some extent. Some alternative theories of inductive inference are presented whose validities appear to be corollaries of the validity of the first method described."
            },
            "slug": "A-PRELIMINARY-REPORT-ON-A-GENERAL-THEORY-OF-Solomonoff",
            "title": {
                "fragments": [],
                "text": "A PRELIMINARY REPORT ON A GENERAL THEORY OF INDUCTIVE INFERENCE"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33515643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "843ffb9898cedf899ddcdb9c4bdd10881c122429",
            "isKey": false,
            "numCitedBy": 228,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A boosting algorithm, based on the probably approximately correct (PAC) learning model is used to construct an ensemble of neural networks that significantly improves performance (compared to a single network) in optical character recognition (OCR) problems. The effect of boosting is reported on four handwritten image databases consisting of 12000 digits from segmented ZIP Codes from the United States Postal Service and the following from the National Institute of Standards and Technology: 220000 digits, 45000 upper case letters, and 45000 lower case letters. We use two performance measures: the raw error rate (no rejects) and the reject rate required to achieve a 1% error rate on the patterns not rejected. Boosting improved performance significantly, and, in some cases, dramatically."
            },
            "slug": "Boosting-Performance-in-Neural-Networks-Drucker-Schapire",
            "title": {
                "fragments": [],
                "text": "Boosting Performance in Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "The boosting algorithm is used to construct an ensemble of neural networks that significantly improves performance (compared to a single network) in optical character recognition (OCR) problems and improved performance significantly, and, in some cases, dramatically."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416462"
                        ],
                        "name": "G. Cybenko",
                        "slug": "G.-Cybenko",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Cybenko",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cybenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 121
                            }
                        ],
                        "text": "In 1989 Cybenko proved that using a superposition of sigmoid functions (neurons) one can approximate any smooth function (Cybenko, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3958369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8da1dda34ecc96263102181448c94ec7d645d085",
            "isKey": false,
            "numCitedBy": 6368,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks."
            },
            "slug": "Approximation-by-superpositions-of-a-sigmoidal-Cybenko",
            "title": {
                "fragments": [],
                "text": "Approximation by superpositions of a sigmoidal function"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "It is demonstrated that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Control. Signals Syst."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115302"
                        ],
                        "name": "N. Sauer",
                        "slug": "N.-Sauer",
                        "structuredName": {
                            "firstName": "Norbert",
                            "lastName": "Sauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Sauer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 48
                            }
                        ],
                        "text": "2In 1972 this bound was also published by Sauer (Sauer, 1972)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7231983,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "788c6d1b1419a0f7b7695c0e7e9e41cf54fbfe1b",
            "isKey": false,
            "numCitedBy": 893,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-Density-of-Families-of-Sets-Sauer",
            "title": {
                "fragments": [],
                "text": "On the Density of Families of Sets"
            },
            "venue": {
                "fragments": [],
                "text": "J. Comb. Theory, Ser. A"
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1881615"
                        ],
                        "name": "R. Dudley",
                        "slug": "R.-Dudley",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Dudley",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dudley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 91
                            }
                        ],
                        "text": "One of these generalizations is based on the concept of a VC subgraph introduced by Dudley (Dudley, 1978) (in the AI literature, this concept was renamed pseudo-dimension)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121416923,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "10fd7180b2c0f14e5575b4892e74932b983af822",
            "isKey": false,
            "numCitedBy": 570,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Let $(X, \\mathscr{A}, P)$ be a probability space. Let $X_1, X_2,\\cdots,$ be independent $X$-valued random variables with distribution $P$. Let $P_n := n^{-1}(\\delta_{X_1} + \\cdots + \\delta_{X_n})$ be the empirical measure and let $\\nu_n := n^\\frac{1}{2}(P_n - P)$. Given a class $\\mathscr{C} \\subset \\mathscr{a}$, we study the convergence in law of $\\nu_n$, as a stochastic process indexed by $\\mathscr{C}$, to a certain Gaussian process indexed by $\\mathscr{C}$. If convergence holds with respect to the supremum norm $\\sup_{C \\in \\mathscr{C}}|f(C)|$, in a suitable (usually nonseparable) function space, we call $\\mathscr{C}$ a Donsker class. For measurability, $X$ may be a complete separable metric space, $\\mathscr{a} =$ Borel sets, and $\\mathscr{C}$ a suitable collection of closed sets or open sets. Then for the Donsker property it suffices that for some $m$, and every set $F \\subset X$ with $m$ elements, $\\mathscr{C}$ does not cut all subsets of $F$ (Vapnik-Cervonenkis classes). Another sufficient condition is based on metric entropy with inclusion. If $\\mathscr{C}$ is a sequence $\\{C_m\\}$ independent for $P$, then $\\mathscr{C}$ is a Donsker class if and only if for some $r, \\sigma_m(P(C_m)(1 - P(C_m)))^r < \\infty$."
            },
            "slug": "Central-Limit-Theorems-for-Empirical-Measures-Dudley",
            "title": {
                "fragments": [],
                "text": "Central Limit Theorems for Empirical Measures"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070422946"
                        ],
                        "name": "D. L. Phillips",
                        "slug": "D.-L.-Phillips",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Phillips",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. L. Phillips"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 483,
                                "start": 467
                            }
                        ],
                        "text": "In the middle of the 1960s it was discovered that if instead of the functional R(f) one minimizes another so-called regularized functional R*(f) = IIAI - Flil12 + 'Y(b)f2(f), where f2(f) is some functional (that belongs to a special type of functionals) and 'Y( b) is an appropriately chosen constant (depending on the level of noise), then one obtains a sequence of solutions that converges to the desired one as b tends to zero (Tikhonov, 1963), (Ivanov,1962), and (Phillips, 1962)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35368397,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8dc685b0ba73ad1b279a3cd6ff531d12a9d71388",
            "isKey": false,
            "numCitedBy": 1791,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "where the known functions h(x) , K(x, y) and g(x) are assumed to be bounded and usually to be continuous. If h(x) ~0 the equation is of first kind; if h(x) ~ 0 for a -<_ x ~ b, the equation is of second kind; if h(x) vanishes somewhere but not identically, the equation is of third kind. If the range of integration is infinite or if the kernel K(x, y) is not bounded, the equation is singular. Here we will consider only nonsingular linear integral equations of the first kind:"
            },
            "slug": "A-Technique-for-the-Numerical-Solution-of-Certain-Phillips",
            "title": {
                "fragments": [],
                "text": "A Technique for the Numerical Solution of Certain Integral Equations of the First Kind"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Here the authors will consider only nonsingular linear integral equations of the first kind, where the known functions h(x), K(x, y) and g(x) are assumed to be bounded and usually to be continuous."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2545803"
                        ],
                        "name": "M. Aizerman",
                        "slug": "M.-Aizerman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Aizerman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aizerman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60493317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3caf34c1c86633b6e80dca29e3cb2b6367a0f93",
            "isKey": false,
            "numCitedBy": 1692,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theoretical-Foundations-of-the-Potential-Function-Aizerman",
            "title": {
                "fragments": [],
                "text": "Theoretical Foundations of the Potential Function Method in Pattern Recognition Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35238,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1881615"
                        ],
                        "name": "R. Dudley",
                        "slug": "R.-Dudley",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Dudley",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dudley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122230395,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c59250736174b8d80c40de6e1d5e1641366ef416",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "When \\(\\mathfrak{F}\\) is a universal Donsker class, then for independent, indetically distributed (i.i.d) observation \\(\\mathbf{X}_1,\\ldots,\\mathbf{X}_n\\) with an unknown law P, for any \\(\\mathfrak{f}_i\\)in \\(\\mathfrak{F},\\) \\(i=1,\\ldots,m,\\quad n^{-1/2}\\left\\{ \\mathfrak{f}_1\\left(\\mathbf{X}_1\\right)+\\ldots+\\mathfrak{f}_i\\left(\\mathbf{X}_n\\right)\\right\\}_{1\\leq i\\leq m}\\) is asymptotically normal with mean Vector \\(n^{1/2}\\left\\{\\int\\mathfrak{f}_i\\left(\\mathbf{X}_n\\right)d\\mathbf{P}\\left(x\\right)\\right\\}_{1_\\leq i\\leq m}\\) and covariance matrix \\(\\int\\mathfrak{f}_i\\mathfrak{f}_j d\\mathbf{P}-\\int\\mathfrak{f}_id\\mathbf{P}\\int\\mathfrak{f}_jd\\mathbf{P},\\) uniformly for \\({\\mathfrak{f}_i}\\in \\mathfrak{F}.\\) Then, for certain Statistics formed frome the \\(\\mathfrak{f}_i\\left(\\mathbf{X}_k\\right),\\) even where \\(\\mathfrak{f}_i\\) may be chosen depending on the \\(\\mathbf{X}_k\\) there will be asymptotic distribution as \\(n \\rightarrow \\infty.\\) For example, for \\(\\mathbf{X}^2\\) statistics, where \\(f_i\\) are indicators of disjoint intervals, depending suitably on \\(\\mathbf{X}_1,\\ldots,\\mathbf{X}_n\\), whose union is the real line, \\(\\mathbf{X}^2\\) quadratic forms have limiting distributions [Roy (1956) and Watson (1958)] which may, however, not be \\(\\mathbf{X}^2\\) distributions and may depend on P [Chernoff and Lehmann (1954)]. Universal Donsker classes of sets are, up to mild measurability conditions, just classes satisfying the Vapnik\u2013Cervonenkis comdinatorial conditions defined later in this section Donsker the Vapnik-Cervonenkis combinatorial conditions defined later in this section [Durst and Dudley (1981) and Dudley (1984) Chapter 11]. The use of such classes allows a variety of extensions of the Roy\u2013Watson results to general (multidimensional) sample spaces [Pollard (1979) and Moore and Subblebine (1981)]. Vapnik and Cervonenkis (1974) indicated application of their families of sets to classification (pattern recognition) problems. More recently, the classes have been applied to tree-structured classifiacation [Breiman, Friedman, Olshen and Stone (1984), Chapter 12]."
            },
            "slug": "Universal-Donsker-Classes-and-Metric-Entropy-Dudley",
            "title": {
                "fragments": [],
                "text": "Universal Donsker Classes and Metric Entropy"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11382731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8314dda1ec43ce57ff877f8f02ed89acb68ca035",
            "isKey": false,
            "numCitedBy": 581,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Memory-based classification algorithms such as radial basis functions or K-nearest neighbors typically rely on simple distances (Euclidean, dot product...), which are not particularly meaningful on pattern vectors. More complex, better suited distance measures are often expensive and rather ad-hoc (elastic matching, deformable templates). We propose a new distance measure which (a) can be made locally invariant to any set of transformations of the input and (b) can be computed efficiently. We tested the method on large handwritten character databases provided by the Post Office and the NIST. Using invariances with respect to translation, rotation, scaling, shearing and line thickness, the method consistently outperformed all other systems tested on the same databases."
            },
            "slug": "Efficient-Pattern-Recognition-Using-a-New-Distance-Simard-LeCun",
            "title": {
                "fragments": [],
                "text": "Efficient Pattern Recognition Using a New Transformation Distance"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new distance measure which can be made locally invariant to any set of transformations of the input and can be computed efficiently is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 117
                            }
                        ],
                        "text": "(By 1979 this theory had been generalized for any problem of minimization of the risk on the basis of empirical data (Vapnik, 1979)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 44
                            }
                        ],
                        "text": "The VC dimension of a set of real functions (Vapnik, 1979) Let A ~ Q(z, a) ~ B, a E A, be a set of real functions bounded by constants A and B (A can be -00 and B can be 00)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 108
                            }
                        ],
                        "text": "This concept (as well as the theory of estimating the function at given points) was considered in the 1970s (Vapnik, 1979)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 164
                            }
                        ],
                        "text": "However, in contrast to previous considerations, we use a special type of hyperplane, the so-called optimal separating hyperplanes (Vapnik and Chervonenkis, 1974), (Vapnik, 1979)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 142
                            }
                        ],
                        "text": "6In the first result obtained in 1968 the constant was a = 1/8 (Vapnik and Chervonenkis, 1968, 1971); then in 1979 it was improved to a = 1/4 (Vapnik, 1979)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 186
                            }
                        ],
                        "text": "where we denote by x*(l) some (any) support vector belonging to the first class and we denote by x* ( -1) a support vector belonging to the second class (Vapnik and Chervonenkis, 1974), (Vapnik, 1979)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of dependencies based on empirical Data, (in Russian), Nauka, Moscow"
            },
            "venue": {
                "fragments": [],
                "text": "(English translation: Vladimir Vapnik"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 182
                            }
                        ],
                        "text": "In particular, using a continuous piecewise linear (polygonal) approximation we obtained (in the one-dimensional case) a Parzen's-type estimator with a new kernel defined as follows (Vapnik, 1988):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 119
                            }
                        ],
                        "text": "At the end of the 1980s the residual method for estimating the regularization parameter (width parameter) was proposed (Vapnik 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 240
                            }
                        ],
                        "text": ", a polygon in the one-dimensional case) one can obtain estimators that in addition to nice asymptotic properties (shared by the classical estimators) possess some useful properties from the point of view of limited numbers of observations (Vapnik, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Inductive principles of statistics and learning theory\" Yearbook of the Academy of Sciences of the USSR on Recognition, Classification, and Forecasting, 1, Nauka, Moscow"
            },
            "venue": {
                "fragments": [],
                "text": "(English translation:"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 88
                            }
                        ],
                        "text": "It then appeared in the method for nonparametric density estimation: (i) Parzen windows (Parzen, 1962), (ii) projection methods (Chentsov, 1963), (iii) conditional maximum likelihood method (the method of sieves (Grenander, 1981)), (iv) maximum penalized likelihood method (Tapia and Thompson, 1978)), etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 19
                            }
                        ],
                        "text": "Rosenblatt, 1956), (Parzen, 1962), and (Chentsov, 1963); in the middle of the 1970s the general way for creating these kinds of algorithms on the basis of standard procedures for solving ill-posed problems was found (Vapnik and S1efanyuk, 1978)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 89
                            }
                        ],
                        "text": "First the methods were proposed: The histogram method (Rosenblatt 1956), Parzen's method (Parzen 1962), projection method (Chentsov 1963) and so on followed by proofs of their consistency."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 19
                            }
                        ],
                        "text": "Rosenblatt, 1957), (Parzen, 1962), and (Chentsov, 1963)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On estimation of probability function and mode."
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Mathematical Statistics"
            },
            "year": 1962
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 128
                            }
                        ],
                        "text": "It then appeared in the method for nonparametric density estimation: (i) Parzen windows (Parzen, 1962), (ii) projection methods (Chentsov, 1963), (iii) conditional maximum likelihood method (the method of sieves (Grenander, 1981)), (iv) maximum penalized likelihood method (Tapia and Thompson, 1978)), etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 39
                            }
                        ],
                        "text": "Rosenblatt, 1956), (Parzen, 1962), and (Chentsov, 1963); in the middle of the 1970s the general way for creating these kinds of algorithms on the basis of standard procedures for solving ill-posed problems was found (Vapnik and S1efanyuk, 1978)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 122
                            }
                        ],
                        "text": "First the methods were proposed: The histogram method (Rosenblatt 1956), Parzen's method (Parzen 1962), projection method (Chentsov 1963) and so on followed by proofs of their consistency."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 39
                            }
                        ],
                        "text": "Rosenblatt, 1957), (Parzen, 1962), and (Chentsov, 1963)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evaluation of an unknown distribution density from observations,"
            },
            "venue": {
                "fragments": [],
                "text": "Soviet Math"
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118969901"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 238440002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eccfb47fa72551b2951ab927eadc8358b2609027",
            "isKey": false,
            "numCitedBy": 295,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Internal-Representations-by-Error-Parallel-Rumelhart",
            "title": {
                "fragments": [],
                "text": "Learning Internal Representations by Error Propagation, Parallel Distributed Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102790204"
                        ],
                        "name": "A. Novikoff",
                        "slug": "A.-Novikoff",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Novikoff",
                            "middleNames": [
                                "B.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Novikoff"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 109
                            }
                        ],
                        "text": "Beginning the Analysis of Learning Processes In 1962 Novikoff proved the first theorem about the percept ron (Novikoff, 1962)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122810543,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "eba6b55f7d3302471579eedd5c1558b38f4ea8da",
            "isKey": false,
            "numCitedBy": 439,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "ON-CONVERGENCE-PROOFS-FOR-PERCEPTRONS-Novikoff",
            "title": {
                "fragments": [],
                "text": "ON CONVERGENCE PROOFS FOR PERCEPTRONS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121589596,
            "fieldsOfStudy": [],
            "id": "c8c66af8271d17092b8e2fa575d6e129bc06a564",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ridge regression: biased estimation for nonorthogonal problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1834278"
                        ],
                        "name": "A. Chervonenkis",
                        "slug": "A.-Chervonenkis",
                        "structuredName": {
                            "firstName": "Alexey",
                            "lastName": "Chervonenkis",
                            "middleNames": [
                                "Ya."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Chervonenkis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120020259,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1861a3dcc41bbab284766d57e19accc2a86eadbc",
            "isKey": false,
            "numCitedBy": 211,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Necessary-and-Sufficient-Conditions-for-the-Uniform-Vapnik-Chervonenkis",
            "title": {
                "fragments": [],
                "text": "Necessary and Sufficient Conditions for the Uniform Convergence of Means to their Expectations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687119"
                        ],
                        "name": "E. Wegman",
                        "slug": "E.-Wegman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Wegman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Wegman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 118461270,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ee2da4f75272f0b47ef795655992ec19791d20f4",
            "isKey": false,
            "numCitedBy": 222,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Nonparametric-probability-density-estimation-Wegman",
            "title": {
                "fragments": [],
                "text": "Nonparametric probability density estimation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98400912"
                        ],
                        "name": "Le Cam",
                        "slug": "Le-Cam",
                        "structuredName": {
                            "firstName": "Le",
                            "lastName": "Cam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Le Cam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47433270"
                        ],
                        "name": "M. Lucien",
                        "slug": "M.-Lucien",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Lucien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lucien"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118317967,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bc03436a3f87781c00c636973ab2f3e1d5d09ab5",
            "isKey": false,
            "numCitedBy": 438,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-some-asymptotic-properties-of-maximum-likelihood-Cam-Lucien",
            "title": {
                "fragments": [],
                "text": "On some asymptotic properties of maximum likelihood estimates and related Bayes' estimates"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1953
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 215
                            }
                        ],
                        "text": "Idea of Neural Networks In 1986 several authors independently proposed a method for simultaneously constructing the vector coefficients for all neurons of the Percept ron using the so-called back-propagation method (LeCun, 1986), (Rumelhart, Hinton, and Williams, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 210
                            }
                        ],
                        "text": "The method for calculating the gradient of the empirical risk for the sigmoid approximation of neural networks, called the back-propagation method, was proposed in 1986 (Rumelhart, Hinton, and Williams, 1986), (LeCun, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 64582816,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56b771c4c3a54910dc3e7ff838940de89ed282db",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-processes-in-an-asymmetric-threshold-LeCun",
            "title": {
                "fragments": [],
                "text": "Learning processes in an asymmetric threshold network"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1927644"
                        ],
                        "name": "F. R. Forst",
                        "slug": "F.-R.-Forst",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Forst",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. R. Forst"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 154
                            }
                        ],
                        "text": "The solution to this problem actually has the following form: Choose an appropriate density function and then estimate the parameters using the ML method (Huber, 1964)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61846277,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c87d57da3b1f2b467ef4995d30df832ee2281107",
            "isKey": false,
            "numCitedBy": 3971,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-robust-estimation-of-the-location-parameter-Forst",
            "title": {
                "fragments": [],
                "text": "On robust estimation of the location parameter"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052516042"
                        ],
                        "name": "G. Schwarz",
                        "slug": "G.-Schwarz",
                        "structuredName": {
                            "firstName": "Gideon",
                            "lastName": "Schwarz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Schwarz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 129
                            }
                        ],
                        "text": "Therefore, several ideas for estimating the degree of the approximating polynomial were suggested, including (Akaike, 1970), and (Schwartz, 1978) (see (Miller, 1990))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123722079,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "37e44d1de8003d8394d158ec6afd1ff0e87e595b",
            "isKey": false,
            "numCitedBy": 39556,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimating-the-Dimension-of-a-Model-Schwarz",
            "title": {
                "fragments": [],
                "text": "Estimating the Dimension of a Model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Handwritten digit recognition with backpropagation network,"
            },
            "venue": {
                "fragments": [],
                "text": "Jackel"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 122
                            }
                        ],
                        "text": "On the basis of this bound, Pollard derived a bound for the rate of uniform convergence of the means to their expectation (Pollard, 1984)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Convergence of stochastic processes, Springer, New York"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonparametric methods for estimating probability densities,\" Autom. and Remote Gontr"
            },
            "venue": {
                "fragments": [],
                "text": "Stefanyuk"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Glivenko (1933), \"Sulla determinazione empirica di probabilita',\" Giornale dell' Instituto Italiano degli Attuari"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1933
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A History of Western Philosophy, Unwin, London"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 150
                            }
                        ],
                        "text": "First this idea appeared in the methods for solving ill-posed problems: (i) Methods of quasi-solutions (Ivanov, 1962), (ii) methods of regularization (Tikhonov, 1963))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 446,
                                "start": 430
                            }
                        ],
                        "text": "In the middle of the 1960s it was discovered that if instead of the functional R(f) one minimizes another so-called regularized functional R*(f) = IIAI - Flil12 + 'Y(b)f2(f), where f2(f) is some functional (that belongs to a special type of functionals) and 'Y( b) is an appropriately chosen constant (depending on the level of noise), then one obtains a sequence of solutions that converges to the desired one as b tends to zero (Tikhonov, 1963), (Ivanov,1962), and (Phillips, 1962)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On solving ill-posed problem and method of regularization,"
            },
            "venue": {
                "fragments": [],
                "text": "Doklady Akademii Nauk USSR,"
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Incorporating invariance in support vector learning machines,\" in book"
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Neural Network - ICANN'96. Springer Lecture Notes in Computer Science"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Approximation properties of a multi-layer feedforward artificial neural network,"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Computational Mathematics"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "German translation: W.N. Wapnik, A.Ja"
            },
            "venue": {
                "fragments": [],
                "text": "Tschervonenkis"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 188
                            }
                        ],
                        "text": "More attention is now focused on the alternatives to neural nets, for example, a great deal of effort has been devoted to the study of the radial basis functions method (see the review in (Powell, 1992))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 54
                            }
                        ],
                        "text": "For the theory of RBF machines see (Micchelli, 1986), (Powell, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The theory of radial basis functions approximation in 1990,\" W.A. Light ed., Advances in Numerical Analysis Volume II: Wavelets, Subdivision algorithms and radial basis functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On multivariant Kolmogorov-Smirnov distribution,"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics & Probability Letters"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic complexity and statistical inquiry, World Scientific"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 116
                            }
                        ],
                        "text": "The Perceptron Model To construct such a rule the percept ron uses adaptive properties of the simplest neuron model (Rosenblatt, 1962)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Principles of neurodinamics: Perceptron and theory of brain mechanisms, Spartan Books, Washington D.C"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Geostatistical case stadies (Quantitative geology and geostatistics), D"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of probability and mathematical statistics (Selected works), Nauka, Moscow"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Rate of convergence for radial basis functions and neural networks,\" Artificial Neuml Networks for Speech and Vision"
            },
            "venue": {
                "fragments": [],
                "text": "Anzellotti"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A formal theory of inductive inference,\" Parts 1 and 2, Inform. Contr.,7"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sulla determinatione empirica di una leggi di distributione,"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1933
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal programming problem with inequality constraints. I: Necessary conditions for extremal solutions"
            },
            "venue": {
                "fragments": [],
                "text": "AIAA Journal"
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical estimation: Asymptotic theory, Springer, New York"
            },
            "venue": {
                "fragments": [],
                "text": "Hasminskii"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 86
                            }
                        ],
                        "text": "Popper suggested his famous criterion for demarcation between true and false theories (Popper, 1968)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Logic of Scientific Discovery, 2nd cd"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sulla determinazione empirica della leggi di probabilita',\" Giornale dell' Institute Italiano degli Attuari"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1933
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 141
                            }
                        ],
                        "text": "In the 1970s, on the basis of these ideas, Rissanen suggested the minimum description length (MDL) inductive inference for learning problems (Rissanen, 1978)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 220
                            }
                        ],
                        "text": "Ten years after the concept of algorithmic complexity was introduced, Rissanen suggested using Kolmogorov's concept as the main tool of inductive inference of learning machines; he suggested the so-called MDL principle6 (Rissanen, 1978])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modeling by shortest data"
            },
            "venue": {
                "fragments": [],
                "text": "description,\" Automatica,"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 54
                            }
                        ],
                        "text": "First the methods were proposed: The histogram method (Rosenblatt 1956), Parzen's method (Parzen 1962), projection method (Chentsov 1963) and so on followed by proofs of their consistency."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Remarks on some nonparametric estimation of density functions,"
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Mathematical Statistics"
            },
            "year": 1956
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 103
                            }
                        ],
                        "text": "First this idea appeared in the methods for solving ill-posed problems: (i) Methods of quasi-solutions (Ivanov, 1962), (ii) methods of regularization (Tikhonov, 1963))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On linear problems which are not well-posed,"
            },
            "venue": {
                "fragments": [],
                "text": "Soviet Math. Dod"
            },
            "year": 1962
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Robbince-Monroe process and the method of potential functions,"
            },
            "venue": {
                "fragments": [],
                "text": "Rozonoer"
            },
            "year": 1965
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 27,
            "methodology": 14,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 73,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/The-Nature-of-Statistical-Learning-Theory-Vapnik/8213dbed4db44e113af3ed17d6dad57471a0c048?sort=total-citations"
}