{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821130"
                        ],
                        "name": "David J. Crandall",
                        "slug": "David-J.-Crandall",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Crandall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Crandall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2612372"
                        ],
                        "name": "L. Backstrom",
                        "slug": "L.-Backstrom",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Backstrom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Backstrom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3371403"
                        ],
                        "name": "J. Kleinberg",
                        "slug": "J.-Kleinberg",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Kleinberg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kleinberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 116
                            }
                        ],
                        "text": "On average, subjects were correct 79% of the time (std = 6.3), with chance at 50% (when allowed to scrutinize the text, performance for some subjects went up as high as 90%)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 186
                            }
                        ],
                        "text": "Flickr has emerged as the data-source of choice for most recently developed data-driven applications in computer vision and graphics, including visual geo-location [Hays and Efros 2008; Crandall et al. 2009; Li et al. 2009]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2106641,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a4dc773c76b016542206375eef7d8fe79fb7cd4",
            "isKey": true,
            "numCitedBy": 858,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate how to organize a large collection of geotagged photos, working with a dataset of about 35 million images collected from Flickr. Our approach combines content analysis based on text tags and image data with structural analysis based on geospatial data. We use the spatial distribution of where people take photos to define a relational structure between the photos that are taken at popular places. We then study the interplay between this structure and the content, using classification methods for predicting such locations from visual, textual and temporal features of the photos. We find that visual and temporal features improve the ability to estimate the location of a photo, compared to using just textual features. We illustrate using these techniques to organize a large photo collection, while also revealing various interesting properties about popular cities and landmarks at a global scale."
            },
            "slug": "Mapping-the-world's-photos-Crandall-Backstrom",
            "title": {
                "fragments": [],
                "text": "Mapping the world's photos"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work uses the spatial distribution of where people take photos to define a relational structure between the photos that are taken at popular places, and finds that visual and temporal features improve the ability to estimate the location of a photo, compared to using just textual features."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726249"
                        ],
                        "name": "T. Quack",
                        "slug": "T.-Quack",
                        "structuredName": {
                            "firstName": "Till",
                            "lastName": "Quack",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Quack"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789756"
                        ],
                        "name": "B. Leibe",
                        "slug": "B.-Leibe",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Leibe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Leibe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 202
                            }
                        ],
                        "text": "Finally, our paper is related to a line of work on unsupervised object discovery [Russell et al. 2006; Chum et al. 2009; Karlinsky et al. 2009; Lee and Grauman 2009; Singh et al. 2012] (and espe-\ncially [Quack et al. 2008], who also deal with mining geo-tagged image data)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 180
                            }
                        ],
                        "text": "Specifically, given tens of thousands of geo-localized images of some geographic region R, we aim to find a few hundred visual elements that are both: 1) repeating, i.e. they occur often in R, and 2) geographically discriminative, i.e. they occur much more often in R than in RC ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16379501,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9923b1167ad8f2ed341c39faa8b016274a4e1ae2",
            "isKey": false,
            "numCitedBy": 295,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe an approach for mining images of objects (such as touristic sights) from community photo collections in an unsupervised fashion. Our approach relies on retrieving geotagged photos from those web-sites using a grid of geospatial tiles. The downloaded photos are clustered into potentially interesting entities through a processing pipeline of several modalities, including visual, textual and spatial proximity. The resulting clusters are analyzed and are automatically classified into objects and events. Using mining techniques, we then find text labels for these clusters, which are used to again assign each cluster to a corresponding Wikipedia article in a fully unsupervised manner. A final verification step uses the contents (including images) from the selected Wikipedia article to verify the cluster-article assignment. We demonstrate this approach on several urban areas, densely covering an area of over 700 square kilometers and mining over 200,000 photos, making it probably the largest experiment of its kind to date."
            },
            "slug": "World-scale-mining-of-objects-and-events-from-photo-Quack-Leibe",
            "title": {
                "fragments": [],
                "text": "World-scale mining of objects and events from community photo collections"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This paper describes an approach for mining images of objects from community photo collections in an unsupervised fashion, and demonstrates this approach on several urban areas, densely covering an area of over 700 square kilometers and mining over 200,000 photos, making it probably the largest experiment of its kind to date."
            },
            "venue": {
                "fragments": [],
                "text": "CIVR '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781242"
                        ],
                        "name": "Abhinav Shrivastava",
                        "slug": "Abhinav-Shrivastava",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Shrivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abhinav Shrivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3045340"
                        ],
                        "name": "Tomasz Malisiewicz",
                        "slug": "Tomasz-Malisiewicz",
                        "structuredName": {
                            "firstName": "Tomasz",
                            "lastName": "Malisiewicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomasz Malisiewicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 11
                            }
                        ],
                        "text": "Recently, [Shrivastava et al. 2011] showed how one can improve visual retrieval by adapting the distance metric to the given query using discriminative learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "A standard technique for finding repeated patterns in data is clustering."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13898083,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "39689bd427f5668a8fbb3113019c2b8393f0b1a7",
            "isKey": false,
            "numCitedBy": 247,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this work is to find visually similar images even if they appear quite different at the raw pixel level. This task is particularly important for matching images across visual domains, such as photos taken over different seasons or lighting conditions, paintings, hand-drawn sketches, etc. We propose a surprisingly simple method that estimates the relative importance of different features in a query image based on the notion of \"data-driven uniqueness\". We employ standard tools from discriminative object detection in a novel way, yielding a generic approach that does not depend on a particular image representation or a specific visual domain. Our approach shows good performance on a number of difficult cross-domain visual tasks e.g., matching paintings or sketches to real photographs. The method also allows us to demonstrate novel applications such as Internet re-photography, and painting2gps. While at present the technique is too computationally intensive to be practical for interactive image retrieval, we hope that some of the ideas will eventually become applicable to that domain as well."
            },
            "slug": "Data-driven-visual-similarity-for-cross-domain-Shrivastava-Malisiewicz",
            "title": {
                "fragments": [],
                "text": "Data-driven visual similarity for cross-domain image matching"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A surprisingly simple method that estimates the relative importance of different features in a query image based on the notion of \"data-driven uniqueness\" is proposed, yielding a generic approach that does not depend on a particular image representation or a specific visual domain."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066445700"
                        ],
                        "name": "Jan Knopp",
                        "slug": "Jan-Knopp",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Knopp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Knopp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758039"
                        ],
                        "name": "T. Pajdla",
                        "slug": "T.-Pajdla",
                        "structuredName": {
                            "firstName": "Tom\u00e1s",
                            "lastName": "Pajdla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pajdla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "But what are those features?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 86
                            }
                        ],
                        "text": "Geo-tagged datasets have also been used for place recognition [Schindler et al. 2007; Knopp et al. 2010; Chen et al. 2011] including famous landmarks [Li et al. 2008; Li et al. 2009; Zheng et al. 2009]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 60
                            }
                        ],
                        "text": "Our work is particularly related to [Schindler et al. 2007; Knopp et al. 2010], where geotags are also used as a supervisory signal to find sets of image features discriminative for a particular place."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12680385,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f50032d3d3251069179baee50f78e2e2b076b2ad",
            "isKey": false,
            "numCitedBy": 236,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We seek to recognize the place depicted in a query image using a database of \"street side\" images annotated with geolocation information. This is a challenging task due to changes in scale, viewpoint and lighting between the query and the images in the database. One of the key problems in place recognition is the presence of objects such as trees or road markings, which frequently occur in the database and hence cause significant confusion between different places. As the main contribution, we show how to avoid features leading to confusion of particular places by using geotags attached to database images as a form of supervision. We develop a method for automatic detection of image-specific and spatially-localized groups of confusing features, and demonstrate that suppressing them significantly improves place recognition performance while reducing the database size. We show the method combines well with the state of the art bag-of-features model including query expansion, and demonstrate place recognition that generalizes over wide range of viewpoints and lighting conditions. Results are shown on a geotagged database of over 17K images of Paris downloaded from Google Street View."
            },
            "slug": "Avoiding-Confusing-Features-in-Place-Recognition-Knopp-Sivic",
            "title": {
                "fragments": [],
                "text": "Avoiding Confusing Features in Place Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A method for automatic detection of image-specific and spatially-localized groups of confusing features is developed, and it is demonstrated that suppressing them significantly improves place recognition performance while reducing the database size."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12600623"
                        ],
                        "name": "David M. Chen",
                        "slug": "David-M.-Chen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chen",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49589443"
                        ],
                        "name": "Georges Baatz",
                        "slug": "Georges-Baatz",
                        "structuredName": {
                            "firstName": "Georges",
                            "lastName": "Baatz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Georges Baatz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3154228"
                        ],
                        "name": "K. K\u00f6ser",
                        "slug": "K.-K\u00f6ser",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "K\u00f6ser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. K\u00f6ser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778174"
                        ],
                        "name": "Sam S. Tsai",
                        "slug": "Sam-S.-Tsai",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Tsai",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sam S. Tsai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2201723"
                        ],
                        "name": "Ramakrishna Vedantham",
                        "slug": "Ramakrishna-Vedantham",
                        "structuredName": {
                            "firstName": "Ramakrishna",
                            "lastName": "Vedantham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ramakrishna Vedantham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2453822"
                        ],
                        "name": "Timo Pylv\u00e4n\u00e4inen",
                        "slug": "Timo-Pylv\u00e4n\u00e4inen",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "Pylv\u00e4n\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timo Pylv\u00e4n\u00e4inen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3318092"
                        ],
                        "name": "Kimmo Roimela",
                        "slug": "Kimmo-Roimela",
                        "structuredName": {
                            "firstName": "Kimmo",
                            "lastName": "Roimela",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kimmo Roimela"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145230804"
                        ],
                        "name": "Xin Chen",
                        "slug": "Xin-Chen",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060584870"
                        ],
                        "name": "Jeff Bach",
                        "slug": "Jeff-Bach",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Bach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Bach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742208"
                        ],
                        "name": "M. Pollefeys",
                        "slug": "M.-Pollefeys",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Pollefeys",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pollefeys"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739786"
                        ],
                        "name": "B. Girod",
                        "slug": "B.-Girod",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Girod",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Girod"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2026212"
                        ],
                        "name": "R. Grzeszczuk",
                        "slug": "R.-Grzeszczuk",
                        "structuredName": {
                            "firstName": "Radek",
                            "lastName": "Grzeszczuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grzeszczuk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5881441,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e5bc2c934ec78550bf6b3e0743a3388838fce31",
            "isKey": false,
            "numCitedBy": 400,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "With recent advances in mobile computing, the demand for visual localization or landmark identification on mobile devices is gaining interest. We advance the state of the art in this area by fusing two popular representations of street-level image data \u2014 facade-aligned and viewpoint-aligned \u2014 and show that they contain complementary information that can be exploited to significantly improve the recall rates on the city scale. We also improve feature detection in low contrast parts of the street-level data, and discuss how to incorporate priors on a user's position (e.g. given by noisy GPS readings or network cells), which previous approaches often ignore. Finally, and maybe most importantly, we present our results according to a carefully designed, repeatable evaluation scheme and make publicly available a set of 1.7 million images with ground truth labels, geotags, and calibration data, as well as a difficult set of cell phone query images. We provide these resources as a benchmark to facilitate further research in the area."
            },
            "slug": "City-scale-landmark-identification-on-mobile-Chen-Baatz",
            "title": {
                "fragments": [],
                "text": "City-scale landmark identification on mobile devices"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work fuses two popular representations of street-level image data \u2014 facade-aligned and viewpoint-aligned \u2014 and shows that they contain complementary information that can be exploited to significantly improve the recall rates on the city scale."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110423572"
                        ],
                        "name": "Yunpeng Li",
                        "slug": "Yunpeng-Li",
                        "structuredName": {
                            "firstName": "Yunpeng",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunpeng Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821130"
                        ],
                        "name": "David J. Crandall",
                        "slug": "David-J.-Crandall",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Crandall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Crandall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "In informal debriefings, our subjects suggested that for most images, a few localized, distinctive elements \u201cimmediately gave it away\u201d."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 167
                            }
                        ],
                        "text": "Geo-tagged datasets have also been used for place recognition [Schindler et al. 2007; Knopp et al. 2010; Chen et al. 2011] including famous landmarks [Li et al. 2008; Li et al. 2009; Zheng et al. 2009]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 208
                            }
                        ],
                        "text": "Flickr has emerged as the data-source of choice for most recently developed data-driven applications in computer vision and graphics, including visual geo-location [Hays and Efros 2008; Crandall et al. 2009; Li et al. 2009]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206769680,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aacd2fefca976b963701669a77808fde973c1d02",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "With the rise of photo-sharing websites such as Facebook and Flickr has come dramatic growth in the number of photographs online. Recent research in object recognition has used such sites as a source of image data, but the test images have been selected and labeled by hand, yielding relatively small validation sets. In this paper we study image classification on a much larger dataset of 30 million images, including nearly 2 million of which have been labeled into one of 500 categories. The dataset and categories are formed automatically from geotagged photos from Flickr, by looking for peaks in the spatial geotag distribution corresponding to frequently-photographed landmarks. We learn models for these landmarks with a multiclass support vector machine, using vector-quantized interest point descriptors as features. We also explore the non-visual information available on modern photo-sharing sites, showing that using textual tags and temporal constraints leads to significant improvements in classification rate. We find that in some cases image features alone yield comparable classification accuracy to using text tags as well as to the performance of human observers."
            },
            "slug": "Landmark-classification-in-large-scale-image-Li-Crandall",
            "title": {
                "fragments": [],
                "text": "Landmark classification in large-scale image collections"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is found that in some cases image features alone yield comparable classification accuracy to using text tags as well as to the performance of human observers, showing that using textual tags and temporal constraints leads to significant improvements in classification rate."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "In this work, we want to find such local geo-informative features automatically, directly from a large database of photographs from a particular place, such as a city."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2061602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b48d90cfebb8fbff29d161f6704d31b6909eb7ad",
            "isKey": false,
            "numCitedBy": 894,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Estimating geographic information from an image is an excellent, difficult high-level computer vision problem whose time has come. The emergence of vast amounts of geographically-calibrated image data is a great reason for computer vision to start looking globally - on the scale of the entire planet! In this paper, we propose a simple algorithm for estimating a distribution over geographic locations from a single image using a purely data-driven scene matching approach. For this task, we leverage a dataset of over 6 million GPS-tagged images from the Internet. We represent the estimated image location as a probability distribution over the Earthpsilas surface. We quantitatively evaluate our approach in several geolocation tasks and demonstrate encouraging performance (up to 30 times better than chance). We show that geolocation estimates can provide the basis for numerous other image understanding tasks such as population density estimation, land cover estimation or urban/rural classification."
            },
            "slug": "IM2GPS:-estimating-geographic-information-from-a-Hays-Efros",
            "title": {
                "fragments": [],
                "text": "IM2GPS: estimating geographic information from a single image"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes a simple algorithm for estimating a distribution over geographic locations from a single image using a purely data-driven scene matching approach and shows that geolocation estimates can provide the basis for numerous other image understanding tasks such as population density estimation, land cover estimation or urban/rural classification."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144756076"
                        ],
                        "name": "Yong Jae Lee",
                        "slug": "Yong-Jae-Lee",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Lee",
                            "middleNames": [
                                "Jae"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Jae Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1406708,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0dd12ca4e927b15b4eb5006827bc91d8c58c3d2a",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a weakly-supervised visual data mining approach that discovers connections between recurring mid-level visual elements in historic (temporal) and geographic (spatial) image collections, and attempts to capture the underlying visual style. In contrast to existing discovery methods that mine for patterns that remain visually consistent throughout the dataset, our goal is to discover visual elements whose appearance changes due to change in time or location; i.e., exhibit consistent stylistic variations across the label space (date or geo-location). To discover these elements, we first identify groups of patches that are style-sensitive. We then incrementally build correspondences to find the same element across the entire dataset. Finally, we train style-aware regressors that model each element's range of stylistic differences. We apply our approach to date and geo-location prediction and show substantial improvement over several baselines that do not model visual style. We also demonstrate the method's effectiveness on the related task of fine-grained classification."
            },
            "slug": "Style-Aware-Mid-level-Representation-for-Visual-in-Lee-Efros",
            "title": {
                "fragments": [],
                "text": "Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "This work presents a weakly-supervised visual data mining approach that discovers connections between recurring mid-level visual elements in historic and geographic image collections, and attempts to capture the underlying visual style."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111092227"
                        ],
                        "name": "Yantao Zheng",
                        "slug": "Yantao-Zheng",
                        "structuredName": {
                            "firstName": "Yantao",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yantao Zheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92590083"
                        ],
                        "name": "Ming Zhao",
                        "slug": "Ming-Zhao",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144404428"
                        ],
                        "name": "Yang Song",
                        "slug": "Yang-Song",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2595180"
                        ],
                        "name": "Hartwig Adam",
                        "slug": "Hartwig-Adam",
                        "structuredName": {
                            "firstName": "Hartwig",
                            "lastName": "Adam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hartwig Adam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095305139"
                        ],
                        "name": "Ulrich Buddemeier",
                        "slug": "Ulrich-Buddemeier",
                        "structuredName": {
                            "firstName": "Ulrich",
                            "lastName": "Buddemeier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ulrich Buddemeier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726358"
                        ],
                        "name": "A. Bissacco",
                        "slug": "A.-Bissacco",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Bissacco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bissacco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064290457"
                        ],
                        "name": "Fernando Brucher",
                        "slug": "Fernando-Brucher",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Brucher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando Brucher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144078686"
                        ],
                        "name": "Tat-Seng Chua",
                        "slug": "Tat-Seng-Chua",
                        "structuredName": {
                            "firstName": "Tat-Seng",
                            "lastName": "Chua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tat-Seng Chua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2665814"
                        ],
                        "name": "H. Neven",
                        "slug": "H.-Neven",
                        "structuredName": {
                            "firstName": "Hartmut",
                            "lastName": "Neven",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Neven"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1953336,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "48190636ce372f9a9420ca2395db886351eccca5",
            "isKey": false,
            "numCitedBy": 358,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling and recognizing landmarks at world-scale is a useful yet challenging task. There exists no readily available list of worldwide landmarks. Obtaining reliable visual models for each landmark can also pose problems, and efficiency is another challenge for such a large scale system. This paper leverages the vast amount of multimedia data on the Web, the availability of an Internet image search engine, and advances in object recognition and clustering techniques, to address these issues. First, a comprehensive list of landmarks is mined from two sources: (1) ~20 million GPS-tagged photos and (2) online tour guide Web pages. Candidate images for each landmark are then obtained from photo sharing Websites or by querying an image search engine. Second, landmark visual models are built by pruning candidate images using efficient image matching and unsupervised clustering techniques. Finally, the landmarks and their visual models are validated by checking authorship of their member images. The resulting landmark recognition engine incorporates 5312 landmarks from 1259 cities in 144 countries. The experiments demonstrate that the engine can deliver satisfactory recognition performance with high efficiency."
            },
            "slug": "Tour-the-world:-Building-a-web-scale-landmark-Zheng-Zhao",
            "title": {
                "fragments": [],
                "text": "Tour the world: Building a web-scale landmark recognition engine"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper leverages the vast amount of multimedia data on the Web, the availability of an Internet image search engine, and advances in object recognition and clustering techniques, to address issues of modeling and recognizing landmarks at world-scale."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3413968"
                        ],
                        "name": "Petr Gron\u00e1t",
                        "slug": "Petr-Gron\u00e1t",
                        "structuredName": {
                            "firstName": "Petr",
                            "lastName": "Gron\u00e1t",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Petr Gron\u00e1t"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3276694"
                        ],
                        "name": "M. Havlena",
                        "slug": "M.-Havlena",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Havlena",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Havlena"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758039"
                        ],
                        "name": "T. Pajdla",
                        "slug": "T.-Pajdla",
                        "structuredName": {
                            "firstName": "Tom\u00e1s",
                            "lastName": "Pajdla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pajdla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 31289311,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dfd982490a8f0e8d8d8a9508b7bf162b01d0b2fd",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Google Maps API combined with Street View images can serve as a powerful tool for place recognition or city reconstruction tasks. In this paper, we present a way how to build geotagged datasets of perspective views from Google Maps. Given the initial GPS coordinates, the algorithm can build a list of panoramas in a certain area, download corresponding panoramas, and generate perspective views. In more detail, each panorama on Google Maps Street View contains meta data from which the GPS location and the direction of the view can be extracted. Moreover, the information about the neighbouring panoramas can be obtained as well, hence, a list of panoramas covering a certain area can be built. Downloading panoramas from the list and combining it with the meta data, each downloaded panorama is cut into a set of overlaping perspective views and stored while the camera GPS location, yaw, and pitch are coded in the filename of the perspective view. The geotagged database is subsequently used for place recognition and structure from motion 3D reconstruction."
            },
            "slug": "Building-Streetview-Datasets-for-Place-Recognition-Gron\u00e1t-Havlena",
            "title": {
                "fragments": [],
                "text": "Building Streetview Datasets for Place Recognition and City Reconstruction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents a way how to build geotagged datasets of perspective views from Google Maps, and shows how this database is used for place recognition and structure from motion 3D reconstruction."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2786693"
                        ],
                        "name": "Carl Doersch",
                        "slug": "Carl-Doersch",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Doersch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl Doersch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6334137,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b222b3b05d8d313420dbde8b163e4336a85dcde9",
            "isKey": false,
            "numCitedBy": 268,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work on mid-level visual representations aims to capture information at the level of complexity higher than typical \"visual words\", but lower than full-blown semantic objects. Several approaches [5,6,12,23] have been proposed to discover mid-level visual elements, that are both 1) representative, i.e., frequently occurring within a visual dataset, and 2) visually discriminative. However, the current approaches are rather ad hoc and difficult to analyze and evaluate. In this work, we pose visual element discovery as discriminative mode seeking, drawing connections to the the well-known and well-studied mean-shift algorithm [2, 1, 4, 8]. Given a weakly-labeled image collection, our method discovers visually-coherent patch clusters that are maximally discriminative with respect to the labels. One advantage of our formulation is that it requires only a single pass through the data. We also propose the Purity-Coverage plot as a principled way of experimentally analyzing and evaluating different visual discovery approaches, and compare our method against prior work on the Paris Street View dataset of [5]. We also evaluate our method on the task of scene classification, demonstrating state-of-the-art performance on the MIT Scene-67 dataset."
            },
            "slug": "Mid-level-Visual-Element-Discovery-as-Mode-Seeking-Doersch-Gupta",
            "title": {
                "fragments": [],
                "text": "Mid-level Visual Element Discovery as Discriminative Mode Seeking"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Given a weakly-labeled image collection, this method discovers visually-coherent patch clusters that are maximally discriminative with respect to the labels, and proposes the Purity-Coverage plot as a principled way of experimentally analyzing and evaluating different visual discovery approaches."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144556482"
                        ],
                        "name": "Saurabh Singh",
                        "slug": "Saurabh-Singh",
                        "structuredName": {
                            "firstName": "Saurabh",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Saurabh Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 166
                            }
                        ],
                        "text": "Finally, our paper is related to a line of work on unsupervised object discovery [Russell et al. 2006; Chum et al. 2009; Karlinsky et al. 2009; Lee and Grauman 2009; Singh et al. 2012] (and espe-\ncially [Quack et al. 2008], who also deal with mining geo-tagged image data)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 289
                            }
                        ],
                        "text": "We have experimented with such discriminative clustering methods [Moosmann et al. 2007; Fulkerson et al. 2008; Shotton et al. 2008], but found they did not provide the right behavior for our data: they either produce inhomogeneous clusters or focus too much on the most common visual features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "Unfortunately, standard visual words tend to be dominated by low-level features, like edges and corners (Figure 2a), not the larger visual structures we are hoping to find."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 113
                            }
                        ],
                        "text": "We adopt similar machinery, training a linear SVM detector for each visual element in an iterative manner as in [Singh et al. 2012] while also adding a weak geographical constraint."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 159
                            }
                        ],
                        "text": "Specifically, given tens of thousands of geo-localized images of some geographic region R, we aim to find a few hundred visual elements that are both: 1) repeating, i.e. they occur often in R, and 2) geographically discriminative, i.e. they occur much more often in R than in RC ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14970392,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce854ea2c797bd10cbdf4563a558cd8652c4946e",
            "isKey": false,
            "numCitedBy": 575,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this paper is to discover a set of discriminative patches which can serve as a fully unsupervised mid-level visual representation. The desired patches need to satisfy two requirements: 1) to be representative, they need to occur frequently enough in the visual world; 2) to be discriminative, they need to be different enough from the rest of the visual world. The patches could correspond to parts, objects, \"visual phrases\", etc. but are not restricted to be any one of them. We pose this as an unsupervised discriminative clustering problem on a huge dataset of image patches. We use an iterative procedure which alternates between clustering and training discriminative classifiers, while applying careful cross-validation at each step to prevent overfitting. The paper experimentally demonstrates the effectiveness of discriminative patches as an unsupervised mid-level visual representation, suggesting that it could be used in place of visual words for many tasks. Furthermore, discriminative patches can also be used in a supervised regime, such as scene classification, where they demonstrate state-of-the-art performance on the MIT Indoor-67 dataset."
            },
            "slug": "Unsupervised-Discovery-of-Mid-Level-Discriminative-Singh-Gupta",
            "title": {
                "fragments": [],
                "text": "Unsupervised Discovery of Mid-Level Discriminative Patches"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "The paper experimentally demonstrates the effectiveness of discriminative patches as an unsupervised mid-level visual representation, suggesting that it could be used in place of visual words for many tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143868587"
                        ],
                        "name": "A. Oliva",
                        "slug": "A.-Oliva",
                        "structuredName": {
                            "firstName": "Aude",
                            "lastName": "Oliva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Oliva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 93
                            }
                        ],
                        "text": "Alternatively, global image representations from scene recognition, such as GIST descriptor [Oliva and Torralba 2006] have been used for geo-localization of generic scenes on the global Earth scale [Hays and Efros 2008; Kalogerakis et al. 2009]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "Finding those features can be difficult though, since every image can contain more than 25, 000 candidate patches, and only a tiny fraction will be truly distinctive."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2432623,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d94fc289d82738a4d1071470b16ba861ea12169",
            "isKey": false,
            "numCitedBy": 1395,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Building-the-gist-of-a-scene:-the-role-of-global-in-Oliva-Torralba",
            "title": {
                "fragments": [],
                "text": "Building the gist of a scene: the role of global image features in recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Progress in brain research"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144756076"
                        ],
                        "name": "Yong Jae Lee",
                        "slug": "Yong-Jae-Lee",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Lee",
                            "middleNames": [
                                "Jae"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Jae Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794409"
                        ],
                        "name": "K. Grauman",
                        "slug": "K.-Grauman",
                        "structuredName": {
                            "firstName": "Kristen",
                            "lastName": "Grauman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Grauman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 32131539,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3979fc22340bb275e5d1d97f00ba0dc2cf1e881d",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to automatically discover meaningful features in unlabeled image collections. Each image is decomposed into semi-local features that describe neighborhood appearance and geometry. The goal is to determine for each image which of these parts are most relevant, given the image content in the remainder of the collection. Our method first computes an initial image-level grouping based on feature correspondences, and then iteratively refines cluster assignments based on the evolving intra-cluster pattern of local matches. As a result, the significance attributed to each feature influences an image\u2019s cluster membership, while related images in a cluster affect the estimated significance of their features. We show that this mutual reinforcement of object-level and feature-level similarity improves unsupervised image clustering, and apply the technique to automatically discover categories and foreground regions in images from benchmark datasets."
            },
            "slug": "Foreground-Focus:-Unsupervised-Learning-from-Images-Lee-Grauman",
            "title": {
                "fragments": [],
                "text": "Foreground Focus: Unsupervised Learning from Partially Matching Images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that this mutual reinforcement of object-level and feature-level similarity improves unsupervised image clustering, and the technique is applied to automatically discover categories and foreground regions in images from benchmark datasets."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2808670"
                        ],
                        "name": "E. Kalogerakis",
                        "slug": "E.-Kalogerakis",
                        "structuredName": {
                            "firstName": "Evangelos",
                            "lastName": "Kalogerakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Kalogerakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2606576"
                        ],
                        "name": "O. Vesselova",
                        "slug": "O.-Vesselova",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Vesselova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Vesselova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747779"
                        ],
                        "name": "Aaron Hertzmann",
                        "slug": "Aaron-Hertzmann",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Hertzmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron Hertzmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "In this work, we want to find such local geo-informative features automatically, directly from a large database of photographs from a particular place, such as a city."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 220
                            }
                        ],
                        "text": "Alternatively, global image representations from scene recognition, such as GIST descriptor [Oliva and Torralba 2006] have been used for geo-localization of generic scenes on the global Earth scale [Hays and Efros 2008; Kalogerakis et al. 2009]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2343164,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b1a3014af054cbc73b1d8912bf39eb76d6023ec1",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for estimating geographic location for sequences of time-stamped photographs. A prior distribution over travel describes the likelihood of traveling from one location to another during a given time interval. This distribution is based on a training database of 6 million photographs from Flickr.com. An image likelihood for each location is defined by matching a test photograph against the training database. Inferring location for images in a test sequence is then performed using the Forward-Backward algorithm, and the model can be adapted to individual users as well. Using temporal constraints allows our method to geolocate images without recognizable landmarks, and images with no geographic cues whatsoever. This method achieves a substantial performance improvement over the best-available baseline, and geolocates some users' images with near-perfect accuracy."
            },
            "slug": "Image-sequence-geolocation-with-human-travel-priors-Kalogerakis-Vesselova",
            "title": {
                "fragments": [],
                "text": "Image sequence geolocation with human travel priors"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This paper presents a method for estimating geographic location for sequences of time-stamped photographs based on a training database of 6 million photographs from Flickr.com, and achieves a substantial performance improvement over the best-available baseline."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2582852"
                        ],
                        "name": "Grant Schindler",
                        "slug": "Grant-Schindler",
                        "structuredName": {
                            "firstName": "Grant",
                            "lastName": "Schindler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Grant Schindler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144735785"
                        ],
                        "name": "Matthew A. Brown",
                        "slug": "Matthew-A.-Brown",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Brown",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew A. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "But what are those features?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 63
                            }
                        ],
                        "text": "Geo-tagged datasets have also been used for place recognition [Schindler et al. 2007; Knopp et al. 2010; Chen et al. 2011] including famous landmarks [Li et al. 2008; Li et al. 2009; Zheng et al. 2009]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 37
                            }
                        ],
                        "text": "Our work is particularly related to [Schindler et al. 2007; Knopp et al. 2010], where geotags are also used as a supervisory signal to find sets of image features discriminative for a particular place."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14222017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cbd2b4bbdf5b0b16e2f9db342bd902f3d851d094",
            "isKey": false,
            "numCitedBy": 660,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We look at the problem of location recognition in a large image dataset using a vocabulary tree. This entails finding the location of a query image in a large dataset containing 3times104 streetside images of a city. We investigate how the traditional invariant feature matching approach falls down as the size of the database grows. In particular we show that by carefully selecting the vocabulary using the most informative features, retrieval performance is significantly improved, allowing us to increase the number of database images by a factor of 10. We also introduce a generalization of the traditional vocabulary tree search algorithm which improves performance by effectively increasing the branching factor of a fixed vocabulary tree."
            },
            "slug": "City-Scale-Location-Recognition-Schindler-Brown",
            "title": {
                "fragments": [],
                "text": "City-Scale Location Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that by carefully selecting the vocabulary using the most informative features, retrieval performance is significantly improved, allowing us to increase the number of database images by a factor of 10."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 145
                            }
                        ],
                        "text": "\u2026Vol. 31, No. 4, Article 101, Publication Date: July 2012\na scene from photo collections [Simon et al. 2007], finding iconic images of an object [Berg and Berg 2009], etc. 2) More practically, one possible future application of the ideas presented here might be to help CG modelers by\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "CR Categories: I.3.m [Computer Graphics]: Miscellaneous\u2014 Application I.4.10 [Image Processing and Computer Vision]: Image Representation\u2014Statistical\nKeywords: data mining, visual summarization, reference art, big data, computational geography, visual perception\nLinks: DL PDF WEB"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15311570,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "453c90a8ad2d0b26a82d0478d05281736e411e6a",
            "isKey": true,
            "numCitedBy": 75,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We demonstrate that is it possible to automatically find representative example images of a specified object category. These canonical examples are perhaps the kind of images that one would show a child to teach them what, for example a horse is - images with a large object clearly separated from the background. Given a large collection of images returned by a web search for an object category, our approach proceeds without any user supplied training data for the category. First images are ranked according to a category independent composition model that predicts whether they contain a large clearly depicted object, and outputs an estimated location of that object. Then local features calculated on the proposed object regions are used to eliminate images not distinctive to the category and to cluster images by similarity of object appearance. We present results and a user evaluation on a variety of object categories, demonstrating the effectiveness of the approach."
            },
            "slug": "Finding-iconic-images-Berg-Berg",
            "title": {
                "fragments": [],
                "text": "Finding iconic images"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "It is demonstrated that is it possible to automatically find representative example images of a specified object category using a category independent composition model that predicts whether they contain a large clearly depicted object, and outputs an estimated location of that object."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47364998"
                        ],
                        "name": "R. Raguram",
                        "slug": "R.-Raguram",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Raguram",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Raguram"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38955885"
                        ],
                        "name": "Changchang Wu",
                        "slug": "Changchang-Wu",
                        "structuredName": {
                            "firstName": "Changchang",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changchang Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40454588"
                        ],
                        "name": "Jan-Michael Frahm",
                        "slug": "Jan-Michael-Frahm",
                        "structuredName": {
                            "firstName": "Jan-Michael",
                            "lastName": "Frahm",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan-Michael Frahm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749609"
                        ],
                        "name": "S. Lazebnik",
                        "slug": "S.-Lazebnik",
                        "structuredName": {
                            "firstName": "Svetlana",
                            "lastName": "Lazebnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lazebnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "In informal debriefings, our subjects suggested that for most images, a few localized, distinctive elements \u201cimmediately gave it away\u201d."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 151
                            }
                        ],
                        "text": "Geo-tagged datasets have also been used for place recognition [Schindler et al. 2007; Knopp et al. 2010; Chen et al. 2011] including famous landmarks [Li et al. 2008; Li et al. 2009; Zheng et al. 2009]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1320747,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af6890ae521cb39bea56f61368f062df4221747a",
            "isKey": false,
            "numCitedBy": 335,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents an approach for modeling landmarks based on large-scale, heavily contaminated image collections gathered from the Internet. Our system efficiently combines 2D appearance and 3D geometric constraints to extract scene summaries and construct 3D models. In the first stage of processing, images are clustered based on low-dimensional global appearance descriptors, and the clusters are refined using 3D geometric constraints. Each valid cluster is represented by a single iconic view, and the geometric relationships between iconic views are captured by an iconic scene graph. Using structure from motion techniques, the system then registers the iconic images to efficiently produce 3D models of the different aspects of the landmark. To improve coverage of the scene, these 3D models are subsequently extended using additional, non-iconic views. We also demonstrate the use of iconic images for recognition and browsing. Our experimental results demonstrate the ability to process datasets containing up to 46,000 images in less than 20 hours, using a single commodity PC equipped with a graphics card. This is a significant advance towards Internet-scale operation."
            },
            "slug": "Modeling-and-Recognition-of-Landmark-Image-Using-Raguram-Wu",
            "title": {
                "fragments": [],
                "text": "Modeling and Recognition of Landmark Image Collections Using Iconic Scene Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This article presents an approach for modeling landmarks based on large-scale, heavily contaminated image collections gathered from the Internet that efficiently combines 2D appearance and 3D geometric constraints to extract scene summaries and construct 3D models."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143774737"
                        ],
                        "name": "J. Shotton",
                        "slug": "J.-Shotton",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Shotton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shotton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143945334"
                        ],
                        "name": "Matthew Johnson",
                        "slug": "Matthew-Johnson",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 111
                            }
                        ],
                        "text": "We have experimented with such discriminative clustering methods [Moosmann et al. 2007; Fulkerson et al. 2008; Shotton et al. 2008], but found they did not provide the right behavior for our data: they either produce inhomogeneous clusters or focus too much on the most common visual features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9952478,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d136d77dcdfb34381d8f581f3866d10293a519fd",
            "isKey": false,
            "numCitedBy": 1004,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose semantic texton forests, efficient and powerful new low-level features. These are ensembles of decision trees that act directly on image pixels, and therefore do not need the expensive computation of filter-bank responses or local descriptors. They are extremely fast to both train and test, especially compared with k-means clustering and nearest-neighbor assignment of feature descriptors. The nodes in the trees provide (i) an implicit hierarchical clustering into semantic textons, and (ii) an explicit local classification estimate. Our second contribution, the bag of semantic textons, combines a histogram of semantic textons over an image region with a region prior category distribution. The bag of semantic textons is computed over the whole image for categorization, and over local rectangular regions for segmentation. Including both histogram and region prior allows our segmentation algorithm to exploit both textural and semantic context. Our third contribution is an image-level prior for segmentation that emphasizes those categories that the automatic categorization believes to be present. We evaluate on two datasets including the very challenging VOC 2007 segmentation dataset. Our results significantly advance the state-of-the-art in segmentation accuracy, and furthermore, our use of efficient decision forests gives at least a five-fold increase in execution speed."
            },
            "slug": "Semantic-texton-forests-for-image-categorization-Shotton-Johnson",
            "title": {
                "fragments": [],
                "text": "Semantic texton forests for image categorization and segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "The proposed semantic texton forests are ensembles of decision trees that act directly on image pixels, and therefore do not need the expensive computation of filter-bank responses or local descriptors, and give at least a five-fold increase in execution speed."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2487006"
                        ],
                        "name": "B. Fulkerson",
                        "slug": "B.-Fulkerson",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Fulkerson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Fulkerson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715959"
                        ],
                        "name": "Stefano Soatto",
                        "slug": "Stefano-Soatto",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Soatto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefano Soatto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 88
                            }
                        ],
                        "text": "We have experimented with such discriminative clustering methods [Moosmann et al. 2007; Fulkerson et al. 2008; Shotton et al. 2008], but found they did not provide the right behavior for our data: they either produce inhomogeneous clusters or focus too much on the most common visual features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 149880,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31482a39b91561efa140e2537f5edf2efbc17aa8",
            "isKey": false,
            "numCitedBy": 233,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an approach to determine the category and location of objects in images. It performs very fast categorization of each pixel in an image, a brute-force approach made feasible by three key developments: First, our method reduces the size of a large generic dictionary (on the order of ten thousand words) to the low hundreds while increasing classification performance compared to k-means. This is achieved by creating a discriminative dictionary tailored to the task by following the information bottleneck principle. Second, we perform feature-based categorization efficiently on a dense grid by extending the concept of integral images to the computation of local histograms. Third, we compute SIFT descriptors densely in linear time. We compare our method to the state of the art and find that it excels in accuracy and simplicity, performing better while assuming less."
            },
            "slug": "Localizing-Objects-with-Smart-Dictionaries-Fulkerson-Vedaldi",
            "title": {
                "fragments": [],
                "text": "Localizing Objects with Smart Dictionaries"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This work presents an approach to determine the category and location of objects in images that reduces the size of a large generic dictionary to the low hundreds while increasing classification performance compared to k-means, and performs feature-based categorization efficiently on a dense grid."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 82
                            }
                        ],
                        "text": "Finally, our paper is related to a line of work on unsupervised object discovery [Russell et al. 2006; Chum et al. 2009; Karlinsky et al. 2009; Lee and Grauman 2009; Singh et al. 2012] (and espe-\ncially [Quack et al. 2008], who also deal with mining geo-tagged image data)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 154
                            }
                        ],
                        "text": "Specifically, given tens of thousands of geo-localized images of some geographic region R, we aim to find a few hundred visual elements that are both: 1) repeating, i.e. they occur often in R, and 2) geographically discriminative, i.e. they occur much more often in R than in RC ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2066830,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56766bab76cdcd541bf791730944a5e453006239",
            "isKey": false,
            "numCitedBy": 740,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a large dataset of images, we seek to automatically determine the visually similar object and scene classes together with their image segmentation. To achieve this we combine two ideas: (i) that a set of segmented objects can be partitioned into visual object classes using topic discovery models from statistical text analysis; and (ii) that visual object classes can be used to assess the accuracy of a segmentation. To tie these ideas together we compute multiple segmentations of each image and then: (i) learn the object classes; and (ii) choose the correct segmentations. We demonstrate that such an algorithm succeeds in automatically discovering many familiar objects in a variety of image datasets, including those from Caltech, MSRC and LabelMe."
            },
            "slug": "Using-Multiple-Segmentations-to-Discover-Objects-in-Russell-Freeman",
            "title": {
                "fragments": [],
                "text": "Using Multiple Segmentations to Discover Objects and their Extent in Image Collections"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work compute multiple segmentations of each image and then learns the object classes and chooses the correct segmentations, demonstrating that such an algorithm succeeds in automatically discovering many familiar objects in a variety of image datasets, including those from Caltech, MSRC and LabelMe."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35577716"
                        ],
                        "name": "Ian Simon",
                        "slug": "Ian-Simon",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Simon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian Simon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830653"
                        ],
                        "name": "Noah Snavely",
                        "slug": "Noah-Snavely",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Snavely",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah Snavely"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679223"
                        ],
                        "name": "S. Seitz",
                        "slug": "S.-Seitz",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Seitz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Seitz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "CR Categories: I.3.m [Computer Graphics]: Miscellaneous\u2014 Application I.4.10 [Image Processing and Computer Vision]: Image Representation\u2014Statistical\nKeywords: data mining, visual summarization, reference art, big data, computational geography, visual perception\nLinks: DL PDF WEB"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 145
                            }
                        ],
                        "text": "\u2026et al. 2011], summarizing\nACM Transactions on Graphics, Vol. 31, No. 4, Article 101, Publication Date: July 2012\na scene from photo collections [Simon et al. 2007], finding iconic images of an object [Berg and Berg 2009], etc. 2) More practically, one possible future application of the ideas\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7870764,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65cf5bdb2a29e30bc35e085ca27189cac6994b97",
            "isKey": true,
            "numCitedBy": 387,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We formulate the problem of scene summarization as selecting a set of images that efficiently represents the visual content of a given scene. The ideal summary presents the most interesting and important aspects of the scene with minimal redundancy. We propose a solution to this problem using multi-user image collections from the Internet. Our solution examines the distribution of images in the collection to select a set of canonical views to form the scene summary, using clustering techniques on visual features. The summaries we compute also lend themselves naturally to the browsing of image collections, and can be augmented by analyzing user-specified image tag data. We demonstrate the approach using a collection of images of the city of Rome, showing the ability to automatically decompose the images into separate scenes, and identify canonical views for each scene."
            },
            "slug": "Scene-Summarization-for-Online-Image-Collections-Simon-Snavely",
            "title": {
                "fragments": [],
                "text": "Scene Summarization for Online Image Collections"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This work proposes a solution to the problem of scene summarization by examining the distribution of images in the collection to select a set of canonical views to form the scene summary, using clustering techniques on visual features."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144914140"
                        ],
                        "name": "Hao Su",
                        "slug": "Hao-Su",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977260"
                        ],
                        "name": "E. Xing",
                        "slug": "E.-Xing",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Xing",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Xing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 67
                            }
                        ],
                        "text": "However, adding supervision, such as by explicitly training object [Li et al. 2010] or object part detectors [Bourdev and Malik 2009], requires a labor-intensive labeling process for each object/part."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 194
                            }
                        ],
                        "text": "In particular, we construct a feature vector of the query image by building a spatial pyramid and max-pooling the SVM scores of the correspondence detectors in each spatial bin in the manner of [Li et al. 2010]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 591187,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6286a82f72f632672c1890f3dd6bbb15b8e5168b",
            "isKey": false,
            "numCitedBy": 996,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classification; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classifiers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efficient and scalable for large scene datasets, and reveal semantically meaningful feature patterns."
            },
            "slug": "Object-Bank:-A-High-Level-Image-Representation-for-Li-Su",
            "title": {
                "fragments": [],
                "text": "Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A high-level image representation, called the Object Bank, is proposed, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769383"
                        ],
                        "name": "Lubomir D. Bourdev",
                        "slug": "Lubomir-D.-Bourdev",
                        "structuredName": {
                            "firstName": "Lubomir",
                            "lastName": "Bourdev",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lubomir D. Bourdev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9320620,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55b29a2505149d06d8c1d616cd30edca40cb029c",
            "isKey": false,
            "numCitedBy": 1048,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the classic problems of detection, segmentation and pose estimation of people in images with a novel definition of a part, a poselet. We postulate two criteria (1) It should be easy to find a poselet given an input image (2) it should be easy to localize the 3D configuration of the person conditioned on the detection of a poselet. To permit this we have built a new dataset, H3D, of annotations of humans in 2D photographs with 3D joint information, inferred using anthropometric constraints. This enables us to implement a data-driven search procedure for finding poselets that are tightly clustered in both 3D joint configuration space as well as 2D image appearance. The algorithm discovers poselets that correspond to frontal and profile faces, pedestrians, head and shoulder views, among others. Each poselet provides examples for training a linear SVM classifier which can then be run over the image in a multiscale scanning mode. The outputs of these poselet detectors can be thought of as an intermediate layer of nodes, on top of which one can run a second layer of classification or regression. We show how this permits detection and localization of torsos or keypoints such as left shoulder, nose, etc. Experimental results show that we obtain state of the art performance on people detection in the PASCAL VOC 2007 challenge, among other datasets. We are making publicly available both the H3D dataset as well as the poselet parameters for use by other researchers."
            },
            "slug": "Poselets:-Body-part-detectors-trained-using-3D-pose-Bourdev-Malik",
            "title": {
                "fragments": [],
                "text": "Poselets: Body part detectors trained using 3D human pose annotations"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A new dataset, H3D, is built of annotations of humans in 2D photographs with 3D joint information, inferred using anthropometric constraints, to address the classic problems of detection, segmentation and pose estimation of people in images with a novel definition of a part, a poselet."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143868587"
                        ],
                        "name": "A. Oliva",
                        "slug": "A.-Oliva",
                        "structuredName": {
                            "firstName": "Aude",
                            "lastName": "Oliva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Oliva"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 58
                            }
                        ],
                        "text": "Figure 12 shows object-centric averages (in the style of [Torralba and Oliva 2003]) for the detector in the top row of Figure 11 for Paris and London."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11919476,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "697b45e902c9dd2f31d205f0720e7079f71db200",
            "isKey": false,
            "numCitedBy": 838,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we study the statistical properties of natural images belonging to different categories and their relevance for scene and object categorization tasks. We discuss how second-order statistics are correlated with image categories, scene scale and objects. We propose how scene categorization could be computed in a feedforward manner in order to provide top-down and contextual information very early in the visual processing chain. Results show how visual categorization based directly on low-level features, without grouping or segmentation stages, can benefit object localization and identification. We show how simple image statistics can be used to predict the presence and absence of objects in the scene before exploring the image."
            },
            "slug": "Statistics-of-natural-image-categories-Torralba-Oliva",
            "title": {
                "fragments": [],
                "text": "Statistics of natural image categories"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Results show how visual categorization based directly on low-level features, without grouping or segmentation stages, can benefit object localization and identification."
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2386687"
                        ],
                        "name": "O. Teboul",
                        "slug": "O.-Teboul",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Teboul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Teboul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145304110"
                        ],
                        "name": "Lo\u00efc Simon",
                        "slug": "Lo\u00efc-Simon",
                        "structuredName": {
                            "firstName": "Lo\u00efc",
                            "lastName": "Simon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lo\u00efc Simon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2606519"
                        ],
                        "name": "P. Koutsourakis",
                        "slug": "P.-Koutsourakis",
                        "structuredName": {
                            "firstName": "Panagiotis",
                            "lastName": "Koutsourakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Koutsourakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680727"
                        ],
                        "name": "N. Paragios",
                        "slug": "N.-Paragios",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Paragios",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Paragios"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "In an informal survey, we presented 11 subjects with 100 random Street View images of which 50% were from Paris, and the rest from eleven other cities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 281,
                                "start": 263
                            }
                        ],
                        "text": "Such local elements and rules for combining them have been used in computer systems for procedural modeling of architecture to generate 3D models of entire cities in an astonishing level of detail, e.g. [Mueller et al. 2006], or to parse images of facades, e.g. [Teboul et al. 2010]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14031967,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0caaa5f60e53b6f22a06d405b8c598334b4d6ee3",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose a novel approach to the perceptual interpretation of building facades that combines shape grammars, supervised classification and random walks. Procedural modeling is used to model the geometric and the photometric variation of buildings. This is fused with visual classification techniques (randomized forests) that provide a crude probabilistic interpretation of the observation space in order to measure the appropriateness of a procedural generation with respect to the image. A random exploration of the grammar space is used to optimize the sequence of derivation rules towards a semantico-geometric interpretation of the observations. Experiments conducted on complex architecture facades with ground truth validate the approach."
            },
            "slug": "Segmentation-of-building-facades-using-procedural-Teboul-Simon",
            "title": {
                "fragments": [],
                "text": "Segmentation of building facades using procedural shape priors"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A novel approach to the perceptual interpretation of building facades that combines shape grammars, supervised classification and random walks is proposed that is validated on complex architecture facades with ground truth."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 57
                            }
                        ],
                        "text": "For example, in computer vision, \u201cvisual word\u201d approaches [Sivic and Zisserman 2003] use k-means clustering on image patches represented by SIFT descriptors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14457153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "642e328cae81c5adb30069b680cf60ba6b475153",
            "isKey": false,
            "numCitedBy": 6760,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieved is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching in two full length feature films."
            },
            "slug": "Video-Google:-a-text-retrieval-approach-to-object-Sivic-Zisserman",
            "title": {
                "fragments": [],
                "text": "Video Google: a text retrieval approach to object matching in videos"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "An approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video, represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700928"
                        ],
                        "name": "O. Chum",
                        "slug": "O.-Chum",
                        "structuredName": {
                            "firstName": "Ond\u0159ej",
                            "lastName": "Chum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2356443"
                        ],
                        "name": "Michal Perdoch",
                        "slug": "Michal-Perdoch",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Perdoch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michal Perdoch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12755531,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c4c8070e439106a2ecaf92d643cad4e9342ab7e",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel hashing scheme for image retrieval, clustering and automatic object discovery. Unlike commonly used bag-of-words approaches, the spatial extent of image features is exploited in our method. The geometric information is used both to construct repeatable hash keys and to increase the discriminability of the description. Each hash key combines visual appearance (visual words) with semi-local geometric information. Compared with the state-of-the-art min-hash, the proposed method has both higher recall (probability of collision for hashes on the same object) and lower false positive rates (random collisions). The advantages of geometric min-hashing approach are most pronounced in the presence of viewpoint and scale change, significant occlusion or small physical overlap of the viewing fields. We demonstrate the power of the proposed method on small object discovery in a large unordered collection of images and on a large scale image clustering problem."
            },
            "slug": "Geometric-min-Hashing:-Finding-a-(thick)-needle-in-Chum-Perdoch",
            "title": {
                "fragments": [],
                "text": "Geometric min-Hashing: Finding a (thick) needle in a haystack"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "The proposed geometric min-hashing approach is a novel hashing scheme for image retrieval, clustering and automatic object discovery that has both higher recall and lower false positive rates than the state-of-the-art min-hash."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We have also scraped suburbs of Paris for one experiment."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 43
                            }
                        ],
                        "text": "Patches are represented with standard HOG [Dalal and Triggs 2005] (8x8x31 cells), plus a 8x8 color image in L*a*b colorspace (a and b only)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 115
                            }
                        ],
                        "text": "While we can try clustering using larger image patches (with a higher-dimensional feature descriptor, such as HOG [Dalal and Triggs 2005]), k-means behaves poorly in very high dimensions because the distance metric becomes less meaningful, producing visually inhomogeneous clusters (Figure 2b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 82
                            }
                        ],
                        "text": "Throughout the algorithm, we represent such patches using a HOG+color descriptor [Dalal and Triggs 2005]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The following section presents the details of this algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206590483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cec734d7097ab6b1e60d95228ffd64248eb89d66",
            "isKey": true,
            "numCitedBy": 29263,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds."
            },
            "slug": "Histograms-of-oriented-gradients-for-human-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Histograms of oriented gradients for human detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection, and the influence of each stage of the computation on performance is studied."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32074103"
                        ],
                        "name": "A. Sutcliffe",
                        "slug": "A.-Sutcliffe",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Sutcliffe",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sutcliffe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 161670403,
            "fieldsOfStudy": [
                "History",
                "Art"
            ],
            "id": "23d7f05ee1b7adf0c8a55acd77d170ba3f849b2d",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Paris, with its majestic buildings, elegant boulevards, and colourful neighbourhoods, is often hailed as the most beautiful city in the world. In this lavishly illustrated book, one of the city's leading historians links the beauty of Paris to its harmonious architecture, the product of a powerful tradition of classical design running from the Renaissance to the twentieth century. Anthony Sutcliffe traces the main features of the development of Parisian building and architecture since Roman times, explaining the interaction of continuity and innovation and relating it to power, social structure, the property market, fashion, and the creativity of its architects. Three hundred illustrations, most in colour, complement the text, expressing the full character of Paris architecture. Sutcliffe describes in fascinating detail how Paris merged medieval tradition with a Renaissance architecture imported from Italy - first by order of the Crown, then by the aristocracy, the Church, and the middle classes. Under Louis XIV this style became clearly French. After 1789 revolutions and industrialization threatened to undermine Parisian classicism, but it was reinforced by Haussmann in mid-century as part of the most impressive urban development project of all time. Because of Haussmann, says Sutcliffe, public and private buildings conformed to a more rigid design convention than any that Paris had previously known, a classical tradition that remained entrenched until the 1950s, when modernism made its impact in a high-rise revolution during the de Gaulle era. However, explains Sutcliffe, by 1970 this modernist architecture was rejected by the Paris public, and in the last decade the city has seen theemergence of a restrained neo-modern architecture that blends sensitively with the Parisian tradition."
            },
            "slug": "Paris:-An-Architectural-History-Sutcliffe",
            "title": {
                "fragments": [],
                "text": "Paris: An Architectural History"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2428823"
                        ],
                        "name": "Leonid Karlinsky",
                        "slug": "Leonid-Karlinsky",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Karlinsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leonid Karlinsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2888605"
                        ],
                        "name": "Michael Dinerstein",
                        "slug": "Michael-Dinerstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Dinerstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Dinerstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743045"
                        ],
                        "name": "S. Ullman",
                        "slug": "S.-Ullman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Ullman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ullman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7133451,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "174cc61c6a6c2f8eddbf4e0d1b071ab0b403bc0e",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Class learning, both supervised and unsupervised, requires feature selection, which includes two main components. The first is the selection of a discriminative subset of features from a larger pool. The second is the selection of detection parameters for each feature to optimize classification performance. In this paper we present a method for the discovery of multiple classification features, their detection parameters and their consistent configurations, in the fully unsupervised setting. This is achieved by a global optimization of joint consistency between the features as a function of the detection parameters, without assuming any prior parametric model. We demonstrate how the proposed framework can be applied for learning different types of feature parameters, such as detection thresholds and geometric relations, resulting in the unsupervised discovery of informative configurations of objects parts. We test our approach on a wide range of classes and show good results. We also demonstrate how the approach can be used to unsupervisedly separate and learn visually similar sub-classes of a single category, such as facial views or hand poses. We use the approach to compare various criteria for feature consistency, including Mutual Information, Suspicious Coincidence, L2 and Jaccard index. Finally, we compare our approach to aparametric consistency optimization technique such as pLSA and show significantly better performance."
            },
            "slug": "Unsupervised-feature-optimization-(UFO):-selection-Karlinsky-Dinerstein",
            "title": {
                "fragments": [],
                "text": "Unsupervised feature optimization (UFO): Simultaneous selection of multiple features with their detection parameters"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents a method for the discovery of multiple classification features, their detection parameters and their consistent configurations, in the fully unsupervised setting, by a global optimization of joint consistency between the features as a function of the detection parameters, without assuming any prior parametric model."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3128253"
                        ],
                        "name": "F. Moosmann",
                        "slug": "F.-Moosmann",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Moosmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Moosmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82117876"
                        ],
                        "name": "F. Jurie",
                        "slug": "F.-Jurie",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Jurie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jurie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11904287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2d3746a1f755928b5011932285d686eb5a9127b",
            "isKey": false,
            "numCitedBy": 569,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Some of the most effective recent methods for content-based image classification work by extracting dense or sparse local image descriptors, quantizing them according to a coding rule such as k-means vector quantization, accumulating histograms of the resulting \"visual word\" codes over the image, and classifying these with a conventional classifier such as an SVM. Large numbers of descriptors and large codebooks are needed for good results and this becomes slow using k-means. We introduce Extremely Randomized Clustering Forests - ensembles of randomly created clustering trees - and show that these provide more accurate results, much faster training and testing and good resistance to background clutter in several state-of-the-art image classification tasks."
            },
            "slug": "Fast-Discriminative-Visual-Codebooks-using-Forests-Moosmann-Triggs",
            "title": {
                "fragments": [],
                "text": "Fast Discriminative Visual Codebooks using Randomized Clustering Forests"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work introduces Extremely Randomized Clustering Forests - ensembles of randomly created clustering trees - and shows that these provide more accurate results, much faster training and testing and good resistance to background clutter in several state-of-the-art image classification tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5115386"
                        ],
                        "name": "Yunchao Gong",
                        "slug": "Yunchao-Gong",
                        "structuredName": {
                            "firstName": "Yunchao",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunchao Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749609"
                        ],
                        "name": "S. Lazebnik",
                        "slug": "S.-Lazebnik",
                        "structuredName": {
                            "firstName": "Svetlana",
                            "lastName": "Lazebnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lazebnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 52800221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab3c98282db470f75b26f852f255fae67e359907",
            "isKey": false,
            "numCitedBy": 1079,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of learning similarity-preserving binary codes for efficient retrieval in large-scale image collections. We propose a simple and efficient alternating minimization scheme for finding a rotation of zero-centered data so as to minimize the quantization error of mapping this data to the vertices of a zero-centered binary hypercube. This method, dubbed iterative quantization (ITQ), has connections to multi-class spectral clustering and to the orthogonal Procrustes problem, and it can be used both with unsupervised data embeddings such as PCA and supervised embeddings such as canonical correlation analysis (CCA). Our experiments show that the resulting binary coding schemes decisively outperform several other state-of-the-art methods."
            },
            "slug": "Iterative-quantization:-A-procrustean-approach-to-Gong-Lazebnik",
            "title": {
                "fragments": [],
                "text": "Iterative quantization: A procrustean approach to learning binary codes"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A simple and efficient alternating minimization scheme for finding a rotation of zero- centered data so as to minimize the quantization error of mapping this data to the vertices of a zero-centered binary hypercube is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763981"
                        ],
                        "name": "Juliet Fiss",
                        "slug": "Juliet-Fiss",
                        "structuredName": {
                            "firstName": "Juliet",
                            "lastName": "Fiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juliet Fiss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696487"
                        ],
                        "name": "A. Agarwala",
                        "slug": "A.-Agarwala",
                        "structuredName": {
                            "firstName": "Aseem",
                            "lastName": "Agarwala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Agarwala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800609"
                        ],
                        "name": "B. Curless",
                        "slug": "B.-Curless",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Curless",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Curless"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "CR Categories: I.3.m [Computer Graphics]: Miscellaneous\u2014 Application I.4.10 [Image Processing and Computer Vision]: Image Representation\u2014Statistical\nKeywords: data mining, visual summarization, reference art, big data, computational geography, visual perception\nLinks: DL PDF WEB"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 150
                            }
                        ],
                        "text": "\u2026imagery, but rather propose ways of finding and visualizing existing image data in better ways, be it selecting candid portraits from a video stream [Fiss et al. 2011], summarizing\nACM Transactions on Graphics, Vol. 31, No. 4, Article 101, Publication Date: July 2012\na scene from photo collections\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13943523,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9486056dd59ec33f05657af0c03d04a188900a4e",
            "isKey": true,
            "numCitedBy": 33,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we train a computer to select still frames from video that work well as candid portraits. Because of the subjective nature of this task, we conduct a human subjects study to collect ratings of video frames across multiple videos. Then, we compute a number of features and train a model to predict the average rating of a video frame. We evaluate our model with cross-validation, and show that it is better able to select quality still frames than previous techniques, such as simply omitting frames that contain blinking or motion blur, or selecting only smiles. We also evaluate our technique qualitatively on videos that were not part of our validation set, and were taken outdoors and under different lighting conditions."
            },
            "slug": "Candid-portrait-selection-from-video-Fiss-Agarwala",
            "title": {
                "fragments": [],
                "text": "Candid portrait selection from video"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A computer is trained to select still frames from video that work well as candid portraits and is better able to select quality still frames than previous techniques, such as simply omitting frames that contain blinking or motion blur, or selecting only smiles."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40136495"
                        ],
                        "name": "Pascal M\u00fcller",
                        "slug": "Pascal-M\u00fcller",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798011"
                        ],
                        "name": "Peter Wonka",
                        "slug": "Peter-Wonka",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Wonka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Wonka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2469595"
                        ],
                        "name": "Simon Haegler",
                        "slug": "Simon-Haegler",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Haegler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Haegler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32638775"
                        ],
                        "name": "Andreas Ulmer",
                        "slug": "Andreas-Ulmer",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Ulmer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Ulmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 55
                            }
                        ],
                        "text": "In an informal survey, we presented 11 subjects with 100 random Street View images of which 50% were from Paris, and the rest from eleven other cities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 204
                            }
                        ],
                        "text": "Such local elements and rules for combining them have been used in computer systems for procedural modeling of architecture to generate 3D models of entire cities in an astonishing level of detail, e.g. [Mueller et al. 2006], or to parse images of facades, e.g. [Teboul et al. 2010]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53236631,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d1bbbe3379e3912768fc3f1b3083094a7133908",
            "isKey": false,
            "numCitedBy": 903,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "CGA shape, a novel shape grammar for the procedural modeling of CG architecture, produces building shells with high visual quality and geometric detail. It produces extensive architectural models for computer games and movies, at low cost. Context sensitive shape rules allow the user to specify interactions between the entities of the hierarchical shape descriptions. Selected examples demonstrate solutions to previously unsolved modeling problems, especially to consistent mass modeling with volumetric shapes of arbitrary orientation. CGA shape is shown to efficiently generate massive urban models with unprecedented level of detail, with the virtual rebuilding of the archaeological site of Pompeii as a case in point."
            },
            "slug": "Procedural-modeling-of-buildings-M\u00fcller-Wonka",
            "title": {
                "fragments": [],
                "text": "Procedural modeling of buildings"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "CGA shape is shown to efficiently generate massive urban models with unprecedented level of detail, with the virtual rebuilding of the archaeological site of Pompeii as a case in point."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH 2006"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2032526876"
                        ],
                        "name": "FissJuliet",
                        "slug": "FissJuliet",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "FissJuliet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "FissJuliet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644528752"
                        ],
                        "name": "AgarwalaAseem",
                        "slug": "AgarwalaAseem",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "AgarwalaAseem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "AgarwalaAseem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643731980"
                        ],
                        "name": "CurlessBrian",
                        "slug": "CurlessBrian",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "CurlessBrian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "CurlessBrian"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 227432064,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "449da75d80332dbfebe4d1c201f4f271f3633d39",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we train a computer to select still frames from video that work well as candid portraits. Because of the subjective nature of this task, we conduct a human subjects study to collect ..."
            },
            "slug": "Candid-portrait-selection-from-video-FissJuliet-AgarwalaAseem",
            "title": {
                "fragments": [],
                "text": "Candid portrait selection from video"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A computer is trained to select still frames from video that work well as candid portraits and a human subjects study is conducted to collect still frames for this task."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52153279"
                        ],
                        "name": "Fran\u00e7ois Loyer",
                        "slug": "Fran\u00e7ois-Loyer",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Loyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fran\u00e7ois Loyer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 10
                            }
                        ],
                        "text": "Can you tell which is which?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 148
                            }
                        ],
                        "text": "However, doing this directly would not produce much improvement because the SVM tends to overfit to the initial positive examples [Singh et al.\nACM Transactions on Graphics, Vol. 31, No. 4, Article 101, Publication Date: July 2012\n2012], and will prefer them in each next round over new (and better) ones."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 98
                            }
                        ],
                        "text": "To answer the first question, we consulted a respected volume on 19th century Paris architecture [Loyer 1988]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 7
                            }
                        ],
                        "text": "Pre-\nACM Transactions on Graphics, Vol. 31, No. 4, Article 101, Publication Date: July 2012\ndictably, Louvre/Opera is differentiated from the rest of Paris by the presence of big palatial facades."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 144
                            }
                        ],
                        "text": "Recently, [Shrivastava et al. 2011] showed how one can improve visual retrieval by adapting the distance metric to the given query using discriminative learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 86
                            }
                        ],
                        "text": "For example, in the left column, the initial nearest neighbors contain only a few\nACM Transactions on Graphics, Vol. 31, No. 4, Article 101, Publication Date: July 2012\nwindows with railings."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 151
                            }
                        ],
                        "text": "\u2026doing this directly would not produce much improvement because the SVM tends to overfit to the initial positive examples [Singh et al.\nACM Transactions on Graphics, Vol. 31, No. 4, Article 101, Publication Date: July 2012\n2012], and will prefer them in each next round over new (and better) ones."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 4
                            }
                        ],
                        "text": "ACM Transactions on Graphics (SIGGRAPH) 25, 3, 614\u2013623."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 4
                            }
                        ],
                        "text": "ACM Transactions on Graphics, Vol. 31, No. 4, Article 101, Publication Date: July 2012\nLEE, Y., AND GRAUMAN, K. 2009."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 4
                            }
                        ],
                        "text": "ACM Transactions on Graphics, Vol. 31, No. 4, Article 101, Publication Date: July 2012"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 69
                            }
                        ],
                        "text": "Another interesting observation is that some discovered visual el-\nACM Transactions on Graphics, Vol. 31, No. 4, Article 101, Publication Date: July 2012\nQuery&Image&in&Prague& Retrieved&Images&in&Paris& Retrieved&Images&in&Paris&\nFigure 13: Geographically-informed retrieval."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 8
                            }
                        ],
                        "text": "Our\nACM Transactions on Graphics, Vol. 31, No. 4, Article 101, Publication Date: July 2012\ninput& matches& kN N & ES VM & ite r.& 1& ite r.& 2&\nite r.& 3&\ninput& matches& kN N & ES VM & ite r.& 1& ite r.& 2&\nfin al &\ninput& matches& kN N & ES VM & ite r.& 1& ite r.& 2&\nfin al &\nbiggest challenge is that the overwhelming majority of our data is uninteresting, so matching the occurrences of the rare interesting elements is like finding a few needles in a haystack."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 4
                            }
                        ],
                        "text": "ACM Transactions on Graphics (SIGGRAPH Asia) 30, 6, 154."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 142
                            }
                        ],
                        "text": "In the field of architectural history, descriptions of urban and regional architectural styles and their elements are well established, e.g. [Loyer 1988; Sutcliffe 1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 4
                            }
                        ],
                        "text": "ACM Transactions on Graphics (SIGGRAPH Asia) 30, 6, 128."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 306,
                                "start": 295
                            }
                        ],
                        "text": "Our paper shares this motivation with a number of other recent works that don\u2019t actually synthesize new visual imagery, but rather propose ways of finding and visualizing existing image data in better ways, be it selecting candid portraits from a video stream [Fiss et al. 2011], summarizing\nACM Transactions on Graphics, Vol. 31, No. 4, Article 101, Publication Date: July 2012\na scene from photo collections [Simon et al. 2007], finding iconic images of an object [Berg and Berg 2009], etc. 2) More practically, one possible future application of the ideas presented here might be to help CG modelers by generating so-called \u201creference art\u201d for a city."
                    },
                    "intents": []
                }
            ],
            "corpusId": 160760321,
            "fieldsOfStudy": [
                "History"
            ],
            "id": "8f6b0c3555d1efd58bca1121e5ecc9d9b45e0754",
            "isKey": true,
            "numCitedBy": 16,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Paris-Nineteenth-Century:-Architecture-and-Urbanism-Loyer",
            "title": {
                "fragments": [],
                "text": "Paris Nineteenth Century: Architecture and Urbanism"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 238
                            }
                        ],
                        "text": "Their solution was to \u201crun around Paris for a week like mad tourists, just looking at things, talking about them, and taking lots of pictures\u201d not just of the Eiffel Tower but of the many stylistic Paris details, such as signs, doors etc. [Paik 2006](see photos on pp.120\u2013121)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "CR Categories: I.3.m [Computer Graphics]: Miscellaneous\u2014 Application I.4.10 [Image Processing and Computer Vision]: Image Representation\u2014Statistical\nKeywords: data mining, visual summarization, reference art, big data, computational geography, visual perception\nLinks: DL PDF WEB"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 1
                            }
                        ],
                        "text": "[Paik 2006]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Art of Ratatouille"
            },
            "venue": {
                "fragments": [],
                "text": "Chronicle Books"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 121
                            }
                        ],
                        "text": "Finally, our paper is related to a line of work on unsupervised object discovery [Russell et al. 2006; Chum et al. 2009; Karlinsky et al. 2009; Lee and Grauman 2009; Singh et al. 2012] (and espe-\ncially [Quack et al. 2008], who also deal with mining geo-tagged image data)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62013773,
            "fieldsOfStudy": [],
            "id": "e76d3c8579e473924ae831e488608b6e1c82ba45",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unsupervised feature optimization (UFO): Simultaneous selection of multiple features with their detection parameters"
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055460158"
                        ],
                        "name": "John Hart",
                        "slug": "John-Hart",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 64813808,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6cae8a08979c34110a85428f06c973b38828eac6",
            "isKey": false,
            "numCitedBy": 1472,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "ACM-Transactions-on-Graphics-Hart",
            "title": {
                "fragments": [],
                "text": "ACM Transactions on Graphics"
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH 2004"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Random'Images'for'Paris'Street2view' Extracted'Visual'Elements'from'Paris' Random'Images'for'Prague'Street2view' Extracted'Visual'Elements'from"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Art of Ratatouille. Chronicle Books"
            },
            "venue": {
                "fragments": [],
                "text": "The Art of Ratatouille. Chronicle Books"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 63
                            }
                        ],
                        "text": "Geo-tagged datasets have also been used for place recognition [Schindler et al. 2007; Knopp et al. 2010; Chen et al. 2011] including famous landmarks [Li et al. 2008; Li et al. 2009; Zheng et al. 2009]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 37
                            }
                        ],
                        "text": "Our work is particularly related to [Schindler et al. 2007; Knopp et al. 2010], where geotags are also used as a supervisory signal to find sets of image features discriminative for a particular place."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 62
                            }
                        ],
                        "text": "Geo-tagged datasets have also been used for place recognition [Schindler et al. 2007; Knopp et al. 2010; Chen et al. 2011] including famous landmarks [Li et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cityscale location recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1\u20137."
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Random'Images'for'London'Street2view' Extracted'Elements'from'London' Random'Images'for'Barcelona'Street2view' Extracted'Elements'from"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extracted'Elements'from'SF' Extracted'Elements'from'Boston' Random'Images'for"
            },
            "venue": {
                "fragments": [],
                "text": "The 2nd Internet Vision Workshop at Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 58
                            }
                        ],
                        "text": "Figure 12 shows object-centric averages (in the style of [Torralba and Oliva 2003]) for the detector in the top row of Figure 11 for Paris and London."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistics of natural image categories. Network: Computation in Neural Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 21,
            "methodology": 25
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 46,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/What-makes-Paris-look-like-Paris-Doersch-Singh/d913b9d742b99119d96ad2b661f3e7e7c2fa5e2b?sort=total-citations"
}