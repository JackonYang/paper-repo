{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8275028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8778bb692cf105254fe767ef11a3a8afac4a068",
            "isKey": false,
            "numCitedBy": 3817,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial neural net models have been studied for many years in the hope of achieving human-like performance in the fields of speech and image recognition. These models are composed of many nonlinear computational elements operating in parallel and arranged in patterns reminiscent of biological neural nets. Computational elements or nodes are connected via weights that are typically adapted during use to improve performance. There has been a recent resurgence in the field of artificial neural nets caused by new net topologies and algorithms, analog VLSI implementation techniques, and the belief that massive parallelism is essential for high performance speech and image recognition. This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification. These nets are highly parallel building blocks that illustrate neural net components and design principles and can be used to construct more complex systems. In addition to describing these nets, a major emphasis is placed on exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components. Single-layer nets can implement algorithms required by Gaussian maximum-likelihood classifiers and optimum minimum-error classifiers for binary patterns corrupted by noise. More generally, the decision regions required by any classification algorithm can be generated in a straightforward manner by three-layer feed-forward nets."
            },
            "slug": "An-introduction-to-computing-with-neural-nets-Lippmann",
            "title": {
                "fragments": [],
                "text": "An introduction to computing with neural nets"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification and exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE ASSP Magazine"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807117"
                        ],
                        "name": "T. Sanger",
                        "slug": "T.-Sanger",
                        "structuredName": {
                            "firstName": "Terence",
                            "lastName": "Sanger",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sanger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 707975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3975117d907c0e582a35c34137231c87956aa93b",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an optimality principle for training an unsupervised feedforward neural network based upon maximal ability to reconstruct the input data from the network outputs. We describe an algorithm which can be used to train either linear or nonlinear networks with certain types of nonlinearity. Examples of applications to the problems of image coding, feature detection, and analysis of random-dot stereograms are presented."
            },
            "slug": "An-Optimality-Principle-for-Unsupervised-Learning-Sanger",
            "title": {
                "fragments": [],
                "text": "An Optimality Principle for Unsupervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "An optimality principle is proposed for training an unsupervised feedforward neural network based upon maximal ability to reconstruct the input data from the network outputs and an algorithm which can be used to train either linear or nonlinear networks with certain types of nonlinearity."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Baldi and Hornik (1989) proved that the backpropagation energy function has a unique minimum at this solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 67
                            }
                        ],
                        "text": "Since the hidden units will all have approximately equal variance (Baldi & Hornik, 1989; Cottrell et al., 1987), it is not possible to quantize them with different numbers of bits, as it was for the KLT."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 160
                            }
                        ],
                        "text": "As mentioned above, such a network will not actually find the KLT, but will tend to converge to outputs which represent linear combinations of the KLT vectors (Baldi & Hornik, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 135
                            }
                        ],
                        "text": "The algorithm generalizes the Oja (1982) network algorithm to the multiple-output case, and verifies conjectures of Linsker (1988) and Baldi and Hornik (1988) about the existence of such an algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 108
                            }
                        ],
                        "text": "Several authors have noted that SSBP tends to produce hidden units which have approximately equal variance (Baldi & Hornik, 1989; Cottrell et al., 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14333248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9552ac39a57daacf3d75865a268935b5a0df9bbb",
            "isKey": true,
            "numCitedBy": 1336,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-networks-and-principal-component-analysis:-Baldi-Hornik",
            "title": {
                "fragments": [],
                "text": "Neural networks and principal component analysis: Learning from examples without local minima"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143809344"
                        ],
                        "name": "G. Carpenter",
                        "slug": "G.-Carpenter",
                        "structuredName": {
                            "firstName": "Gail",
                            "lastName": "Carpenter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Carpenter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682174"
                        ],
                        "name": "S. Grossberg",
                        "slug": "S.-Grossberg",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Grossberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Grossberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14625094,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de7cf7c01258719cc3be4321f780db378831f2f4",
            "isKey": false,
            "numCitedBy": 1334,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The adaptive resonance theory (ART) suggests a solution to the stability-plasticity dilemma facing designers of learning systems, namely how to design a learning system that will remain plastic, or adaptive, in response to significant events and yet remain stable in response to irrelevant events. ART architectures are discussed that are neural networks that self-organize stable recognition codes in real time in response to arbitrary sequences of input patterns. Within such an ART architecture, the process of adaptive pattern recognition is a special case of the more general cognitive process of hypothesis discovery, testing, search, classification, and learning. This property opens up the possibility of applying ART systems to more general problems of adaptively processing large abstract information sources and databases. The main computational properties of these ART architectures are outlined and contrasted with those of alternative learning and recognition systems.<<ETX>>"
            },
            "slug": "The-ART-of-adaptive-pattern-recognition-by-a-neural-Carpenter-Grossberg",
            "title": {
                "fragments": [],
                "text": "The ART of adaptive pattern recognition by a self-organizing neural network"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Art architectures are discussed that are neural networks that self-organize stable recognition codes in real time in response to arbitrary sequences of input patterns, which opens up the possibility of applying ART systems to more general problems of adaptively processing large abstract information sources and databases."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691804"
                        ],
                        "name": "D. Ballard",
                        "slug": "D.-Ballard",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ballard",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ballard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 38968420,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa109a5c8440332a05ac538d98c4f93d25500c81",
            "isKey": false,
            "numCitedBy": 282,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "In the development of large-scale knowledge networks much recent progress has been inspired by connections to neurobiology. An important component of any \"neural\" network is an accompanying learning algorithm. Such an algorithm, to be biologically plausible, must work for very large numbers of units. Studies of large-scale systems have so far been restricted to systems Without internal units (units With no direct connections to the input or output). Internal units are crucial to such systems as they are the means by which a system can encode high-order regularities (or invariants) that are Implicit in its inputs and outputs. Computer simulations of learning using internal units have been restricted to small-scale systems. This paper describes away of coupling autoassociative learning modules Into hierarchies that should greatly improve the performance of learning algorithms in large-scale systems. The Idea has been tested experimentally with positive results."
            },
            "slug": "Modular-Learning-in-Neural-Networks-Ballard",
            "title": {
                "fragments": [],
                "text": "Modular Learning in Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper describes away of coupling autoassociative learning modules Into hierarchies that should greatly improve the performance of learning algorithms in large-scale systems."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7840452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a57c6d627ffc667ae3547073876c35d6420accff",
            "isKey": false,
            "numCitedBy": 1575,
            "numCiting": 122,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-Learning-Procedures-Hinton",
            "title": {
                "fragments": [],
                "text": "Connectionist Learning Procedures"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Linsker (1988) has suggested that maximization of the output information may be a fundamental principle for the organization of biological neural networks ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 187
                            }
                        ],
                        "text": "The assumption that subsequent processing must be linear means that this criterion is not equivalent to maximizing the Shannon information in the outputs, except in certain special cases (Linsker, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 30
                            }
                        ],
                        "text": "The algorithm generalizes the Oja (1982) network algorithm to the multiple-output case, and verifies conjectures of Linsker (1988) and Baldi and Hornik (1988) about the existence of such an algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 189
                            }
                        ],
                        "text": "The assumption that subsequent processing must be linear means that this criterion is not equivalent to maximizing the Shannon information in the outputs , except in certain special cases (Linsker, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 58
                            }
                        ],
                        "text": "Maximization of output information was first suggested by Linsker (1988) as a principle for the design of neural networks ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1527671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16d70e8af45ca0ae2c1bb73f3be6628518d40b8f",
            "isKey": true,
            "numCitedBy": 1417,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The emergence of a feature-analyzing function from the development rules of simple, multilayered networks is explored. It is shown that even a single developing cell of a layered network exhibits a remarkable set of optimization properties that are closely related to issues in statistics, theoretical physics, adaptive signal processing, the formation of knowledge representation in artificial intelligence, and information theory. The network studied is based on the visual system. These results are used to infer an information-theoretic principle that can be applied to the network as a whole, rather than a single cell. The organizing principle proposed is that the network connections develop in such a way as to maximize the amount of information that is preserved when signals are transformed at each processing stage, subject to certain constraints. The operation of this principle is illustrated for some simple cases.<<ETX>>"
            },
            "slug": "Self-organization-in-a-perceptual-network-Linsker",
            "title": {
                "fragments": [],
                "text": "Self-organization in a perceptual network"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "It is shown that even a single developing cell of a layered network exhibits a remarkable set of optimization properties that are closely related to issues in statistics, theoretical physics, adaptive signal processing, the formation of knowledge representation in artificial intelligence, and information theory."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 31
                            }
                        ],
                        "text": "This approach has been used by Linsker (1986) and Silverman and Noetzel (1988), for example."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5175050,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "06f5a18780d7332ed68a9c786e1c597b27a8e0f6",
            "isKey": false,
            "numCitedBy": 266,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Orientation-selective cells--cells that are selectively responsive to bars and edges at particular orientations--are a salient feature of the architecture of mammalian visual cortex. In the previous paper of this series, I showed that such cells emerge spontaneously during the development of a simple multilayered network having local but initially random feedforward connections that mature, one layer at a time, according to a simple development rule (of Hebb type). In this paper, I show that, in the presence of lateral connections between developing orientation cells, these cells self-organize into banded patterns of cells of similar orientation. These patterns are similar to the \"orientation columns\" found in mammalian visual cortex. No orientation preference is specified to the system at any stage, none of the basic developmental rules is specific to visual processing, and the results emerge even in the absence of visual input to the system (as has been observed in macaque monkey)."
            },
            "slug": "From-basic-network-principles-to-neural-emergence-Linsker",
            "title": {
                "fragments": [],
                "text": "From basic network principles to neural architecture: emergence of orientation columns."
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is shown that, in the presence of lateral connections between developing orientation cells, these cells self-organize into banded patterns of cells of similar orientation, similar to the \"orientation columns\" found in mammalian visual cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 205001834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "isKey": false,
            "numCitedBy": 20334,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."
            },
            "slug": "Learning-representations-by-back-propagating-errors-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating errors"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246319"
                        ],
                        "name": "E. Bienenstock",
                        "slug": "E.-Bienenstock",
                        "structuredName": {
                            "firstName": "Elie",
                            "lastName": "Bienenstock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bienenstock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8884630"
                        ],
                        "name": "L. Cooper",
                        "slug": "L.-Cooper",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Cooper",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Cooper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094648"
                        ],
                        "name": "P. Munro",
                        "slug": "P.-Munro",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Munro",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Munro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1607496,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "9f22cf81654dd50b95e65b86b1125cfe6803a67b",
            "isKey": false,
            "numCitedBy": 2695,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "The development of stimulus selectivity in the primary sensory cortex of higher vertebrates is considered in a general mathematical framework. A synaptic evolution scheme of a new kind is proposed in which incoming patterns rather than converging afferents compete. The change in the efficacy of a given synapse depends not only on instantaneous pre- and postsynaptic activities but also on a slowly varying time-averaged value of the postsynaptic activity. Assuming an appropriate nonlinear form for this dependence, development of selectivity is obtained under quite general conditions on the sensory environment. One does not require nonlinearity of the neuron's integrative power nor does one need to assume any particular form for intracortical circuitry. This is first illustrated in simple cases, e.g., when the environment consists of only two different stimuli presented alternately in a random manner. The following formal statement then holds: the state of the system converges with probability 1 to points of maximum selectivity in the state space. We next consider the problem of early development of orientation selectivity and binocular interaction in primary visual cortex. Giving the environment an appropriate form, we obtain orientation tuning curves and ocular dominance comparable to what is observed in normally reared adult cats or monkeys. Simulations with binocular input and various types of normal or altered environments show good agreement with the relevant experimental data. Experiments are suggested that could test our theory further."
            },
            "slug": "Theory-for-the-development-of-neuron-selectivity:-Bienenstock-Cooper",
            "title": {
                "fragments": [],
                "text": "Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The development of stimulus selectivity in the primary sensory cortex of higher vertebrates is considered in a general mathematical framework and a synaptic evolution scheme of a new kind is proposed in which incoming patterns rather than converging afferents compete."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of neuroscience : the official journal of the Society for Neuroscience"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46617541"
                        ],
                        "name": "D. Kazakos",
                        "slug": "D.-Kazakos",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Kazakos",
                            "middleNames": [],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kazakos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121001128,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6739c73a8e0d9ccaf7a2d00eb2dd20e02f97d236",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Optimal-constrained-representation-and-filtering-of-Kazakos",
            "title": {
                "fragments": [],
                "text": "Optimal constrained representation and filtering of signals"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781325"
                        ],
                        "name": "J. Daugman",
                        "slug": "J.-Daugman",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Daugman",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Daugman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1984348,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6513888c5ef473bdbb3167c7b52f0985be071f7a",
            "isKey": false,
            "numCitedBy": 1899,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "A three-layered neural network is described for transforming two-dimensional discrete signals into generalized nonorthogonal 2-D Gabor representations for image analysis, segmentation, and compression. These transforms are conjoint spatial/spectral representations, which provide a complete image description in terms of locally windowed 2-D spectral coordinates embedded within global 2-D spatial coordinates. In the present neural network approach, based on interlaminar interactions involving two layers with fixed weights and one layer with adjustable weights, the network finds coefficients for complete conjoint 2-D Gabor transforms without restrictive conditions. In wavelet expansions based on a biologically inspired log-polar ensemble of dilations, rotations, and translations of a single underlying 2-D Gabor wavelet template, image compression is illustrated with ratios up to 20:1. Also demonstrated is image segmentation based on the clustering of coefficients in the complete 2-D Gabor transform. >"
            },
            "slug": "Complete-discrete-2-D-Gabor-transforms-by-neural-Daugman",
            "title": {
                "fragments": [],
                "text": "Complete discrete 2-D Gabor transforms by neural networks for image analysis and compression"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A three-layered neural network based on interlaminar interactions involving two layers with fixed weights and one layer with adjustable weights finds coefficients for complete conjoint 2-D Gabor transforms without restrictive conditions for image analysis, segmentation, and compression."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1994273"
                        ],
                        "name": "R. Shapley",
                        "slug": "R.-Shapley",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Shapley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Shapley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4433870"
                        ],
                        "name": "P. Lennie",
                        "slug": "P.-Lennie",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Lennie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Lennie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 967753,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "72dafa24626298cf09f8c96c8aeb6508ff1ff27c",
            "isKey": false,
            "numCitedBy": 303,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "Within the last 15 years, a method called \"spatial frequency analysis\" has been applied widely to the study of receptive fields of neurons in the visual pathway. Out of this work have emerged new concepts of how the brain analyzes and recognizes visual images. The aim of this paper is to explain why \"spatial frequency analysis\" is useful, and to review the insights into visual function that have resulted from its application. It is important to note at the outset that while spatial frequency analysis can provide a comprehensive description of the behavior of neurons in which signals are summed linearly (see below), it has much more limited application to the behavior of neurons that combine signals nonlinearly. Because of this, the greatest insights into visual information processing have come and probably will continue to come from a combined use of space, time, spatial frequency, and temporal frequency measurements. The earliest applications of spatial frequency analysis were motivated by the idea that visual physiology and psychophysics could be more closely related by a uniform approach to the problems of spatial vision. This approach has continued to bind together the psychophysics and physiology of spatial vision, and the interested reader will find relevant psychophysical work discussed in reviews by Braddick et al (1978), De Valois & De Valois (1980), Robson (1980), Westheimer (1984), and Kelly & Burbeck (1984)."
            },
            "slug": "Spatial-frequency-analysis-in-the-visual-system.-Shapley-Lennie",
            "title": {
                "fragments": [],
                "text": "Spatial frequency analysis in the visual system."
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The aim of this paper is to explain why \"spatial frequency analysis\" is useful, and to review the insights into visual function that have resulted from its application."
            },
            "venue": {
                "fragments": [],
                "text": "Annual review of neuroscience"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790264"
                        ],
                        "name": "Eduardo Sontag",
                        "slug": "Eduardo-Sontag",
                        "structuredName": {
                            "firstName": "Eduardo",
                            "lastName": "Sontag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eduardo Sontag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1901890"
                        ],
                        "name": "H. Sussmann",
                        "slug": "H.-Sussmann",
                        "structuredName": {
                            "firstName": "H\u00e9ctor",
                            "lastName": "Sussmann",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sussmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 69947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1943969ee23bf0e7bfe079d76cdd08bcd006d5c4",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We give an example of a neural net without hidden layers and with a sigmoid transfer function, together with a training set of binary vectors, for which the sum of the squared errors, regarded as a function of the weights, has a local minimum which is not a global minimum. The example consists of a set of 125 training instances, with four weights and a threshold to be learnt. We do not know if substantially smaller binary examples exist."
            },
            "slug": "Backpropagation-Can-Give-Rise-to-Spurious-Local-for-Sontag-Sussmann",
            "title": {
                "fragments": [],
                "text": "Backpropagation Can Give Rise to Spurious Local Minima Even for Networks without Hidden Layers"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "An example of a neural net without hidden layers and with a sigmoid transfer function, together with a training set of binary vectors, for which the sum of the squared errors, regarded as a function of the weights, has a local minimum which is not a global minimum."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699388"
                        ],
                        "name": "L. Ljung",
                        "slug": "L.-Ljung",
                        "structuredName": {
                            "firstName": "Lennart",
                            "lastName": "Ljung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ljung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123327817,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "d6af193556d970da366332d1301ea0d5ea5511c2",
            "isKey": false,
            "numCitedBy": 807,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Recursive algorithms where random observations enter are studied in a fairly general framework. An important feature is that the observations my depend on previous \"outputs\" of the algorithm. The considered class of algorithms contains, e.g., stochastic approximation algorithm, recursive identification algorithm, and algorithms for adaptive control of linear systems. It is shown how a deterministic differential equation can be associated with the algorithm. Problems like convergence with probability one, possible convergence points and asymptotic behavior of the algorithm can all be studied in terms of this differential equation. Theorems stating the precise relationships between the differential equation and the algorithm are given as well as examples of applications of the results to problems in identification and adaptive control."
            },
            "slug": "Analysis-of-recursive-stochastic-algorithms-Ljung",
            "title": {
                "fragments": [],
                "text": "Analysis of recursive stochastic algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown how a deterministic differential equation can be associated with the algorithm and examples of applications of the results to problems in identification and adaptive control."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1925241"
                        ],
                        "name": "D. Pollen",
                        "slug": "D.-Pollen",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Pollen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pollen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1992154"
                        ],
                        "name": "S. Ronner",
                        "slug": "S.-Ronner",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Ronner",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ronner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 23970541,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "5ec37637d34c792a3798e9724be81d041ca0ec32",
            "isKey": false,
            "numCitedBy": 376,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper relates to the receptive field properties of neurons in the primary visual cortex, i.e. the striate cortex, to current issues in spatial visual information processing. Particular attention is given to the fact that receptive field profiles of simple cells in the visual cortex often resemble even-symmetric or odd-symmetric Gabor filters; i.e. their receptive field profiles can be described by the product of a Gaussian and either a cosine or sine function. Their spatial frequency tuning is of medium bandwidth (~one octave) which is narrow enough for a cell to distinguish the third harmonic from the fundamental frequency for square-wave gratings of low spatial frequency. The responses of adjacent simple cells, tuned to the same spatial frequency, orientation, and direction, differ in their phase response to drifting sine-wave gratings by approximately either 90\u00b0 or 180\u00b0. This latter result makes it possible to consider two adjacent simple cell pairs as operating like paired Gaussian-attenuated sine and cosine filters of Gabor filters for restricted regions of visual space. The entire set of simple cells provides a complete representation of the visual scene, yet each simple cell is unique in its response properties. At the complex cell stage, the cell's mean firing rate appears to represent the amplitude of a local Fourier coefficient, but phase information is seldom conveyed with much precision in the action potential code."
            },
            "slug": "Visual-cortical-neurons-as-localized-spatial-Pollen-Ronner",
            "title": {
                "fragments": [],
                "text": "Visual cortical neurons as localized spatial frequency filters"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The receptive field properties of neurons in the primary visual cortex, i.e. the striate cortex, are related to current issues in spatial visual information processing to consider two adjacent simple cell pairs as operating like paired Gaussian-attenuated sine and cosine filters of Gabor filters for restricted regions of visual space."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2488592"
                        ],
                        "name": "R. Brockett",
                        "slug": "R.-Brockett",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Brockett",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Brockett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123144601,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1b23f4c1726b2acb13b78b5391a84403d7ecc5d9",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The author establishes a number of properties associated with the dynamical system H=(H,(H,N)), where H and N are symmetric n-by-n matrices and (A,B)=AB-BA. The most important of these come from the fact that this equation is equivalent to a certain gradient flow on the space of orthogonal matrices. Particular emphasis is placed on the role of this equation as an analog computer. For example, it is shown how to map the data associated with a linear programming problem into H(0) and N in such a way as to have H=(H(H,N)) evolve to a solution of the linear programming problem. This result can be applied to find systems that solve a variety of generic combinatorial optimization problems, and it also provides an algorithm for diagonalizing symmetric matrices.<<ETX>>"
            },
            "slug": "Dynamical-systems-that-sort-lists,-diagonalize-and-Brockett",
            "title": {
                "fragments": [],
                "text": "Dynamical systems that sort lists, diagonalize matrices and solve linear programming problems"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 27th IEEE Conference on Decision and Control"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2334226"
                        ],
                        "name": "D. Hubel",
                        "slug": "D.-Hubel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hubel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hubel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2629471"
                        ],
                        "name": "T. Wiesel",
                        "slug": "T.-Wiesel",
                        "structuredName": {
                            "firstName": "Torsten",
                            "lastName": "Wiesel",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Wiesel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17055992,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7",
            "isKey": false,
            "numCitedBy": 12432,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "What chiefly distinguishes cerebral cortex from other parts of the central nervous system is the great diversity of its cell types and interconnexions. It would be astonishing if such a structure did not profoundly modify the response patterns of fibres coming into it. In the cat's visual cortex, the receptive field arrangements of single cells suggest that there is indeed a degree of complexity far exceeding anything yet seen at lower levels in the visual system. In a previous paper we described receptive fields of single cortical cells, observing responses to spots of light shone on one or both retinas (Hubel & Wiesel, 1959). In the present work this method is used to examine receptive fields of a more complex type (Part I) and to make additional observations on binocular interaction (Part II). This approach is necessary in order to understand the behaviour of individual cells, but it fails to deal with the problem of the relationship of one cell to its neighbours. In the past, the technique of recording evoked slow waves has been used with great success in studies of functional anatomy. It was employed by Talbot & Marshall (1941) and by Thompson, Woolsey & Talbot (1950) for mapping out the visual cortex in the rabbit, cat, and monkey. Daniel & Whitteiidge (1959) have recently extended this work in the primate. Most of our present knowledge of retinotopic projections, binocular overlap, and the second visual area is based on these investigations. Yet the method of evoked potentials is valuable mainly for detecting behaviour common to large populations of neighbouring cells; it cannot differentiate functionally between areas of cortex smaller than about 1 mm2. To overcome this difficulty a method has in recent years been developed for studying cells separately or in small groups during long micro-electrode penetrations through nervous tissue. Responses are correlated with cell location by reconstructing the electrode tracks from histological material. These techniques have been applied to"
            },
            "slug": "Receptive-fields,-binocular-interaction-and-in-the-Hubel-Wiesel",
            "title": {
                "fragments": [],
                "text": "Receptive fields, binocular interaction and functional architecture in the cat's visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This method is used to examine receptive fields of a more complex type and to make additional observations on binocular interaction and this approach is necessary in order to understand the behaviour of individual cells, but it fails to deal with the problem of the relationship of one cell to its neighbours."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of physiology"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 147
                            }
                        ],
                        "text": "There exist several different but closely related algorithms for finding multiple eigenvectors which have been proven to converge (Brockett, 1989; Karhunen , 1984a, 1984b, 1985; Karhunen & Oja, 1982; Kuusela & Oja, 1982; Oja, 1983; Oja & Karhunen, 1980, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 133
                            }
                        ],
                        "text": "Proofs of convergence for some of these algorithms, and an excellent summary of the different methods may be found in Oja (1983) and Karhunen (1984b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20111284,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b4b7e81992ebc33ff4b9ceed1c4e71a37940c66",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In several applications of signal processing recursive algorithms for estimating a few eigenvectors of correlation or covariance matrices directly from the incoming samples are desirable. In this paper such algorithms are derived by starting from an extension of the classical power method of numerical analysis, instead of the usual gradient approach. This viewpoint leads to useful and relatively simple rules for determining the gain parameters of Owsley's stochastic gradient ascent algorithm for sensor array processing and Thompson's adaptive algorithm for unbiased frequency estimation using the Pisarenko method. A new, promising algorithm for adaptive estimation of eigenvectors corresponding to the smallest eigenvalues is introduced. Preliminary numerical results and comparisons are given, and a generalization of Thompson's algorithm for estimating several eigenvectors is represented."
            },
            "slug": "Adaptive-algorithms-for-estimating-eigenvectors-of-Karhunen",
            "title": {
                "fragments": [],
                "text": "Adaptive algorithms for estimating eigenvectors of correlation type matrices"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A new, promising algorithm for adaptive estimation of eigenvectors corresponding to the smallest eigenvalues is introduced, and a generalization of Thompson's algorithm for estimating several eigenvctors is represented."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2251137"
                        ],
                        "name": "V. Brailovsky",
                        "slug": "V.-Brailovsky",
                        "structuredName": {
                            "firstName": "Viktor",
                            "lastName": "Brailovsky",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Brailovsky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 71
                            }
                        ],
                        "text": "If the network is designed with too many hidden units (in the sense of Brailovsky, 1983a, 1983b, 1985) then the additional error introduced is spread evenly throughout the units and cannot be easily detected or removed by looking at the signal to noise ratio of the individual units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 211
                            }
                        ],
                        "text": "Once we find an output whose variance is below the threshold, we can stop computing additional terms since all later outputs will have variances which are even lower and may decrease our approximation accuracy (Brailovsky, 1983a, 1983b, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 84372763,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d696bec72b0503f5c6bf616a60209dfac091f85a",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of function approximation by experimental data is usually considered within the framework of a general linear model. One assumes there exists a finite system of known functions p,(x), p2(x) . . . , pk(x) such that a desired functionf(x) may be expressed as a linear combination of these functions. The unknown coefficients may be found by experimental data, where the values of the functionf(x) are given with an additive error t. A number of related models are developed and different deviations from the basic model are studied. The case when the system of functions {pi(x)} is not known completely and there exists a so-called bias error was studied by Box and Draper in Reference 1 and by some other authors with reference to finding an optimal design. Another situation when an incomplete set of regressors is considered is that of determining a best equation based on a subset of the original complete set of regressors (see, e.g., References 2 and 9). In practice an investigator often faces the problem that only observational data (as opposed to data from designed experiments) are available and the system of functions {pi(x)} is not known completely. Either the form of functional dependence or the very set of arguments upon which the functions depend may be known only approximately. Such a situation is characteristic of economics problems,3 forecasting complex physical processes: modeling in medicine, and so on. Even if an investigator accepts a general linear model as a basic one for his study, in later stages, trying to get more precise results and to use more experimental data, he is faced with phenomena that hardly may be understood within the framework of well-known properties of a general linear model (see, e.g., Reference 3). Having this situation in mind, an appropriate model, which we call incompletely determined, was considered in References 5 and 6, the main results of these studies being briefly mentioned here. This article is concerned with the same problem, some new results, as well as a discussion and some simulation experiments being presented."
            },
            "slug": "On-an-Incompletely-Determined-Model-for-Function-by-Brailovsky",
            "title": {
                "fragments": [],
                "text": "On an Incompletely Determined Model for Function Approximation by Experimental Data"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The problem of function approximation by experimental data is usually considered within the framework of a general linear model, but in practice an investigator often faces the problem that only observational data are available and the system of functions {pi(x)} is not known completely."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34814308"
                        ],
                        "name": "R. Silverman",
                        "slug": "R.-Silverman",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Silverman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Silverman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144664041"
                        ],
                        "name": "A. Noetzel",
                        "slug": "A.-Noetzel",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Noetzel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Noetzel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 50
                            }
                        ],
                        "text": "This approach has been used by Linsker (1986) and Silverman and Noetzel (1988), for example."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7240055,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e07a06db24c86fa895b3dc3384a86506fbf28cd",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Self-organization of multi-layered networks can be realized by time-sequential organization of successive neural layers. Lateral inhibition operating in the surround of firing cells in each layer provides for unsupervised capture of excitation patterns presented by the previous layer. By presenting patterns of increasing complexity, in co-ordination with network self-organization, higher levels of the hierarchy capture concepts implicit in the pattern set."
            },
            "slug": "Time-Sequential-Self-Organization-of-Hierarchical-Silverman-Noetzel",
            "title": {
                "fragments": [],
                "text": "Time-Sequential Self-Organization of Hierarchical Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "Self-organization of multi-layered networks can be realized by time-sequential organization of successive neural layers by providing for unsupervised capture of excitation patterns presented by the previous layer."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16577977,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3e00dd12caea7c4dab1633a35d1da3cb2e76b420",
            "isKey": false,
            "numCitedBy": 2357,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple linear neuron model with constrained Hebbian-type synaptic modification is analyzed and a new class of unconstrained learning rules is derived. It is shown that the model neuron tends to extract the principal component from a stationary input vector sequence."
            },
            "slug": "Simplified-neuron-model-as-a-principal-component-Oja",
            "title": {
                "fragments": [],
                "text": "Simplified neuron model as a principal component analyzer"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A simple linear neuron model with constrained Hebbian-type synaptic modification is analyzed and a new class of unconstrained learning rules is derived."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of mathematical biology"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688681"
                        ],
                        "name": "T. Kohonen",
                        "slug": "T.-Kohonen",
                        "structuredName": {
                            "firstName": "Teuvo",
                            "lastName": "Kohonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kohonen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 127
                            }
                        ],
                        "text": "This technique is used to ensure that different outputs learn different functions of the input (Barrow, 1987; Grossberg, 1976; Kohonen, 1982, 1988, among others)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13699908,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45099a4db6d8d71590ae37f6b629e40c5ac2c833",
            "isKey": false,
            "numCitedBy": 671,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The factors that make speech recognition difficult are examined, and the potential of neural computers for this purpose is discussed. A speaker-adaptive system that transcribes dictation using an unlimited vocabulary is presented that is based on a neural network processor for the recognition of phonetic units of speech. The acoustic preprocessing, vector quantization, neural network model, and shortcut learning algorithm used are described. The utilization of phonotopic maps and of postprocessing in symbolic forms are discussed. Hardware implementations and performance of the neural networks are considered.<<ETX>>"
            },
            "slug": "The-'neural'-phonetic-typewriter-Kohonen",
            "title": {
                "fragments": [],
                "text": "The 'neural' phonetic typewriter"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A speaker-adaptive system that transcribes dictation using an unlimited vocabulary is presented that is based on a neural network processor for the recognition of phonetic units of speech."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2251137"
                        ],
                        "name": "V. Brailovsky",
                        "slug": "V.-Brailovsky",
                        "structuredName": {
                            "firstName": "Viktor",
                            "lastName": "Brailovsky",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Brailovsky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 71
                            }
                        ],
                        "text": "If the network is designed with too many hidden units (in the sense of Brailovsky, 1983a, 1983b, 1985) then the additional error introduced is spread evenly throughout the units and cannot be easily detected or removed by looking at the signal to noise ratio of the individual units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 211
                            }
                        ],
                        "text": "Once we find an output whose variance is below the threshold, we can stop computing additional terms since all later outputs will have variances which are even lower and may decrease our approximation accuracy (Brailovsky, 1983a, 1983b, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 84565601,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "065f3eccc5be2bf966a5c3a93e2ba8e16ea5134d",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "In mathematical statistics, the problem of function approximation is usually considered in the following way. One is given a finite system of functions, {+ i (x ) } , such that an unknown function to be approximated can be expressed as a linear combination of the functions 4i(x) plus a random error. The problem of estimating the unknown coefficients of the linear combination using a sample set has been well studied for different cases. However, in many practical problems, the system of functions (+,(x)} is not known completely, but only partially. We shall call such models incompletely determined. The model for predicting solar flares4 and some models for medical diagnosis are typical examples. As one considers such models, one finds some new problems that do not exist for completely determined models. Among them is the question, Should one use all available functions 4i(x) in the model or discard some of them? If the latter is preferable (and we shall see that it is) then what is the criterion of selection? If it is possible to use the complete model, is that always preferable to discarding some functions? We shall show that the use of an incompletely determined model sometimes leads to better results. The influence of the precision of the model and the experimental data on the selection criterion is also considered. The case with errors in both dependent and independent variables is especially discussed. A study of such problems was made by the author for a simplified and, in a sense, artificial model of function approximation in Reference 5. In this paper, the study is performed for the method of least squares, which is often used for function approximation in practice."
            },
            "slug": "ON-THE-PROBLEM-OF-FUNCTION-APPROXIMATION-BY-SAMPLE-Brailovsky",
            "title": {
                "fragments": [],
                "text": "ON THE PROBLEM OF FUNCTION APPROXIMATION BY SAMPLE SET PROCESSING FOR AN INCOMPLETELY DETERMINED MODEL"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The study is performed for the method of least squares, which is often used for function approximation in practice, and it is shown that the use of an incompletely determined model sometimes leads to better results."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2753465"
                        ],
                        "name": "H. Voorhees",
                        "slug": "H.-Voorhees",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Voorhees",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Voorhees"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4324241,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef14b1c144dd71c98160d26aeaf601489a73e68f",
            "isKey": false,
            "numCitedBy": 189,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent computational and psychological theories of human texture vision1\u20133 assert that texture discrimination is based on first-order differences in geometric and luminance attributes of texture elements, called 'textons'4. Significant differences in the density, orientation, size, or contrast of line segments or other small features in an image have been shown to cause immediate perception of texture boundaries. However, the psychological theories, which are based on the perception of synthetic images composed of lines and symbols, neglect two important issues. First, how can textons be computed from grey-level images of natural scenes? And second, how, exactly, can texture boundaries be found? Our analysis of these two issues has led to an algorithm that is fully implemented and which successfully detects boundaries in natural images5. We propose that blobs computed by a centre-surround operator are useful as texture elements, and that a simple non-parametric statistic can be used to compare local distributions of blob attributes to locate texture boundaries. Although designed for natural images, our computation agrees with some psycho-physical findings, in particular, those of Adelson and Bergen (described in the preceding article6), which cast doubt on the hypothesis that line segment crossings or termination points are textons."
            },
            "slug": "Computing-texture-boundaries-from-images-Voorhees-Poggio",
            "title": {
                "fragments": [],
                "text": "Computing texture boundaries from images"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work proposes that blobs computed by a centre-surround operator are useful as texture elements, and that a simple non-parametric statistic can be used to compare local distributions of blob attributes to locate texture boundaries."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48089152"
                        ],
                        "name": "B. W. Andrews",
                        "slug": "B.-W.-Andrews",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Andrews",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. W. Andrews"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1925241"
                        ],
                        "name": "D. Pollen",
                        "slug": "D.-Pollen",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Pollen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pollen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 114
                            }
                        ],
                        "text": "The second and third crosssections appear similar to the receptive field shapes of cortical simple cells shown in Andrews and Pollen (1979). Many of the remaining masks which the network has learned do not correspond to any receptive field shape which has been found in primate visual cortex (see Sanger, in press for further discussion of these issues."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28588085,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "dda59b551f8c72c29ee580f6627f1f75a6d3b38b",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Receptive fields of simple cells in area 17 of the cat were mapped with stationary stimuli. Spatial frequency selectivities of the same cells were measured with drifting sinusoidal gratings. 2. The reconstructed field profile (inverse Fourier transform of selectivity curve) shows qualitative agreement with the mapped profile, and suggests the existence of additional side\u2010lobes in the field. The side\u2010lobes may correspond to the 'unresponsive regions' investigated by Maffei & Fiorentini (1976). 3. Our data suggest that the simple cell may perform approximately linear spatial summation of inputs to the visual system. However, the output of the simple cell is generally non\u2010linear as reflected in its truncated responses to gratings."
            },
            "slug": "Relationship-between-spatial-frequency-selectivity-Andrews-Pollen",
            "title": {
                "fragments": [],
                "text": "Relationship between spatial frequency selectivity and receptive field profile of simple cells."
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The data suggest that the simple cell may perform approximately linear spatial summation of inputs to the visual system, however, the output of thesimple cell is generally non\u2010linear as reflected in its truncated responses to gratings."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of physiology"
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3085659"
                        ],
                        "name": "H. Kushner",
                        "slug": "H.-Kushner",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Kushner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kushner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35229585"
                        ],
                        "name": "D. Clark",
                        "slug": "D.-Clark",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Clark",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Clark"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117328031,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0009c5a2b4b07751a99bcf407d95e911a3064d0f",
            "isKey": false,
            "numCitedBy": 948,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "I. Introduction.- 1.1. General Remarks.- 1.2. The Robbins-Monro Process.- 1.3. A \"Continuous\" Process Version of Section 2.- 1.4. Regulation of a Dynamical System a simple example.- 1.5. Function Minimization: The Kiefer-Wolfowitz Procedure.- 1.6. Constrained Problems.- 1.7. An Economics Example.- II. Convergence w.p.1 for Unconstrained Systems.- 2.1. Preliminaries and Motivation.- 2.2. The Robbins-Monro and Kiefer-Wolfowitz Algorithms: Conditions and Discussion.- 2.3. Convergence Proofs for RM and KW-like Procedures.- 2.3.1. A Basic RM-like Procedure.- 2.3.2. One Dimensional RM and Accelerated RM Procedures.- 2.3.3. A Continuous Parameter RM Procedure.- 2.3.4. The Basic Kiefer-Wolfowitz Procedure.- 2.3.5. Random Directions KW Methods.- 2.4. A General Robbins-Monro Process: \"Exogenous Noise\".- 2.4.1. The Case of Bounded h(*,*).- 2.4.2. Unbounded h(*,*): Exogenous Noise.- 2.5. A General RM Process State Dependent Noise.- 2.5.1. Extensions and Localizations of Theorem 2.5.2.- 2.6. Some Applications.- 2.7. Mensov-Rademacher Estimates.- III. Weak Convergence of Probability Measures.- IV. Weak Convergence for Unconstrained Systems.- 4.1. Conditions and General Discussion.- 4.2. The Robbins-Monro and Kiefer-Wolfowitz Procedures.- 4.2.1. The Basic Robbins-Monro Procedure.- 4.2.2. The One-Dimensional Robbins-Monro Procedure.- 4.2.3. The Kiefer-Wolfowitz Procedure.- 4.2.4. A Case Where the Limit Satisfies a Generalized ODE.- 4.2.5. A Continuous Parameter KW Procedure.- 4.3. A General Robbins-Monro Process: Exogenous Noise.- 4.4. A General RM Process: State Dependent Noise.- 4.5. The Identification Problem.- 4.6. A Counter-Example to Tightness.- 4.7. Boundedness of {Xn} and Tightness of {Xn(*)}.- V. Convergence w.p.1 For Constrained Systems.- 5.1. A Penalty-Multiplier Algorithm for Equality Constraints.- 5.1.1. A Basic RM-like Algorithm, Conditions and Discussion.- 5.1.2. The Noise Condition, Discussion and Generalization.- 5.1.3. Boundedness of {Xn}.- 5.1.4. Proof of the Main Theorem.- 5.1.5. Constrained Function Minimization and Other Extensions.- 5.2. A Lagrangian Method for Inequality Constraints.- 5.2.1. The Algorithm and Conditions.- 5.2.2. The Convergence Theorem 18.- 5.2.3. A Non-Convergent but Useful Algorithm.- 5.2.4. An Application to the Identification Problem.- 5.3. A Projection Algorithm.- 5.4. A Penalty-Multiplier Method for Inequality Constraints.- VI. Weak Convergence: Constrained Systems.- 6.1. A Multiplier Type Algorithm for Equality Constraints.- 6.1.1. Boundedness of {Xn}.- 6.1.2. The Noise Condition, Discussion.- 6.1.3. The Convergence Theorem.- 6.2. The Lagrangian Method.- 6.3. A Projection Algorithm.- 6.4. A Penalty-Multiplier Algorithm for Inequality Constraints.- VII. Rates of Convergence.- 7.1. The Problem Formulation.- 7.2. Conditions and Discussions.- 7.3. Rates of Convergence for Case 1, the KW Algorithm.- 7.4. Discussion of Rates of Convergence for Two KW Algorithms."
            },
            "slug": "wchastic.-approximation-methods-for-constrained-and-Kushner-Clark",
            "title": {
                "fragments": [],
                "text": "wchastic. approximation methods for constrained and unconstrained systems"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Robbins-Monro and Kiefer-Wolfowitz Algorithm for Inequality Constraints and the Weak Convergence of Probability Measures, a simple example, and the Convergence Theorem, a proof of the Main Theorem."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1925241"
                        ],
                        "name": "D. Pollen",
                        "slug": "D.-Pollen",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Pollen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pollen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1992154"
                        ],
                        "name": "S. Ronner",
                        "slug": "S.-Ronner",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Ronner",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ronner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11912279,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "818e8a5f1e8b3cc5d0cb72a60ecbef2ba4549b81",
            "isKey": false,
            "numCitedBy": 414,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Adjacent simple cells recorded and \"isolated\" simultaneously from the same microelectrode placement were usually tuned to the same orientation and spatial frequency. The responses of the members of these \"spatial frequency pairs\" to drifting sine-wave gratings were cross-correlates. Within the middle range of the spatial frequency selectivity curves, the responses of the paired cells differed in phase by approximately 90 percent. This phase relationship suggests that adjacent simple cells tuned to the same spatial frequency and orientation represent paired sine and cosine filters in terms of their processing of afferent spatial inputs and truncated sine and cosine filters in terms of the output of simple cells."
            },
            "slug": "Phase-relationships-between-adjacent-simple-cells-Pollen-Ronner",
            "title": {
                "fragments": [],
                "text": "Phase relationships between adjacent simple cells in the visual cortex."
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This phase relationship suggests that adjacent simple cells tuned to the same spatial frequency and orientation represent paired sine and cosine filters in terms of their processing of afferent spatial inputs and truncated sines and cosines in Terms of the output of simple cells."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116003860"
                        ],
                        "name": "J. Bergen",
                        "slug": "J.-Bergen",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bergen",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bergen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4288650,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "7ef6d7f96721edd46297524e80fbd8d65a57ecc7",
            "isKey": false,
            "numCitedBy": 362,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Texture perception has frequently been studied using textures constructed by repeated placement of micropatterns or texture elements. Theories have been developed to explain the discrimina-bility of such textures in terms of specific features within the micropatterns themselves. For example, Beck1,2 observed that a region filled with vertical Ts is readily distinguished from one filled with tilted Ts but not from one filled with vertical Ls. He attributed this to the different distribution of oriented line segments present in the former case but not in the latter. However, Bergen and Julesz3 found that a region of randomly oriented Xs segregated from one filled with randomly oriented Ls, in spite of the identical distribution of oriented line segments in the two cases. They suggested that this discrimination might be based on the density of such features as terminators, corners, and intersections within the patterns. We note here that simpler, lower-level mechanisms tuned for size may be sufficient to explain this discrimination. We tested this by varying the relative sizes of the Xs and the Ls; when they produce equal responses in size-tuned mechanisms they are hard to discriminate, and when they produce different size-tuned responses they are easy to discriminate."
            },
            "slug": "Early-vision-and-texture-perception-Bergen-Adelson",
            "title": {
                "fragments": [],
                "text": "Early vision and texture perception"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is noted here that simpler, lower-level mechanisms tuned for size may be sufficient to explain this discrimination of micropatterns based on the density of such features as terminators, corners, and intersections within the patterns."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2251137"
                        ],
                        "name": "V. Brailovsky",
                        "slug": "V.-Brailovsky",
                        "structuredName": {
                            "firstName": "Viktor",
                            "lastName": "Brailovsky",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Brailovsky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 71
                            }
                        ],
                        "text": "If the network is designed with too many hidden units (in the sense of Brailovsky, 1983a, 1983b, 1985) then the additional error introduced is spread evenly throughout the units and cannot be easily detected or removed by looking at the signal to noise ratio of the individual units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 211
                            }
                        ],
                        "text": "Once we find an output whose variance is below the threshold, we can stop computing additional terms since all later outputs will have variances which are even lower and may decrease our approximation accuracy (Brailovsky, 1983a, 1983b, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 83752344,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2c41a3aa89b676507dcc65005ca5dc6674913de8",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of function approximation is considered in mathematical statistics as follows. One knows a finite system of functions, {di(x)], such that an unknown function to be approximated can be expressed as a linear combination of the functions {di(x)] plus a random error, the coefficients of the linear combination being determined by a sample set with the help of a well-known procedure, such as the method of least squares. But, under real conditions, the investigator rarely knows the system of functions { d i ( x ) ) precisely. Even if he is certain about some of them, the others may well be considered only candidates that should be tried beforehand. In any case, he almost never knows the whole system { d i ( x ) ] . That is why such a model, the incompletely determined model, was considered in Reference 1, highlighting the problem of the selection of the functions, di(x), to be used. If the coefficients of the model are determined with the help of the method of least squares, the main result of the study of the orthonormalized system of functions, { d i ( x ) } , may be formulated as follows.* If the modulus of the coefficient of the proposed function is less than a threshold value, the function must be discarded; otherwise, it must be included in the model. The threshold value has the form"
            },
            "slug": "ON-THE-PROBLEM-OF-FUNCTION-SYSTEM-SELECTION-FOR-ON-Brailovsky",
            "title": {
                "fragments": [],
                "text": "ON THE PROBLEM OF FUNCTION SYSTEM SELECTION FOR FUNCTION APPROXIMATION BASED ON THE USE OF A SAMPLE SET WITH DEFECTS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2753465"
                        ],
                        "name": "H. Voorhees",
                        "slug": "H.-Voorhees",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Voorhees",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Voorhees"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 129833069,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "47c8fc6b7dfd23d9eb3b314c04486d07d2af0e50",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Texture provides one cue for identifying the physical cause of an intensity edge, such as occlusion, shadow, surface orientation or reflectance change. Marr, Julesz, and others have proposed that texture is represented by small lines or blobs, called 'textons' by Julesz (1981a), together with their attributes, such as orientation, elongation, and intensity. Psychophysical studies suggest that texture boundaries are perceived where distributions of attributes over neighborhoods of textons differ significantly. However, these studies, which deal with synthetic images, neglect to consider two important questions: How can these textons be extracted from images of natural scenes? And how, exactly, are texture boundaries then found? This thesis proposes answers to these questions by presenting an algorithm for computing blobs from natural images and a statistic for measuring the difference between two sample distributions of blob attributes. As part of the blob detection algorithm, methods for estimating image noise are presented, which are applicable to edge detection as well."
            },
            "slug": "Finding-Texture-Boundaries-in-Images-Voorhees",
            "title": {
                "fragments": [],
                "text": "Finding Texture Boundaries in Images"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2300091"
                        ],
                        "name": "N. Owsley",
                        "slug": "N.-Owsley",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Owsley",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Owsley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 82
                            }
                        ],
                        "text": "In the field of signal processing, an almost equivalent algorithm was proposed in Owsley (1978), Oja (1983), and Oja and Karhunen (1985), although convergence to the matrix of eigenvectors was never proven, To our knowledge, the proof given here is the first convergence proof for this algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43194778,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a683be1b73ffba620b01e1358e25681eae09bfe",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The decomposition of vector time series data into orthogonal components can be applied in both temporal and spatial discrete frequency analysis. If the observed multidimensional data is non-stationary, then adaptive procedures can be used for estimation of the eigendata. This paper presents the relationship between multicomponent, spectral signals in noise and the corresponding eigendata. Two adaptive realizations of the eigendata estimation process are considered. Examples are given which allow a comparison between signal detection by data orthogonalization, power spectrum estimation and two channel magnitude squared coherence computation."
            },
            "slug": "Adaptive-data-orthogonalization-Owsley",
            "title": {
                "fragments": [],
                "text": "Adaptive data orthogonalization"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The relationship between multicomponent, spectral signals in noise and the corresponding eigendata is presented and examples are given which allow a comparison between signal detection by data orthogonalization, power spectrum estimation and two channel magnitude squared coherence computation."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 113
                            }
                        ],
                        "text": "In the field of signal processing, an almost equivalent algorithm was proposed in Owsley (1978), Oja (1983), and Oja and Karhunen (1985), although convergence to the matrix of eigenvectors was never proven, To our knowledge, the proof given here is the first convergence proof for this algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 232
                            }
                        ],
                        "text": "There exist several different but closely related algorithms for finding multiple eigenvectors which have been proven to converge (Brockett, 1989; Karhunen , 1984a, 1984b, 1985; Karhunen & Oja, 1982; Kuusela & Oja, 1982; Oja, 1983; Oja & Karhunen, 1980, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122159626,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "320dd7814f28ae2cb5a01f514dd4af860f600fd7",
            "isKey": false,
            "numCitedBy": 570,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-stochastic-approximation-of-the-eigenvectors-and-Oja-Karhunen",
            "title": {
                "fragments": [],
                "text": "On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49719219"
                        ],
                        "name": "J. Lim",
                        "slug": "J.-Lim",
                        "structuredName": {
                            "firstName": "Jae",
                            "lastName": "Lim",
                            "middleNames": [
                                "Sung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58729789,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e24c2a95284b6c3f6f2b4485c9d2bbfbb3e45775",
            "isKey": false,
            "numCitedBy": 1153,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "New to P-H Signal Processing Series (Alan Oppenheim, Series Ed) this text covers the principles and applications of \"multidimensional\" and \"image\" digital signal processing. For Sr/grad level courses in image processing in EE departments."
            },
            "slug": "Two-Dimensional-Signal-and-Image-Processing-Lim",
            "title": {
                "fragments": [],
                "text": "Two-Dimensional Signal and Image Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This text covers the principles and applications of \"multidimensional\" and \"image\" digital signal processing and is suitable for Sr/grad level courses in image processing in EE departments."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1407756324"
                        ],
                        "name": "C. Enroth-Cugell",
                        "slug": "C.-Enroth-Cugell",
                        "structuredName": {
                            "firstName": "Christina",
                            "lastName": "Enroth-Cugell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Enroth-Cugell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36155815"
                        ],
                        "name": "J. Robson",
                        "slug": "J.-Robson",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Robson",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Robson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3052969,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "c6609ae352d9e1c99530136e41798f337496ff27",
            "isKey": false,
            "numCitedBy": 2635,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Spatial summation within cat retinal receptive fields was studied by recording from optic\u2010tract fibres the responses of ganglion cells to grating patterns whose luminance perpendicular to the bars varied sinusoidally about the mean level."
            },
            "slug": "The-contrast-sensitivity-of-retinal-ganglion-cells-Enroth-Cugell-Robson",
            "title": {
                "fragments": [],
                "text": "The contrast sensitivity of retinal ganglion cells of the cat"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Spatial summation within cat retinal receptive fields was studied by recording from optic\u2010tract fibres the responses of ganglion cells to grating patterns whose luminance perpendicular to the bars varied sinusoidally about the mean level."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of physiology"
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19356,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144265584"
                        ],
                        "name": "E. Ross",
                        "slug": "E.-Ross",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Ross",
                            "middleNames": [
                                "Alsworth"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ross"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 138
                            }
                        ],
                        "text": "Hebbian learning rules modify the connection between two units by an amount proportional to the product of the activation of those units (Hebb, 1949)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 143518681,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "9e8de2ccaa1c5f988a522dd59b4061d4ee45d26a",
            "isKey": false,
            "numCitedBy": 680,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The combining of the efforts of a number of persons for the accomplishment of a particular purpose results in the organization of effort.' Such an organization may receive its direction either from the will of an individual or from the will of a group. The process by which a group will is arrived at may be termed the organization of will. In the organization of effort, he movement is from the one toward the many, i.e., from the controlling purpose to the coordinated efforts of the various persons who contribute to its accomplishment. In the organization of will, the movement is from the many toward the one, i.e., from the wills of individual members to the single purpose which comes to direct and unify the activities of the group. Organizations may be represented graphically by the cone, the base of the cone representing the individuals organized, the apex their unifying purpose. The organizing of will may be thought of as a movement from base toward apex; the organizing of effort as a movement from apex toward base. These two types of organization may exist separately or combined. In an army, a railroad, a government department, and a x See paper on this topic in the July number of this Journal."
            },
            "slug": "The-Organization-of-Will-Ross",
            "title": {
                "fragments": [],
                "text": "The Organization of Will"
            },
            "venue": {
                "fragments": [],
                "text": "American Journal of Sociology"
            },
            "year": 1916
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48801723"
                        ],
                        "name": "F. Downton",
                        "slug": "F.-Downton",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Downton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Downton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 962,
                                "start": 14
                            }
                        ],
                        "text": "lation of Q are not feasible. For instance, if a network has 4000 inputs, then Q = E[xx r] has 16 million elements, and it may be hard to find the eigenvectors by traditional methods. However, GHA requires the computation only of the outer products yx T and y y r so that if the number of outputs is small the computational and storage requirements can be correspondingly decreased. If there are 16 outputs, for example, yx r will have only 64,000 elements, and yyr will have only 256. The Generalized Hebbian Algorithm takes advantage of this network structure. If the number of outputs of the network is close to the number of inputs, then the algorithm may not have a distinct advantage over other methods. But when the number of inputs is large and the number of required outputs is small, GHA provides a practical and useful procedure for finding eigenvectors. In the field of signal processing, an almost equivalent algorithm was proposed in Owsley (1978), Oja (1983), and Oja and Karhunen (1985), although convergence to the matrix of eigenvectors was never proven, To our knowledge, the proof given here is the first convergence proof for this algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 113
                            }
                        ],
                        "text": "In the field of signal processing, an almost equivalent algorithm was proposed in Owsley (1978), Oja (1983), and Oja and Karhunen (1985), although convergence to the matrix of eigenvectors was never proven, To our knowledge, the proof given here is the first convergence proof for this algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 232
                            }
                        ],
                        "text": "There exist several different but closely related algorithms for finding multiple eigenvectors which have been proven to converge (Brockett, 1989; Karhunen , 1984a, 1984b, 1985; Karhunen & Oja, 1982; Kuusela & Oja, 1982; Oja, 1983; Oja & Karhunen, 1980, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4224834,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "555a7921231b86464a8db34ada9edade765bf8f0",
            "isKey": false,
            "numCitedBy": 440,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic ApproximationBy M. T. Wasan. (Cambridge Tracts in Mathematics and Mathematical Physics, No. 58.) Pp. x + 202. (Cambridge University Press: London, June 1969.) 70s; $9.50."
            },
            "slug": "Stochastic-Approximation-Downton",
            "title": {
                "fragments": [],
                "text": "Stochastic Approximation"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Cottrell et al. (1987) point out that if channel errors affect certain units more than others, then it may be an advantage to distribute the information evenly so that high-variance channels are not corrupted excessively."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Cottrell et al, (1987) used self-supervised backpropagation to perform image coding, with bit rates as low as 0.625 bits per pixel."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 89
                            }
                        ],
                        "text": "Since the hidden units will all have approximately equal variance (Baldi & Hornik, 1989; Cottrell et al., 1987), it is not possible to quantize them with different numbers of bits, as it was for the KLT."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 102
                            }
                        ],
                        "text": "The resulting image is shown in Figure 4, We calculate the normalized mean square error (NMSE) as in (Cottrell et al., 1987) to be the ratio of the error variance to the data variance NMSE ~ E [(I .... -"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 130
                            }
                        ],
                        "text": "Several authors have noted that SSBP tends to produce hidden units which have approximately equal variance (Baldi & Hornik, 1989; Cottrell et al., 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 29
                            }
                        ],
                        "text": "It should also be noted that Cottrell et al. (1987) trained their network for 150,000 iterations, while the network which learned the masks of Figure 3 was trained for 2048 iterations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning internal representations from gray-scale images: An example of extensional programming"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 9th Annual Conference of the Cognitive Science Society"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 189
                            }
                        ],
                        "text": "There exist several different but closely related algorithms for finding multiple eigenvectors which have been proven to converge (Brockett, 1989; Karhunen , 1984a, 1984b, 1985; Karhunen & Oja, 1982; Kuusela & Oja, 1982; Oja, 1983; Oja & Karhunen, 1980, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "Oja (1983) and others proposed an iterative method called \"Stochastic Gradient Ascent\" based on multiplication by an estimated correlation matrix followed by Gram-Schmidt or thonormal iza t ion ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 91
                            }
                        ],
                        "text": "The first row of the differential equation is given by: which is equivalent to eqn (7) of (Oja, 1982)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 161
                            }
                        ],
                        "text": "To help understand the operation of the Generalized Hebbian Algorithm, we rewrite (1) as In this form, we see that the algorithm is equivalent to performing the Oja (1982) learning rule (10) using a modified version of the input given by"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 152
                            }
                        ],
                        "text": "The combination of the two terms has fixed points at all eigenvectors, but the only asymptotically stable solution is e~, which maximizes the variance (Oja, 1982)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 34
                            }
                        ],
                        "text": "which is equivalent to eqn (7) of (Oja, 1982)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 169
                            }
                        ],
                        "text": "He then proves that for an arbitrary choice of initial weights, c~ will converge to the principal eigenvector e~ (or its negative) so long as ci(O)re~ ~ 0 at time zero (Oja, 1982, 1983: Oja & Karhunen, 1980)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 72
                            }
                        ],
                        "text": "In this form, we see that the algorithm is equivalent to performing the Oja (1982) learning rule (10) using a modified version of the input given by"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 42
                            }
                        ],
                        "text": "He proposed a network learning algorithm (Oja, 1982): ac, = ,,'(y,x -y~c,) (10) where cf is a row of C, and y~ = c[x (Oja, 1982)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 213
                            }
                        ],
                        "text": "Oja has shown that if we maintain the diagonal elements of CC r equal to 1 (so that the norm of each row is 1), then a Hebbian learning rule will cause the rows of C to converge to the principal eigenvector of Q (Oja, 1982, 1983; Oja & Karhunen, 1980)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 37
                            }
                        ],
                        "text": "where cf is a row of C, and y~ = c[x (Oja, 1982)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 41
                            }
                        ],
                        "text": "He proposed a network learning algorithm (Oja, 1982):"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A simplified neuron model as a principal"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 527,
                                "start": 11
                            }
                        ],
                        "text": "Although the images are different, their statistics may be similar enough that their respective KLTs are similar. A network trained on either image will compute a set of masks which will be useful for the other. This generalization property is a direct consequence of the statistical similarity of the two images. Filters similar to those given in Figure 3 have been used for image coding by many authors. The Discrete Cosine Transform masks are qualitatively similar (see Lira, in press for a description), and Daugman (1988) has performed image coding using two-dimensional Gabor filters which are also similar to the masks which the network learned."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 5
                            }
                        ],
                        "text": "Cottrell et al. (1987) point out that if channel errors affect certain units more than others, then it may be an advantage to distribute the information"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 510,
                                "start": 4
                            }
                        ],
                        "text": "above, such a network will not actually find the KLT, but will tend to converge to outputs which represent linear combinations of the KLT vectors (Baldi & Hornik, 1989). Since the hidden units will all have approximately equal variance (Baldi & Hornik, 1989; Cottrell et al., 1987), it is not possible to quantize them with different numbers of bits, as it was for the KLT. This fact significantly reduces the maximum compression rate which can be achieved. It should also be noted that Cottrell et al. (1987) trained their network for 150,000 iterations, while the network which learned the masks of Figure 3 was trained for 2048 iterations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1735,
                                "start": 5
                            }
                        ],
                        "text": "and vertical \"edge-detectors,\" and the fourth is a horizontal \"bar-detector.\" We next convolve each of the masks with the original image in Figure 7 to show which regions of the image give high response for each mask. The convolution results are shown in Figure 8b. To estimate the local variance, we full-wave rectify the images of Figure 8b, and low-pass filter the results with a Gaussian of standard deviation 4 pixels. This gives a measure of local \"energy\" or variance, and the result is shown in Figure 8c for each mask. Here we can clearly see that certain masks respond preferentially to one or the other of the two textures in the image. The first mask is a low-pass filter, which achieves equal and maximal variance everywhere in the image. The next three masks have larger variance when responding to a particular texture (lines of one or the other orientation). Note that the texture preference of a mask is not immediately apparent from the convolution result in Figure 8b. We must approximate the local variance as in Figure 8c in order to distinguish the texture regions. An equivalent technique was developed by Turner (1986), although he used a fixed set of masks which were not learned. His masks were Gabor filters, and he computed the squared amplitude of the response to a given filter at different points in the image. This operation is almost equivalent to taking the absolute value and low-pass filtering as we have done here. Both techniques find an estimate of the local amplitude (variance) of the response to a filter. Gabor filters were also used in (Daugman, 1988) to perform texture segmentation, although the local variance or response amplitude was not explicitly computed. Similarly, Voorhees (1987) and Voorhees and Poggio (1988) used 72G filtering to di~riminate texture regions, while Bergen and Adetson (I988) used a center-surround with a rectification nonlinearity."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1143,
                                "start": 5
                            }
                        ],
                        "text": "and vertical \"edge-detectors,\" and the fourth is a horizontal \"bar-detector.\" We next convolve each of the masks with the original image in Figure 7 to show which regions of the image give high response for each mask. The convolution results are shown in Figure 8b. To estimate the local variance, we full-wave rectify the images of Figure 8b, and low-pass filter the results with a Gaussian of standard deviation 4 pixels. This gives a measure of local \"energy\" or variance, and the result is shown in Figure 8c for each mask. Here we can clearly see that certain masks respond preferentially to one or the other of the two textures in the image. The first mask is a low-pass filter, which achieves equal and maximal variance everywhere in the image. The next three masks have larger variance when responding to a particular texture (lines of one or the other orientation). Note that the texture preference of a mask is not immediately apparent from the convolution result in Figure 8b. We must approximate the local variance as in Figure 8c in order to distinguish the texture regions. An equivalent technique was developed by Turner (1986), although he used a fixed set of masks which were not learned."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 1
                            }
                        ],
                        "text": "We now apply theorem 1 of Ljung (1977) which states (in our notation): If"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 5
                            }
                        ],
                        "text": "Cottrell et al, (1987) used self-supervised backpropagation to perform image coding, with bit rates as low as 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some remarks on the hackpropagation algorithm ]or neural net learning (Tech"
            },
            "venue": {
                "fragments": [],
                "text": "Rep. No. SYCON-88-"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 221
                            }
                        ],
                        "text": "There exist several different but closely related algorithms for finding multiple eigenvectors which have been proven to converge (Brockett, 1989; Karhunen , 1984a, 1984b, 1985; Karhunen & Oja, 1982; Kuusela & Oja, 1982; Oja, 1983; Oja & Karhunen, 1980, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 165
                            }
                        ],
                        "text": "This is equivalent to the procedure used here, except that here all the components converge at the same time~ A summary of other related algorithms may be found in (Oja, 1983)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 138
                            }
                        ],
                        "text": "Algorithms exist which can find the eigenvectors given only samples of the input distribution , without the need to explicitly compute Q. Oja (1983) and others proposed an iterative method called \"Stochastic Gradient Ascent\" based on multiplication by an estimated correlation matrix followed by\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 97
                            }
                        ],
                        "text": "In the field of signal processing, an almost equivalent algorithm was proposed in Owsley (1978), Oja (1983), and Oja and Karhunen (1985), although convergence to the matrix of eigenvectors was never proven, To our knowledge, the proof given here is the first convergence proof for this algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 118
                            }
                        ],
                        "text": "Proofs of convergence for some of these algorithms, and an excellent summary of the different methods may be found in Oja (1983) and Karhunen (1984b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 169
                            }
                        ],
                        "text": "He then proves that for an arbitrary choice of initial weights, c~ will converge to the principal eigenvector e~ (or its negative) so long as ci(O)re~ ~ 0 at time zero (Oja, 1982, 1983: Oja & Karhunen, 1980)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 213
                            }
                        ],
                        "text": "Oja has shown that if we maintain the diagonal elements of CC r equal to 1 (so that the norm of each row is 1), then a Hebbian learning rule will cause the rows of C to converge to the principal eigenvector of Q (Oja, 1982, 1983; Oja & Karhunen, 1980)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58166686,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "718c68c25886484c922d6d33702a726cdd27ca3a",
            "isKey": true,
            "numCitedBy": 840,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Subspace-methods-of-pattern-recognition-Oja",
            "title": {
                "fragments": [],
                "text": "Subspace methods of pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 221
                            }
                        ],
                        "text": "There exist several different but closely related algorithms for finding multiple eigenvectors which have been proven to converge (Brockett, 1989; Karhunen , 1984a, 1984b, 1985; Karhunen & Oja, 1982; Kuusela & Oja, 1982; Oja, 1983; Oja & Karhunen, 1980, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 165
                            }
                        ],
                        "text": "This is equivalent to the procedure used here, except that here all the components converge at the same time~ A summary of other related algorithms may be found in (Oja, 1983)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 130
                            }
                        ],
                        "text": "There exist several different but closely related algorithms for finding multiple eigenvectors which have been proven to converge (Brockett, 1989; Karhunen, 1984a, 1984b, 1985; Karhunen & Oja, 1982; Kuusela & Oja, 1982; Oja, 1983; Oja & Karhunen, 1980, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 138
                            }
                        ],
                        "text": "Algorithms exist which can find the eigenvectors given only samples of the input distribution , without the need to explicitly compute Q. Oja (1983) and others proposed an iterative method called \"Stochastic Gradient Ascent\" based on multiplication by an estimated correlation matrix followed by\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 97
                            }
                        ],
                        "text": "In the field of signal processing, an almost equivalent algorithm was proposed in Owsley (1978), Oja (1983), and Oja and Karhunen (1985), although convergence to the matrix of eigenvectors was never proven, To our knowledge, the proof given here is the first convergence proof for this algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 118
                            }
                        ],
                        "text": "Proofs of convergence for some of these algorithms, and an excellent summary of the different methods may be found in Oja (1983) and Karhunen (1984b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 169
                            }
                        ],
                        "text": "He then proves that for an arbitrary choice of initial weights, c~ will converge to the principal eigenvector e~ (or its negative) so long as ci(O)re~ ~ 0 at time zero (Oja, 1982, 1983: Oja & Karhunen, 1980)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 213
                            }
                        ],
                        "text": "Oja has shown that if we maintain the diagonal elements of CC r equal to 1 (so that the norm of each row is 1), then a Hebbian learning rule will cause the rows of C to converge to the principal eigenvector of Q (Oja, 1982, 1983; Oja & Karhunen, 1980)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Suh~'paee methods qf pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Letch-"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688681"
                        ],
                        "name": "T. Kohonen",
                        "slug": "T.-Kohonen",
                        "structuredName": {
                            "firstName": "Teuvo",
                            "lastName": "Kohonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kohonen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 127
                            }
                        ],
                        "text": "This technique is used to ensure that different outputs learn different functions of the input (Barrow, 1987; Grossberg, 1976; Kohonen, 1982, 1988, among others)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 112
                            }
                        ],
                        "text": "the use of unsupervised learning algorithms (e.g., Carpenter & Grossberg, 1988; Grossberg, 1976; Hinton , 1987; Kohonen, 1982)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 126292379,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "509d1af21b3e182e748c93b9fed2880f022e9f93",
            "isKey": false,
            "numCitedBy": 2910,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Self-organized-formation-of-topographically-correct-Kohonen",
            "title": {
                "fragments": [],
                "text": "Self-organized formation of topographically correct feature maps"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712211"
                        ],
                        "name": "G. Golub",
                        "slug": "G.-Golub",
                        "structuredName": {
                            "firstName": "Gene",
                            "lastName": "Golub",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Golub"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 126299280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9efffc63f81bf3f6cb6357ddc15e9cd9da75d16",
            "isKey": false,
            "numCitedBy": 27000,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Matrix-computations-Golub",
            "title": {
                "fragments": [],
                "text": "Matrix computations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125968047,
            "fieldsOfStudy": [],
            "id": "a6891af0e04724965532095dec1b8198b943e31e",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Advanced Engineering Mathematics."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122917865,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "661cf494566c9119be68f6b35aa32c0fbbd7d8bb",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recursive-estimation-of-eigenvectors-of-correlation-Karhunen",
            "title": {
                "fragments": [],
                "text": "Recursive estimation of eigenvectors of correlation type matrices for signal processing applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3547091"
                        ],
                        "name": "D. Hebb",
                        "slug": "D.-Hebb",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Hebb",
                            "middleNames": [
                                "Olding"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hebb"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62285311,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2760c82b71bbcd586f786ca8017d5916f2a7c8ec",
            "isKey": false,
            "numCitedBy": 4624,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-organization-of-behavior-Hebb",
            "title": {
                "fragments": [],
                "text": "The organization of behavior"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 31
                            }
                        ],
                        "text": "This approach has been used by Linsker (1986) and Silverman and Noetzel (1988), for example."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60683349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1094e087102df1686de0a21d7e2cc233d1821386",
            "isKey": false,
            "numCitedBy": 208,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "From-basic-network-principles-to-neural-Linsker",
            "title": {
                "fragments": [],
                "text": "From basic network principles to neural architecture"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144441876"
                        ],
                        "name": "J. Lira",
                        "slug": "J.-Lira",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Lira",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 54157570,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d13d8dfe7bcb841959273909bbc363000bb5517a",
            "isKey": false,
            "numCitedBy": 1458,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Two-dimensional-signal-and-image-processing-Lira",
            "title": {
                "fragments": [],
                "text": "Two dimensional signal and image processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143880192"
                        ],
                        "name": "V. Dyck",
                        "slug": "V.-Dyck",
                        "structuredName": {
                            "firstName": "V.",
                            "lastName": "Dyck",
                            "middleNames": [
                                "Arnie"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Dyck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068725286"
                        ],
                        "name": "Smith",
                        "slug": "Smith",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "83099779"
                        ],
                        "name": "Barbara Lawson",
                        "slug": "Barbara-Lawson",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Lawson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barbara Lawson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59771503,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "068fcf47be1c4c3b3f5416e0002a2d41888fd172",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-Computing-Dyck-Smith",
            "title": {
                "fragments": [],
                "text": "Introduction to Computing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 5
                            }
                        ],
                        "text": "(See Watanabe, 1965 for a theoretical description of the KLT and some remarks on its applications.)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 75
                            }
                        ],
                        "text": "The entropy term M -~ E[y~]log E[y~] i 1 is minimized for all values of M (Watanabe, 1965), so that information is concentrated in the first few outputs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Karhunen-Lo6ve expansion and factor"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Barrow (1987) has proposed a model of receptive field development which is qualitatively very similar to that presented here, His input is bandpass-filtered white noise, and he hypothesizes that the bandpass operation occurs in retina and lateral geniculate."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 946,
                                "start": 0
                            }
                        ],
                        "text": "Barrow (1987) has proposed a model of receptive field development which is qualitatively very similar to that presented here, His input is bandpass-filtered white noise, and he hypothesizes that the bandpass operation occurs in retina and lateral geniculate. He uses a Hebbian learning rule and maintains the row norms at one. To avoid having all the outputs converge to the same set of weights, Barrow uses a winner-take-all strategy in which only the strongest output has its weights updated. We suspect that this form of competitive learning may be formally equivalent to the Generalized Hebbian Algorithm, Barrow's algorithm does not order the outputs by decreasing variance, but it is possible that the actual eigenvectors are discovered, rather than a linear combination of them. It is not clear whether or not the outputs of his network are orthogonal, but his results seem very similar to Figures 9 and 10. Recently, Yuille et al. (1988) have proposed a model for cortical cell development based on a Hebbian learning rule and lateral interactions between pairs of cells."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 96
                            }
                        ],
                        "text": "This technique is used to ensure that different outputs learn different functions of the input (Barrow, 1987; Grossberg, 1976; Kohonen, 1982, 1988, among others)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning receptive helds"
            },
            "venue": {
                "fragments": [],
                "text": "Proceeding.~ of the IEEE 1st Annual Conference on Neural Networks (Vol, 4, pp. 115-121). Washington. DC: IEEE Computer Society Press."
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning internal representations from grayscale images : An example of extensional programming"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 9 th Annual Conference of the Cognitive Science Society"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 5
                            }
                        ],
                        "text": "(See Watanabe, 1965 for a theoretical description of the KLT and some remarks on its applications.)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 75
                            }
                        ],
                        "text": "The entropy term M -~ E[y~]log E[y~] i 1 is minimized for all values of M (Watanabe, 1965), so that information is concentrated in the first few outputs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Karhunen-Lo6ve expansion and factor analysis: Thcoretical remarks and applications. Transactions o/'the 4th Prague ConJ~'rence on htj~>rmation Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Karhunen-Lo6ve expansion and factor analysis: Thcoretical remarks and applications. Transactions o/'the 4th Prague ConJ~'rence on htj~>rmation Theory"
            },
            "year": 1965
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 5
                            }
                        ],
                        "text": "(See Watanabe, 1965 for a theoretical description of the KLT and some remarks on its applications.)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 75
                            }
                        ],
                        "text": "The entropy term M -~ E[y~]log E[y~] i 1 is minimized for all values of M (Watanabe, 1965), so that information is concentrated in the first few outputs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Karhunen - Lo 6 ve expansion and factor analysis : Thcoretical remarks and applications"
            },
            "venue": {
                "fragments": [],
                "text": "Transactions o / ' the 4 th Prague ConJ ~ ' rence on htj ~ > rmation Theory"
            },
            "year": 1965
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Thc averaged learning subspace method for spectral pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the oth International Conference on Pattern Recognition Two - dimensional signal and intage processing"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recursive construction of Karhuncn-Lo ,Sve expansions for pattern recognition purposes"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 5th International Conference on Pattern Recognition"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 403,
                                "start": 6
                            }
                        ],
                        "text": "In linear SSBP, a two-layer network is trained to perform the identity mapping, yet the number of hidden units is set to be fewer than the number of inputs. The hidden units must therefore discover an efficient encoding of the input data. Since efficient coding is also the goal for the Generalized Hebbian Algorithm. we would expect both algorithms to produce similar results. Bourlard and Kamp (1988) have shown that a set of vectors which span the Singular Value Decomposition (which is equivalent to the KLT as used here) gives the optimal set of hidden units of such a network."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 15
                            }
                        ],
                        "text": "matrix Q and then finding the eigenvector decomposition using matrix techniques (see Golub and Van Loan (1983) or Kreyszig (1988) for review)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An optimality principle for unsupervised"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Backpropagation ean give rise to spurious local minima even .l~r networks without hidden layers (Tech"
            },
            "venue": {
                "fragments": [],
                "text": "Rep. No. SYCON-88-08)"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "Ballard."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modular learning m neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the Sixth National Conference on Artificial Intelligence (AAAI-87)"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural networks , principal components analysis , and Gabor filters in low - level vision"
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics ."
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 97
                            }
                        ],
                        "text": "the use of unsupervised learning algorithms (e.g., Carpenter & Grossberg, 1988; Grossberg, 1976; Hinton , 1987; Kohonen, 1982)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Conneetionist learning procedures"
            },
            "venue": {
                "fragments": [],
                "text": "Conneetionist learning procedures"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 121
                            }
                        ],
                        "text": "In the field of signal processing, an almost equivalent algorithm was proposed in Owsley (1978), Oja (1983), and Oja and Karhunen (1985), although convergence to the matrix of eigenvectors was never proven, To our knowledge, the proof given here is the first convergence proof for this algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 147
                            }
                        ],
                        "text": "There exist several different but closely related algorithms for finding multiple eigenvectors which have been proven to converge (Brockett, 1989; Karhunen , 1984a, 1984b, 1985; Karhunen & Oja, 1982; Kuusela & Oja, 1982; Oja, 1983; Oja & Karhunen, 1980, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Simple gradient o'pe algorithms J`or"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recursive construction of Kar"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "New methods for stochastic approximation of truncated KarhunenLo 6 ve expansions"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 6 th International Conference on Pattern Recognition"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 121
                            }
                        ],
                        "text": "In the field of signal processing, an almost equivalent algorithm was proposed in Owsley (1978), Oja (1983), and Oja and Karhunen (1985), although convergence to the matrix of eigenvectors was never proven, To our knowledge, the proof given here is the first convergence proof for this algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 147
                            }
                        ],
                        "text": "There exist several different but closely related algorithms for finding multiple eigenvectors which have been proven to converge (Brockett, 1989; Karhunen , 1984a, 1984b, 1985; Karhunen & Oja, 1982; Kuusela & Oja, 1982; Oja, 1983; Oja & Karhunen, 1980, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Simple gradient o'pe algorithms J`or dataadaptive eigenvector estimation"
            },
            "venue": {
                "fragments": [],
                "text": "Simple gradient o'pe algorithms J`or dataadaptive eigenvector estimation"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 153
                            }
                        ],
                        "text": "Then the linear least squares estimate (LLSE) of x given y is is minimized when the rows of C span the first M eigenvectors of Q (Bourlard & Kamp, 1988; Fukunaga, 1972; Golub & Van Loan, 1983; Kazakos, 1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction of statistical pattern reco~mlion"
            },
            "venue": {
                "fragments": [],
                "text": "Introduction of statistical pattern reco~mlion"
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Backpropagation ean give rise to spurious local minima even"
            },
            "venue": {
                "fragments": [],
                "text": "l~r networks without hidden layers (Tech. Rep. No. SYCON-88-08). New Jersey: Rutgers Center lk~r Systems and Control"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 71
                            }
                        ],
                        "text": "If the network is designed with too many hidden units (in the sense of Brailovsky, 1983a, 1983b, 1985) then the additional error introduced is spread evenly throughout the units and cannot be easily detected or removed by looking at the signal to noise ratio of the individual units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 211
                            }
                        ],
                        "text": "Once we find an output whose variance is below the threshold, we can stop computing additional terms since all later outputs will have variances which are even lower and may decrease our approximation accuracy (Brailovsky, 1983a, 1983b, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the problem of function approxmlauon bv sample set processing for an incompletely determined model, Annals\" of the New York Academy of,Science"
            },
            "venue": {
                "fragments": [],
                "text": "410, 137147."
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Thc averaged learning subspace method for spectral pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the oth International Conference on Pattern Recognition"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 686,
                                "start": 121
                            }
                        ],
                        "text": "In the field of signal processing, an almost equivalent algorithm was proposed in Owsley (1978), Oja (1983), and Oja and Karhunen (1985), although convergence to the matrix of eigenvectors was never proven, To our knowledge, the proof given here is the first convergence proof for this algorithm. There exist several different but closely related algorithms for finding multiple eigenvectors which have been proven to converge (Brockett, 1989; Karhunen, 1984a, 1984b, 1985; Karhunen & Oja, 1982; Kuusela & Oja, 1982; Oja, 1983; Oja & Karhunen, 1980, 1985). Proofs of convergence for some of these algorithms, and an excellent summary of the different methods may be found in Oja (1983) and Karhunen (1984b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 121
                            }
                        ],
                        "text": "In the field of signal processing, an almost equivalent algorithm was proposed in Owsley (1978), Oja (1983), and Oja and Karhunen (1985), although convergence to the matrix of eigenvectors was never proven, To our knowledge, the proof given here is the first convergence proof for this algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 707,
                                "start": 121
                            }
                        ],
                        "text": "In the field of signal processing, an almost equivalent algorithm was proposed in Owsley (1978), Oja (1983), and Oja and Karhunen (1985), although convergence to the matrix of eigenvectors was never proven, To our knowledge, the proof given here is the first convergence proof for this algorithm. There exist several different but closely related algorithms for finding multiple eigenvectors which have been proven to converge (Brockett, 1989; Karhunen, 1984a, 1984b, 1985; Karhunen & Oja, 1982; Kuusela & Oja, 1982; Oja, 1983; Oja & Karhunen, 1980, 1985). Proofs of convergence for some of these algorithms, and an excellent summary of the different methods may be found in Oja (1983) and Karhunen (1984b). Recently, Brockett (1989) has proposed a related algorithm which is similar but which does not use the LT operator, and which he has proven converges to the eigenvectors in any desired order."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recursive estimation of eigenvectors q/'cor"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning internal representations from grayscale images : An example of extensional programming"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 9 th Annual Conference of the Cognitive Science Society"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 178
                            }
                        ],
                        "text": "There exist several different but closely related algorithms for finding multiple eigenvectors which have been proven to converge (Brockett, 1989; Karhunen , 1984a, 1984b, 1985; Karhunen & Oja, 1982; Kuusela & Oja, 1982; Oja, 1983; Oja & Karhunen, 1980, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "New methods for stochastic approximation of truncated Karhunen-Lo6ve expansions"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 6th International Conference on Pattern Recognition"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Anah,'sis of rccursive stochastic algorithms. IEEE 7)'an.saction~s on Automatic Control"
            },
            "venue": {
                "fragments": [],
                "text": "Anah,'sis of rccursive stochastic algorithms. IEEE 7)'an.saction~s on Automatic Control"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "T h en e u r a l \" phonetic typewriter"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Conneetionist learning procedures, (Tech"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 232
                            }
                        ],
                        "text": "There exist several different but closely related algorithms for finding multiple eigenvectors which have been proven to converge (Brockett, 1989; Karhunen , 1984a, 1984b, 1985; Karhunen & Oja, 1982; Kuusela & Oja, 1982; Oja, 1983; Oja & Karhunen, 1980, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 186
                            }
                        ],
                        "text": "He then proves that for an arbitrary choice of initial weights, c~ will converge to the principal eigenvector e~ (or its negative) so long as ci(O)re~ ~ 0 at time zero (Oja, 1982, 1983: Oja & Karhunen, 1980)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 230
                            }
                        ],
                        "text": "Oja has shown that if we maintain the diagonal elements of CC r equal to 1 (so that the norm of each row is 1), then a Hebbian learning rule will cause the rows of C to converge to the principal eigenvector of Q (Oja, 1982, 1983; Oja & Karhunen, 1980)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recursive construction of Kar - huncn - Lo , Sve expansions for pattern recognition purposes"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 5 th International Conference on Pattern Recognition"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some remarks on the hackpropagation algorithm ]or neural net learning New Jersey: Rutgcrs Center lk)r Systems and Control"
            },
            "venue": {
                "fragments": [],
                "text": "Some remarks on the hackpropagation algorithm ]or neural net learning New Jersey: Rutgcrs Center lk)r Systems and Control"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "New methods for stochastic approximation of truncated Karhunen - Lo 6 ve expansions"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 6 th International Conference on Pattern Recognition"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spatial frequency analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Thc averaged learning subspace"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Visual cortical neurons"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 193
                            }
                        ],
                        "text": "Then the linear least squares estimate (LLSE) of x given y is is minimized when the rows of C span the first M eigenvectors of Q (Bourlard & Kamp, 1988; Fukunaga, 1972; Golub & Van Loan, 1983; Kazakos, 1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 109
                            }
                        ],
                        "text": "and the mean squared error E[(x - / ) 2 ] is minimized when the rows of C span the first M eigenvectors of Q (Bourlard & Kamp, 1988; Fukunaga, 1972; Golub & Van Loan, 1983; Kazakos, 1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal constrained representation and fil"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recursive construction of KarhuncnLo , Sve expansions for pattern recognition purposes"
            },
            "venue": {
                "fragments": [],
                "text": "Suh ~ ' paee methods qf pattern recognition"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 127
                            }
                        ],
                        "text": "This technique is used to ensure that different outputs learn different functions of the input (Barrow, 1987; Grossberg, 1976; Kohonen, 1982, 1988, among others)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 44
                            }
                        ],
                        "text": "the use of unsupervised learning algorithms (e.g., Carpenter & Grossberg, 1988; Grossberg, 1976; Hinton, 1987; Kohonen, 1982)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 112
                            }
                        ],
                        "text": "the use of unsupervised learning algorithms (e.g., Carpenter & Grossberg, 1988; Grossberg, 1976; Hinton , 1987; Kohonen, 1982)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Self-organized formation of topologically"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Auto-association by multilayet perceptrons and singular value decomposition"
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 50
                            }
                        ],
                        "text": "This approach has been used by Linsker (1986) and Silverman and Noetzel (1988), for example."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Time-sequential selforganization of hierarchical neural net~,orks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural inJormation processing .sTstems"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 97
                            }
                        ],
                        "text": "In the field of signal processing, an almost equivalent algorithm was proposed in Owsley (1978), Oja (1983), and Oja and Karhunen (1985), although convergence to the matrix of eigenvectors was never proven, To our knowledge, the proof given here is the first convergence proof for this algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Anah,'sis of rccursive stochastic algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE 7 ) ' an . saction ~ s on Automatic Control"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prague: Publishing House of the Czechoskwak Academy of Sciences"
            },
            "venue": {
                "fragments": [],
                "text": "Prague: Publishing House of the Czechoskwak Academy of Sciences"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 127
                            }
                        ],
                        "text": "This technique is used to ensure that different outputs learn different functions of the input (Barrow, 1987; Grossberg, 1976; Kohonen, 1982, 1988, among others)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 112
                            }
                        ],
                        "text": "the use of unsupervised learning algorithms (e.g., Carpenter & Grossberg, 1988; Grossberg, 1976; Hinton , 1987; Kohonen, 1982)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Self-Organized Formation of Correct Feature Maps"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 71
                            }
                        ],
                        "text": "If the network is designed with too many hidden units (in the sense of Brailovsky, 1983a, 1983b, 1985) then the additional error introduced is spread evenly throughout the units and cannot be easily detected or removed by looking at the signal to noise ratio of the individual units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 211
                            }
                        ],
                        "text": "Once we find an output whose variance is below the threshold, we can stop computing additional terms since all later outputs will have variances which are even lower and may decrease our approximation accuracy (Brailovsky, 1983a, 1983b, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the problem of function approxmlauon bv sample set processing for an incompletely determined model, Annals\" of the New York Academy of"
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1983
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 26,
            "methodology": 26,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 92,
        "totalPages": 10
    },
    "page_url": "https://www.semanticscholar.org/paper/Optimal-unsupervised-learning-in-a-single-layer-Sanger/709b4bfc5198336ba5d70da987889a157f695c1e?sort=total-citations"
}