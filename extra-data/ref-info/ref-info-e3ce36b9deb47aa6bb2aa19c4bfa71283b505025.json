{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 41
                            }
                        ],
                        "text": "gence (Hinton, 2002), and score matching (Hyv\u00e4rinen, 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 165
                            }
                        ],
                        "text": "; \u03b1) without computation of the integral which defines the normalization constant; the most recent ones are contrastive divergence (Hinton, 2002) and score matching (Hyva\u0308rinen, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 165
                            }
                        ],
                        "text": "; \u03b1) without computation of the integral which defines the normalization constant; the most recent ones are contrastive divergence (Hinton, 2002) and score matching (Hyv\u00e4rinen, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 280
                            }
                        ],
                        "text": "In the setting of logistic regression, however, we will then have to learn that the two distributions are equal and that the posterior probability for any point belonging to any of the two classes is 50%, which is a well defined problem.\ngence (Hinton, 2002), and score matching (Hyva\u0308rinen, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1152227,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9966e890f2eedb4577e11b9d5a66380a4d9341fe",
            "isKey": false,
            "numCitedBy": 639,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simplifies to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete filter set for natural image data."
            },
            "slug": "Estimation-of-Non-Normalized-Statistical-Models-by-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "Estimation of Non-Normalized Statistical Models by Score Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, it is proved a surprising result that gives a simple formula that simplifies to a sample average of a sum of some derivatives of the log- density given by the model."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145992652"
                        ],
                        "name": "Michael U Gutmann",
                        "slug": "Michael-U-Gutmann",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Gutmann",
                            "middleNames": [
                                "U"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael U Gutmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 100
                            }
                        ],
                        "text": "We used in previous work classification based on logistic regression to learn features from images (Gutmann & Hyva\u0308rinen, 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6651358,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba37a893c8d4c1d0bf152da07a565998296308cf",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling the statistical structure of natural images is interesting for reasons related to neuroscience as well as engineering. Currently, this modeling relies heavily on generative probabilistic models. The estimation of such models is, however, difficult, especially when they consist of multiple layers. If the goal lies only in estimating the features, i.e. in pinpointing structure in natural images, one could also estimate instead a discriminative probabilistic model where multiple layers are more easily handled. For that purpose, we propose to estimate a classifier that can tell natural images apart from reference data which has been constructed to contain some known structure of natural images. The features of the classifier then reveal the interesting structure. Here, we use a classifier with one layer of features and reference data which contains the covariance-structure of natural images. We show that the features of the classifier are similar to those which are obtained from generative probabilistic models. Furthermore, we investigate the optimal shape of the nonlinearity that is used within the classifier."
            },
            "slug": "Learning-Features-by-Contrasting-Natural-Images-Gutmann-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "Learning Features by Contrasting Natural Images with Noise"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes to estimate a classifier that can tell natural images apart from reference data which has been constructed to contain some known structure of natural images, and shows that the features of the classifier are similar to those which are obtained from generative probabilistic models."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633389"
                        ],
                        "name": "Yan Karklin",
                        "slug": "Yan-Karklin",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Karklin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Karklin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 97
                            }
                        ],
                        "text": "Patch-based models are mostly two-layer models (Osindero et al., 2006; Ko\u0308ster & Hyva\u0308rinen, 2007; Karklin & Lewicki, 2005), although in (Osindero & Hinton, 2008) a three-layer model is presented."
                    },
                    "intents": []
                }
            ],
            "corpusId": 490453,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68f58d5b4b4797955b5965f10d424764bd6ee839",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Capturing statistical regularities in complex, high-dimensional data is an important problem in machine learning and signal processing. Models such as principal component analysis (PCA) and independent component analysis (ICA) make few assumptions about the structure in the data and have good scaling properties, but they are limited to representing linear statistical regularities and assume that the distribution of the data is stationary. For many natural, complex signals, the latent variables often exhibit residual dependencies as well as nonstationary statistics. Here we present a hierarchical Bayesian model that is able to capture higher-order nonlinear structure and represent nonstationary data distributions. The model is a generalization of ICA in which the basis function coefficients are no longer assumed to be independent; instead, the dependencies in their magnitudes are captured by a set of density components. Each density component describes a common pattern of deviation from the marginal density of the pattern ensemble; in different combinations, they can describe nonstationary distributions. Adapting the model to image or audio data yields a nonlinear, distributed code for higher-order statistical regularities that reflect more abstract, invariant properties of the signal."
            },
            "slug": "A-Hierarchical-Bayesian-Model-for-Learning-in-Karklin-Lewicki",
            "title": {
                "fragments": [],
                "text": "A Hierarchical Bayesian Model for Learning Nonlinear Statistical Regularities in Nonstationary Natural Signals"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A hierarchical Bayesian model is presented that is able to capture higher-order nonlinear structure and represent nonstationary data distributions and Adapting the model to image or audio data yields a nonlinear, distributed code for higher- order statistical regularities that reflect more abstract, invariant properties of the signal."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49627521"
                        ],
                        "name": "Urs K\u00f6ster",
                        "slug": "Urs-K\u00f6ster",
                        "structuredName": {
                            "firstName": "Urs",
                            "lastName": "K\u00f6ster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Urs K\u00f6ster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 288,
                                "start": 264
                            }
                        ],
                        "text": "Examples of statistical models where the normalization constraint poses a problem can be found in Markov random fields (Roth & Black, 2009; Ko\u0308ster et al., 2009), energy-based models (Hinton, 2002; Teh et al., 2004), and multilayer networks (Osindero et al., 2006; Ko\u0308ster & Hyva\u0308rinen, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 71
                            }
                        ],
                        "text": "Patch-based models are mostly two-layer models (Osindero et al., 2006; Ko\u0308ster & Hyva\u0308rinen, 2007; Karklin & Lewicki, 2005), although in (Osindero & Hinton, 2008) a three-layer model is presented."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 41
                            }
                        ],
                        "text": "The data pdf pd(.) is modeled by a parameterized family of functions {pm(."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 45
                            }
                        ],
                        "text": "The results correspond to those reported in (Ko\u0308ster & Hyva\u0308rinen, 2007) and (Osindero et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 18039559,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9cca5a7c3327f6075c58361d7f98188dc4f9d7c",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Capturing regularities in high-dimensional data is an important problem in machine learning and signal processing. Here we present a statistical model that learns a nonlinear representation from the data that reflects abstract, invariant properties of the signal without making requirements about the kind of signal that can be processed. The model has a hierarchy of two layers, with the first layer broadly corresponding to Independent Component Analysis (ICA) and a second layer to represent higher order structure. We estimate the model using the mathematical framework of Score Matching (SM), a novel method for the estimation of non-normalized statistical models. The model incorporates a squaring nonlinearity, which we propose to be suitable for forming a higher-order code of invariances. Additionally the squaring can be viewed as modelling subspaces to capture residual dependencies, which linear models cannot capture."
            },
            "slug": "A-Two-Layer-ICA-Like-Model-Estimated-by-Score-K\u00f6ster-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "A Two-Layer ICA-Like Model Estimated by Score Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A statistical model is presented that learns a nonlinear representation from the data that reflects abstract, invariant properties of the signal without making requirements about the kind of signal that can be processed."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 197
                            }
                        ],
                        "text": "Examples of statistical models where the normalization constraint poses a problem can be found in Markov random fields (Roth & Black, 2009; Ko\u0308ster et al., 2009), energy-based models (Hinton, 2002; Teh et al., 2004), and multilayer networks (Osindero et al., 2006; Ko\u0308ster & Hyva\u0308rinen, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 29
                            }
                        ],
                        "text": ", 2009), energy-based models (Hinton, 2002; Teh et al., 2004), and multilayer networks (Osindero et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52865368,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b95799a25def71b100bd12e7ebb32cbcee6590bf",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new way of extending independent components analysis (ICA) to overcomplete representations. In contrast to the causal generative extensions of ICA which maintain marginal independence of sources, we define features as deterministic (linear) functions of the inputs. This assumption results in marginal dependencies among the features, but conditional independence of the features given the inputs. By assigning energies to the features a probability distribution over the input states is defined through the Boltzmann distribution. Free parameters of this model are trained using the contrastive divergence objective (Hinton, 2002). When the number of features is equal to the number of input dimensions this energy-based model reduces to noiseless ICA and we show experimentally that the proposed learning algorithm is able to perform blind source separation on speech data. In additional experiments we train overcomplete energy-based models to extract features from various standard data-sets containing speech, natural images, hand-written digits and faces."
            },
            "slug": "Energy-Based-Models-for-Sparse-Overcomplete-Teh-Welling",
            "title": {
                "fragments": [],
                "text": "Energy-Based Models for Sparse Overcomplete Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A new way of extending independent components analysis (ICA) to overcomplete representations that defines features as deterministic (linear) functions of the inputs and assigns energies to the features through the Boltzmann distribution."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 75
                            }
                        ],
                        "text": "The results correspond to those reported in (K\u00f6ster & Hyv\u00e4rinen, 2007) and (Osindero et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 76
                            }
                        ],
                        "text": "The results correspond to those reported in (Ko\u0308ster & Hyva\u0308rinen, 2007) and (Osindero et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 47
                            }
                        ],
                        "text": "Patch-based models are mostly two-layer models (Osindero et al., 2006; K\u00f6ster & Hyv\u00e4rinen, 2007; Karklin & Lewicki, 2005), although in (Osindero & Hinton, 2008) a three-layer model is presented."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 48
                            }
                        ],
                        "text": "Patch-based models are mostly two-layer models (Osindero et al., 2006; Ko\u0308ster & Hyva\u0308rinen, 2007; Karklin & Lewicki, 2005), although in (Osindero & Hinton, 2008) a three-layer model is presented."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 262,
                                "start": 241
                            }
                        ],
                        "text": "Examples of statistical models where the normalization constraint poses a problem can be found in Markov random fields (Roth & Black, 2009; Ko\u0308ster et al., 2009), energy-based models (Hinton, 2002; Teh et al., 2004), and multilayer networks (Osindero et al., 2006; Ko\u0308ster & Hyva\u0308rinen, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6699891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8d01934cb26064b253dbd0f1627519133c3df3e",
            "isKey": true,
            "numCitedBy": 120,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an energy-based model that uses a product of generalized Student-t distributions to capture the statistical structure in data sets. This model is inspired by and particularly applicable to natural data sets such as images. We begin by providing the mathematical framework, where we discuss complete and overcomplete models and provide algorithms for training these models from data. Using patches of natural scenes, we demonstrate that our approach represents a viable alternative to independent component analysis as an interpretive model of biological visual systems. Although the two approaches are similar in flavor, there are also important differences, particularly when the representations are overcomplete. By constraining the interactions within our model, we are also able to study the topographic organization of Gabor-like receptive fields that our model learns. Finally, we discuss the relation of our new approach to previous workin particular, gaussian scale mixture models and variants of independent components analysis."
            },
            "slug": "Topographic-Product-Models-Applied-to-Natural-Scene-Osindero-Welling",
            "title": {
                "fragments": [],
                "text": "Topographic Product Models Applied to Natural Scene Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "An energy-based model is presented that uses a product of generalized Student-t distributions to capture the statistical structure in data sets to study the topographic organization of Gabor-like receptive fields that the model learns."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794837"
                        ],
                        "name": "Siwei Lyu",
                        "slug": "Siwei-Lyu",
                        "structuredName": {
                            "firstName": "Siwei",
                            "lastName": "Lyu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siwei Lyu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 80
                            }
                        ],
                        "text": "Projection onto a sphere can be considered as a form of divisive normalization (Lyu & Simoncelli, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2041757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b4cc7d091bcb1e2ff4a1d4c8a28cfbe0e44d99a",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 107,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of efficiently encoding a signal by transforming it to a new representation whose components are statistically independent. A widely studied linear solution, known as independent component analysis (ICA), exists for the case when the signal is generated as a linear transformation of independent nongaussian sources. Here, we examine a complementary case, in which the source is nongaussian and elliptically symmetric. In this case, no invertible linear transform suffices to decompose the signal into independent components, but we show that a simple nonlinear transformation, which we call radial gaussianization (RG), is able to remove all dependencies. We then examine this methodology in the context of natural image statistics. We first show that distributions of spatially proximal bandpass filter responses are better described as elliptical than as linearly transformed independent sources. Consistent with this, we demonstrate that the reduction in dependency achieved by applying RG to either nearby pairs or blocks of bandpass filter responses is significantly greater than that achieved by ICA. Finally, we show that the RG transformation may be closely approximated by divisive normalization, which has been used to model the nonlinear response properties of visual neurons."
            },
            "slug": "Nonlinear-Extraction-of-Independent-Components-of-Lyu-Simoncelli",
            "title": {
                "fragments": [],
                "text": "Nonlinear Extraction of Independent Components of Natural Images Using Radial Gaussianization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that distributions of spatially proximal bandpass filter responses are better described as elliptical than as linearly transformed independent sources, and it is demonstrated that the reduction in dependency achieved by applying RG to either nearby pairs or blocks of bandpass filters is significantly greater than that achieved by ICA."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 120
                            }
                        ],
                        "text": "Examples of statistical models where the normalization constraint poses a problem can be found in Markov random fields (Roth & Black, 2009; Ko\u0308ster et al., 2009), energy-based models (Hinton, 2002; Teh et al., 2004), and multilayer networks (Osindero et al., 2006; Ko\u0308ster & Hyva\u0308rinen, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 31
                            }
                        ],
                        "text": "This is different compared to (Roth & Black, 2009), where the filters had no clear structure."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 86
                            }
                        ],
                        "text": "For the learning of MRF from natural images, contrastive divergence has been used in (Roth & Black, 2009), while (Ko\u0308ster et al., 2009) employs score matching."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 18
                            }
                        ],
                        "text": "Assume a sample of a random vector x \u2208 Rn is observed which follows an unknown probability density function (pdf) pd(.)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13058320,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dfcae80f4d34ac09ba8063c5cfb5be954d0bf5f1",
            "isKey": true,
            "numCitedBy": 704,
            "numCiting": 109,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach provides a practical method for learning high-order Markov random field (MRF) models with potential functions that extend over large pixel neighborhoods. These clique potentials are modeled using the Product-of-Experts framework that uses non-linear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field-of-Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we obtain results that compete with specialized techniques."
            },
            "slug": "Fields-of-Experts-Roth-Black",
            "title": {
                "fragments": [],
                "text": "Fields of Experts"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The approach provides a practical method for learning high-order Markov random field models with potential functions that extend over large pixel neighborhoods with non-linear functions of many linear filter responses."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108774414"
                        ],
                        "name": "Yuhong Yang",
                        "slug": "Yuhong-Yang",
                        "structuredName": {
                            "firstName": "Yuhong",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuhong Yang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 68
                            }
                        ],
                        "text": "In our implementation, we used one step of Hamiltonian Monte Carlo (MacKay, 2002) with three leapfrog steps."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122759122,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "277b6accc82872010ebebfc79214ca2913c483b7",
            "isKey": false,
            "numCitedBy": 919,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The fundamental step in a systematic development of analytical methods for Markov processes, particularly, for processes with stationary and independent increments (PSI), was taken by Kolmogorov (1931). Since 1990, many authors have referred to PSI as the theory of L\u00e9vy processes. After a period of intense development in 1930\u20131980, applications in mathematical finance have motivated more interest in L\u00e9vy processes (see Cont and Tankov 2004; Schoutens 2003). There is also a persistent interest by the physical community in the theory of L\u00e9vy processes, also called \u201cL\u00e9vy flights\u201d (see Shlesinger, Zaslavsky, and Frisch 1995). Both finanacial and physical motivations come from the fact that models with underlying L\u00e9vy processes can be (at least, potentially) more accurately fitted to real data than traditional models with underlying Brownian motion (but, still keeping the assumption on independent increments, which often simplifies calculations). This book presents an interplay between the classical theory of general L\u00e9vy processes described by Skorohod (1991), Bertoin (1996), Sato (2003), and modern stochastic analysis as presented by Liptser and Shiryayev (1989), Protter (2004), and others. Most applications in finance and physics do not require the full power of stochastic analysis, which allows us to consider discrete and continuous time models in one framework. I think the author made a wise decision to limit attention to the aspects of stochastic analysis related to stochastically continuous L\u00e9vy processes. This approach allows one to economically describe basic tools of stochastic analysis such as \u201cMartingales, Stopping Times, and Random Measures\u201d (Chap. 2); \u201cStochastic Integration\u201d (Chap. 4); \u201cExponential Martingales, Change of Measure and Finance Applications\u201d (Chap. 5); and \u201cStochastic Differential Equations\u201d (Chap. 6). Chapter 1, \u201cL\u00e9vy Processes,\u201d contains a nice review of measure and probability including the detailed discussion of the key notions of infinite divisibility and L\u00e9vy processes. Chapter 3, \u201cMarkov Processes, Semigroups, and Generators,\u201d is more analytical than the rest of the book and could be skipped by \u201creaders with specific interest in finance,\u201d as the author recommends in his preface. Actually, aspects of mathematical finance are outlined only briefly in Sections 5.4. There is no discussion of statistical aspects of fitting L\u00e9vy models, and it seems that this topic has not yet been treated in textbooks. The background of the readers should be, at least at the level of graduate students, a solid knowledge of probability, Fourier transforms, and, for the readers of Chapter 3, a basic knowledge of linear operators in Banach spaces (although Appendix 3.8 contains some key results on this topic). Most of the exposition is clearly presented and contains useful exercises (without solutions), notes, and suggestions for further reading. My only complaint is that some numberings of references in the range 100\u2013312 are incorrect (although in most cases this can be easily recognized and corrected). The author has promised to post corrections on his website. I would recommend this book as a reference textbook for advanced courses like stochastic modeling or stochastic calculus in finance."
            },
            "slug": "Information-Theory,-Inference,-and-Learning-Yang",
            "title": {
                "fragments": [],
                "text": "Information Theory, Inference, and Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This book presents an interplay between the classical theory of general L\u00e9vy processes described by Skorohod (1991), Bertoin (1996), Sato (2003), and modern stochastic analysis as presented by Liptser and Shiryayev (1989), Protter (2004), and others."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 29
                            }
                        ],
                        "text": ", 2009), energy-based models (Hinton, 2002; Teh et al., 2004), and multilayer networks (Osindero et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 131
                            }
                        ],
                        "text": "; \u03b1) without computation of the integral which defines the normalization constant; the most recent ones are contrastive divergence (Hinton, 2002) and score matching (Hyv\u00e4rinen, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 6
                            }
                        ],
                        "text": "gence (Hinton, 2002), and score matching (Hyv\u00e4rinen, 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 183
                            }
                        ],
                        "text": "Examples of statistical models where the normalization constraint poses a problem can be found in Markov random fields (Roth & Black, 2009; Ko\u0308ster et al., 2009), energy-based models (Hinton, 2002; Teh et al., 2004), and multilayer networks (Osindero et al., 2006; Ko\u0308ster & Hyva\u0308rinen, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 131
                            }
                        ],
                        "text": "; \u03b1) without computation of the integral which defines the normalization constant; the most recent ones are contrastive divergence (Hinton, 2002) and score matching (Hyva\u0308rinen, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 257,
                                "start": 245
                            }
                        ],
                        "text": "In the setting of logistic regression, however, we will then have to learn that the two distributions are equal and that the posterior probability for any point belonging to any of the two classes is 50%, which is a well defined problem.\ngence (Hinton, 2002), and score matching (Hyva\u0308rinen, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207596505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9360e5ce9c98166bb179ad479a9d2919ff13d022",
            "isKey": true,
            "numCitedBy": 4571,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data."
            },
            "slug": "Training-Products-of-Experts-by-Minimizing-Hinton",
            "title": {
                "fragments": [],
                "text": "Training Products of Experts by Minimizing Contrastive Divergence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A product of experts (PoE) is an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary because it is hard even to approximate the derivatives of the renormalization term in the combination rule."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 136
                            }
                        ],
                        "text": "Patch-based models are mostly two-layer models (Osindero et al., 2006; Ko\u0308ster & Hyva\u0308rinen, 2007; Karklin & Lewicki, 2005), although in (Osindero & Hinton, 2008) a three-layer model is presented."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2054939,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80c330eee12decb84aaebcc85dc7ce414134ad61",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an efficient learning procedure for multilayer generative models that combine the best aspects of Markov random fields and deep, directed belief nets. The generative models can be learned one layer at a time and when learning is complete they have a very fast inference procedure for computing a good approximation to the posterior distribution in all of the hidden layers. Each hidden layer has its own MRF whose energy function is modulated by the top-down directed connections from the layer above. To generate from the model, each layer in turn must settle to equilibrium given its top-down input. We show that this type of model is good at capturing the statistics of patches of natural images."
            },
            "slug": "Modeling-image-patches-with-a-directed-hierarchy-of-Osindero-Hinton",
            "title": {
                "fragments": [],
                "text": "Modeling image patches with a directed hierarchy of Markov random fields"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An efficient learning procedure for multilayer generative models that combine the best aspects of Markov random fields and deep, directed belief nets is described and it is shown that this type of model is good at capturing the statistics of patches of natural images."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733999"
                        ],
                        "name": "L. Wasserman",
                        "slug": "L.-Wasserman",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Wasserman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Wasserman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 166
                            }
                        ],
                        "text": "\u2026compare its performance with other estimation methods, namely MLE, MLE where the normalization (partition function) is calculated with importance sampling (see e.g. (Wasserman, 2004) for an introduction to importance sampling), contrastive diver-\n4At a first glance, this might be counterintuitive."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 64
                            }
                        ],
                        "text": "Conditions (b) and (c) have their counterparts in MLE, see e.g.(Wasserman, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "(Wasserman, 2004) for an introduction to importance sampling), contrastive diver-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 64035189,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "ca78cb7c1d841d6bde6cd54a6b50ff9157b0cd63",
            "isKey": true,
            "numCitedBy": 750,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "regression methods. The authors expect \u201ca working knowledge of the statistical concepts discussed in the book\u201d (\u201cUsing This Book,\u201d p. vii). Because JMP is Windows software, one would hope that the user could just figure out how to do everything by looking at the menus. That is certainly my intent with any Windows software. Chapter 2, \u201cRegression in JMP,\u201d presents lists of instructions like \u201cselect, click, click, repeat, and click,\u201d each followed by direction to the appropriate item. It is patently clear from the very first illustration that this is a book about how to do regression in JMP, not a book about regression. The two subsequent chapters on \u201cObservations\u201d and \u201cCollinearity\u201d are both excellent. They take full advantage of both the visual and computational capabilities of JMP. Many of the outputs are annotated with text boxes. The next three chapters focus on several different types of regression models. There are chapters on polynomials, which includes smoothing, and nonlinear models. In between these two chapters is a chapter concerned with several different linear models applications, including errors-in-variables, multiplicative models, splines, indicator variables, and logistic regression for binary responses. Curiously, the book ends with a chapter titled \u201cRegression With the JMP Scripting Language.\u201d I had thought that the whole point of JMP was to avoid writing SAS code. Nevertheless, for a new JMP user, this book should certainly jump-start the process of using JMP effectively. As for the authors, I should note that they have two other excellent users\u2019 books for SAS. Each would provide an excellent basis for a JMP book similar to this one. See their recent update, (Littell, Stroup, and Freund 2002), for their linear models book, and their excellent book on mixed models (Littell, Milliken, Stroup, and Wolfinger 1996). Technometrics reviews for these books were published by Moore (2003) and Ziegel (1997)."
            },
            "slug": "All-of-Statistics-Wasserman",
            "title": {
                "fragments": [],
                "text": "All of statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "The first \u20ac price and the \u00a3 and $ price are net prices, subject to local VAT, and the \u20ac(D) includes 7% for Germany, the\u20ac(A) includes 10% for Austria."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118850142"
                        ],
                        "name": "Seungjin Choi",
                        "slug": "Seungjin-Choi",
                        "structuredName": {
                            "firstName": "Seungjin",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seungjin Choi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 106
                            }
                        ],
                        "text": "In our implementation for the estimation of the ICA model, we used the faster natural gradient, see e.g. (Hyva\u0308rinen et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 80
                            }
                        ],
                        "text": "We illustrate noise-contrastive estimation with the estimation of an ICA model (Hyva\u0308rinen et al., 2001), and compare its performance with other estimation methods, namely MLE, MLE where the normalization (partition function) is calculated with importance sampling (see e.g. (Wasserman, 2004) for an\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 79
                            }
                        ],
                        "text": "We illustrate noise-contrastive estimation with the estimation of an ICA model (Hyv\u00e4rinen et al., 2001), and compare its performance with other estimation methods, namely MLE, MLE where the normalization (partition function) is calculated with importance sampling (see e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1299275,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4dce52e86359dfebd40037084e6b5e46188243ed",
            "isKey": true,
            "numCitedBy": 2909,
            "numCiting": 228,
            "paperAbstract": {
                "fragments": [],
                "text": "In the independent component (IC) model it is assumed that the p-variate random vector x = \u03a9z + \u03bc, where \u03bc is a location vector, \u03a9 is a full rank p\u00d7 p mixing matrix, and z is a p-variate vector with mutually independent components. In the independent component analysis (ICA) the aim is to find an estimate of an unmixing matrix \u0393 such that \u0393x has independent components. We talk about standardization of the IC model, and on the basis of n independent copies of x, we consider one-sample testing and estimation procedures for \u03a9 (or \u0393). We also discuss comparison of different unmixing matrix estimates."
            },
            "slug": "Independent-Component-Analysis-Choi",
            "title": {
                "fragments": [],
                "text": "Independent Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The standardization of the IC model is talked about, and on the basis of n independent copies of x, the aim is to find an estimate of an unmixing matrix \u0393 such that \u0393x has independent components."
            },
            "venue": {
                "fragments": [],
                "text": "Handbook of Natural Computing"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49627521"
                        ],
                        "name": "Urs K\u00f6ster",
                        "slug": "Urs-K\u00f6ster",
                        "structuredName": {
                            "firstName": "Urs",
                            "lastName": "K\u00f6ster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Urs K\u00f6ster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1957992"
                        ],
                        "name": "J. T. Lindgren",
                        "slug": "J.-T.-Lindgren",
                        "structuredName": {
                            "firstName": "Jussi",
                            "lastName": "Lindgren",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. T. Lindgren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 191
                            }
                        ],
                        "text": "For a tractable ICA model, we compared noisecontrastive estimation with other methods that can\n6Although the MRF is a model for an entire image, training can be done with image patches, see (Ko\u0308ster et al., 2009).\nbe used to estimate unnormalized models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 4
                            }
                        ],
                        "text": "In (Ko\u0308ster et al., 2009), the filters, which were shown in the whitened space, were also Gabor-like."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 38
                            }
                        ],
                        "text": "Assume a sample of a random vector x \u2208 Rn is observed which follows an unknown probability density function (pdf) pd(.)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 140
                            }
                        ],
                        "text": "Examples of statistical models where the normalization constraint poses a problem can be found in Markov random fields (Roth & Black, 2009; Ko\u0308ster et al., 2009), energy-based models (Hinton, 2002; Teh et al., 2004), and multilayer networks (Osindero et al., 2006; Ko\u0308ster & Hyva\u0308rinen, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 114
                            }
                        ],
                        "text": "For the learning of MRF from natural images, contrastive divergence has been used in (Roth & Black, 2009), while (Ko\u0308ster et al., 2009) employs score matching."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6048599,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "b8beb6d1738ba8911c33bf2be90cf7b76094900d",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A magnetic recording medium which comprises a non-magnetic support, a magnetic recording layer formed on one side of the support, and a back coat layer formed on the other side. The back coat layer is made of a dispersion, in a binder resin, of non-magnetic particles on which there is adsorbed carbon black having an average size not larger than 100 millimicrons and a specific surface area not less than 30 m2/g."
            },
            "slug": "Estimating-Markov-Random-Field-Potentials-for-K\u00f6ster-Lindgren",
            "title": {
                "fragments": [],
                "text": "Estimating Markov Random Field Potentials for Natural Images"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A magnetic recording medium which comprises a non-magnetic support, a magnetic recording layerformed on one side of the support, and a back coat layer formed on the other side."
            },
            "venue": {
                "fragments": [],
                "text": "ICA"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In our implementation, we used one step of Hamiltonian Monte Carlo (MacKay, 2002) with three leapfrog steps."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5436619,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7f15848cd0fbb3d08f351595da833b1627de9c3",
            "isKey": false,
            "numCitedBy": 8764,
            "numCiting": 249,
            "paperAbstract": {
                "fragments": [],
                "text": "Fun and exciting textbook on the mathematics underpinning the most dynamic areas of modern science and engineering."
            },
            "slug": "Information-Theory,-Inference,-and-Learning-Mackay",
            "title": {
                "fragments": [],
                "text": "Information Theory, Inference, and Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "A fun and exciting textbook on the mathematics underpinning the most dynamic areas of modern science and engineering."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 53
                            }
                        ],
                        "text": "The optimization is then done by conjugate gradient (Rasmussen, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 62
                            }
                        ],
                        "text": "For the optimization, we used a conjugate gradient algorithm (Rasmussen, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 71
                            }
                        ],
                        "text": "For NCE and score matching (SM), we relied on the built-in criteria of (Rasmussen, 2006) to determine convergence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 62
                            }
                        ],
                        "text": "The optimization is done with a conjugate gradient algorithm (Rasmussen, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Conjugate gradient algorithm, version 2006-09-08"
            },
            "venue": {
                "fragments": [],
                "text": "available online."
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 53
                            }
                        ],
                        "text": "The optimization is then done by conjugate gradient (Rasmussen, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 62
                            }
                        ],
                        "text": "For the optimization, we used a conjugate gradient algorithm (Rasmussen, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 62
                            }
                        ],
                        "text": "The optimization is done with a conjugate gradient algorithm (Rasmussen, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Conjugate gradient algorithm, version 2006-09-08. available online"
            },
            "venue": {
                "fragments": [],
                "text": "Conjugate gradient algorithm, version 2006-09-08. available online"
            },
            "year": 2006
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 9,
            "methodology": 9,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 17,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Noise-contrastive-estimation:-A-new-estimation-for-Gutmann-Hyv\u00e4rinen/e3ce36b9deb47aa6bb2aa19c4bfa71283b505025?sort=total-citations"
}