{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24590005"
                        ],
                        "name": "Alex Perelygin",
                        "slug": "Alex-Perelygin",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Perelygin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Perelygin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110402830"
                        ],
                        "name": "Jean Wu",
                        "slug": "Jean-Wu",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1964541"
                        ],
                        "name": "Jason Chuang",
                        "slug": "Jason-Chuang",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Chuang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Chuang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144922861"
                        ],
                        "name": "Christopher Potts",
                        "slug": "Christopher-Potts",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Potts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Potts"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 145
                            }
                        ],
                        "text": "They give human-level performance on two commonly used word similarity datasets, WordSim353 (Finkelstein etal., 2001) and Simlex-999 (Hill et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 257
                            }
                        ],
                        "text": "We compare the same suite of compositional architectur s for three supervised NLP tasks: sentence similarity and textual entailment using the 2014 SemEval SICK dataset (Marelli et al., 2014), and sentiment classification using the Stanford Sentim Treebank (Socher et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 221
                            }
                        ],
                        "text": "To address these questions, we\n13Available athttp://ttic.uchicago.edu/ \u02dc wieting .\nused three tasks: The SICK similarity task, the SICK entailment task, and the Stanford Sentiment Treebank (SST) binary classification task (Socher et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We would also like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) and thank NVIDIA Corporation for donati g GPUs used in this research."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 990233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "687bac2d3320083eb4530bf18bb8f8f721477600",
            "isKey": true,
            "numCitedBy": 5371,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases."
            },
            "slug": "Recursive-Deep-Models-for-Semantic-Compositionality-Socher-Perelygin",
            "title": {
                "fragments": [],
                "text": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A Sentiment Treebank that includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality, and introduces the Recursive Neural Tensor Network."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3450996"
                        ],
                        "name": "Ryan Kiros",
                        "slug": "Ryan-Kiros",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Kiros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Kiros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844940337"
                        ],
                        "name": "Yukun Zhu",
                        "slug": "Yukun-Zhu",
                        "structuredName": {
                            "firstName": "Yukun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yukun Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37895334"
                        ],
                        "name": "S. Fidler",
                        "slug": "S.-Fidler",
                        "structuredName": {
                            "firstName": "Sanja",
                            "lastName": "Fidler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fidler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 155
                            }
                        ],
                        "text": "We compare the same suite of compositional architectur s for three supervised NLP tasks: sentence similarity and textual entailment using the 2014 SemEval SICK dataset (Marelli et al., 2014), and sentiment classification using the Stanford Sentim Treebank (Socher et al., 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 108
                            }
                        ],
                        "text": "Other recently proposed methods, including paragraph vectors (Le & Mikolov, 2014) and skip-thought vectors (Kiros et al., 2015), learn sequence representations that are predictive of words inside the sequenceor in neighboring sequences."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 137
                            }
                        ],
                        "text": "We also useo r sentence embeddings as an effective black box feature extractor for downstream tasks, comparing favorably to recent work (Kiros et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "This leads to performance rivaling the state of the art on the SICK similarity and entailment tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 30
                            }
                        ],
                        "text": "We use a similar set-up as in Kiros et al. (2015) and encode the sentences by averaging our PARAGRAM-PHRASEembeddings and then just learn the classification parameters without updating the embeddings."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 177
                            }
                        ],
                        "text": "The intuition is that we want the two phrases to be more similar to each other (cos g(x1), g(x2))) than either is to their respective negative examplest1 andt2, by a margin of at least\u03b4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 54
                            }
                        ],
                        "text": "We compare to three baselines: skip-thoughtvectors10 (Kiros et al., 2015), denoted \u201cST\u201d, averaged GloVe11 vectors (Pennington et al., 2014), and averagedPARAGRAM-SL999 vectors (Wieting et al., 2015), denoted \u201cPSL\u201d."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 32
                            }
                        ],
                        "text": "We also investigate how ourPARAGRAM-PHRASE embeddings perform as features for supervised tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 8
                            }
                        ],
                        "text": "For the SICK similarity task, we minimize the objective function14 from Tai et al. (2015)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "Secondly, we note that Tai et al. (2015) had a similar LSTM result on the SICK dataset (Pearson\u2019sr of 85.28 to our 85.45) to show that our LSTM implementation/tu ing procedure is able to match or exceed performance of another published LSTM result."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "Recent strong results on the SICK similarity task include 86.86 using a convolutional neural network (He et al., 2015) and 86.76 using a tree-LSTM (Tai et al., 2015)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 31
                            }
                        ],
                        "text": "Models in related work such as Kiros et al. (2015) and Li et al.(2015a) require significant training time on GPUs, on the order of multiple weeks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 134
                            }
                        ],
                        "text": "To address these questions, we\n13Available athttp://ttic.uchicago.edu/ \u02dc wieting .\nused three tasks: The SICK similarity task, the SICK entailment task, and the Stanford Sentiment Treebank (SST) binary classification task (Socher et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "The results on all STS tasks as well as the SICK and Twitter tasks re shown in Table 2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 91
                            }
                        ],
                        "text": "Note that 2400 dimensions is the same dimensionality as the uni-skip and bi-skip models in Kiros et al. (2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9126867,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e795c6e9916174ae12349f5dc3f516570c17ce8",
            "isKey": true,
            "numCitedBy": 1928,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice."
            },
            "slug": "Skip-Thought-Vectors-Kiros-Zhu",
            "title": {
                "fragments": [],
                "text": "Skip-Thought Vectors"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The approach for unsupervised learning of a generic, distributed sentence encoder is described, using the continuity of text from books to train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136562"
                        ],
                        "name": "Mohit Iyyer",
                        "slug": "Mohit-Iyyer",
                        "structuredName": {
                            "firstName": "Mohit",
                            "lastName": "Iyyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohit Iyyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1977256"
                        ],
                        "name": "Varun Manjunatha",
                        "slug": "Varun-Manjunatha",
                        "structuredName": {
                            "firstName": "Varun",
                            "lastName": "Manjunatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Varun Manjunatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389036863"
                        ],
                        "name": "Jordan L. Boyd-Graber",
                        "slug": "Jordan-L.-Boyd-Graber",
                        "structuredName": {
                            "firstName": "Jordan",
                            "lastName": "Boyd-Graber",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jordan L. Boyd-Graber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722360"
                        ],
                        "name": "Hal Daum\u00e9",
                        "slug": "Hal-Daum\u00e9",
                        "structuredName": {
                            "firstName": "Hal",
                            "lastName": "Daum\u00e9",
                            "middleNames": [],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hal Daum\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 102
                            }
                        ],
                        "text": "These include neural bag-of-words models (Kalchbrenner et al., 2014), deep averaging networks (DANs) (Iyyer et al., 2015), feature-weighted aver ging (Yu & Dredze, 2015), recursive neural networks based on parse structure (Socher et al., 2011; 2; 2013;\u0307Irsoy & Cardie, 2014; Wieting et al., 2015),\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 29
                            }
                        ],
                        "text": "Our simplest averaging model is even competitive with systems tuned for the particular tasks while also being extremely efficient and easy to use."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 207
                            }
                        ],
                        "text": "\u2026work, we experimented with recursive neural networks on binarized parses of the PPDB (Wieting et al., 2015), but we found that many of the phrases in PPDB are not sentences or even constituents, causing the parser to have unexpected behavior.\nwhereWp is the projection matrix andb is a bias vector."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Future work will focus on improving our embeddings by effectively handling undertrained words as well as by exploring new models that generalize even better to the large suite of text similaritytasks used in our experiments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 148
                            }
                        ],
                        "text": "\u2026model compositionality in such sequences, ranging from those based on simple operations like addition (Mitchell & Lapata, 2010; Yu & Dredze, 2015; Iyyer et al., 2015) to those based on richly-structured functions like recursive neural networks (Socher et al., 2011), convolutional neural networks\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 55
                            }
                        ],
                        "text": "Our third model is the deep averaging network (DAN) of Iyyer et al. (2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 84
                            }
                        ],
                        "text": "We add complexity by adding layers, leading to variants of deep averaging networks (Iyyer et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 216848261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d86227948b6000e5d7ed63cf2054ad600b7994a0",
            "isKey": true,
            "numCitedBy": 673,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Many existing deep learning models for natural language processing tasks focus on learning the compositionality of their inputs, which requires many expensive computations. We present a simple deep neural network that competes with and, in some cases, outperforms such models on sentiment analysis and factoid question answering tasks while taking only a fraction of the training time. While our model is syntactically-ignorant, we show significant improvements over previous bag-of-words models by deepening our network and applying a novel variant of dropout. Moreover, our model performs better than syntactic models on datasets with high syntactic variance. We show that our model makes similar errors to syntactically-aware models, indicating that for the tasks we consider, nonlinearly transforming the input is more important than tailoring a network to incorporate word order and syntax."
            },
            "slug": "Deep-Unordered-Composition-Rivals-Syntactic-Methods-Iyyer-Manjunatha",
            "title": {
                "fragments": [],
                "text": "Deep Unordered Composition Rivals Syntactic Methods for Text Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work presents a simple deep neural network that competes with and, in some cases, outperforms such models on sentiment analysis and factoid question answering tasks while taking only a fraction of the training time."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We release all of our resources to the research community1 with the hope that they can serve as the new baseline for further work on universal sentence embeddings."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 175
                            }
                        ],
                        "text": "\u2026variants, culminating in LSTMs because they have been foundto be effective for many types of sequential data (Graves et al., 2008; 2013; Greff et al., 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7961699,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cea967b59209c6be22829699f05b8b1ac4dc092d",
            "isKey": false,
            "numCitedBy": 14885,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier."
            },
            "slug": "Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals",
            "title": {
                "fragments": [],
                "text": "Sequence to Sequence Learning with Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure, and finds that reversing the order of the words in all source sentences improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2583391"
                        ],
                        "name": "Nal Kalchbrenner",
                        "slug": "Nal-Kalchbrenner",
                        "structuredName": {
                            "firstName": "Nal",
                            "lastName": "Kalchbrenner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nal Kalchbrenner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1864353"
                        ],
                        "name": "Edward Grefenstette",
                        "slug": "Edward-Grefenstette",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward Grefenstette"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685771"
                        ],
                        "name": "P. Blunsom",
                        "slug": "P.-Blunsom",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Blunsom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Blunsom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 187
                            }
                        ],
                        "text": "\u2026& Lapata, 2010; Yu & Dredze, 2015; Iyyer et al., 2015) to those based on richly-structured functions like recursive neural networks (Socher et al., 2011), convolutional neural networks (Kalchbrenner et al., 2014), and recurrent neural networks using long short-termmemory (LSTM) (Tai et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 42
                            }
                        ],
                        "text": "These include neural bag-of-words models (Kalchbrenner et al., 2014), deep averaging networks (DANs) (Iyyer et al., 2015), feature-weighted aver ging (Yu & Dredze, 2015), recursive neural networks based on parse structure (Socher et al., 2011; 2; 2013;\u0307Irsoy & Cardie, 2014; Wieting et al., 2015),\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 147
                            }
                        ],
                        "text": "\u2026prior work, we experimented with recursive neural networks on binarized parses of the PPDB (Wieting et al., 2015), but we found that many of the phrases in PPDB are not sentences or even constituents, causing the parser to have unexpected behavior.\nwhereWp is the projection matrix andb is a\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 149
                            }
                        ],
                        "text": "\u2026al., 2015), recursive networks based on non-syntactic hierarchical structure (Zhao et al., 2015; Chen et al., 2015b), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014; Yin & Schu\u0308tze, 2015; He et al., 2015), and recu rent neural networks using long short-term\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 33
                            }
                        ],
                        "text": "In order to better understand how these architectures compare, we conduct further experiments on three supervised NLP tasks: sentence similarity, entailment, and sentiment classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1306065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27725a2d2a8cee9bf9fffc6c2167017103aba0fa",
            "isKey": true,
            "numCitedBy": 2991,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline."
            },
            "slug": "A-Convolutional-Neural-Network-for-Modelling-Kalchbrenner-Grefenstette",
            "title": {
                "fragments": [],
                "text": "A Convolutional Neural Network for Modelling Sentences"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) is described that is adopted for the semantic modelling of sentences and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49298465"
                        ],
                        "name": "Jiwei Li",
                        "slug": "Jiwei-Li",
                        "structuredName": {
                            "firstName": "Jiwei",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiwei Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707242"
                        ],
                        "name": "Minh-Thang Luong",
                        "slug": "Minh-Thang-Luong",
                        "structuredName": {
                            "firstName": "Minh-Thang",
                            "lastName": "Luong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minh-Thang Luong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746807"
                        ],
                        "name": "Dan Jurafsky",
                        "slug": "Dan-Jurafsky",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Jurafsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 220
                            }
                        ],
                        "text": "One approach is to train an autoencoder in an attempt to learnthe latent structure of the sequence, whether it be a sentence with a parse tree (Socher et al., 2011), or a longer sequence such as a paragraph or document (Li et al., 2015b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 55
                            }
                        ],
                        "text": "Models in related work such as Kiros et al. (2015) and Li et al.(2015a) require significant training time on GPUs, on the order of multiple weeks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 179
                            }
                        ],
                        "text": "\u2026initialized weight matrices and nonlinear activations:\nht = f(WxW xt w +Whht\u22121 + b)\ngRNN(x) = h\u22121\nwheref is the activation function (eithertanh or rectified linear unit; the choice is tuned),Wx and Wh are parameter matrices,b is a bias vector, andh\u22121 refers to the hidden vector of the last token."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b21c78a62fbb945a19ae9a8935933711647e7d70",
            "isKey": false,
            "numCitedBy": 548,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models. In this paper, we explore an important step toward this generation task: training an LSTM (Longshort term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs. We introduce an LSTM model that hierarchically builds an embedding for a paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph. We evaluate the reconstructed paragraph using standard metrics like ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization1."
            },
            "slug": "A-Hierarchical-Neural-Autoencoder-for-Paragraphs-Li-Luong",
            "title": {
                "fragments": [],
                "text": "A Hierarchical Neural Autoencoder for Paragraphs and Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper introduces an LSTM model that hierarchically builds an embedding for a paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph and evaluates the reconstructed paragraph using standard metrics to show that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40150953"
                        ],
                        "name": "E. Huang",
                        "slug": "E.-Huang",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Huang",
                            "middleNames": [
                                "Hsin-Chun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143845796"
                        ],
                        "name": "Jeffrey Pennington",
                        "slug": "Jeffrey-Pennington",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Pennington",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey Pennington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 247
                            }
                        ],
                        "text": "\u2026\u2016Wc\u2016 2 + \u03bbw \u2016Wwinitial \u2212Ww\u2016 2 (1)\nwhereg is the embedding function in use (e.g.,LSTM), \u03b4 is the margin,\u03bbc and\u03bbw are regularization parameters,Wwinitial is the initial word embedding matrix, andt1 andt2 are carefully-selected negative examples taken from a mini-batch during optimization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 227
                            }
                        ],
                        "text": "\u2026have developed and shared word embeddings trained on large datasets (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014), and these have been used effectively for many downstream tasks (Turian et al., 2010; Socher et al., 2011; Kim, 2014; Bansal et al., 2014; Tai et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 144
                            }
                        ],
                        "text": "\u2026(Mitchell & Lapata, 2010; Yu & Dredze, 2015; Iyyer et al., 2015) to those based on richly-structured functions like recursive neural networks (Socher et al., 2011), convolutional neural networks (Kalchbrenner et al., 2014), and recurrent neural networks using long short-termmemory (LSTM)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "\u2026averaging networks (DANs) (Iyyer et al., 2015), feature-weighted aver ging (Yu & Dredze, 2015), recursive neural networks based on parse structure (Socher et al., 2011; 2; 2013;\u0307Irsoy & Cardie, 2014; Wieting et al., 2015), recursive networks based on non-syntactic hierarchical structure (Zhao et\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 144
                            }
                        ],
                        "text": "One approach is to train an autoencoder in an attempt to learnthe latent structure of the sequence, whether it be a sentence with a parse tree (Socher et al., 2011), or a longer sequence such as a paragraph or document (Li et al., 2015b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 106
                            }
                        ],
                        "text": "We compare six compositional architectures, evaluating them on annotated textual similarity datasets drawn both from the same distribu ion as the training data and from a wide range of other domains."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6979578,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ae5e6c6f5513613a161b2c85563f9708bf2e9178",
            "isKey": true,
            "numCitedBy": 888,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Paraphrase detection is the task of examining two sentences and determining whether they have the same meaning. In order to obtain high accuracy on this task, thorough syntactic and semantic analysis of the two statements is needed. We introduce a method for paraphrase detection based on recursive autoencoders (RAE). Our unsupervised RAEs are based on a novel unfolding objective and learn feature vectors for phrases in syntactic trees. These features are used to measure the word- and phrase-wise similarity between two sentences. Since sentences may be of arbitrary length, the resulting matrix of similarity measures is of variable size. We introduce a novel dynamic pooling layer which computes a fixed-sized representation from the variable-sized matrices. The pooled representation is then used as input to a classifier. Our method outperforms other state-of-the-art approaches on the challenging MSRP paraphrase corpus."
            },
            "slug": "Dynamic-Pooling-and-Unfolding-Recursive-for-Socher-Huang",
            "title": {
                "fragments": [],
                "text": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work introduces a method for paraphrase detection based on recursive autoencoders (RAE) and unsupervised RAEs based on a novel unfolding objective and learns feature vectors for phrases in syntactic trees to measure word- and phrase-wise similarity between two sentences."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977268"
                        ],
                        "name": "Mohit Bansal",
                        "slug": "Mohit-Bansal",
                        "structuredName": {
                            "firstName": "Mohit",
                            "lastName": "Bansal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohit Bansal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700980"
                        ],
                        "name": "Kevin Gimpel",
                        "slug": "Kevin-Gimpel",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Gimpel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Gimpel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2924113"
                        ],
                        "name": "Karen Livescu",
                        "slug": "Karen-Livescu",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Livescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Livescu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 138
                            }
                        ],
                        "text": "We compare six compositional architectures, evaluating them on annotated textual similarity datasets drawn both from the same distribu ion as the training data and from a wide range of other domains."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 278,
                                "start": 259
                            }
                        ],
                        "text": "\u2026have developed and shared word embeddings trained on large datasets (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014), and these have been used effectively for many downstream tasks (Turian et al., 2010; Socher et al., 2011; Kim, 2014; Bansal et al., 2014; Tai et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7803700,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9f91e7bac46b13444eddeb2438b01089e73b786",
            "isKey": false,
            "numCitedBy": 295,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Word representations have proven useful for many NLP tasks, e.g., Brown clusters as features in dependency parsing (Koo et al., 2008). In this paper, we investigate the use of continuous word representations as features for dependency parsing. We compare several popular embeddings to Brown clusters, via multiple types of features, in both news and web domains. We find that all embeddings yield significant parsing gains, including some recent ones that can be trained in a fraction of the time of others. Explicitly tailoring the representations for the task leads to further improvements. Moreover, an ensemble of all representations achieves the best results, suggesting their complementarity."
            },
            "slug": "Tailoring-Continuous-Word-Representations-for-Bansal-Gimpel",
            "title": {
                "fragments": [],
                "text": "Tailoring Continuous Word Representations for Dependency Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is found that all embeddings yield significant parsing gains, including some recent ones that can be trained in a fraction of the time of others, suggesting their complementarity."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153160559"
                        ],
                        "name": "Joseph P. Turian",
                        "slug": "Joseph-P.-Turian",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Turian",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph P. Turian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2335225"
                        ],
                        "name": "Lev-Arie Ratinov",
                        "slug": "Lev-Arie-Ratinov",
                        "structuredName": {
                            "firstName": "Lev-Arie",
                            "lastName": "Ratinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lev-Arie Ratinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 85
                            }
                        ],
                        "text": "We compare six compositional architectures, evaluating them on annotated textual similarity datasets drawn both from the same distribu ion as the training data and from a wide range of other domains."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 206
                            }
                        ],
                        "text": "\u2026have developed and shared word embeddings trained on large datasets (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014), and these have been used effectively for many downstream tasks (Turian et al., 2010; Socher et al., 2011; Kim, 2014; Bansal et al., 2014; Tai et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 629094,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dac72f2c509aee67524d3321f77e97e8eff51de6",
            "isKey": false,
            "numCitedBy": 2163,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/"
            },
            "slug": "Word-Representations:-A-Simple-and-General-Method-Turian-Ratinov",
            "title": {
                "fragments": [],
                "text": "Word Representations: A Simple and General Method for Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work evaluates Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeds of words on both NER and chunking, and finds that each of the three word representations improves the accuracy of these baselines."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2329943"
                        ],
                        "name": "Ozan Irsoy",
                        "slug": "Ozan-Irsoy",
                        "structuredName": {
                            "firstName": "Ozan",
                            "lastName": "Irsoy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ozan Irsoy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748501"
                        ],
                        "name": "Claire Cardie",
                        "slug": "Claire-Cardie",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cardie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Cardie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9792203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "60dda7f5efd67758bde1ee7f45e6d3ef86445495",
            "isKey": false,
            "numCitedBy": 265,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Recursive neural networks comprise a class of architecture that can operate on structured input. They have been previously successfully applied to model com-positionality in natural language using parse-tree-based structural representations. Even though these architectures are deep in structure, they lack the capacity for hierarchical representation that exists in conventional deep feed-forward networks as well as in recently investigated deep recurrent neural networks. In this work we introduce a new architecture \u2014 a deep recursive neural network (deep RNN) \u2014 constructed by stacking multiple recursive layers. We evaluate the proposed model on the task of fine-grained sentiment classification. Our results show that deep RNNs outperform associated shallow counterparts that employ the same number of parameters. Furthermore, our approach outperforms previous baselines on the sentiment analysis task, including a multiplicative RNN variant as well as the recently introduced paragraph vectors, achieving new state-of-the-art results. We provide exploratory analyses of the effect of multiple layers and show that they capture different aspects of compositionality in language."
            },
            "slug": "Deep-Recursive-Neural-Networks-for-Compositionality-Irsoy-Cardie",
            "title": {
                "fragments": [],
                "text": "Deep Recursive Neural Networks for Compositionality in Language"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The results show that deep RNNs outperform associated shallow counterparts that employ the same number of parameters and outperforms previous baselines on the sentiment analysis task, including a multiplicative RNN variant as well as the recently introduced paragraph vectors."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8421815"
                        ],
                        "name": "Kai Sheng Tai",
                        "slug": "Kai-Sheng-Tai",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Tai",
                            "middleNames": [
                                "Sheng"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Sheng Tai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 236
                            }
                        ],
                        "text": "\u2026structure (Zhao et al., 2015; Chen et al., 2015b), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014; Yin & Schu\u0308tze, 2015; He et al., 2015), and recu rent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu etal., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 281
                            }
                        ],
                        "text": "\u2026& Lapata, 2010; Yu & Dredze, 2015; Iyyer et al., 2015) to those based on richly-structured functions like recursive neural networks (Socher et al., 2011), convolutional neural networks (Kalchbrenner et al., 2014), and recurrent neural networks using long short-termmemory (LSTM) (Tai et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 280
                            }
                        ],
                        "text": "\u2026have developed and shared word embeddings trained on large datasets (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014), and these have been used effectively for many downstream tasks (Turian et al., 2010; Socher et al., 2011; Kim, 2014; Bansal et al., 2014; Tai et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 146
                            }
                        ],
                        "text": "\u2026model is a standard recurrent network (RNN) with randomly initialized weight matrices and nonlinear activations:\nht = f(WxW xt w +Whht\u22121 + b)\ngRNN(x) = h\u22121\nwheref is the activation function (eithertanh or rectified linear unit; the choice is tuned),Wx and Wh are parameter matrices,b is a bias\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 72
                            }
                        ],
                        "text": "For the SICK similarity task, we minimize the objective function14 from Tai et al. (2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 23
                            }
                        ],
                        "text": "Secondly, we note that Tai et al. (2015) had a similar LSTM result on the SICK dataset (Pearson\u2019sr of 85.28 to our 85.45) to show that our LSTM implementation/tu ing procedure is able to match or exceed performance of another published LSTM result."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 148
                            }
                        ],
                        "text": "Recent strong results on the SICK similarity task include 86.86 using a convolutional neural network (He et al., 2015) and 86.76 using a tree-LSTM (Tai et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 159
                            }
                        ],
                        "text": "We compare six compositional architectures, evaluating them on annotated textual similarity datasets drawn both from the same distribu ion as the training data and from a wide range of other domains."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Interestingly, it seems that regularizingto our universal parameters significantly improves results for the similarity and entailment tasks which are competitive or better than the state-of-the-art, but harms the LSTM\u2019s performance on the sentiment classification task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3033526,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32de44f01a96d4473d21099d15e25bc2b9f08e2f",
            "isKey": true,
            "numCitedBy": 2501,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank)."
            },
            "slug": "Improved-Semantic-Representations-From-Long-Memory-Tai-Socher",
            "title": {
                "fragments": [],
                "text": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Tree-LSTM is introduced, a generalization of LSTMs to tree-structured network topologies that outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences and sentiment classification."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3017324"
                        ],
                        "name": "Katja Filippova",
                        "slug": "Katja-Filippova",
                        "structuredName": {
                            "firstName": "Katja",
                            "lastName": "Filippova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katja Filippova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727837"
                        ],
                        "name": "Enrique Alfonseca",
                        "slug": "Enrique-Alfonseca",
                        "structuredName": {
                            "firstName": "Enrique",
                            "lastName": "Alfonseca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Enrique Alfonseca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39973883"
                        ],
                        "name": "Carlos A. Colmenares",
                        "slug": "Carlos-A.-Colmenares",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Colmenares",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos A. Colmenares"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We tune over the strategy in our experiments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 130
                            }
                        ],
                        "text": "LSTMs have recently been shown to produce state-of-the-art results in a variety of sequence processing tasks (Chen et al., 2015a; Filippova et al., 2015; Xu et al., 2015c; Belinkov & Glass, 2015; Wang & Nyberg, 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1992250,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b064b714107ffc724e0d477f4083e9e507c22fec",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an LSTM approach to deletion-based sentence compression where the task is to translate a sentence into a sequence of zeros and ones, corresponding to token deletion decisions. We demonstrate that even the most basic version of the system, which is given no syntactic information (no PoS or NE tags, or dependencies) or desired compression length, performs surprisingly well: around 30% of the compressions from a large test set could be regenerated. We compare the LSTM system with a competitive baseline which is trained on the same amount of data but is additionally provided with all kinds of linguistic features. In an experiment with human raters the LSTMbased model outperforms the baseline achieving 4.5 in readability and 3.8 in informativeness."
            },
            "slug": "Sentence-Compression-by-Deletion-with-LSTMs-Filippova-Alfonseca",
            "title": {
                "fragments": [],
                "text": "Sentence Compression by Deletion with LSTMs"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is demonstrated that even the most basic version of the LSTM system, given no syntactic information or desired compression length, performs surprisingly well: around 30% of the compressions from a large test set could be regenerated."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733049"
                        ],
                        "name": "Eneko Agirre",
                        "slug": "Eneko-Agirre",
                        "structuredName": {
                            "firstName": "Eneko",
                            "lastName": "Agirre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eneko Agirre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2271847"
                        ],
                        "name": "Carmen Banea",
                        "slug": "Carmen-Banea",
                        "structuredName": {
                            "firstName": "Carmen",
                            "lastName": "Banea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carmen Banea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748501"
                        ],
                        "name": "Claire Cardie",
                        "slug": "Claire-Cardie",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cardie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Cardie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46724030"
                        ],
                        "name": "Daniel Matthew Cer",
                        "slug": "Daniel-Matthew-Cer",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Cer",
                            "middleNames": [
                                "Matthew"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Matthew Cer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700007"
                        ],
                        "name": "Mona T. Diab",
                        "slug": "Mona-T.-Diab",
                        "structuredName": {
                            "firstName": "Mona",
                            "lastName": "Diab",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mona T. Diab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403836100"
                        ],
                        "name": "A. Gonzalez-Agirre",
                        "slug": "A.-Gonzalez-Agirre",
                        "structuredName": {
                            "firstName": "Aitor",
                            "lastName": "Gonzalez-Agirre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gonzalez-Agirre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2579583"
                        ],
                        "name": "Weiwei Guo",
                        "slug": "Weiwei-Guo",
                        "structuredName": {
                            "firstName": "Weiwei",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiwei Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145557251"
                        ],
                        "name": "Rada Mihalcea",
                        "slug": "Rada-Mihalcea",
                        "structuredName": {
                            "firstName": "Rada",
                            "lastName": "Mihalcea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rada Mihalcea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785173"
                        ],
                        "name": "German Rigau",
                        "slug": "German-Rigau",
                        "structuredName": {
                            "firstName": "German",
                            "lastName": "Rigau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "German Rigau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144120827"
                        ],
                        "name": "J. Wiebe",
                        "slug": "J.-Wiebe",
                        "structuredName": {
                            "firstName": "Janyce",
                            "lastName": "Wiebe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wiebe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 144
                            }
                        ],
                        "text": "For more details on these tasks please refer to the relevant publications for the 2012 (Agirre et al., 2012), 2013 (Agirre et al., 2013), 2014 (Agirre et al., 2014), and 2015 (Agirre et al., 2015) tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11650107,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16084914bc3729f86f46ac6267ea7a42e7951d41",
            "isKey": false,
            "numCitedBy": 357,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In Semantic Textual Similarity, systems rate the degree of semantic equivalence between two text snippets. This year, the participants were challenged with new data sets for English, as well as the introduction of Spanish, as a new language in which to assess semantic similarity. For the English subtask, we exposed the systems to a diversity of testing scenarios, by preparing additional OntoNotesWordNet sense mappings and news headlines, as well as introducing new genres, including image descriptions, DEFT discussion forums, DEFT newswire, and tweet-newswire headline mappings. For Spanish, since, to our knowledge, this is the first time that official evaluations are conducted, we used well-formed text, by featuring sentences extracted from encyclopedic content and newswire. The annotations for both tasks leveraged crowdsourcing. The Spanish subtask engaged 9 teams participating with 22 system runs, and the English subtask attracted 15 teams with 38 system runs."
            },
            "slug": "SemEval-2014-Task-10:-Multilingual-Semantic-Textual-Agirre-Banea",
            "title": {
                "fragments": [],
                "text": "SemEval-2014 Task 10: Multilingual Semantic Textual Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This year, the participants were challenged with new data sets for English, as well as the introduction of Spanish, as a new language in which to assess semantic similarity, and the annotations for both tasks leveraged crowdsourcing."
            },
            "venue": {
                "fragments": [],
                "text": "*SEMEVAL"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144629034"
                        ],
                        "name": "Di Wang",
                        "slug": "Di-Wang",
                        "structuredName": {
                            "firstName": "Di",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Di Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144287919"
                        ],
                        "name": "Eric Nyberg",
                        "slug": "Eric-Nyberg",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Nyberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Nyberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We tune over the strategy in our experiments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 196
                            }
                        ],
                        "text": "LSTMs have recently been shown to produce state-of-the-art results in a variety of sequence processing tasks (Chen et al., 2015a; Filippova et al., 2015; Xu et al., 2015c; Belinkov & Glass, 2015; Wang & Nyberg, 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2654931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "828dbeb7cf922dc9b6657dd169b8d26d2b58eedb",
            "isKey": false,
            "numCitedBy": 325,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present an approach that address the answer sentence selection problem for question answering. The proposed method uses a stacked bidirectional Long-Short Term Memory (BLSTM) network to sequentially read words from question and answer sentences, and then outputs their relevance scores. Unlike prior work, this approach does not require any syntactic parsing or external knowledge resources such as WordNet which may not be available in some domains or languages. The full system is based on a combination of the stacked BLSTM relevance model and keywords matching. The results of our experiments on a public benchmark dataset from TREC show that our system outperforms previous work which requires syntactic features and external knowledge resources."
            },
            "slug": "A-Long-Short-Term-Memory-Model-for-Answer-Sentence-Wang-Nyberg",
            "title": {
                "fragments": [],
                "text": "A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The proposed method uses a stacked bidirectional Long-Short Term Memory network to sequentially read words from question and answer sentences, and then outputs their relevance scores, which outperforms previous work which requires syntactic features and external knowledge resources."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055010814"
                        ],
                        "name": "Ang Lu",
                        "slug": "Ang-Lu",
                        "structuredName": {
                            "firstName": "Ang",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ang Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702290"
                        ],
                        "name": "Weiran Wang",
                        "slug": "Weiran-Wang",
                        "structuredName": {
                            "firstName": "Weiran",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiran Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977268"
                        ],
                        "name": "Mohit Bansal",
                        "slug": "Mohit-Bansal",
                        "structuredName": {
                            "firstName": "Mohit",
                            "lastName": "Bansal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohit Bansal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700980"
                        ],
                        "name": "Kevin Gimpel",
                        "slug": "Kevin-Gimpel",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Gimpel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Gimpel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2924113"
                        ],
                        "name": "Karen Livescu",
                        "slug": "Karen-Livescu",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Livescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Livescu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 228
                            }
                        ],
                        "text": "\u2026input representations in order to maximize similarity in the learned space (Weston et al., 2010; Yih et al., 2011; Huang et al., 2013; Hermann & Blunsom, 2014; Socher et al., 2014; Faruqui & Dyer, 2014; Bordes et al., 2014b;a; Lu et al., 2015), including our priorw rk (Wieting et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The intuition is that we want the two phrases to be more similar to each other (cos g(x1), g(x2))) than either is to their respective negative examplest1 andt2, by a margin of at least\u03b4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 874413,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "389ab0215cfd94b73040a5f2d5c84b10162f4567",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Word embeddings have been found useful for many NLP tasks, including part-of-speech tagging, named entity recognition, and parsing. Adding multilingual context when learning embeddings can improve their quality, for example via canonical correlation analysis (CCA) on embeddingsfromtwo languages. In this paper, we extend this idea to learn deep non-linear transformations of word embeddings of the two languages, using the recently proposed deep canonical correlation analysis. The resulting embeddings, when evaluated on multiple word and bigram similarity tasks, consistently improve over monolingual embeddings and over embeddings transformed with linear CCA."
            },
            "slug": "Deep-Multilingual-Correlation-for-Improved-Word-Lu-Wang",
            "title": {
                "fragments": [],
                "text": "Deep Multilingual Correlation for Improved Word Embeddings"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Deep non-linear transformations of word embeddings of the two languages are learned, using the recently proposed deep canonical correlation analysis, to improve their quality and consistency on multiple word and bigram similarity tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30424564"
                        ],
                        "name": "W. Xu",
                        "slug": "W.-Xu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763608"
                        ],
                        "name": "Chris Callison-Burch",
                        "slug": "Chris-Callison-Burch",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Callison-Burch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Callison-Burch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "83415753"
                        ],
                        "name": "W. Dolan",
                        "slug": "W.-Dolan",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Dolan",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Dolan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 154
                            }
                        ],
                        "text": "LSTMs have recently been shown to produce state-of-the-art results in a variety of sequence processing tasks (Chen et al., 2015a; Filippova et al., 2015; Xu et al., 2015c; Belinkov & Glass, 2015; Wang & Nyberg, 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 221
                            }
                        ],
                        "text": "\u2026variants, culminating in LSTMs because they have been foundto be effective for many types of sequential data (Graves et al., 2008; 2013; Greff et al., 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 51
                            }
                        ],
                        "text": "We also evaluate on the SemEval 2015 Twitter task (Xu et al., 2015b) and the SemEval 2014 Semantic Relatedness task (Marelli et al., 2014), as well as two tasks that use PPDB data(Wieting et al., 2015; Pavlick et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15213264,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "c75cdf042e42dae19399d393b5f3d24c10054a10",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "In this shared task, we present evaluations on two related tasks Paraphrase Identification (PI) and Semantic Textual Similarity (SS) systems for the Twitter data. Given a pair of sentences, participants are asked to produce a binary yes/no judgement or a graded score to measure their semantic equivalence. The task features a newly constructed Twitter Paraphrase Corpus that contains 18,762 sentence pairs. A total of 19 teams participated, submitting 36 runs to the PI task and 26 runs to the SS task. The evaluation shows encouraging results and open challenges for future research. The best systems scored a F1-measure of 0.674 for the PI task and a Pearson correlation of 0.619 for the SS task respectively, comparing to a strong baseline using logistic regression model of 0.589 F1 and 0.511 Pearson; while the best SS systems can often reach >0.80 Pearson on well-formed text. This shared task also provides insights into the relation between the PI and SS tasks and suggests the importance to bringing these two research areas together. We make all the data, baseline systems and evaluation scripts publicly available. 1"
            },
            "slug": "SemEval-2015-Task-1:-Paraphrase-and-Semantic-in-Xu-Callison-Burch",
            "title": {
                "fragments": [],
                "text": "SemEval-2015 Task 1: Paraphrase and Semantic Similarity in Twitter (PIT)"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "In this shared task, evaluations on two related tasks Paraphrase Identification and Semantic Textual Similarity (SS) systems for the Twitter data are presented and the importance to bringing these two research areas together is suggested."
            },
            "venue": {
                "fragments": [],
                "text": "*SEMEVAL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713934"
                        ],
                        "name": "Antoine Bordes",
                        "slug": "Antoine-Bordes",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Bordes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoine Bordes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746841"
                        ],
                        "name": "Nicolas Usunier",
                        "slug": "Nicolas-Usunier",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Usunier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Usunier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1849689,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a584211768d49f80192f13b8ed2fda9c058dec34",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Building computers able to answer questions on any subject is a long standing goal of artificial intelligence. Promising progress has recently been achieved by methods that learn to map questions to logical forms or database queries. Such approaches can be effective but at the cost of either large amounts of human-labeled data or by defining lexicons and grammars tailored by practitioners. In this paper, we instead take the radical approach of learning to map questions to vectorial feature representations. By mapping answers into the same space one can query any knowledge base independent of its schema, without requiring any grammar or lexicon. Our method is trained with a new optimization procedure combining stochastic gradient descent followed by a fine-tuning step using the weak supervision provided by blending automatically and collaboratively generated resources. We empirically demonstrate that our model can capture meaningful signals from its noisy supervision leading to major improvements over paralex, the only existing method able to be trained on similar weakly labeled data."
            },
            "slug": "Open-Question-Answering-with-Weakly-Supervised-Bordes-Weston",
            "title": {
                "fragments": [],
                "text": "Open Question Answering with Weakly Supervised Embedding Models"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper empirically demonstrate that the model can capture meaningful signals from its noisy supervision leading to major improvements over paralex, the only existing method able to be trained on similar weakly labeled data."
            },
            "venue": {
                "fragments": [],
                "text": "ECML/PKDD"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354728"
                        ],
                        "name": "A. Karpathy",
                        "slug": "A.-Karpathy",
                        "structuredName": {
                            "firstName": "Andrej",
                            "lastName": "Karpathy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Karpathy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 161
                            }
                        ],
                        "text": "\u2026input representations in order to maximize similarity in the learned space (Weston et al., 2010; Yih et al., 2011; Huang et al., 2013; Hermann & Blunsom, 2014; Socher et al., 2014; Faruqui & Dyer, 2014; Bordes et al., 2014b;a; Lu et al., 2015), including our priorw rk (Wieting et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The intuition is that we want the two phrases to be more similar to each other (cos g(x1), g(x2))) than either is to their respective negative examplest1 andt2, by a margin of at least\u03b4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2317858,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ca7d208ff8d81377e0eaa9723820aeae7a7322d",
            "isKey": false,
            "numCitedBy": 785,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images. However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DT-RNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image."
            },
            "slug": "Grounded-Compositional-Semantics-for-Finding-and-Socher-Karpathy",
            "title": {
                "fragments": [],
                "text": "Grounded Compositional Semantics for Finding and Describing Images with Sentences"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The DT-RNN model, which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences, outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa."
            },
            "venue": {
                "fragments": [],
                "text": "Transactions of the Association for Computational Linguistics"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2482533"
                        ],
                        "name": "Mo Yu",
                        "slug": "Mo-Yu",
                        "structuredName": {
                            "firstName": "Mo",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mo Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782853"
                        ],
                        "name": "Mark Dredze",
                        "slug": "Mark-Dredze",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Dredze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Dredze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 281,
                                "start": 263
                            }
                        ],
                        "text": "\u2026work, we experimented with recursive neural networks on binarized parses of the PPDB (Wieting et al., 2015), but we found that many of the phrases in PPDB are not sentences or even constituents, causing the parser to have unexpected behavior.\nwhereWp is the projection matrix andb is a bias vector."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 146
                            }
                        ],
                        "text": "\u2026include neural bag-of-words models (Kalchbrenner et al., 2014), deep averaging networks (DANs) (Iyyer et al., 2015), feature-weighted aver ging (Yu & Dredze, 2015), recursive neural networks based on parse structure (Socher et al., 2011; 2; 2013;\u0307Irsoy & Cardie, 2014; Wieting et al., 2015),\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 146
                            }
                        ],
                        "text": "\u2026been proposed to model compositionality in such sequences, ranging from those based on simple operations like addition (Mitchell & Lapata, 2010; Yu & Dredze, 2015; Iyyer et al., 2015) to those based on richly-structured functions like recursive neural networks (Socher et al., 2011),\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7807920,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da0cc33fac4d926eaa61f86566572a4653b3e990",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Lexical embeddings can serve as useful representations for words for a variety of NLP tasks, but learning embeddings for phrases can be challenging. While separate embeddings are learned for each word, this is infeasible for every phrase. We construct phrase embeddings by learning how to compose word embeddings using features that capture phrase structure and context. We propose efficient unsupervised and task-specific learning objectives that scale our model to large datasets. We demonstrate improvements on both language modeling and several phrase semantic similarity tasks with various phrase lengths. We make the implementation of our model and the datasets available for general use."
            },
            "slug": "Learning-Composition-Models-for-Phrase-Embeddings-Yu-Dredze",
            "title": {
                "fragments": [],
                "text": "Learning Composition Models for Phrase Embeddings"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes efficient unsupervised and task-specific learning objectives that scale the model to large datasets and demonstrates improvements on both language modeling and several phrase semantic similarity tasks with various phrase lengths."
            },
            "venue": {
                "fragments": [],
                "text": "TACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48615144"
                        ],
                        "name": "Yan Xu",
                        "slug": "Yan-Xu",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38956216"
                        ],
                        "name": "Lili Mou",
                        "slug": "Lili-Mou",
                        "structuredName": {
                            "firstName": "Lili",
                            "lastName": "Mou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lili Mou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1410115257"
                        ],
                        "name": "Ge Li",
                        "slug": "Ge-Li",
                        "structuredName": {
                            "firstName": "Ge",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ge Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691213"
                        ],
                        "name": "Yunchuan Chen",
                        "slug": "Yunchuan-Chen",
                        "structuredName": {
                            "firstName": "Yunchuan",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunchuan Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1818378366"
                        ],
                        "name": "Hao Peng",
                        "slug": "Hao-Peng",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Peng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700880"
                        ],
                        "name": "Zhi Jin",
                        "slug": "Zhi-Jin",
                        "structuredName": {
                            "firstName": "Zhi",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi Jin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We tune over the strategy in our experiments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 154
                            }
                        ],
                        "text": "LSTMs have recently been shown to produce state-of-the-art results in a variety of sequence processing tasks (Chen et al., 2015a; Filippova et al., 2015; Xu et al., 2015c; Belinkov & Glass, 2015; Wang & Nyberg, 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 221
                            }
                        ],
                        "text": "\u2026variants, culminating in LSTMs because they have been foundto be effective for many types of sequential data (Graves et al., 2008; 2013; Greff et al., 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 51
                            }
                        ],
                        "text": "We also evaluate on the SemEval 2015 Twitter task (Xu et al., 2015b) and the SemEval 2014 Semantic Relatedness task (Marelli et al., 2014), as well as two tasks that use PPDB data(Wieting et al., 2015; Pavlick et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5403702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "032fbcb58e2282f02426a0f09c6d5b42787936ec",
            "isKey": true,
            "numCitedBy": 509,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Relation classification is an important research arena in the field of natural language processing (NLP). In this paper, we present SDP-LSTM, a novel neural network to classify the relation of two entities in a sentence. Our neural architecture leverages the shortest dependency path (SDP) between two entities; multichannel recurrent neural networks, with long short term memory (LSTM) units, pick up heterogeneous information along the SDP. Our proposed model has several distinct features: (1) The shortest dependency paths retain most relevant information (to relation classification), while eliminating irrelevant words in the sentence. (2) The multichannel LSTM networks allow effective information integration from heterogeneous sources over the dependency paths. (3) A customized dropout strategy regularizes the neural network to alleviate overfitting. We test our model on the SemEval 2010 relation classification task, and achieve an $F_1$-score of 83.7\\%, higher than competing methods in the literature."
            },
            "slug": "Classifying-Relations-via-Long-Short-Term-Memory-Xu-Mou",
            "title": {
                "fragments": [],
                "text": "Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This paper presents SDP-LSTM, a novel neural network to classify the relation of two entities in a sentence, which leverages the shortest dependency path (SDP) between two entities; multichannel recurrent neural networks, with long short term memory (L STM) units, pick up heterogeneous information along the SDP."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2407601,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee",
            "isKey": false,
            "numCitedBy": 7045,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks."
            },
            "slug": "Distributed-Representations-of-Sentences-and-Le-Mikolov",
            "title": {
                "fragments": [],
                "text": "Distributed Representations of Sentences and Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Paragraph Vector is an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents, and its construction gives the algorithm the potential to overcome the weaknesses of bag-of-words models."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111892870"
                        ],
                        "name": "Hua He",
                        "slug": "Hua-He",
                        "structuredName": {
                            "firstName": "Hua",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hua He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700980"
                        ],
                        "name": "Kevin Gimpel",
                        "slug": "Kevin-Gimpel",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Gimpel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Gimpel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145580839"
                        ],
                        "name": "Jimmy J. Lin",
                        "slug": "Jimmy-J.-Lin",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Lin",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy J. Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 158
                            }
                        ],
                        "text": "\u2026structure (Zhao et al., 2015; Chen et al., 2015b), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014; Yin & Schu\u0308tze, 2015; He et al., 2015), and recu rent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu etal., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 102
                            }
                        ],
                        "text": "Recent strong results on the SICK similarity task include 86.86 using a convolutional neural network (He et al., 2015) and 86.76 using a tree-LSTM (Tai et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We would also like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) and thank NVIDIA Corporation for donati g GPUs used in this research."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 42
                            }
                        ],
                        "text": "Our third model is the deep averaging network (DAN) of Iyyer et al. (2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We release all of our resources to the research community1 with the hope that they can serve as the new baseline for further work on universal sentence embeddings."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 71
                            }
                        ],
                        "text": "Our fourth model is a standard recurrent network (RNN) with randomly initialized weight matrices and nonlinear activations:\nht = f(WxW xt w +Whht\u22121 + b)\ngRNN(x) = h\u22121\nwheref is the activation function (eithertanh or rectified linear unit; the choice is tuned),Wx and Wh are parameter matrices,b is a\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Interestingly, it seems that regularizingto our universal parameters significantly improves results for the similarity and entailment tasks which are competitive or better than the state-of-the-art, but harms the LSTM\u2019s performance on the sentiment classification task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 94
                            }
                        ],
                        "text": "However, in out-of-domain scenarios, simple architectures such as word averaging vastly outperformLSTMs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7413367,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ae3e2451491f7d6ea1ee0c587fd8c811b4200c07",
            "isKey": false,
            "numCitedBy": 340,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling sentence similarity is complicated by the ambiguity and variability of linguistic expression. To cope with these challenges, we propose a model for comparing sentences that uses a multiplicity of perspectives. We first model each sentence using a convolutional neural network that extracts features at multiple levels of granularity and uses multiple types of pooling. We then compare our sentence representations at several granularities using multiple similarity metrics. We apply our model to three tasks, including the Microsoft Research paraphrase identification task and two SemEval semantic textual similarity tasks. We obtain strong performance on all tasks, rivaling or exceeding the state of the art without using external resources such as WordNet or parsers."
            },
            "slug": "Multi-Perspective-Sentence-Similarity-Modeling-with-He-Gimpel",
            "title": {
                "fragments": [],
                "text": "Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a model for comparing sentences that uses a multiplicity of perspectives, first model each sentence using a convolutional neural network that extracts features at multiple levels of granularity and uses multiple types of pooling."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2910877"
                        ],
                        "name": "K. Hermann",
                        "slug": "K.-Hermann",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Hermann",
                            "middleNames": [
                                "Moritz"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685771"
                        ],
                        "name": "P. Blunsom",
                        "slug": "P.-Blunsom",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Blunsom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Blunsom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The intuition is that we want the two phrases to be more similar to each other (cos g(x1), g(x2))) than either is to their respective negative examplest1 andt2, by a margin of at least\u03b4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17589422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b75d707eb3ffe4607c8cdd5436c8d7f8573fed9",
            "isKey": false,
            "numCitedBy": 300,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel technique for learning semantic representations, which extends the distributional hypothesis to multilingual data and joint-space embeddings. Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences, while maintaining sufficient distance between those of dissimilar sentences. The models do not rely on word alignments or any syntactic information and are successfully applied to a number of diverse languages. We extend our approach to learn semantic representations at the document level, too. We evaluate these models on two cross-lingual document classification tasks, outperforming the prior state of the art. Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic relationships across languages without parallel data."
            },
            "slug": "Multilingual-Models-for-Compositional-Distributed-Hermann-Blunsom",
            "title": {
                "fragments": [],
                "text": "Multilingual Models for Compositional Distributed Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A novel technique for learning semantic representations, which extends the distributional hypothesis to multilingual data and joint-space embeddings and demonstrates that these representations are semantically plausible and can capture semantic relationships across languages without parallel data."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733049"
                        ],
                        "name": "Eneko Agirre",
                        "slug": "Eneko-Agirre",
                        "structuredName": {
                            "firstName": "Eneko",
                            "lastName": "Agirre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eneko Agirre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2271847"
                        ],
                        "name": "Carmen Banea",
                        "slug": "Carmen-Banea",
                        "structuredName": {
                            "firstName": "Carmen",
                            "lastName": "Banea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carmen Banea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748501"
                        ],
                        "name": "Claire Cardie",
                        "slug": "Claire-Cardie",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cardie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Cardie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46724030"
                        ],
                        "name": "Daniel Matthew Cer",
                        "slug": "Daniel-Matthew-Cer",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Cer",
                            "middleNames": [
                                "Matthew"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Matthew Cer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700007"
                        ],
                        "name": "Mona T. Diab",
                        "slug": "Mona-T.-Diab",
                        "structuredName": {
                            "firstName": "Mona",
                            "lastName": "Diab",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mona T. Diab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403836100"
                        ],
                        "name": "A. Gonzalez-Agirre",
                        "slug": "A.-Gonzalez-Agirre",
                        "structuredName": {
                            "firstName": "Aitor",
                            "lastName": "Gonzalez-Agirre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gonzalez-Agirre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2579583"
                        ],
                        "name": "Weiwei Guo",
                        "slug": "Weiwei-Guo",
                        "structuredName": {
                            "firstName": "Weiwei",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiwei Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405529953"
                        ],
                        "name": "I. Lopez-Gazpio",
                        "slug": "I.-Lopez-Gazpio",
                        "structuredName": {
                            "firstName": "I\u00f1igo",
                            "lastName": "Lopez-Gazpio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Lopez-Gazpio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2156201"
                        ],
                        "name": "M. Maritxalar",
                        "slug": "M.-Maritxalar",
                        "structuredName": {
                            "firstName": "Montse",
                            "lastName": "Maritxalar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maritxalar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145557251"
                        ],
                        "name": "Rada Mihalcea",
                        "slug": "Rada-Mihalcea",
                        "structuredName": {
                            "firstName": "Rada",
                            "lastName": "Mihalcea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rada Mihalcea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785173"
                        ],
                        "name": "German Rigau",
                        "slug": "German-Rigau",
                        "structuredName": {
                            "firstName": "German",
                            "lastName": "Rigau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "German Rigau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2962373"
                        ],
                        "name": "L. Uria",
                        "slug": "L.-Uria",
                        "structuredName": {
                            "firstName": "Larraitz",
                            "lastName": "Uria",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Uria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144120827"
                        ],
                        "name": "J. Wiebe",
                        "slug": "J.-Wiebe",
                        "structuredName": {
                            "firstName": "Janyce",
                            "lastName": "Wiebe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wiebe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 176
                            }
                        ],
                        "text": "For more details on these tasks please refer to the relevant publications for the 2012 (Agirre et al., 2012), 2013 (Agirre et al., 2013), 2014 (Agirre et al., 2014), and 2015 (Agirre et al., 2015) tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11879061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6346f4fb03a25ff3a4bd887ba45d627a01983ea4",
            "isKey": false,
            "numCitedBy": 371,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In semantic textual similarity (STS), systems rate the degree of semantic equivalence between two text snippets. This year, the participants were challenged with new datasets in English and Spanish. The annotations for both subtasks leveraged crowdsourcing. The English subtask attracted 29 teams with 74 system runs, and the Spanish subtask engaged 7 teams participating with 16 system runs. In addition, this year we ran a pilot task on interpretable STS, where the systems needed to add an explanatory layer, that is, they had to align the chunks in the sentence pair, explicitly annotating the kind of relation and the score of the chunk pair. The train and test data were manually annotated by an expert, and included headline and image sentence pairs from previous years. 7 teams participated with 29 runs."
            },
            "slug": "SemEval-2015-Task-2:-Semantic-Textual-Similarity,-Agirre-Banea",
            "title": {
                "fragments": [],
                "text": "SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This year, the participants were challenged with new datasets in English and Spanish, and the annotations for both subtasks leveraged crowdsourcing, and a pilot task on interpretable STS, where systems needed to add an explanatory layer."
            },
            "venue": {
                "fragments": [],
                "text": "SemEval@NAACL-HLT"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51209090"
                        ],
                        "name": "H. Zhao",
                        "slug": "H.-Zhao",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11955007"
                        ],
                        "name": "Zhengdong Lu",
                        "slug": "Zhengdong-Lu",
                        "structuredName": {
                            "firstName": "Zhengdong",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhengdong Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807041"
                        ],
                        "name": "P. Poupart",
                        "slug": "P.-Poupart",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Poupart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Poupart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 122
                            }
                        ],
                        "text": "This is a generalization of the above models that typically uses multiple layers as well as nonlinear activation functions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 142
                            }
                        ],
                        "text": "\u2026(Socher et al., 2011; 2; 2013;\u0307Irsoy & Cardie, 2014; Wieting et al., 2015), recursive networks based on non-syntactic hierarchical structure (Zhao et al., 2015; Chen et al., 2015b), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014; Yin & Schu\u0308tze, 2015;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14182215,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d41cfe9b2ada4e09d53262bc75c473d8043936fc",
            "isKey": false,
            "numCitedBy": 174,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability to accurately model a sentence at varying stages (e.g., word-phrase-sentence) plays a central role in natural language processing. As an effort towards this goal we propose a self-adaptive hierarchical sentence model (AdaSent). AdaSent effectively forms a hierarchy of representations from words to phrases and then to sentences through recursive gated local composition of adjacent segments. We design a competitive mechanism (through gating networks) to allow the representations of the same sentence to be engaged in a particular learning task (e.g., classification), therefore effectively mitigating the gradient vanishing problem persistent in other recursive models. Both qualitative and quantitative analysis shows that AdaSent can automatically form and select the representations suitable for the task at hand during training, yielding superior classification performance over competitor models on 5 benchmark data sets."
            },
            "slug": "Self-Adaptive-Hierarchical-Sentence-Model-Zhao-Lu",
            "title": {
                "fragments": [],
                "text": "Self-Adaptive Hierarchical Sentence Model"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Both qualitative and quantitative analysis shows that AdaSent can automatically form and select the representations suitable for the task at hand during training, yielding superior classification performance over competitor models on 5 benchmark data sets."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3295137"
                        ],
                        "name": "William Blacoe",
                        "slug": "William-Blacoe",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Blacoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Blacoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 18
                            }
                        ],
                        "text": "Follow-up work by Blacoe & Lapata (2012) found again that simple operations such as vector ad-\n2We used the publicly available 300-dimensional vectors that were trained on the 840 billion token Common Crawl corpus, available athttp://nlp.stanford.edu/projects/glove/ ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11567084,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7988ef10dc9770e5fa4dc40d5d2f3693fd2ed917",
            "isKey": false,
            "numCitedBy": 279,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods. We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication. Some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora. We find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection. The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method."
            },
            "slug": "A-Comparison-of-Vector-based-Representations-for-Blacoe-Lapata",
            "title": {
                "fragments": [],
                "text": "A Comparison of Vector-based Representations for Semantic Composition"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "Shallow approaches to modeling compositional meaning for phrases and sentences using distributional methods are found to be as good as more computationally intensive alternatives with regards to two particular tests: phrase similarity and paraphrase detection."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40483594"
                        ],
                        "name": "Wenpeng Yin",
                        "slug": "Wenpeng-Yin",
                        "structuredName": {
                            "firstName": "Wenpeng",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenpeng Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 150
                            }
                        ],
                        "text": "\u2026hierarchical structure (Zhao et al., 2015; Chen et al., 2015b), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014; Yin & Schu\u0308tze, 2015; He et al., 2015), and recu rent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu etal.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 48
                            }
                        ],
                        "text": "Our fourth model is a standard recurrent network (RNN) with randomly initialized weight matrices and nonlinear activations:\nht = f(WxW xt w +Whht\u22121 + b)\ngRNN(x) = h\u22121\nwheref is the activation function (eithertanh or rectified linear unit; the choice is tuned),Wx and Wh are parameter matrices,b is a\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17578970,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1682b8b395c7d7fa30b3cec961ac81fdda53e72d",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new deep learning architecture Bi-CNN-MI for paraphrase identification (PI). Based on the insight that PI requires comparing two sentences on multiple levels of granularity, we learn multigranular sentence representations using convolutional neural network (CNN) and model interaction features at each level. These features are then the input to a logistic classifier for PI. All parameters of the model (for embeddings, convolution and classification) are directly optimized for PI. To address the lack of training data, we pretrain the network in a novel way using a language modeling task. Results on the MSRP corpus surpass that of previous NN competitors."
            },
            "slug": "Convolutional-Neural-Network-for-Paraphrase-Yin-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Convolutional Neural Network for Paraphrase Identification"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A new deep learning architecture Bi-CNN-MI for paraphrase identification based on the insight that PI requires comparing two sentences on multiple levels of granularity using convolutional neural network and model interaction features at each level is presented."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143845796"
                        ],
                        "name": "Jeffrey Pennington",
                        "slug": "Jeffrey-Pennington",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Pennington",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey Pennington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 113
                            }
                        ],
                        "text": "We compare to three baselines: skip-thoughtvectors10 (Kiros et al., 2015), denoted \u201cST\u201d, averaged GloVe11 vectors (Pennington et al., 2014), and averagedPARAGRAM-SL999 vectors (Wieting et al., 2015), denoted \u201cPSL\u201d."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 178
                            }
                        ],
                        "text": "C L]\n4 M\nar 2\n01 6\nWe consider the problem of learning general-purpose, paraphrastic sentence embeddings based on supervision from the Paraphrase Database(G nitkevitch et al., 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 135
                            }
                        ],
                        "text": "Several researchers have developed and shared word embeddings trained on large datasets (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014), and these have been used effectively for many downstream tasks (Turian et al., 2010; Socher et al., 2011; Kim, 2014; Bansal et al., 2014;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 207
                            }
                        ],
                        "text": "To address these questions, we\n13Available athttp://ttic.uchicago.edu/ \u02dc wieting .\nused three tasks: The SICK similarity task, the SICK entailment task, and the Stanford Sentiment Treebank (SST) binary classification task (Socher et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1957433,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
            "isKey": true,
            "numCitedBy": 22557,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition."
            },
            "slug": "GloVe:-Global-Vectors-for-Word-Representation-Pennington-Socher",
            "title": {
                "fragments": [],
                "text": "GloVe: Global Vectors for Word Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods and produces a vector space with meaningful substructure."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751569"
                        ],
                        "name": "Samy Bengio",
                        "slug": "Samy-Bengio",
                        "structuredName": {
                            "firstName": "Samy",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samy Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746841"
                        ],
                        "name": "Nicolas Usunier",
                        "slug": "Nicolas-Usunier",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Usunier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Usunier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 150
                            }
                        ],
                        "text": "\u2026learning formulation draws from a large body of related work on learning input representations in order to maximize similarity in the learned space (Weston et al., 2010; Yih et al., 2011; Huang et al., 2013; Hermann & Blunsom, 2014; Socher et al., 2014; Faruqui & Dyer, 2014; Bordes et al.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Our learning formulation draws from a large body of related work on learning input representations in order to maximize similarity in the learned space (Weston et al., 2010; Yih et al., 2011; Huang et al., 2013; Hermann & Blunsom, 2014; Socher et al., 2014; Faruqui & Dyer, 2014; Bordes et al., 2014b;a; Lu et al., 2015), including our prior work (Wieting et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7587705,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aea0f946e8dcddb65cc2e907456c42453f246a50",
            "isKey": false,
            "numCitedBy": 420,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Image annotation datasets are becoming larger and larger, with tens of millions of images and tens of thousands of possible annotations. We propose a strongly performing method that scales to such datasets by simultaneously learning to optimize precision at k of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations. Our method both outperforms several baseline methods and, in comparison to them, is faster and consumes less memory. We also demonstrate how our method learns an interpretable model, where annotations with alternate spellings or even languages are close in the embedding space. Hence, even when our model does not predict the exact annotation given by a human labeler, it often predicts similar annotations, a fact that we try to quantify by measuring the newly introduced \u201csibling\u201d precision metric, where our method also obtains excellent results."
            },
            "slug": "Large-scale-image-annotation:-learning\u00a0to\u00a0rank-Weston-Bengio",
            "title": {
                "fragments": [],
                "text": "Large scale image annotation: learning\u00a0to\u00a0rank with\u00a0joint word-image embeddings"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This work proposes a strongly performing method that scales to image annotation datasets by simultaneously learning to optimize precision at k of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46181066"
                        ],
                        "name": "Iz Beltagy",
                        "slug": "Iz-Beltagy",
                        "structuredName": {
                            "firstName": "Iz",
                            "lastName": "Beltagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iz Beltagy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144745718"
                        ],
                        "name": "Stephen Roller",
                        "slug": "Stephen-Roller",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Roller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Roller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2904366"
                        ],
                        "name": "Pengxiang Cheng",
                        "slug": "Pengxiang-Cheng",
                        "structuredName": {
                            "firstName": "Pengxiang",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pengxiang Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708114"
                        ],
                        "name": "Katrin Erk",
                        "slug": "Katrin-Erk",
                        "structuredName": {
                            "firstName": "Katrin",
                            "lastName": "Erk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katrin Erk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 57
                            }
                        ],
                        "text": "For entailment, the best result we are aware of is 85.1 (Beltagy et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We would also like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) and thank NVIDIA Corporation for donati g GPUs used in this research."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14392769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fca1e631b8f93036065311eb92727c509423475a",
            "isKey": true,
            "numCitedBy": 17,
            "numCiting": 125,
            "paperAbstract": {
                "fragments": [],
                "text": "NLP tasks differ in the semantic information they require, and at this time no single semantic representation fulfills all requirements. Logic-based representations characterize sentence structure, but do not capture the graded aspect of meaning. Distributional models give graded similarity ratings for words and phrases, but do not adequately capture overall sentence structure. So it has been argued that the two are complementary. In this paper, we adopt a hybrid approach that combines logic-based and distributional semantics through probabilistic logic inference in Markov Logic Networks (MLNs). We focus on textual entailment (RTE), a task that can utilize the strengths of both representations. Our system is three components, 1) parsing and task representation, where input RTE problems are represented in probabilistic logic. This is quite different from representing them in standard first-order logic. 2) knowledge base construction in the form of weighted inference rules from different sources like WordNet, paraphrase collections, and lexical and phrasal distributional rules generated on the fly. We use a variant of Robinson resolution to determine the necessary inference rules. More sources can easily be added by mapping them to logical rules; our system learns a resource-specific weight that counteract scaling differences between resources. 3) inference, where we show how to solve the inference problems efficiently. In this paper we focus on the SICK dataset, and we achieve a state-of-the-art result. Our system handles overall sentence structure and phenomena like negation in the logic, then uses our Robinson resolution variant to query distributional systems about words and short phrases. Therefor, we use our system to evaluate distributional lexical entailment approaches. We also publish the set of rules queried from the SICK dataset, which can be a good resource to evaluate them."
            },
            "slug": "Representing-Meaning-with-a-Combination-of-Logical-Beltagy-Roller",
            "title": {
                "fragments": [],
                "text": "Representing Meaning with a Combination of Logical Form and Vectors"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A hybrid approach that combines logic-based and distributional semantics through probabilistic logic inference in Markov Logic Networks (MLNs) is adopted, and a state-of-the-art result is achieved on the SICK dataset."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144256365"
                        ],
                        "name": "Tsung-Hsien Wen",
                        "slug": "Tsung-Hsien-Wen",
                        "structuredName": {
                            "firstName": "Tsung-Hsien",
                            "lastName": "Wen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Hsien Wen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768624"
                        ],
                        "name": "Milica Gasic",
                        "slug": "Milica-Gasic",
                        "structuredName": {
                            "firstName": "Milica",
                            "lastName": "Gasic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Milica Gasic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3334541"
                        ],
                        "name": "N. Mrksic",
                        "slug": "N.-Mrksic",
                        "structuredName": {
                            "firstName": "Nikola",
                            "lastName": "Mrksic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Mrksic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2131709"
                        ],
                        "name": "Pei-hao Su",
                        "slug": "Pei-hao-Su",
                        "structuredName": {
                            "firstName": "Pei-hao",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pei-hao Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92480907"
                        ],
                        "name": "David Vandyke",
                        "slug": "David-Vandyke",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Vandyke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Vandyke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145259603"
                        ],
                        "name": "S. Young",
                        "slug": "S.-Young",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Young",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Young"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We release all of our resources to the research community1 with the hope that they can serve as the new baseline for further work on universal sentence embeddings."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 280
                            }
                        ],
                        "text": "\u2026variants, culminating in LSTMs because they have been foundto be effective for many types of sequential data (Graves et al., 2008; 2013; Greff et al., 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 739696,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d4b46e545e1a3f6871b49cc69640ef2eb1a4654",
            "isKey": false,
            "numCitedBy": 756,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "\u00a9 2015 Association for Computational Linguistics. Natural language generation (NLG) is a critical component of spoken dialogue and it has a significant impact both on usability and perceived quality. Most NLG systems in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language. They are also not easily scaled to systems covering multiple domains and languages. This paper presents a statistical language generator based on a semantically controlled Long Short-term Memory (LSTM) structure. The LSTM generator can learn from unaligned data by jointly optimising sentence planning and surface realisation using a simple cross entropy training criterion, and language variation can be easily achieved by sampling from output candidates. With fewer heuristics, an objective evaluation in two differing test domains showed the proposed method improved performance compared to previous methods. Human judges scored the LSTM system higher on informativeness and naturalness and overall preferred it to the other systems.."
            },
            "slug": "Semantically-Conditioned-LSTM-based-Natural-for-Wen-Gasic",
            "title": {
                "fragments": [],
                "text": "Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A statistical language generator based on a semantically controlled Long Short-term Memory (LSTM) structure that can learn from unaligned data by jointly optimising sentence planning and surface realisation using a simple cross entropy training criterion, and language variation can be easily achieved by sampling from output candidates."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2570381"
                        ],
                        "name": "Brody Huval",
                        "slug": "Brody-Huval",
                        "structuredName": {
                            "firstName": "Brody",
                            "lastName": "Huval",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brody Huval"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 806709,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27e38351e48fe4b7da2775bf94341738bc4da07e",
            "isKey": false,
            "numCitedBy": 1265,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them."
            },
            "slug": "Semantic-Compositionality-through-Recursive-Spaces-Socher-Huval",
            "title": {
                "fragments": [],
                "text": "Semantic Compositionality through Recursive Matrix-Vector Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A recursive neural network model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length and can learn the meaning of operators in propositional logic and natural language is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144118452"
                        ],
                        "name": "Pengfei Liu",
                        "slug": "Pengfei-Liu",
                        "structuredName": {
                            "firstName": "Pengfei",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pengfei Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767521"
                        ],
                        "name": "Xipeng Qiu",
                        "slug": "Xipeng-Qiu",
                        "structuredName": {
                            "firstName": "Xipeng",
                            "lastName": "Qiu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xipeng Qiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3064800"
                        ],
                        "name": "Xinchi Chen",
                        "slug": "Xinchi-Chen",
                        "structuredName": {
                            "firstName": "Xinchi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinchi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112379225"
                        ],
                        "name": "Shiyu Wu",
                        "slug": "Shiyu-Wu",
                        "structuredName": {
                            "firstName": "Shiyu",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shiyu Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790227"
                        ],
                        "name": "Xuanjing Huang",
                        "slug": "Xuanjing-Huang",
                        "structuredName": {
                            "firstName": "Xuanjing",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuanjing Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6764076,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cdd2906f29d8103632dba24484571a8a05c09076",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural network based methods have obtained great progress on a variety of natural language processing tasks. However, it is still a challenge task to model long texts, such as sentences and documents. In this paper, we propose a multi-timescale long short-termmemory (MT-LSTM) neural network to model long texts. MTLSTM partitions the hidden states of the standard LSTM into several groups. Each group is activated at different time periods. Thus, MT-LSTM can model very long documents as well as short sentences. Experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task."
            },
            "slug": "Multi-Timescale-Long-Short-Term-Memory-Neural-for-Liu-Qiu",
            "title": {
                "fragments": [],
                "text": "Multi-Timescale Long Short-Term Memory Neural Network for Modelling Sentences and Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A multi-timescale long short-termmemory (MT-LSTM) neural network to model long texts that can model very long documents as well as short sentences is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3035541"
                        ],
                        "name": "Klaus Greff",
                        "slug": "Klaus-Greff",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Greff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Klaus Greff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2100612"
                        ],
                        "name": "R. Srivastava",
                        "slug": "R.-Srivastava",
                        "structuredName": {
                            "firstName": "Rupesh",
                            "lastName": "Srivastava",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Srivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2865775"
                        ],
                        "name": "J. Koutn\u00edk",
                        "slug": "J.-Koutn\u00edk",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Koutn\u00edk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Koutn\u00edk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714059"
                        ],
                        "name": "Bas R. Steunebrink",
                        "slug": "Bas-R.-Steunebrink",
                        "structuredName": {
                            "firstName": "Bas",
                            "lastName": "Steunebrink",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bas R. Steunebrink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 146
                            }
                        ],
                        "text": "\u2026network variants, culminating in LSTMs because they have been foundto be effective for many types of sequential data (Graves et al., 2008; 2013; Greff et al., 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We release all of our resources to the research community1 with the hope that they can serve as the new baseline for further work on universal sentence embeddings."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3356463,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081",
            "isKey": false,
            "numCitedBy": 3297,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Several variants of the long short-term memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful functional ANalysis Of VAriance framework. In total, we summarize the results of 5400 experimental runs ( $\\approx 15$  years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment."
            },
            "slug": "LSTM:-A-Search-Space-Odyssey-Greff-Srivastava",
            "title": {
                "fragments": [],
                "text": "LSTM: A Search Space Odyssey"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling, and observes that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks and Learning Systems"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2267212"
                        ],
                        "name": "Juri Ganitkevitch",
                        "slug": "Juri-Ganitkevitch",
                        "structuredName": {
                            "firstName": "Juri",
                            "lastName": "Ganitkevitch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juri Ganitkevitch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7536576"
                        ],
                        "name": "Benjamin Van Durme",
                        "slug": "Benjamin-Van-Durme",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Durme",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Van Durme"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763608"
                        ],
                        "name": "Chris Callison-Burch",
                        "slug": "Chris-Callison-Burch",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Callison-Burch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Callison-Burch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 142
                            }
                        ],
                        "text": "We consider six compositional architectures based on neural networks and train them on noisy phrase pairs from the Paraphrase Database (PPDB; Ganitkevitch et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 65
                            }
                        ],
                        "text": "This leads to performance rivaling the state of the art on the SICK similarity and entailment tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6067240,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef12383f516840ec1ec998cd5921dfc6e197c9b2",
            "isKey": false,
            "numCitedBy": 691,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the 1.0 release of our paraphrase database, PPDB. Its English portion, PPDB:Eng, contains over 220 million paraphrase pairs, consisting of 73 million phrasal and 8 million lexical paraphrases, as well as 140 million paraphrase patterns, which capture many meaning-preserving syntactic transformations. The paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion English words. We also release PPDB:Spa, a collection of 196 million Spanish paraphrases. Each paraphrase pair in PPDB contains a set of associated scores, including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the Google n-grams and the Annotated Gigaword corpus. Our release includes pruning tools that allow users to determine their own precision/recall tradeoff."
            },
            "slug": "PPDB:-The-Paraphrase-Database-Ganitkevitch-Durme",
            "title": {
                "fragments": [],
                "text": "PPDB: The Paraphrase Database"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "The 1.0 release of the paraphrase database, PPDB, contains over 220 million paraphrase pairs, consisting of 73 million phrasal and 8 million lexical paraphrases, as well as 140million paraphrase patterns, which capture many meaning-preserving syntactic transformations."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38367242"
                        ],
                        "name": "Yoon Kim",
                        "slug": "Yoon-Kim",
                        "structuredName": {
                            "firstName": "Yoon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoon Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 257,
                                "start": 248
                            }
                        ],
                        "text": "\u2026have developed and shared word embeddings trained on large datasets (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014), and these have been used effectively for many downstream tasks (Turian et al., 2010; Socher et al., 2011; Kim, 2014; Bansal et al., 2014; Tai et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 47
                            }
                        ],
                        "text": "On sentiment, the best previousresult is 88.1 (Kim, 2014), which our LSTM surprisingly outperforms by a significant margin."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 145
                            }
                        ],
                        "text": "\u2026based on non-syntactic hierarchical structure (Zhao et al., 2015; Chen et al., 2015b), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014; Yin & Schu\u0308tze, 2015; He et al., 2015), and recu rent neural networks using long short-term memory (Tai et al., 2015;\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We would also like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) and thank NVIDIA Corporation for donati g GPUs used in this research."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 127
                            }
                        ],
                        "text": "We compare six compositional architectures, evaluating them on annotated textual similarity datasets drawn both from the same distribu ion as the training data and from a wide range of other domains."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 15
                            }
                        ],
                        "text": "Our fourth model is a standard recurrent network (RNN) with randomly initialized weight matrices and nonlinear activations:\nht = f(WxW xt w +Whht\u22121 + b)\ngRNN(x) = h\u22121\nwheref is the activation function (eithertanh or rectified linear unit; the choice is tuned),Wx and Wh are parameter matrices,b is a\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9672033,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba",
            "isKey": true,
            "numCitedBy": 10069,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification."
            },
            "slug": "Convolutional-Neural-Networks-for-Sentence-Kim",
            "title": {
                "fragments": [],
                "text": "Convolutional Neural Networks for Sentence Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification, and are proposed to allow for the use of both task-specific and static vectors."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2949185"
                        ],
                        "name": "Ellie Pavlick",
                        "slug": "Ellie-Pavlick",
                        "structuredName": {
                            "firstName": "Ellie",
                            "lastName": "Pavlick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ellie Pavlick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294125"
                        ],
                        "name": "Pushpendre Rastogi",
                        "slug": "Pushpendre-Rastogi",
                        "structuredName": {
                            "firstName": "Pushpendre",
                            "lastName": "Rastogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushpendre Rastogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2267212"
                        ],
                        "name": "Juri Ganitkevitch",
                        "slug": "Juri-Ganitkevitch",
                        "structuredName": {
                            "firstName": "Juri",
                            "lastName": "Ganitkevitch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juri Ganitkevitch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7536576"
                        ],
                        "name": "Benjamin Van Durme",
                        "slug": "Benjamin-Van-Durme",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Durme",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Van Durme"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763608"
                        ],
                        "name": "Chris Callison-Burch",
                        "slug": "Chris-Callison-Burch",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Callison-Burch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Callison-Burch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 68
                            }
                        ],
                        "text": "One is a large sample of 26,456 annotated phrase pairs developed by Pavlick et al. (2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We have made this trained model available to the research community.13"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 202
                            }
                        ],
                        "text": "We also evaluate on the SemEval 2015 Twitter task (Xu et al., 2015b) and the SemEval 2014 Semantic Relatedness task (Marelli et al., 2014), as well as two tasks that use PPDB data(Wieting et al., 2015; Pavlick et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9711750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a47a0b0859547f327760d27bf8ce09c6e6f21fa2",
            "isKey": false,
            "numCitedBy": 270,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new release of the Paraphrase Database. PPDB 2.0 includes a discriminatively re-ranked set of paraphrases that achieve a higher correlation with human judgments than PPDB 1.0\u2019s heuristic rankings. Each paraphrase pair in the database now also includes finegrained entailment relations, word embedding similarities, and style annotations."
            },
            "slug": "PPDB-2.0:-Better-paraphrase-ranking,-fine-grained-Pavlick-Rastogi",
            "title": {
                "fragments": [],
                "text": "PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "PPDB 2.0 includes a discriminatively re-ranked set of paraphrases that achieve a higher correlation with human judgments than PPDB 1.0's heuristic rankings."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21432929"
                        ],
                        "name": "Michael Karlen",
                        "slug": "Michael-Karlen",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Karlen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Karlen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46283650"
                        ],
                        "name": "P. Kuksa",
                        "slug": "P.-Kuksa",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Kuksa",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kuksa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 132
                            }
                        ],
                        "text": "C L]\n4 M\nar 2\n01 6\nWe consider the problem of learning general-purpose, paraphrastic sentence embeddings based on supervision from the Paraphrase Database(G nitkevitch et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 89
                            }
                        ],
                        "text": "Several researchers have developed and shared word embeddings trained on large datasets (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014), and these have been used effectively for many downstream tasks (Turian et al., 2010; Socher et al., 2011; Kim, 2014; Bansal et al., 2014;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 351666,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc1022b031dc6c7019696492e8116598097a8c12",
            "isKey": false,
            "numCitedBy": 6661,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements."
            },
            "slug": "Natural-Language-Processing-(Almost)-from-Scratch-Collobert-Weston",
            "title": {
                "fragments": [],
                "text": "Natural Language Processing (Almost) from Scratch"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2702980"
                        ],
                        "name": "N. Pham",
                        "slug": "N.-Pham",
                        "structuredName": {
                            "firstName": "Nghia",
                            "lastName": "Pham",
                            "middleNames": [
                                "The"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Pham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067996"
                        ],
                        "name": "Germ\u00e1n Kruszewski",
                        "slug": "Germ\u00e1n-Kruszewski",
                        "structuredName": {
                            "firstName": "Germ\u00e1n",
                            "lastName": "Kruszewski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Germ\u00e1n Kruszewski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2672644"
                        ],
                        "name": "Angeliki Lazaridou",
                        "slug": "Angeliki-Lazaridou",
                        "structuredName": {
                            "firstName": "Angeliki",
                            "lastName": "Lazaridou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Angeliki Lazaridou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Pham et al. (2015) noted this same phenomenon in their closely-related compositional model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We found that this fraction had a Spearman\u2019s\u03c1 of -45.1 with the deviation from the75th percentile indicating that there is a significant negative correlationbetween the fraction of OOV words and performance on these STS tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10028211,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55ba959b0193c418399fdda2a38d913a7f0b6ac4",
            "isKey": true,
            "numCitedBy": 47,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce C-PHRASE, a distributional semantic model that learns word representations by optimizing context prediction for phrases at all levels in a syntactic tree, from single words to full sentences. C-PHRASE outperforms the state-of-theart C-BOW model on a variety of lexical tasks. Moreover, since C-PHRASE word vectors are induced through a compositional learning objective (modeling the contexts of words combined into phrases), when they are summed, they produce sentence representations that rival those generated by ad-hoc compositional models."
            },
            "slug": "Jointly-optimizing-word-representations-for-lexical-Pham-Kruszewski",
            "title": {
                "fragments": [],
                "text": "Jointly optimizing word representations for lexical and sentential tasks with the C-PHRASE model"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "C-PHRASE, a distributional semantic model that learns word representations by optimizing context prediction for phrases at all levels in a syntactic tree, outperforms the state-of-theart C-BOW model on a variety of lexical tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48188880"
                        ],
                        "name": "M. Marelli",
                        "slug": "M.-Marelli",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Marelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2486762"
                        ],
                        "name": "L. Bentivogli",
                        "slug": "L.-Bentivogli",
                        "structuredName": {
                            "firstName": "Luisa",
                            "lastName": "Bentivogli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bentivogli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145040726"
                        ],
                        "name": "R. Bernardi",
                        "slug": "R.-Bernardi",
                        "structuredName": {
                            "firstName": "Raffaella",
                            "lastName": "Bernardi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bernardi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2644577"
                        ],
                        "name": "S. Menini",
                        "slug": "S.-Menini",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Menini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Menini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2713535"
                        ],
                        "name": "Roberto Zamparelli",
                        "slug": "Roberto-Zamparelli",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Zamparelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roberto Zamparelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 169
                            }
                        ],
                        "text": "We compare the same suite of compositional architectur s for three supervised NLP tasks: sentence similarity and textual entailment using the 2014 SemEval SICK dataset (Marelli et al., 2014), and sentiment classification using the Stanford Sentim Treebank (Socher et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 117
                            }
                        ],
                        "text": "We also evaluate on the SemEval 2015 Twitter task (Xu et al., 2015b) and the SemEval 2014 Semantic Relatedness task (Marelli et al., 2014), as well as two tasks that use PPDB data(Wieting et al., 2015; Pavlick et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16404002,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "11ec56898a9e7f401a2affe776b5297bd4e25025",
            "isKey": false,
            "numCitedBy": 360,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the task on the evaluation of Compositional Distributional Semantics Models on full sentences organized for the first time within SemEval2014. Participation was open to systems based on any approach. Systems were presented with pairs of sentences and were evaluated on their ability to predict human judgments on (i) semantic relatedness and (ii) entailment. The task attracted 21 teams, most of which participated in both subtasks. We received 17 submissions in the relatedness subtask (for a total of 66 runs) and 18 in the entailment subtask (65 runs)."
            },
            "slug": "SemEval-2014-Task-1:-Evaluation-of-Compositional-on-Marelli-Bentivogli",
            "title": {
                "fragments": [],
                "text": "SemEval-2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "This paper presents the task on the evaluation of Compositional Distributional Semantics Models on full sentences organized for the first time within SemEval2014, and attracted 21 teams, most of which participated in both subtasks."
            },
            "venue": {
                "fragments": [],
                "text": "*SEMEVAL"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33968873"
                        ],
                        "name": "Baotian Hu",
                        "slug": "Baotian-Hu",
                        "structuredName": {
                            "firstName": "Baotian",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baotian Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11955007"
                        ],
                        "name": "Zhengdong Lu",
                        "slug": "Zhengdong-Lu",
                        "structuredName": {
                            "firstName": "Zhengdong",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhengdong Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49404233"
                        ],
                        "name": "Hang Li",
                        "slug": "Hang-Li",
                        "structuredName": {
                            "firstName": "Hang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144159781"
                        ],
                        "name": "Qingcai Chen",
                        "slug": "Qingcai-Chen",
                        "structuredName": {
                            "firstName": "Qingcai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qingcai Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 150
                            }
                        ],
                        "text": "\u2026on non-syntactic hierarchical structure (Zhao et al., 2015; Chen et al., 2015b), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014; Yin & Schu\u0308tze, 2015; He et al., 2015), and recu rent neural networks using long short-term memory (Tai et al., 2015; Ling et al.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 28
                            }
                        ],
                        "text": "Our fourth model is a standard recurrent network (RNN) with randomly initialized weight matrices and nonlinear activations:\nht = f(WxW xt w +Whht\u22121 + b)\ngRNN(x) = h\u22121\nwheref is the activation function (eithertanh or rectified linear unit; the choice is tuned),Wx and Wh are parameter matrices,b is a\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4497054,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f08b01251cb99f4ffae8c7b3e4468d3af9c98d3",
            "isKey": false,
            "numCitedBy": 1104,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic matching is of central importance to many natural language tasks [2,28]. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layer-by-layer composition and pooling, but also capture the rich matching patterns at different levels. Our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages. The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models."
            },
            "slug": "Convolutional-Neural-Network-Architectures-for-Hu-Lu",
            "title": {
                "fragments": [],
                "text": "Convolutional Neural Network Architectures for Matching Natural Language Sentences"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Convolutional neural network models for matching two sentences are proposed, by adapting the convolutional strategy in vision and speech and nicely represent the hierarchical structures of sentences with their layer-by-layer composition and pooling."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733049"
                        ],
                        "name": "Eneko Agirre",
                        "slug": "Eneko-Agirre",
                        "structuredName": {
                            "firstName": "Eneko",
                            "lastName": "Agirre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eneko Agirre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46724030"
                        ],
                        "name": "Daniel Matthew Cer",
                        "slug": "Daniel-Matthew-Cer",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Cer",
                            "middleNames": [
                                "Matthew"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Matthew Cer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700007"
                        ],
                        "name": "Mona T. Diab",
                        "slug": "Mona-T.-Diab",
                        "structuredName": {
                            "firstName": "Mona",
                            "lastName": "Diab",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mona T. Diab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403836100"
                        ],
                        "name": "A. Gonzalez-Agirre",
                        "slug": "A.-Gonzalez-Agirre",
                        "structuredName": {
                            "firstName": "Aitor",
                            "lastName": "Gonzalez-Agirre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gonzalez-Agirre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 88
                            }
                        ],
                        "text": "For more details on these tasks please refer to the relevant publications for the 2012 (Agirre et al., 2012), 2013 (Agirre et al., 2013), 2014 (Agirre et al., 2014), and 2015 (Agirre et al., 2015) tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12549805,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "528fa9bb03644ba752fb9491be49b9dd1bce1d52",
            "isKey": false,
            "numCitedBy": 656,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two texts. This paper presents the results of the STS pilot task in Semeval. The training data contained 2000 sentence pairs from previously existing paraphrase datasets and machine translation evaluation resources. The test data also comprised 2000 sentences pairs for those datasets, plus two surprise datasets with 400 pairs from a different machine translation evaluation corpus and 750 pairs from a lexical resource mapping exercise. The similarity of pairs of sentences was rated on a 0-5 scale (low to high similarity) by human judges using Amazon Mechanical Turk, with high Pearson correlation scores, around 90%. 35 teams participated in the task, submitting 88 runs. The best results scored a Pearson correlation >80%, well above a simple lexical baseline that only scored a 31% correlation. This pilot task opens an exciting way ahead, although there are still open issues, specially the evaluation metric."
            },
            "slug": "SemEval-2012-Task-6:-A-Pilot-on-Semantic-Textual-Agirre-Cer",
            "title": {
                "fragments": [],
                "text": "SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The results of the STS pilot task in Semeval open an exciting way ahead, although there are still open issues, specially the evaluation metric."
            },
            "venue": {
                "fragments": [],
                "text": "*SEMEVAL"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733049"
                        ],
                        "name": "Eneko Agirre",
                        "slug": "Eneko-Agirre",
                        "structuredName": {
                            "firstName": "Eneko",
                            "lastName": "Agirre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eneko Agirre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46724030"
                        ],
                        "name": "Daniel Matthew Cer",
                        "slug": "Daniel-Matthew-Cer",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Cer",
                            "middleNames": [
                                "Matthew"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Matthew Cer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700007"
                        ],
                        "name": "Mona T. Diab",
                        "slug": "Mona-T.-Diab",
                        "structuredName": {
                            "firstName": "Mona",
                            "lastName": "Diab",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mona T. Diab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403836100"
                        ],
                        "name": "A. Gonzalez-Agirre",
                        "slug": "A.-Gonzalez-Agirre",
                        "structuredName": {
                            "firstName": "Aitor",
                            "lastName": "Gonzalez-Agirre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gonzalez-Agirre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2579583"
                        ],
                        "name": "Weiwei Guo",
                        "slug": "Weiwei-Guo",
                        "structuredName": {
                            "firstName": "Weiwei",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiwei Guo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 116
                            }
                        ],
                        "text": "For more details on these tasks please refer to the relevant publications for the 2012 (Agirre et al., 2012), 2013 (Agirre et al., 2013), 2014 (Agirre et al., 2014), and 2015 (Agirre et al., 2015) tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2012), 2013 (Agirre et al., 2013), 2014 (Agirre et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10241043,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "ff5cbefc6766df788919db4c060daedb303ed3e3",
            "isKey": false,
            "numCitedBy": 398,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In Semantic Textual Similarity (STS), systems rate the degree of semantic equivalence, on a graded scale from 0 to 5, with 5 being the most similar. This year we set up two tasks: (i) a core task (CORE), and (ii) a typed-similarity task (TYPED). CORE is similar in set up to SemEval STS 2012 task with pairs of sentences from sources related to those of 2012, yet different in genre from the 2012 set, namely, this year we included newswire headlines, machine translation evaluation datasets and multiple lexical resource glossed sets. TYPED, on the other hand, is novel and tries to characterize why two items are deemed similar, using cultural heritage items which are described with metadata such as title, author or description. Several types of similarity have been defined, including similar author, similar time period or similar location. The annotation for both tasks leverages crowdsourcing, with relative high interannotator correlation, ranging from 62% to 87%. The CORE task attracted 34 participants with 89 runs, and the TYPED task attracted 6 teams with 14 runs."
            },
            "slug": "*SEM-2013-shared-task:-Semantic-Textual-Similarity-Agirre-Cer",
            "title": {
                "fragments": [],
                "text": "*SEM 2013 shared task: Semantic Textual Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "The CORE task attracted 34 participants with 89 runs, and the TYPED task attracted 6 teams with 14 runs, with relative high interannotator correlation, ranging from 62% to 87%."
            },
            "venue": {
                "fragments": [],
                "text": "*SEMEVAL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46181066"
                        ],
                        "name": "Iz Beltagy",
                        "slug": "Iz-Beltagy",
                        "structuredName": {
                            "firstName": "Iz",
                            "lastName": "Beltagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iz Beltagy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144745718"
                        ],
                        "name": "Stephen Roller",
                        "slug": "Stephen-Roller",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Roller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Roller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2904366"
                        ],
                        "name": "Pengxiang Cheng",
                        "slug": "Pengxiang-Cheng",
                        "structuredName": {
                            "firstName": "Pengxiang",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pengxiang Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708114"
                        ],
                        "name": "Katrin Erk",
                        "slug": "Katrin-Erk",
                        "structuredName": {
                            "firstName": "Katrin",
                            "lastName": "Erk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katrin Erk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 57
                            }
                        ],
                        "text": "For entailment, the best result we are aware of is 85.1 (Beltagy et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9707387,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d799ab952c0f677d4526c485cc14555b7265ecc6",
            "isKey": true,
            "numCitedBy": 51,
            "numCiting": 136,
            "paperAbstract": {
                "fragments": [],
                "text": "NLP tasks differ in the semantic information they require, and at this time no single semantic representation fulfills all requirements. Logic-based representations characterize sentence structure, but do not capture the graded aspect of meaning. Distributional models give graded similarity ratings for words and phrases, but do not capture sentence structure in the same detail as logic-based approaches. It has therefore been argued that the two are complementary.We adopt a hybrid approach that combines logical and distributional semantics using probabilistic logic, specifically Markov Logic Networks. In this article, we focus on the three components of a practical system:1 1) Logical representation focuses on representing the input problems in probabilistic logic; 2) knowledge base construction creates weighted inference rules by integrating distributional information with other sources; and 3) probabilistic inference involves solving the resulting MLN inference problems efficiently. To evaluate our approach, we use the task of textual entailment, which can utilize the strengths of both logic-based and distributional representations. In particular we focus on the SICK data set, where we achieve state-of-the-art results. We also release a lexical entailment data set of 10,213 rules extracted from the SICK data set, which is a valuable resource for evaluating lexical entailment systems.2"
            },
            "slug": "Representing-Meaning-with-a-Combination-of-Logical-Beltagy-Roller",
            "title": {
                "fragments": [],
                "text": "Representing Meaning with a Combination of Logical and Distributional Models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This article adopts a hybrid approach that combines logical and distributional semantics using probabilistic logic, specifically Markov Logic Networks and releases a lexical entailment data set of 10,213 rules extracted from the SICK data set, which is a valuable resource for evaluating lexical entailsment systems."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713934"
                        ],
                        "name": "Antoine Bordes",
                        "slug": "Antoine-Bordes",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Bordes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoine Bordes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3295092"
                        ],
                        "name": "S. Chopra",
                        "slug": "S.-Chopra",
                        "structuredName": {
                            "firstName": "Sumit",
                            "lastName": "Chopra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chopra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 204
                            }
                        ],
                        "text": "\u2026input representations in order to maximize similarity in the learned space (Weston et al., 2010; Yih et al., 2011; Huang et al., 2013; Hermann & Blunsom, 2014; Socher et al., 2014; Faruqui & Dyer, 2014; Bordes et al., 2014b;a; Lu et al., 2015), including our priorw rk (Wieting et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12938495,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33261d252218007147a71e40f8367ed152fa2fe0",
            "isKey": false,
            "numCitedBy": 590,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features. Our model learns low-dimensional embeddings of words and knowledge base constituents; these representations are used to score natural language questions against candidate answers. Training our system using pairs of questions and structured representations of their answers, and pairs of question paraphrases, yields competitive results on a recent benchmark of the literature."
            },
            "slug": "Question-Answering-with-Subgraph-Embeddings-Bordes-Chopra",
            "title": {
                "fragments": [],
                "text": "Question Answering with Subgraph Embeddings"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features, using low-dimensional embeddings of words and knowledge base constituents to score natural language questions against candidate answers."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2421691"
                        ],
                        "name": "Po-Sen Huang",
                        "slug": "Po-Sen-Huang",
                        "structuredName": {
                            "firstName": "Po-Sen",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Po-Sen Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800422"
                        ],
                        "name": "Jianfeng Gao",
                        "slug": "Jianfeng-Gao",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723644"
                        ],
                        "name": "A. Acero",
                        "slug": "A.-Acero",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Acero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Acero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46819684"
                        ],
                        "name": "Larry Heck",
                        "slug": "Larry-Heck",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Heck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Larry Heck"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 149
                            }
                        ],
                        "text": "\u2026body of related work on learning input representations in order to maximize similarity in the learned space (Weston et al., 2010; Yih et al., 2011; Huang et al., 2013; Hermann & Blunsom, 2014; Socher et al., 2014; Faruqui & Dyer, 2014; Bordes et al., 2014b;a; Lu et al., 2015), including our\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The intuition is that we want the two phrases to be more similar to each other (cos g(x1), g(x2))) than either is to their respective negative examplest1 andt2, by a margin of at least\u03b4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8384258,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fdb813d8b927bdd21ae1858cafa6c34b66a36268",
            "isKey": false,
            "numCitedBy": 1451,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Latent semantic models, such as LSA, intend to map a query to its relevant documents at the semantic level where keyword-based matching often fails. In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them. The proposed deep structured semantic models are discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthrough data. To make our models applicable to large-scale Web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. The new models are evaluated on a Web document ranking task using a real-world data set. Results show that our best model significantly outperforms other latent semantic models, which were considered state-of-the-art in the performance prior to the work presented in this paper."
            },
            "slug": "Learning-deep-structured-semantic-models-for-web-Huang-He",
            "title": {
                "fragments": [],
                "text": "Learning deep structured semantic models for web search using clickthrough data"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them are developed."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2129425"
                        ],
                        "name": "Denis Paperno",
                        "slug": "Denis-Paperno",
                        "structuredName": {
                            "firstName": "Denis",
                            "lastName": "Paperno",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Denis Paperno"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2702980"
                        ],
                        "name": "N. Pham",
                        "slug": "N.-Pham",
                        "structuredName": {
                            "firstName": "Nghia",
                            "lastName": "Pham",
                            "middleNames": [
                                "The"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Pham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 71
                            }
                        ],
                        "text": "Some have been based on distributional semantics (Baroni et al., 2014; Paperno et al., 2014; Polajnar et al., 2015; Tian et al., 2015), while the current trend is toward development of neural network architectures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 84
                            }
                        ],
                        "text": "In our second model, we learn a projection in addition to the word embeddings:\ngproj(x) = Wp\n(\n1\nn\nn \u2211\ni\nW xiw\n)\n+ b\n7In prior work, we experimented with recursive neural networks on binarized parses of the PPDB (Wieting et al., 2015), but we found that many of the phrases in PPDB are not sentences\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18193242,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "110fa4fa9322f664779400f084aaa2ca717d291c",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Distributional semantic methods to approximate word meaning with context vectors have been very successful empirically, and the last years have seen a surge of interest in their compositional extension to phrases and sentences. We present here a new model that, like those of Coecke et al. (2010) and Baroni and Zamparelli (2010), closely mimics the standard Montagovian semantic treatment of composition in distributional terms. However, our approach avoids a number of issues that have prevented the application of the earlier linguistically-motivated models to full-fledged, real-life sentences. We test the model on a variety of empirical tasks, showing that it consistently outperforms a set of competitive rivals. 1 Compositional distributional semantics The research of the last two decades has established empirically that distributional vectors for words obtained from corpus statistics can be used"
            },
            "slug": "A-practical-and-linguistically-motivated-approach-Paperno-Pham",
            "title": {
                "fragments": [],
                "text": "A practical and linguistically-motivated approach to compositional distributional semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new model that closely mimics the standard Montagovian semantic treatment of composition in distributional terms is presented, showing that it consistently outperforms a set of competitive rivals."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1379953252"
                        ],
                        "name": "Wang Ling",
                        "slug": "Wang-Ling",
                        "structuredName": {
                            "firstName": "Wang",
                            "lastName": "Ling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wang Ling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690706"
                        ],
                        "name": "A. Black",
                        "slug": "A.-Black",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Black",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691021"
                        ],
                        "name": "I. Trancoso",
                        "slug": "I.-Trancoso",
                        "structuredName": {
                            "firstName": "Isabel",
                            "lastName": "Trancoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Trancoso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3394760"
                        ],
                        "name": "Ram\u00f3n Fern\u00e1ndez Astudillo",
                        "slug": "Ram\u00f3n-Fern\u00e1ndez-Astudillo",
                        "structuredName": {
                            "firstName": "Ram\u00f3n",
                            "lastName": "Astudillo",
                            "middleNames": [
                                "Fern\u00e1ndez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ram\u00f3n Fern\u00e1ndez Astudillo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1840645"
                        ],
                        "name": "Silvio Amir",
                        "slug": "Silvio-Amir",
                        "structuredName": {
                            "firstName": "Silvio",
                            "lastName": "Amir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Silvio Amir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728026"
                        ],
                        "name": "Lu\u00eds Marujo",
                        "slug": "Lu\u00eds-Marujo",
                        "structuredName": {
                            "firstName": "Lu\u00eds",
                            "lastName": "Marujo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lu\u00eds Marujo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2979183"
                        ],
                        "name": "T. Lu\u00eds",
                        "slug": "T.-Lu\u00eds",
                        "structuredName": {
                            "firstName": "Tiago",
                            "lastName": "Lu\u00eds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lu\u00eds"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 254
                            }
                        ],
                        "text": "\u2026structure (Zhao et al., 2015; Chen et al., 2015b), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014; Yin & Schu\u0308tze, 2015; He et al., 2015), and recu rent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu etal., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We release all of our resources to the research community1 with the hope that they can serve as the new baseline for further work on universal sentence embeddings."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 278,
                                "start": 261
                            }
                        ],
                        "text": "\u2026variants, culminating in LSTMs because they have been foundto be effective for many types of sequential data (Graves et al., 2008; 2013; Greff et al., 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 157
                            }
                        ],
                        "text": "\u2026initialized weight matrices and nonlinear activations:\nht = f(WxW xt w +Whht\u22121 + b)\ngRNN(x) = h\u22121\nwheref is the activation function (eithertanh or rectified linear unit; the choice is tuned),Wx and Wh are parameter matrices,b is a bias vector, andh\u22121 refers to the hidden vector of the last token."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1689426,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dab1c6491929d396e9e5463bc2e87af88602aa2",
            "isKey": true,
            "numCitedBy": 581,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form\u2010function relationship in language, our \u201ccomposed\u201d word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish)."
            },
            "slug": "Finding-Function-in-Form:-Compositional-Character-Ling-Dyer",
            "title": {
                "fragments": [],
                "text": "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A model for constructing vector representations of words by composing characters using bidirectional LSTMs that requires only a single vector per character type and a fixed set of parameters for the compositional model, which yields state- of-the-art results in language modeling and part-of-speech tagging."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060101052"
                        ],
                        "name": "Terry Koo",
                        "slug": "Terry-Koo",
                        "structuredName": {
                            "firstName": "Terry",
                            "lastName": "Koo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Terry Koo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754497"
                        ],
                        "name": "Slav Petrov",
                        "slug": "Slav-Petrov",
                        "structuredName": {
                            "firstName": "Slav",
                            "lastName": "Petrov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Slav Petrov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We release all of our resources to the research community1 with the hope that they can serve as the new baseline for further work on universal sentence embeddings."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 199
                            }
                        ],
                        "text": "\u2026variants, culminating in LSTMs because they have been foundto be effective for many types of sequential data (Graves et al., 2008; 2013; Greff et al., 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14223,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40",
            "isKey": false,
            "numCitedBy": 845,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation."
            },
            "slug": "Grammar-as-a-Foreign-Language-Vinyals-Kaiser",
            "title": {
                "fragments": [],
                "text": "Grammar as a Foreign Language"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145783676"
                        ],
                        "name": "Felix Hill",
                        "slug": "Felix-Hill",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Hill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Felix Hill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762757"
                        ],
                        "name": "Roi Reichart",
                        "slug": "Roi-Reichart",
                        "structuredName": {
                            "firstName": "Roi",
                            "lastName": "Reichart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roi Reichart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145762466"
                        ],
                        "name": "A. Korhonen",
                        "slug": "A.-Korhonen",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Korhonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Korhonen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3226120,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a96765c147c9c814803c8c9de28a1dd069271da",
            "isKey": false,
            "numCitedBy": 1092,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways. First, in contrast to gold standards such as WordSim-353 and MEN, it explicitly quantifies similarity rather than association or relatedness so that pairs of entities that are associated but not actually similar (Freud, psychology) have a low rating. We show that, via this focus on similarity, SimLex-999 incentivizes the development of models with a different, and arguably wider, range of applications than those which reflect conceptual association. Second, SimLex-999 contains a range of concrete and abstract adjective, noun, and verb pairs, together with an independent rating of concreteness and (free) association strength for each pair. This diversity enables fine-grained analyses of the performance of models on concepts of different types, and consequently greater insight into how architectures can be improved. Further, unlike existing gold standard evaluations, for which automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-of-the-art models perform well below this ceiling on SimLex-999. There is therefore plenty of scope for SimLex-999 to quantify future improvements to distributional semantic models, guiding the development of the next generation of representation-learning architectures."
            },
            "slug": "SimLex-999:-Evaluating-Semantic-Models-With-Hill-Reichart",
            "title": {
                "fragments": [],
                "text": "SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "SimLex-999 is presented, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways, and explicitly quantifies similarity rather than association or relatedness so that pairs of entities that are associated but not actually similar have a low rating."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360972"
                        ],
                        "name": "Abdel-rahman Mohamed",
                        "slug": "Abdel-rahman-Mohamed",
                        "structuredName": {
                            "firstName": "Abdel-rahman",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdel-rahman Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 146
                            }
                        ],
                        "text": "\u2026consider several recurrent network variants, culminating in LSTMs because they have been foundto be effective for many types of sequential data (Graves et al., 2008; 2013; Greff et al., 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We release all of our resources to the research community1 with the hope that they can serve as the new baseline for further work on universal sentence embeddings."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206741496,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d",
            "isKey": false,
            "numCitedBy": 6902,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
            },
            "slug": "Speech-recognition-with-deep-recurrent-neural-Graves-Mohamed",
            "title": {
                "fragments": [],
                "text": "Speech recognition with deep recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118440152"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 156
                            }
                        ],
                        "text": "C L]\n4 M\nar 2\n01 6\nWe consider the problem of learning general-purpose, paraphrastic sentence embeddings based on supervision from the Paraphrase Database(G nitkevitch et al., 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 113
                            }
                        ],
                        "text": "Several researchers have developed and shared word embeddings trained on large datasets (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014), and these have been used effectively for many downstream tasks (Turian et al., 2010; Socher et al., 2011; Kim, 2014; Bansal et al., 2014;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16447573,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
            "isKey": false,
            "numCitedBy": 26066,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. \n \nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible."
            },
            "slug": "Distributed-Representations-of-Words-and-Phrases-Mikolov-Sutskever",
            "title": {
                "fragments": [],
                "text": "Distributed Representations of Words and Phrases and their Compositionality"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents a simple method for finding phrases in text, and shows that learning good vector representations for millions of phrases is possible and describes a simple alternative to the hierarchical softmax called negative sampling."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3064800"
                        ],
                        "name": "Xinchi Chen",
                        "slug": "Xinchi-Chen",
                        "structuredName": {
                            "firstName": "Xinchi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinchi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767521"
                        ],
                        "name": "Xipeng Qiu",
                        "slug": "Xipeng-Qiu",
                        "structuredName": {
                            "firstName": "Xipeng",
                            "lastName": "Qiu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xipeng Qiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115802253"
                        ],
                        "name": "Chenxi Zhu",
                        "slug": "Chenxi-Zhu",
                        "structuredName": {
                            "firstName": "Chenxi",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chenxi Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144118452"
                        ],
                        "name": "Pengfei Liu",
                        "slug": "Pengfei-Liu",
                        "structuredName": {
                            "firstName": "Pengfei",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pengfei Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790227"
                        ],
                        "name": "Xuanjing Huang",
                        "slug": "Xuanjing-Huang",
                        "structuredName": {
                            "firstName": "Xuanjing",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuanjing Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 150
                            }
                        ],
                        "text": "\u2026al., 2011; 2; 2013;\u0307Irsoy & Cardie, 2014; Wieting et al., 2015), recursive networks based on non-syntactic hierarchical structure (Zhao et al., 2015; Chen et al., 2015b), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014; Yin & Schu\u0308tze, 2015; He et al., 2015),\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We tune over the strategy in our experiments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 110
                            }
                        ],
                        "text": "LSTMs have recently been shown to produce state-of-the-art results in a variety of sequence processing tasks (Chen et al., 2015a; Filippova et al., 2015; Xu et al., 2015c; Belinkov & Glass, 2015; Wang & Nyberg, 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5816303,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bef32d4dab1f00b4c3da315fef0c3e5e455bdbef",
            "isKey": false,
            "numCitedBy": 221,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Currently most of state-of-the-art methods for Chinese word segmentation are based on supervised learning, whose features aremostly extracted from a local context. Thesemethods cannot utilize the long distance information which is also crucial for word segmentation. In this paper, we propose a novel neural network model for Chinese word segmentation, which adopts the long short-term memory (LSTM) neural network to keep the previous important information inmemory cell and avoids the limit of window size of local context. Experiments on PKU, MSRA and CTB6 benchmark datasets show that our model outperforms the previous neural network models and state-of-the-art methods."
            },
            "slug": "Long-Short-Term-Memory-Neural-Networks-for-Chinese-Chen-Qiu",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory Neural Networks for Chinese Word Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A novel neural network model for Chinese word segmentation is proposed, which adopts the long short-term memory (LSTM) neural network to keep the previous important information in memory cell and avoids the limit of window size of local context."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 128
                            }
                        ],
                        "text": "We again tuned \u03bbs over{10\u22123, 10\u22124, 10\u22125, 10\u22126} and tuned over the two optimization schemes of AdaGrad with learning rate of 0.05 and Adam with a learning rate of 0.001."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "We tuned hyperparameters on the development set of each dataset16 as well as on two optimization schemes: AdaGrad with learning rate of 0.05 and Adam with a learning rate of 0.001."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 83
                            }
                        ],
                        "text": "We also tune over optimization method (either AdaGrad (Duchi et al., 2011) or Adam(Kingma & Ba, 2014)), learning rate (from{0.05, 0.005, 0.0005}), whether to clip the gradients with threshold 1 (Pascanu etal., 2012), and whether to use MIX or MAX sampling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 2
                            }
                        ],
                        "text": "We would also like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) and thank NVIDIA Corporation for donati g GPUs used in this research."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6628106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "isKey": true,
            "numCitedBy": 90142,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            "slug": "Adam:-A-Method-for-Stochastic-Optimization-Kingma-Ba",
            "title": {
                "fragments": [],
                "text": "Adam: A Method for Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work introduces Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments, and provides a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2910877"
                        ],
                        "name": "K. Hermann",
                        "slug": "K.-Hermann",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Hermann",
                            "middleNames": [
                                "Moritz"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2367821"
                        ],
                        "name": "Tom\u00e1s Kocisk\u00fd",
                        "slug": "Tom\u00e1s-Kocisk\u00fd",
                        "structuredName": {
                            "firstName": "Tom\u00e1s",
                            "lastName": "Kocisk\u00fd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom\u00e1s Kocisk\u00fd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1864353"
                        ],
                        "name": "Edward Grefenstette",
                        "slug": "Edward-Grefenstette",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward Grefenstette"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2311318"
                        ],
                        "name": "Lasse Espeholt",
                        "slug": "Lasse-Espeholt",
                        "structuredName": {
                            "firstName": "Lasse",
                            "lastName": "Espeholt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lasse Espeholt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2062879616"
                        ],
                        "name": "Will Kay",
                        "slug": "Will-Kay",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Kay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Will Kay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2573615"
                        ],
                        "name": "Mustafa Suleyman",
                        "slug": "Mustafa-Suleyman",
                        "structuredName": {
                            "firstName": "Mustafa",
                            "lastName": "Suleyman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mustafa Suleyman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685771"
                        ],
                        "name": "P. Blunsom",
                        "slug": "P.-Blunsom",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Blunsom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Blunsom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 239
                            }
                        ],
                        "text": "\u2026variants, culminating in LSTMs because they have been foundto be effective for many types of sequential data (Graves et al., 2008; 2013; Greff et al., 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6203757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1505c6123c102e53eb19dff312cb25cea840b72",
            "isKey": false,
            "numCitedBy": 2435,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure."
            },
            "slug": "Teaching-Machines-to-Read-and-Comprehend-Hermann-Kocisk\u00fd",
            "title": {
                "fragments": [],
                "text": "Teaching Machines to Read and Comprehend"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new methodology is defined that resolves this bottleneck and provides large scale supervised reading comprehension data that allows a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure to be developed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083259"
                        ],
                        "name": "Yonatan Belinkov",
                        "slug": "Yonatan-Belinkov",
                        "structuredName": {
                            "firstName": "Yonatan",
                            "lastName": "Belinkov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yonatan Belinkov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145898106"
                        ],
                        "name": "James R. Glass",
                        "slug": "James-R.-Glass",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Glass",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James R. Glass"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We tune over the strategy in our experiments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 172
                            }
                        ],
                        "text": "LSTMs have recently been shown to produce state-of-the-art results in a variety of sequence processing tasks (Chen et al., 2015a; Filippova et al., 2015; Xu et al., 2015c; Belinkov & Glass, 2015; Wang & Nyberg, 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16571013,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75928f57b837a1e4a1bf3e46610a1ef0bb8705c0",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Arabic, Hebrew, and similar languages are typically written without diacritics, leading to ambiguity and posing a major challenge for core language processing tasks like speech recognition. Previous approaches to automatic diacritization employed a variety of machine learning techniques. However, they typically rely on existing tools like morphological analyzers and therefore cannot be easily extended to new genres and languages. We develop a recurrent neural network with long shortterm memory layers for predicting diacritics in Arabic text. Our language-independent approach is trained solely from diacritized text without relying on external tools. We show experimentally that our model can rival state-of-the-art methods that have access to additional resources."
            },
            "slug": "Arabic-Diacritization-with-Recurrent-Neural-Belinkov-Glass",
            "title": {
                "fragments": [],
                "text": "Arabic Diacritization with Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work develops a recurrent neural network with long shortterm memory layers for predicting diacritics in Arabic text and shows experimentally that this model can rival state-of-the-art methods that have access to additional resources."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779225"
                        ],
                        "name": "Manaal Faruqui",
                        "slug": "Manaal-Faruqui",
                        "structuredName": {
                            "firstName": "Manaal",
                            "lastName": "Faruqui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manaal Faruqui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 182
                            }
                        ],
                        "text": "\u2026input representations in order to maximize similarity in the learned space (Weston et al., 2010; Yih et al., 2011; Huang et al., 2013; Hermann & Blunsom, 2014; Socher et al., 2014; Faruqui & Dyer, 2014; Bordes et al., 2014b;a; Lu et al., 2015), including our priorw rk (Wieting et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The intuition is that we want the two phrases to be more similar to each other (cos g(x1), g(x2))) than either is to their respective negative examplest1 andt2, by a margin of at least\u03b4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3792324,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d3aaa919c78c06f24588d97ed1028d51860b321",
            "isKey": false,
            "numCitedBy": 556,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "The distributional hypothesis of Harris (1954), according to which the meaning of words is evidenced by the contexts they occur in, has motivated several effective techniques for obtaining vector space semantic representations of words using unannotated text corpora. This paper argues that lexico-semantic content should additionally be invariant across languages and proposes a simple technique based on canonical correlation analysis (CCA) for incorporating multilingual evidence into vectors generated monolingually. We evaluate the resulting word representations on standard lexical semantic evaluation tasks and show that our method produces substantially better semantic representations than monolingual techniques."
            },
            "slug": "Improving-Vector-Space-Word-Representations-Using-Faruqui-Dyer",
            "title": {
                "fragments": [],
                "text": "Improving Vector Space Word Representations Using Multilingual Correlation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper argues that lexico-semantic content should additionally be invariant across languages and proposes a simple technique based on canonical correlation analysis (CCA) for incorporating multilingual evidence into vectors generated monolingually."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3064800"
                        ],
                        "name": "Xinchi Chen",
                        "slug": "Xinchi-Chen",
                        "structuredName": {
                            "firstName": "Xinchi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinchi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767521"
                        ],
                        "name": "Xipeng Qiu",
                        "slug": "Xipeng-Qiu",
                        "structuredName": {
                            "firstName": "Xipeng",
                            "lastName": "Qiu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xipeng Qiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115802253"
                        ],
                        "name": "Chenxi Zhu",
                        "slug": "Chenxi-Zhu",
                        "structuredName": {
                            "firstName": "Chenxi",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chenxi Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112379225"
                        ],
                        "name": "Shiyu Wu",
                        "slug": "Shiyu-Wu",
                        "structuredName": {
                            "firstName": "Shiyu",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shiyu Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790227"
                        ],
                        "name": "Xuanjing Huang",
                        "slug": "Xuanjing-Huang",
                        "structuredName": {
                            "firstName": "Xuanjing",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuanjing Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 150
                            }
                        ],
                        "text": "\u2026al., 2011; 2; 2013;\u0307Irsoy & Cardie, 2014; Wieting et al., 2015), recursive networks based on non-syntactic hierarchical structure (Zhao et al., 2015; Chen et al., 2015b), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014; Yin & Schu\u0308tze, 2015; He et al., 2015),\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 110
                            }
                        ],
                        "text": "LSTMs have recently been shown to produce state-of-the-art results in a variety of sequence processing tasks (Chen et al., 2015a; Filippova et al., 2015; Xu et al., 2015c; Belinkov & Glass, 2015; Wang & Nyberg, 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 20
                            }
                        ],
                        "text": "In our experiments below, we tune over the number of layers and choice of activation function."
                    },
                    "intents": []
                }
            ],
            "corpusId": 820218,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e975e86fff32a9f901b2035688970fcd0fca2a0",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, neural network based sentence modeling methods have achieved great progress. Among these methods, the recursive neural networks (RecNNs) can effectively model the combination of the words in sentence. However, RecNNs need a given external topological structure, like syntactic tree. In this paper, we propose a gated recursive neural network (GRNN) to model sentences, which employs a full binary tree (FBT) structure to control the combinations in recursive structure. By introducing two kinds of gates, our model can better model the complicated combinations of features. Experiments on three text classification datasets show the effectiveness of our model."
            },
            "slug": "Sentence-Modeling-with-Gated-Recursive-Neural-Chen-Qiu",
            "title": {
                "fragments": [],
                "text": "Sentence Modeling with Gated Recursive Neural Network"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A gated recursive neural network (GRNN) to model sentences, which employs a full binary tree (FBT) structure to control the combinations in recursive structure by introducing two kinds of gates."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34902160"
                        ],
                        "name": "Jeff Mitchell",
                        "slug": "Jeff-Mitchell",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 140
                            }
                        ],
                        "text": "\u2026architectures have been proposed to model compositionality in such sequences, ranging from those based on simple operations like addition (Mitchell & Lapata, 2010; Yu & Dredze, 2015; Iyyer et al., 2015) to those based on richly-structured functions like recursive neural networks (Socher\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 0
                            }
                        ],
                        "text": "Mitchell & Lapata (2008; 2010) considered bigram compositionality, comparing many functions for composing two word vectors into a single vector to represent their bigram."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26901423,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "745d86adca56ec50761591733e157f84cfb19671",
            "isKey": false,
            "numCitedBy": 930,
            "numCiting": 253,
            "paperAbstract": {
                "fragments": [],
                "text": "Vector-based models of word meaning have become increasingly popular in cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar. Despite their widespread use, vector-based models are typically directed at representing words in isolation, and methods for constructing representations for phrases or sentences have received little attention in the literature. This is in marked contrast to experimental evidence (e.g., in sentential priming) suggesting that semantic similarity is more complex than simply a relation between isolated words. This article proposes a framework for representing the meaning of word combinations in vector space. Central to our approach is vector composition, which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models that we evaluate empirically on a phrase similarity task."
            },
            "slug": "Composition-in-Distributional-Models-of-Semantics-Mitchell-Lapata",
            "title": {
                "fragments": [],
                "text": "Composition in Distributional Models of Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This article proposes a framework for representing the meaning of word combinations in vector space in terms of additive and multiplicative functions, and introduces a wide range of composition models that are evaluated empirically on a phrase similarity task."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2417051"
                        ],
                        "name": "T. Polajnar",
                        "slug": "T.-Polajnar",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Polajnar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Polajnar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2554988"
                        ],
                        "name": "Laura Rimell",
                        "slug": "Laura-Rimell",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Rimell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laura Rimell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 93
                            }
                        ],
                        "text": "Some have been based on distributional semantics (Baroni et al., 2014; Paperno et al., 2014; Polajnar et al., 2015; Tian et al., 2015), while the current trend is toward development of neural network architectures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 109
                            }
                        ],
                        "text": "In our second model, we learn a projection in addition to the word embeddings:\ngproj(x) = Wp\n(\n1\nn\nn \u2211\ni\nW xiw\n)\n+ b\n7In prior work, we experimented with recursive neural networks on binarized parses of the PPDB (Wieting et al., 2015), but we found that many of the phrases in PPDB are not sentences\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4958263,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb256cee9d3e0ec52f071a8d674517b62f0828d0",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates whether the wider context in which a sentence is located can contribute to a distributional representation of sentence meaning. We compare a vector space for sentences in which the features are words occurring within the sentence, with two new vector spaces that only make use of surrounding context. Experiments on simple subject-verbobject similarity tasks show that all sentence spaces produce results that are comparable with previous work. However, qualitative analysis and user experiments indicate that extra-sentential contexts capture more diverse, yet topically coherent information."
            },
            "slug": "An-Exploration-of-Discourse-Based-Sentence-Spaces-Polajnar-Rimell",
            "title": {
                "fragments": [],
                "text": "An Exploration of Discourse-Based Sentence Spaces for Compositional Distributional Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Qualitative analysis and user experiments indicate that extra-sentential contexts capture more diverse, yet topically coherent information."
            },
            "venue": {
                "fragments": [],
                "text": "LSDSem@EMNLP"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 38
                            }
                        ],
                        "text": "We use long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997), a recurrent neural network (RNN) architecture designed to model sequences with long-distance dependencies."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 27
                            }
                        ],
                        "text": "We call this sampling strategy MIX."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": false,
            "numCitedBy": 51728,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144105277"
                        ],
                        "name": "Wen-tau Yih",
                        "slug": "Wen-tau-Yih",
                        "structuredName": {
                            "firstName": "Wen-tau",
                            "lastName": "Yih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen-tau Yih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50004012"
                        ],
                        "name": "Christopher Meek",
                        "slug": "Christopher-Meek",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Meek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Meek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The intuition is that we want the two phrases to be more similar to each other (cos g(x1), g(x2))) than either is to their respective negative examplest1 andt2, by a margin of at least\u03b4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 150
                            }
                        ],
                        "text": "\u2026draws from a large body of related work on learning input representations in order to maximize similarity in the learned space (Weston et al., 2010; Yih et al., 2011; Huang et al., 2013; Hermann & Blunsom, 2014; Socher et al., 2014; Faruqui & Dyer, 2014; Bordes et al., 2014b;a; Lu et al., 2015),\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1965270,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5b034176d6021bc965fca9aa02f17864d1ccf67",
            "isKey": false,
            "numCitedBy": 224,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional text similarity measures consider each term similar only to itself and do not model semantic relatedness of terms. We propose a novel discriminative training method that projects the raw term vectors into a common, low-dimensional vector space. Our approach operates by finding the optimal matrix to minimize the loss of the pre-selected similarity function (e.g., cosine) of the projected vectors, and is able to efficiently handle a large number of training examples in the high-dimensional space. Evaluated on two very different tasks, cross-lingual document retrieval and ad relevance measure, our method not only outperforms existing state-of-the-art approaches, but also achieves high accuracy at low dimensions and is thus more efficient."
            },
            "slug": "Learning-Discriminative-Projections-for-Text-Yih-Toutanova",
            "title": {
                "fragments": [],
                "text": "Learning Discriminative Projections for Text Similarity Measures"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A novel discriminative training method that projects the raw term vectors into a common, low-dimensional vector space, which not only outperforms existing state-of-the-art approaches, but also achieves high accuracy at low dimensions and is thus more efficient."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21308992"
                        ],
                        "name": "Steven Bird",
                        "slug": "Steven-Bird",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Bird",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Bird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145606490"
                        ],
                        "name": "Ewan Klein",
                        "slug": "Ewan-Klein",
                        "structuredName": {
                            "firstName": "Ewan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ewan Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3213150"
                        ],
                        "name": "E. Loper",
                        "slug": "E.-Loper",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Loper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Loper"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 256
                            }
                        ],
                        "text": "\u2026for regularizing the word parameters toward the learnedWw from PPDB, and\u03bbc for regularizing the compositional parameters (for all models except for the word averaging model) back to their initial values.17 In all cases, we regularize to the universal parameters using L2 regularization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 142
                            }
                        ],
                        "text": "10Note that we pre-processed the training data with the tokenizer from Stanford CoreNLP (Manning et al., 2014) rather than the included NLTK (Bird et al., 2009) tokeniz r."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58803969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a65f23d990231d461418067c808b09d84c19b2c",
            "isKey": false,
            "numCitedBy": 3390,
            "numCiting": 113,
            "paperAbstract": {
                "fragments": [],
                "text": "This book offers a highly accessible introduction to natural language processing, the field that supports a variety of language technologies, from predictive text and email filtering to automatic summarization and translation. With it, you'll learn how to write Python programs that work with large collections of unstructured text. You'll access richly annotated datasets using a comprehensive range of linguistic data structures, and you'll understand the main algorithms for analyzing the content and structure of written communication. Packed with examples and exercises, Natural Language Processing with Python will help you: Extract information from unstructured text, either to guess the topic or identify \"named entities\" Analyze linguistic structure in text, including parsing and semantic analysis Access popular linguistic databases, including WordNet and treebanks Integrate techniques drawn from fields as diverse as linguistics and artificial intelligence This book will help you gain practical skills in natural language processing using the Python programming language and the Natural Language Toolkit (NLTK) open source library. If you're interested in developing web applications, analyzing multilingual news sources, or documenting endangered languages -- or if you're simply curious to have a programmer's perspective on how human language works -- you'll find Natural Language Processing with Python both fascinating and immensely useful."
            },
            "slug": "Natural-Language-Processing-with-Python-Bird-Klein",
            "title": {
                "fragments": [],
                "text": "Natural Language Processing with Python"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This book offers a highly accessible introduction to natural language processing, the field that supports a variety of language technologies, from predictive text and email filtering to automatic summarization and translation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145040725"
                        ],
                        "name": "R. Bernardi",
                        "slug": "R.-Bernardi",
                        "structuredName": {
                            "firstName": "Raffaela",
                            "lastName": "Bernardi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bernardi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2713535"
                        ],
                        "name": "Roberto Zamparelli",
                        "slug": "Roberto-Zamparelli",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Zamparelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roberto Zamparelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61492637,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "3c721b5986f674df242f40ae5cf629b5af81f1e0",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 206,
            "paperAbstract": {
                "fragments": [],
                "text": "The lexicon of any natural language encodes a huge number of distinct word meanings. Just to understand this article, you will need to know what thousands of words mean. The space of possible sentential meanings is infinite: In this article alone, you will encounter many sentences that express ideas you have never heard before, we hope. Statistical semantics has addressed the issue of the vastness of word meaning by proposing methods to harvest meaning automatically from large collections of text (corpora). Formal semantics in the Fregean tradition has developed methods to account for the infinity of sentential meaning based on the crucial insight of compositionality, the idea that meaning of sentences is built incrementally by combining the meanings of their constituents. This article sketches a new approach to semantics that brings together ideas from statistical and formal semantics to account, in parallel, for the richness of lexical meaning and the combinatorial power of sentential semantics. We adopt, in particular, the idea that word meaning can be approximated by the patterns of co-occurrence of words in corpora from statistical semantics, and the idea that compositionality can be captured in terms of a syntax-driven calculus of function application from formal semantics."
            },
            "slug": "Frege-in-Space:-A-Program-for-Composition-Semantics-Baroni-Bernardi",
            "title": {
                "fragments": [],
                "text": "Frege in Space: A Program for Composition Distributional Semantics"
            },
            "venue": {
                "fragments": [],
                "text": "LILT"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117101253"
                        ],
                        "name": "Ke Xu",
                        "slug": "Ke-Xu",
                        "structuredName": {
                            "firstName": "Ke",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ke Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3450996"
                        ],
                        "name": "Ryan Kiros",
                        "slug": "Ryan-Kiros",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Kiros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Kiros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979489"
                        ],
                        "name": "Kyunghyun Cho",
                        "slug": "Kyunghyun-Cho",
                        "structuredName": {
                            "firstName": "Kyunghyun",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyunghyun Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 154
                            }
                        ],
                        "text": "LSTMs have recently been shown to produce state-of-the-art results in a variety of sequence processing tasks (Chen et al., 2015a; Filippova et al., 2015; Xu et al., 2015c; Belinkov & Glass, 2015; Wang & Nyberg, 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 221
                            }
                        ],
                        "text": "\u2026variants, culminating in LSTMs because they have been foundto be effective for many types of sequential data (Graves et al., 2008; 2013; Greff et al., 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015; Ling et al., 2015; Wen et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 51
                            }
                        ],
                        "text": "We also evaluate on the SemEval 2015 Twitter task (Xu et al., 2015b) and the SemEval 2014 Semantic Relatedness task (Marelli et al., 2014), as well as two tasks that use PPDB data(Wieting et al., 2015; Pavlick et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1055111,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d8f2d14af5991d4f0d050d22216825cac3157bd",
            "isKey": false,
            "numCitedBy": 7255,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO."
            },
            "slug": "Show,-Attend-and-Tell:-Neural-Image-Caption-with-Xu-Ba",
            "title": {
                "fragments": [],
                "text": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "An attention based model that automatically learns to describe the content of images is introduced that can be trained in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996134"
                        ],
                        "name": "Razvan Pascanu",
                        "slug": "Razvan-Pascanu",
                        "structuredName": {
                            "firstName": "Razvan",
                            "lastName": "Pascanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Razvan Pascanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 117
                            }
                        ],
                        "text": "We would also like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) and thank NVIDIA Corporation for donati g GPUs used in this research."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14650762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84069287da0a6b488b8c933f3cb5be759cb6237e",
            "isKey": false,
            "numCitedBy": 3803,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section."
            },
            "slug": "On-the-difficulty-of-training-recurrent-neural-Pascanu-Mikolov",
            "title": {
                "fragments": [],
                "text": "On the difficulty of training recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper proposes a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem and validates empirically the hypothesis and proposed solutions."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066087250"
                        ],
                        "name": "Ran Tian",
                        "slug": "Ran-Tian",
                        "structuredName": {
                            "firstName": "Ran",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ran Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764004"
                        ],
                        "name": "Naoaki Okazaki",
                        "slug": "Naoaki-Okazaki",
                        "structuredName": {
                            "firstName": "Naoaki",
                            "lastName": "Okazaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naoaki Okazaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3040648"
                        ],
                        "name": "Kentaro Inui",
                        "slug": "Kentaro-Inui",
                        "structuredName": {
                            "firstName": "Kentaro",
                            "lastName": "Inui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kentaro Inui"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 116
                            }
                        ],
                        "text": "Some have been based on distributional semantics (Baroni et al., 2014; Paperno et al., 2014; Polajnar et al., 2015; Tian et al., 2015), while the current trend is toward development of neural network architectures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16363177,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "896380f5b05ad640ee45d72edd1087da98b365a6",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "Additive composition (Foltz et al. in Discourse Process 15:285\u2013307, 1998; Landauer and Dumais in Psychol Rev 104(2):211, 1997; Mitchell and Lapata in Cognit Sci 34(8):1388\u20131429, 2010) is a widely used method for computing meanings of phrases, which takes the average of vector representations of the constituent words. In this article, we prove an upper bound for the bias of additive composition, which is the first theoretical analysis on compositional frameworks from a machine learning point of view. The bound is written in terms of collocation strength; we prove that the more exclusively two successive words tend to occur together, the more accurate one can guarantee their additive composition as an approximation to the natural phrase vector. Our proof relies on properties of natural language data that are empirically verified, and can be theoretically derived from an assumption that the data is generated from a Hierarchical Pitman\u2013Yor Process. The theory endorses additive composition as a reasonable operation for calculating meanings of phrases, and suggests ways to improve additive compositionality, including: transforming entries of distributional word vectors by a function that meets a specific condition, constructing a novel type of vector representations to make additive composition sensitive to word order, and utilizing singular value decomposition to train word vectors."
            },
            "slug": "The-mechanism-of-additive-composition-Tian-Okazaki",
            "title": {
                "fragments": [],
                "text": "The mechanism of additive composition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "An upper bound for the bias of additive composition is proved, which is the first theoretical analysis on compositional frameworks from a machine learning point of view and suggests ways to improve additive compositionality."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 24
                            }
                        ],
                        "text": "We use the version from Gers et al. (2003) which has the following equations:\nit = \u03c3 (WxiW xt w +Whiht\u22121 +Wcict\u22121 + bi) ft = \u03c3 (WxfW xt w +Whfht\u22121 +Wcfct\u22121 + bf ) ct = ftct\u22121 + it tanh (WxcW xt w +Whcht\u22121 + bc) ot = \u03c3 (WxoW xt w +Whoht\u22121 +Wcoct + bo)\nht = ot tanh(ct)\ngLSTM(x) = h\u22121\nwhere\u03c3 is the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 474078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "047655e733a9eed9a500afd916efa566915b9110",
            "isKey": false,
            "numCitedBy": 1270,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by \"peephole connections\" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars. Without external resets or teacher forcing, our LSTM variant also learns to generate stable streams of precisely timed spikes and other highly nonlinear periodic patterns. This makes LSTM a promising approach for tasks that require the accurate measurement or generation of time intervals."
            },
            "slug": "Learning-Precise-Timing-with-LSTM-Recurrent-Gers-Schraudolph",
            "title": {
                "fragments": [],
                "text": "Learning Precise Timing with LSTM Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work finds that LSTM augmented by \"peephole connections\" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760868"
                        ],
                        "name": "M. Surdeanu",
                        "slug": "M.-Surdeanu",
                        "structuredName": {
                            "firstName": "Mihai",
                            "lastName": "Surdeanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Surdeanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661918"
                        ],
                        "name": "John Bauer",
                        "slug": "John-Bauer",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Bauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784228"
                        ],
                        "name": "J. Finkel",
                        "slug": "J.-Finkel",
                        "structuredName": {
                            "firstName": "Jenny",
                            "lastName": "Finkel",
                            "middleNames": [
                                "Rose"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Finkel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105138"
                        ],
                        "name": "Steven Bethard",
                        "slug": "Steven-Bethard",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Bethard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Bethard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2240597"
                        ],
                        "name": "David McClosky",
                        "slug": "David-McClosky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McClosky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David McClosky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 199
                            }
                        ],
                        "text": "\u2026for regularizing the word parameters toward the learnedWw from PPDB, and\u03bbc for regularizing the compositional parameters (for all models except for the word averaging model) back to their initial values.17 In all cases, we regularize to the universal parameters using L2 regularization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 89
                            }
                        ],
                        "text": "10Note that we pre-processed the training data with the tokenizer from Stanford CoreNLP (Manning et al., 2014) rather than the included NLTK (Bird et al., 2009) tokeniz r."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14068874,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f5102ec3f70d0dea98c957cc2cab4d15d83a2da",
            "isKey": false,
            "numCitedBy": 6061,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage."
            },
            "slug": "The-Stanford-CoreNLP-Natural-Language-Processing-Manning-Surdeanu",
            "title": {
                "fragments": [],
                "text": "The Stanford CoreNLP Natural Language Processing Toolkit"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The design and use of the Stanford CoreNLP toolkit is described, an extensible pipeline that provides core natural language analysis, and it is suggested that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1425082935"
                        ],
                        "name": "Xinyun Chen",
                        "slug": "Xinyun-Chen",
                        "structuredName": {
                            "firstName": "Xinyun",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinyun Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17735339,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94e3e7bc3d23276f0ee2d1cb8f9d14aa19668d5f",
            "isKey": false,
            "numCitedBy": 488,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system."
            },
            "slug": "Under-Review-as-a-Conference-Paper-at-Iclr-2017-Ex-Chen",
            "title": {
                "fragments": [],
                "text": "Under Review as a Conference Paper at Iclr 2017 Delving into Transferable Adversarial Ex- Amples and Black-box Attacks"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work is the first to conduct an extensive study of the transferability over large models and a large scale dataset, and it is also theFirst to study the transferabilities of targeted adversarial examples with their target labels."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34902160"
                        ],
                        "name": "Jeff Mitchell",
                        "slug": "Jeff-Mitchell",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 0
                            }
                        ],
                        "text": "Mitchell & Lapata (2008; 2010) considered bigram compositionality, comparing many functions for composing two word vectors into a single vector to represent their bigram."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18597583,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5d67d1dc671bce42a9daac0c3605adb3fcfc697",
            "isKey": false,
            "numCitedBy": 731,
            "numCiting": 133,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a framework for representing the meaning of phrases and sentences in vector space. Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments."
            },
            "slug": "Vector-based-Models-of-Semantic-Composition-Mitchell-Lapata",
            "title": {
                "fragments": [],
                "text": "Vector-based Models of Semantic Composition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Under this framework, a wide range of composition models are introduced which are evaluated empirically on a sentence similarity task and demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143913738"
                        ],
                        "name": "Santiago Fern\u00e1ndez",
                        "slug": "Santiago-Fern\u00e1ndez",
                        "structuredName": {
                            "firstName": "Santiago",
                            "lastName": "Fern\u00e1ndez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Santiago Fern\u00e1ndez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 146
                            }
                        ],
                        "text": "\u2026consider several recurrent network variants, culminating in LSTMs because they have been foundto be effective for many types of sequential data (Graves et al., 2008; 2013; Greff et al., 2015), including text (Sutskever et al., 2014; Vinyals et al., 2014; Xu et al., 2015a; Hermann et al., 2015;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14986843,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68db33b01ef82cbafb440e5f4bee30458cbb9871",
            "isKey": false,
            "numCitedBy": 234,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In online handwriting recognition the trajectory of the pen is recorded during writing. Although the trajectory provides a compact and complete representation of the written output, it is hard to transcribe directly, because each letter is spread over many pen locations. Most recognition systems therefore employ sophisticated preprocessing techniques to put the inputs into a more localised form. However these techniques require considerable human effort, and are specific to particular languages and alphabets. This paper describes a system capable of directly transcribing raw online handwriting data. The system consists of an advanced recurrent neural network with an output layer designed for sequence labelling, combined with a probabilistic language model. In experiments on an unconstrained online database, we record excellent results using either raw or preprocessed data, well outperforming a state-of-the-art HMM based system in both cases."
            },
            "slug": "Unconstrained-On-line-Handwriting-Recognition-with-Graves-Fern\u00e1ndez",
            "title": {
                "fragments": [],
                "text": "Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A system capable of directly transcribing raw online handwriting data is described, consisting of an advanced recurrent neural network with an output layer designed for sequence labelling, combined with a probabilistic language model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695842"
                        ],
                        "name": "M. Dzikovska",
                        "slug": "M.-Dzikovska",
                        "structuredName": {
                            "firstName": "Myroslava",
                            "lastName": "Dzikovska",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dzikovska"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47147237"
                        ],
                        "name": "Johanna D. Moore",
                        "slug": "Johanna-D.-Moore",
                        "structuredName": {
                            "firstName": "Johanna",
                            "lastName": "Moore",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johanna D. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2402485"
                        ],
                        "name": "Natalie B. Steinhauser",
                        "slug": "Natalie-B.-Steinhauser",
                        "structuredName": {
                            "firstName": "Natalie",
                            "lastName": "Steinhauser",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Natalie B. Steinhauser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26333504"
                        ],
                        "name": "Gwendolyn E. Campbell",
                        "slug": "Gwendolyn-E.-Campbell",
                        "structuredName": {
                            "firstName": "Gwendolyn",
                            "lastName": "Campbell",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gwendolyn E. Campbell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9882120"
                        ],
                        "name": "Elaine Farrow",
                        "slug": "Elaine-Farrow",
                        "structuredName": {
                            "firstName": "Elaine",
                            "lastName": "Farrow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elaine Farrow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730221"
                        ],
                        "name": "Charles B. Callaway",
                        "slug": "Charles-B.-Callaway",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Callaway",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles B. Callaway"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 100
                            }
                        ],
                        "text": "Since we found thatPARAGRAM-PHRASE embeddings have such strong performance, we trained this model on more data from PPDB and also used more data for hyperparameter tuning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 117
                            }
                        ],
                        "text": "Questions and Answers: Paired answers to the same question from StackExchange (answersforums) and the BEETLE corpus (Dzikovska et al., 2010) (answer -students) were used in 2015."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2988106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b19897e1fabfc87093de93857cdfad7c8c8abe9",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present BEETLE II, a tutorial dialogue system designed to accept unrestricted language input and support experimentation with different tutorial planning and dialogue strategies. Our first system evaluation used two different tutorial policies and demonstrated that the system can be successfully used to study the impact of different approaches to tutoring. In the future, the system can also be used to experiment with a variety of natural language interpretation and generation techniques."
            },
            "slug": "Beetle-II:-A-System-for-Tutoring-and-Computational-Dzikovska-Moore",
            "title": {
                "fragments": [],
                "text": "Beetle II: A System for Tutoring and Computational Linguistics Experimentation"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "BEETLE II is presented, a tutorial dialogue system designed to accept unrestricted language input and support experimentation with different tutorial planning and dialogue strategies and can be successfully used to study the impact of different approaches to tutoring."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50064811"
                        ],
                        "name": "L. Finkelstein",
                        "slug": "L.-Finkelstein",
                        "structuredName": {
                            "firstName": "Lev",
                            "lastName": "Finkelstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Finkelstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718798"
                        ],
                        "name": "E. Gabrilovich",
                        "slug": "E.-Gabrilovich",
                        "structuredName": {
                            "firstName": "Evgeniy",
                            "lastName": "Gabrilovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gabrilovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745572"
                        ],
                        "name": "Y. Matias",
                        "slug": "Y.-Matias",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Matias",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Matias"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747801"
                        ],
                        "name": "E. Rivlin",
                        "slug": "E.-Rivlin",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Rivlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rivlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3316511"
                        ],
                        "name": "Zach Solan",
                        "slug": "Zach-Solan",
                        "structuredName": {
                            "firstName": "Zach",
                            "lastName": "Solan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zach Solan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073936"
                        ],
                        "name": "G. Wolfman",
                        "slug": "G.-Wolfman",
                        "structuredName": {
                            "firstName": "Gadi",
                            "lastName": "Wolfman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wolfman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779370"
                        ],
                        "name": "E. Ruppin",
                        "slug": "E.-Ruppin",
                        "structuredName": {
                            "firstName": "Eytan",
                            "lastName": "Ruppin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ruppin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12956853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0c01df98a6b633b25c96c1a99b713ac96f1c5be",
            "isKey": false,
            "numCitedBy": 1725,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Keyword-based search engines are in widespread use today as a popular means for Web-based information retrieval. Although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. This paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. This paradigm is implemented in practice in the IntelliZap system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (\"the context\"). The context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. Search results are then semantically reranked, using context. Experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the Web."
            },
            "slug": "Placing-search-in-context:-the-concept-revisited-Finkelstein-Gabrilovich",
            "title": {
                "fragments": [],
                "text": "Placing search in context: the concept revisited"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new conceptual paradigm for performing search in context is presented, that largely automates the search process, providing even non-professional users with highly relevant results."
            },
            "venue": {
                "fragments": [],
                "text": "TOIS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734693"
                        ],
                        "name": "John C. Duchi",
                        "slug": "John-C.-Duchi",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Duchi",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Duchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34840427"
                        ],
                        "name": "Elad Hazan",
                        "slug": "Elad-Hazan",
                        "structuredName": {
                            "firstName": "Elad",
                            "lastName": "Hazan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elad Hazan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 106
                            }
                        ],
                        "text": "We tuned hyperparameters on the development set of each dataset16 as well as on two optimization schemes: AdaGrad with learning rate of 0.05 and Adam with a learning rate of 0.001."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 89
                            }
                        ],
                        "text": "We again tuned \u03bbs over{10\u22123, 10\u22124, 10\u22125, 10\u22126} and tuned over the two optimization schemes of AdaGrad with learning rate of 0.05 and Adam with a learning rate of 0.001."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 55
                            }
                        ],
                        "text": "We also tune over optimization method (either AdaGrad (Duchi et al., 2011) or Adam(Kingma & Ba, 2014)), learning rate (from{0.05, 0.005, 0.0005}), whether to clip the gradients with threshold 1 (Pascanu etal., 2012), and whether to use MIX or MAX sampling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 21
                            }
                        ],
                        "text": "To optimize, we used AdaGrad with a learning rate of 0.05."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 74
                            }
                        ],
                        "text": "We would like to thank Yoon Kim, the anonymous reviewers, andthe area chair for their valuable comments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We have made this trained model available to the research community.13"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 538820,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "413c1142de9d91804d6d11c67ff3fed59c9fc279",
            "isKey": true,
            "numCitedBy": 8026,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms."
            },
            "slug": "Adaptive-Subgradient-Methods-for-Online-Learning-Duchi-Hazan",
            "title": {
                "fragments": [],
                "text": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work describes and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal functions that can be chosen in hindsight."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8536286"
                        ],
                        "name": "A. Bosselut",
                        "slug": "A.-Bosselut",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Bosselut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bosselut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39455775"
                        ],
                        "name": "Omer Levy",
                        "slug": "Omer-Levy",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14487640"
                        ],
                        "name": "Ari Holtzman",
                        "slug": "Ari-Holtzman",
                        "structuredName": {
                            "firstName": "Ari",
                            "lastName": "Holtzman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ari Holtzman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40479850"
                        ],
                        "name": "C. Ennis",
                        "slug": "C.-Ennis",
                        "structuredName": {
                            "firstName": "Corin",
                            "lastName": "Ennis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145197953"
                        ],
                        "name": "D. Fox",
                        "slug": "D.-Fox",
                        "structuredName": {
                            "firstName": "Dieter",
                            "lastName": "Fox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699545"
                        ],
                        "name": "Yejin Choi",
                        "slug": "Yejin-Choi",
                        "structuredName": {
                            "firstName": "Yejin",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yejin Choi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 51540074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcd857d75841aa3e92cd4284a8818aba9f6c0c3f",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Understanding procedural language requires anticipating the causal effects of actions, even when they are not explicitly stated. In this work, we introduce Neural Process Networks to understand procedural text through (neural) simulation of action dynamics. Our model complements existing memory architectures with dynamic entity tracking by explicitly modeling actions as state transformers. The model updates the states of the entities by executing learned action operators. Empirical results demonstrate that our proposed model can reason about the unstated causal effects of actions, allowing it to provide more accurate contextual information for understanding and generating procedural text, all while offering more interpretable internal representations than existing alternatives."
            },
            "slug": "Published-as-a-conference-paper-at-ICLR-2018-S-A-D-Bosselut-Levy",
            "title": {
                "fragments": [],
                "text": "Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work introduces Neural Process Networks to understand procedural text through (neural) simulation of action dynamics, and complements existing memory architectures with dynamic entity tracking by explicitly modeling actions as state transformers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3227028"
                        ],
                        "name": "Fr\u00e9d\u00e9ric Bastien",
                        "slug": "Fr\u00e9d\u00e9ric-Bastien",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Bastien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fr\u00e9d\u00e9ric Bastien"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3087941"
                        ],
                        "name": "Pascal Lamblin",
                        "slug": "Pascal-Lamblin",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Lamblin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Lamblin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996134"
                        ],
                        "name": "Razvan Pascanu",
                        "slug": "Razvan-Pascanu",
                        "structuredName": {
                            "firstName": "Razvan",
                            "lastName": "Pascanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Razvan Pascanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32837403"
                        ],
                        "name": "J. Bergstra",
                        "slug": "J.-Bergstra",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bergstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bergstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47944877"
                        ],
                        "name": "Arnaud Bergeron",
                        "slug": "Arnaud-Bergeron",
                        "structuredName": {
                            "firstName": "Arnaud",
                            "lastName": "Bergeron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arnaud Bergeron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065828537"
                        ],
                        "name": "Nicolas Bouchard",
                        "slug": "Nicolas-Bouchard",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Bouchard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Bouchard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393680089"
                        ],
                        "name": "David Warde-Farley",
                        "slug": "David-Warde-Farley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Warde-Farley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Warde-Farley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 77
                            }
                        ],
                        "text": "We would also like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) and thank NVIDIA Corporation for donati g GPUs used in this research."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8180128,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "855d0f722d75cc56a66a00ede18ace96bafee6bd",
            "isKey": false,
            "numCitedBy": 1374,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Theano is a linear algebra compiler that optimizes a user's symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks."
            },
            "slug": "Theano:-new-features-and-speed-improvements-Bastien-Lamblin",
            "title": {
                "fragments": [],
                "text": "Theano: new features and speed improvements"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "New features and efficiency improvements to Theano are presented, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 257
                            }
                        ],
                        "text": "We compare the same suite of compositional architectur s for three supervised NLP tasks: sentence similarity and textual entailment using the 2014 SemEval SICK dataset (Marelli et al., 2014), and sentiment classification using the Stanford Sentim Treebank (Socher et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 198
                            }
                        ],
                        "text": "We also investigated how these sentence encoding models do o n three tasks: The SICK similarity task, the SICK entailment task, and the Stanford Sentiment T reebank (SST) binary classification task (Socher et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 145
                            }
                        ],
                        "text": "However, on sentiment classification, we find that the LSTM performs verystrongly\u2014even recording new state-of-the-art performance on the Stanford Sentiment Treebank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 221
                            }
                        ],
                        "text": "To address these questions, we\n13Available athttp://ttic.uchicago.edu/ \u02dc wieting .\nused three tasks: The SICK similarity task, the SICK entailment task, and the Stanford Sentiment Treebank (SST) binary classification task (Socher et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 91
                            }
                        ],
                        "text": "2% on the coarse-grained sentiment classification task of the Sta nford Sentiment Treebank (Socher et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "compositionality over a sentiment treebank"
            },
            "venue": {
                "fragments": [],
                "text": "InProceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771118"
                        ],
                        "name": "J. Wieting",
                        "slug": "J.-Wieting",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Wieting",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wieting"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977268"
                        ],
                        "name": "Mohit Bansal",
                        "slug": "Mohit-Bansal",
                        "structuredName": {
                            "firstName": "Mohit",
                            "lastName": "Bansal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohit Bansal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700980"
                        ],
                        "name": "Kevin Gimpel",
                        "slug": "Kevin-Gimpel",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Gimpel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Gimpel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2924113"
                        ],
                        "name": "Karen Livescu",
                        "slug": "Karen-Livescu",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Livescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Livescu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 68
                            }
                        ],
                        "text": "The second, called Annotated-PPDB, was developed in our prior work (Wieting et al., 2015) and is a small set of 1,000 annotated phrase pairs that were filtered to focus on challenging paraphrase phenomena."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 135
                            }
                        ],
                        "text": "\u2026aver ging (Yu & Dredze, 2015), recursive neural networks based on parse structure (Socher et al., 2011; 2; 2013;\u0307Irsoy & Cardie, 2014; Wieting et al., 2015), recursive networks based on non-syntactic hierarchical structure (Zhao et al., 2015; Chen et al., 2015b), convolutional\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 40
                            }
                        ],
                        "text": "3These are 300-dimensional vectors from Wieting et al. (2015) and are available at http://ttic.uchicago.edu/ \u02dc wieting ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The intuition is that we want the two phrases to be more similar to each other (cos g(x1), g(x2))) than either is to their respective negative examplest1 andt2, by a margin of at least\u03b4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 180
                            }
                        ],
                        "text": "We also evaluate on the SemEval 2015 Twitter task (Xu et al., 2015b) and the SemEval 2014 Semantic Relatedness task (Marelli et al., 2014), as well as two tasks that use PPDB data(Wieting et al., 2015; Pavlick et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 175
                            }
                        ],
                        "text": "We compare to three baselines: skip-thoughtvectors10 (Kiros et al., 2015), denoted \u201cST\u201d, averaged GloVe11 vectors (Pennington et al., 2014), and averagedPARAGRAM-SL999 vectors (Wieting et al., 2015), denoted \u201cPSL\u201d."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 150
                            }
                        ],
                        "text": "\u2026word embeddings:\ngproj(x) = Wp\n(\n1\nn\nn \u2211\ni\nW xiw\n)\n+ b\n7In prior work, we experimented with recursive neural networks on binarized parses of the PPDB (Wieting et al., 2015), but we found that many of the phrases in PPDB are not sentences or even constituents, causing the parser to have unexpected\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 32
                            }
                        ],
                        "text": "For the SICK similarity task, we minimize the objective function14 from Tai et al. (2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 33
                            }
                        ],
                        "text": "We mostly follow the approach of Wieting et al. (2015)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 129
                            }
                        ],
                        "text": "10Note that we pre-processed the training data with the tokenizer from Stanford CoreNLP (Manning et al., 2014) rather than the included NLTK (Bird et al., 2009) tokeniz r."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 271
                            }
                        ],
                        "text": "\u2026input representations in order to maximize similarity in the learned space (Weston et al., 2010; Yih et al., 2011; Huang et al., 2013; Hermann & Blunsom, 2014; Socher et al., 2014; Faruqui & Dyer, 2014; Bordes et al., 2014b;a; Lu et al., 2015), including our priorw rk (Wieting et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57564106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6aed419cb7926218d1ffe378c0fc63b1a82f6302",
            "isKey": false,
            "numCitedBy": 249,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The Paraphrase Database (PPDB; Ganitkevitch et al., 2013) is an extensive semantic resource, consisting of a list of phrase pairs with (heuristic) confidence estimates. However, it is still unclear how it can best be used, due to the heuristic nature of the confidences and its necessarily incomplete coverage. We propose models to leverage the phrase pairs from the PPDB to build parametric paraphrase models that score paraphrase pairs more accurately than the PPDB\u2019s internal scores while simultaneously improving its coverage. They allow for learning phrase embeddings as well as improved word embeddings. Moreover, we introduce two new, manually annotated datasets to evaluate short-phrase paraphrasing models. Using our paraphrase model trained using PPDB, we achieve state-of-the-art results on standard word and bigram similarity tasks and beat strong baselines on our new short phrase paraphrase tasks."
            },
            "slug": "From-Paraphrase-Database-to-Compositional-Model-and-Wieting-Bansal",
            "title": {
                "fragments": [],
                "text": "From Paraphrase Database to Compositional Paraphrase Model and Back"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work proposes models to leverage the phrase pairs from the Paraphrase Database to build parametric paraphrase models that score paraphrase pairs more accurately than the PPDB\u2019s internal scores while simultaneously improving its coverage."
            },
            "venue": {
                "fragments": [],
                "text": "TACL"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deep recursive neural netwo"
            },
            "venue": {
                "fragments": [],
                "text": "hrough data. InProceedings of the 22nd ACM international conference on Conference on information & knowledge management,"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive subgr"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semeval-2012 task 6: A"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning composition models for phr"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning composition models for phr"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 80
                            }
                        ],
                        "text": "To learn the models in this section, we minimize the objective function 12 from Tai et al. (2015). Given a score for a sentence pair in the range[1,K], whereK is an integer, with sentence representations hL andhR, and model parameters\u03b8, they first compute: h\u00d7 = hL \u2299 hR, h+ = |hL \u2212 hR|, hs = \u03c3 ("
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 16
                            }
                        ],
                        "text": "We also mention Tai et al. (2015) which had a simi lar LSTM result on the SICK dataset (Pearson\u2019sr of 85."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "o sequence learning with neural networks. InAdvances in neural information processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ppdb: The paraphrase database. InHLT-NAACL"
            },
            "venue": {
                "fragments": [],
                "text": "The Association for Computational Linguistics,"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "SemEval-2015 task 2: Semantic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 11
                            }
                        ],
                        "text": "6Available athttp://ttic.uchicago.edu/ \u02dc wieting .\ndition performed strongly."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Convolutional neural network for paraphrase iden- tification"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "URL http://papers.nips.cc/paper/3213-unconstrained-on-line-handwriting-recognition-with"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A long short-term memory model for"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Composition in distrib"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A comparison of vector"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 116
                            }
                        ],
                        "text": "Some have been based on distributional semantics (Baroni et al., 2014; Paperno et al., 2014; Polajnar et al., 2015; Tian et al., 2015), while the current trend is toward development of neural network architectures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 49
                            }
                        ],
                        "text": "Some have been based on distributional semantics (Baroni et al., 2014 ; Paperno et al., 2014; Polajnar et al., 2015; Tian et al., 2015), while the current trend is toward de velopment of neural network architectures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The mechanis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving vector space wor"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 11
                            }
                        ],
                        "text": "(2015) and Li et al. (2015a) require significant training time on GPUs, on the order of multiple weeks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A hierarch"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 54
                            }
                        ],
                        "text": "We would also like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) and thank NVIDIA Corporation for donati g GPUs used in this research."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theano: a CPU and GPU math expression compiler"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Python for Scientific Computing Conference (SciPy)"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A practi"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Science,"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Eneko agirre"
            },
            "venue": {
                "fragments": [],
                "text": "Eneko agirre"
            },
            "year": 2013
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 55,
            "methodology": 46,
            "result": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 98,
        "totalPages": 10
    },
    "page_url": "https://www.semanticscholar.org/paper/Towards-Universal-Paraphrastic-Sentence-Embeddings-Wieting-Bansal/395044a2e3f5624b2471fb28826e7dbb1009356e?sort=total-citations"
}