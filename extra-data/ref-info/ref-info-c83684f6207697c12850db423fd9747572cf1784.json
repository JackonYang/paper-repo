{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2546518"
                        ],
                        "name": "D. Plaut",
                        "slug": "D.-Plaut",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Plaut",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Plaut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 28
                            }
                        ],
                        "text": "A common tr aining approach [29, 41] is to minimize a penalized error term such as 1 L (Yi - o(x;,W)) 2+ '\\S (w)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 163
                            }
                        ],
                        "text": "The top graph in figur e 1 shows the case for the smoothing prior just derived , an d the bottom graph shows the case for the prior correspo nding to weight decay [29],"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 139
                            }
                        ],
                        "text": "A prior for a network is related to the notions of \"penalty te rm\" or \"regularizer\" used in the statist ics literatur e [18], weight decay [29] and weight elimination [41], and the \"complexity te rm\" used in minimum enco ding methods [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 175
                            }
                        ],
                        "text": "This sect ion shows how to take a penalty term such as quadratic smoothness and construct a pr ior th at requires no set t ing of additional hyperparameters or decay fact ors [29, 41, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15150815,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4a42b2104ca8ff891ae77c40a915d4c94c8f8428",
            "isKey": true,
            "numCitedBy": 390,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Rumelhart, Hinton and Williams (Rumelhart 86) describe a learning procedure for layered networks of deterministic, neuron-like units. This paper describes further research on the learning procedure. We start by describing the units, the way they are connected, the learning procedure, and the extension to iterative nets. We then give an example in which a network learns a set of filters that enable it to discriminate formant-like patterns in the presence of noise. The speed of learning is strongly dependent on the shape of the surface formed by the error measure in weight space . We give examples of the shape of the error surface for a typical task and illustrate how an acceleration method speeds up descent in weight space. The main drawback of the learning procedure is the way it scales as the size of the task and the network increases. We give some preliminary results on scaling and show how the magnitude of the optimal weight changes depends on the fan-in of the units. Additional results illustrate the effects on learning speed of the amount of interaction between the weights. A variation of the learning procedure that back-propagates desired state information rather than error gradients is developed and compared with the standard procedure. Finally, we discuss the relationship between our iterative networks and the analog networks described by Hopefield and Tank (Hopfield 85). The learning procedure can discover appropriate weights in their kind of network, as well as determine an optimal schedule for varying the nonlinearity of the units during a search."
            },
            "slug": "Experiments-on-Learning-by-Back-Propagation.-Plaut-Nowlan",
            "title": {
                "fragments": [],
                "text": "Experiments on Learning by Back Propagation."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The learning procedure can discover appropriate weights in their kind of network, as well as determine an optimal schedule for varying the nonlinearity of the units during a search."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144924970"
                        ],
                        "name": "H. Seung",
                        "slug": "H.-Seung",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Seung",
                            "middleNames": [
                                "Sebastian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Seung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720547"
                        ],
                        "name": "H. Sompolinsky",
                        "slug": "H.-Sompolinsky",
                        "structuredName": {
                            "firstName": "Haim",
                            "lastName": "Sompolinsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sompolinsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 154
                            }
                        ],
                        "text": "Statist ical mechanics theory has been developed in the context of training a perceptron in a noise-free environment to estimate the generalization error [35]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11178960,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "d66c28b43191d3bd59700430e0530cb722861d64",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning from examples in feedforward neural networks is studied using equilibrium statistical mechanics. Such an analysis is valid for stochastic learning algorithms that lead to a Gibbs distribution in the network weight space. Two simple approximations to the exact theory are presented: the high temperature limit and the annealed approximation. Within these approximations, we study models of perceptron learning of realizable target rules. In each model, the target rule is perfectly realizable because it is given by another perceptron of identical architecture. We focus on the generalization curve , i.e. the average generalization error as a function of the number of examples. For continuously varying weights learning is known to be gradual, with generalization curves that asymptotically obey inverse power laws. Here we discuss two model perceptrons, with weights that are constrained to be discrete, that exhibit sudden learning. For a linear output, there is a first-order transition occurring at low temperatures, from a state of poor generalization to a state of good generalization. Beyond the transition, the generalization error decays exponentially to zero. For a boolean output, the first order transition is to perfect generalization at all temperatures. Monte Carlo simulations confirm that these approximate analytical results are quantitatively accurate at high temperatures and qualitatively correct at low temperatures. For unrealizable rules, however, the annealed approximation breaks down in general. Finally, we propose a general classification of generalization curves in models of realizable rules."
            },
            "slug": "Learning-curves-in-large-neural-networks-Seung-Sompolinsky",
            "title": {
                "fragments": [],
                "text": "Learning curves in large neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Two model perceptrons, with weights that are constrained to be discrete, that exhibit sudden learning are discussed, and a general classification of generalization curves in models of realizable rules is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '91"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794321"
                        ],
                        "name": "B. Huberman",
                        "slug": "B.-Huberman",
                        "structuredName": {
                            "firstName": "Bernardo",
                            "lastName": "Huberman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Huberman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36452235,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2cee043045b529fceda7964a70e626d45657245a",
            "isKey": false,
            "numCitedBy": 842,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the effectiveness of connectionist architectures for predicting the future behavior of nonlinear dynamical systems. We focus on real-world time series of limited record length. Two examples are analyzed: the benchmark sunspot series and chaotic data from a computational ecosystem. The problem of overfitting, particularly serious for short records of noisy data, is addressed both by using the statistical method of validation and by adding a complexity term to the cost function (\"back-propagation with weight-elimination\"). The dimension of the dynamics underlying the time series, its Liapunov coefficient, and its nonlinearity can be determined via the network. We also show why sigmoid units are superior in performance to radial basis functions for high-dimensional input spaces. Furthermore, since the ultimate goal is accuracy in the prediction, we find that sigmoid networks trained with the weight-elimination algorithm outperform traditional nonlinear statistical approaches."
            },
            "slug": "Predicting-the-Future:-a-Connectionist-Approach-Weigend-Huberman",
            "title": {
                "fragments": [],
                "text": "Predicting the Future: a Connectionist Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Since the ultimate goal is accuracy in the prediction, it is found that sigmoid networks trained with the weight-elimination algorithm outperform traditional nonlinear statistical approaches."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Neural Syst."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8992604"
                        ],
                        "name": "E. Levin",
                        "slug": "E.-Levin",
                        "structuredName": {
                            "firstName": "Esther",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Levin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "an ensemble of networks [22] corresponds to the Bayesian post erior."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "Notions such as the \"t rue\" subject ive probab ility function [13], the \"correct\" likelihood funct ion derived from th e addit ive energy funct ion [22], and the use of probabili st ic mod eling in the context of \"unrealizable\" models [35J (when ass umpt ion (I) fails so the search space does not contain the \"true\" model) are only par ti ally consiste nt wit h a Bayesian approach ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5254307,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0658717c77b812cab5943f13564e1aa2f06f6e71",
            "isKey": false,
            "numCitedBy": 253,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A general statistical description of the problem of learning from examples is presented. Learning in layered networks is posed as a search in the network parameter space for a network that minimizes an additive error function of a statistically independent examples. By imposing the equivalence of the minimum error and the maximum likelihood criteria for training the network, the Gibbs distribution on the ensemble of networks with a fixed architecture is derived. The probability of correct prediction of a novel example can be expressed using the ensemble, serving as a measure to the network's generalization ability. The entropy of the prediction distribution is shown to be a consistent measure of the network's performance. The proposed formalism is applied to the problems of selecting an optimal architecture and the prediction of learning curves. >"
            },
            "slug": "A-statistical-approach-to-learning-and-in-layered-Levin-Tishby",
            "title": {
                "fragments": [],
                "text": "A statistical approach to learning and generalization in layered neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The proposed formalism is applied to the problems of selecting an optimal architecture and the prediction of learning curves and the Gibbs distribution on the ensemble of networks with a fixed architecture is derived."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '89"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15659829,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "25406e6733a698bfc4ac836f8e74f458e75dad4f",
            "isKey": false,
            "numCitedBy": 1696,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 < \u220a 1/8. We show that if m O(W/\u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 \u220a of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than (W/\u220a) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 \u220a fraction of the future test examples."
            },
            "slug": "What-Size-Net-Gives-Valid-Generalization-Baum-Haussler",
            "title": {
                "fragments": [],
                "text": "What Size Net Gives Valid Generalization?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if m O(W/ \u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 2 \u220a of future test examples drawn from the same distribution."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144158372"
                        ],
                        "name": "F. Wilczek",
                        "slug": "F.-Wilczek",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Wilczek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Wilczek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 82
                            }
                        ],
                        "text": "These cost functions differ from maximum likelihood methods for network tr aining [8, 11] in that they introduce the important prior te rm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10578219,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d9ed799fcc2ba2f929532a4f403091198bcfd83",
            "isKey": false,
            "numCitedBy": 187,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose that the back propagation algorithm for supervised learning can be generalized, put on a satisfactory conceptual footing, and very likely made more efficient by defining the values of the output and input neurons as probabilities and varying the synaptic weights in the gradient direction of the log likelihood, rather than the 'error'."
            },
            "slug": "Supervised-Learning-of-Probability-Distributions-by-Baum-Wilczek",
            "title": {
                "fragments": [],
                "text": "Supervised Learning of Probability Distributions by Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The back propagation algorithm for supervised learning can be generalized, put on a satisfactory conceptual footing, and very likely made more efficient by defining the values of the output and input neurons as probabilities and varying the synaptic weights in the gradient direction of the log likelihood, rather than the 'error'."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143605816"
                        ],
                        "name": "M. Ishikawa",
                        "slug": "M.-Ishikawa",
                        "structuredName": {
                            "firstName": "Masumi",
                            "lastName": "Ishikawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ishikawa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "So if the magnitude of a weight is within a standard deviation , the weight can safely be set to zero [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1808996,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4327281ca97c5060c14c5e9e9854c6eaf26f94f",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Summary form only given, as follows. Backpropagation learning suffers from serious drawbacks: first the necessity of a priori specification of a model structure, and second the difficulty in interpreting hidden units. To cope with these drawbacks the author proposes a novel learning algorithm, called structural learning algorithm, which generates a skeletal structure of a network: a network in which minimum number of links and a minimum number of hidden units are actually used. The resulting skeletal structure solves the first difficulty of trial and error. It also solves the second difficulty due to its clarity. In addition to these two benefits, the structural learning algorithm is also advantageous in dealing with a network composed of multiple modules. It explains how links from other modules emerge, while pruning those from the outside world full of redundant information.<<ETX>>"
            },
            "slug": "A-structural-learning-algorithm-with-forgetting-of-Ishikawa",
            "title": {
                "fragments": [],
                "text": "A structural learning algorithm with forgetting of link weights"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel learning algorithm is proposed, called structural learning algorithm, which generates a skeletal structure of a network: a network in which Minimum number of links and a minimum number of hidden units are actually used, which solves the first difficulty of trial and error."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804836"
                        ],
                        "name": "G. Towell",
                        "slug": "G.-Towell",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Towell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Towell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734317"
                        ],
                        "name": "J. Shavlik",
                        "slug": "J.-Shavlik",
                        "structuredName": {
                            "firstName": "Jude",
                            "lastName": "Shavlik",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shavlik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767011"
                        ],
                        "name": "M. Noordewier",
                        "slug": "M.-Noordewier",
                        "structuredName": {
                            "firstName": "Michiel",
                            "lastName": "Noordewier",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Noordewier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "\" Schemes for initializat ion of weights as described in [38] are a step in this direct ion ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 364,
                                "start": 360
                            }
                        ],
                        "text": "If network weights have some clear int erpret ation then we can configure the network in coherent ways, an expe rt can view the knowledge represented by the network, and we can more readily assign meaningful priors or some other initialization for the network weight s (for instance, a non-probabili stic scheme for initialization of weight s is describ ed in [38])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14454260,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4918b7578a0fa4a9d88d4de54b14a7307172d5a5",
            "isKey": false,
            "numCitedBy": 427,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard algorithms for explanation-based learning require complete and correct knowledge bases. The KBANN system relaxes this constraint through the use of empirical learning methods to refine approximately correct knowledge. This knowledge is used to determine the structure of an artificial neural network and the weights on its links, thereby making the knowledge accessible for modification by neural learning. KBANN is evaluated by empirical tests in the domain of molecular biology. Networks created by KBANN are shown to be superior, in terms of their ability to correctly classify unseen examples, to randomly initialized neural networks, decision trees, \"nearest neighbor\" matching, and standard techniques reported in the biological literature. In addition, KBANN'S networks improve the initial knowledge in biologically interesting ways."
            },
            "slug": "Refinement-of-approximate-domain-theories-by-neural-Towell-Shavlik",
            "title": {
                "fragments": [],
                "text": "Refinement of approximate domain theories by knowledge-based neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The KBANN system relaxes this constraint through the use of empirical learning methods to refine approximately correct knowledge, used to determine the structure of an artificial neural network and the weights on its links, thereby making the knowledge accessible for modification by neural learning."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI 1990"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402969335"
                        ],
                        "name": "A. El-Jaroudi",
                        "slug": "A.-El-Jaroudi",
                        "structuredName": {
                            "firstName": "Amro",
                            "lastName": "El-Jaroudi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. El-Jaroudi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10080270"
                        ],
                        "name": "J. Makhoul",
                        "slug": "J.-Makhoul",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Makhoul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Makhoul"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40598043,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "adf81acbfb348c7ebacb97858beb3d193766bb2a",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors introduce an error criterion for training which improves the performance of neural nets as posterior probability estimators, as compared to using least squares. The proposed criterion is similar to the Kullback-Leibler information measure and is simple to use. A straightforward iterative algorithm for the minimization of the error criterion which has been shown to have good convergence properties is described. The authors applied the proposed technique to some classification examples and showed it to produce better posterior probability estimates than least squares, especially for low probabilities"
            },
            "slug": "A-new-error-criterion-for-posterior-probability-El-Jaroudi-Makhoul",
            "title": {
                "fragments": [],
                "text": "A new error criterion for posterior probability estimation with neural nets"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "An error criterion for training is introduced which improves the performance of neural nets as posterior probability estimators, as compared to using least squares, and is similar to the Kullback-Leibler information measure."
            },
            "venue": {
                "fragments": [],
                "text": "1990 IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20658113"
                        ],
                        "name": "A. Barron",
                        "slug": "A.-Barron",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barron",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 37
                            }
                        ],
                        "text": "Several minimum encoding approac hes [40, 2, 33] are also exp lained in section 6."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "Wallace and Freeman [40] and Barron and Cover [2] interpret a form like Eval(w) as the cost of encoding y and w given x."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 133
                            }
                        ],
                        "text": "We discuss these br iefly her e to draw the strong connections between Eval(w) and the ofte n discuss ed encod ing measures given in [40, 2, 33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 234
                            }
                        ],
                        "text": "A prior for a network is related to the notions of \"penalty te rm\" or \"regularizer\" used in the statist ics literatur e [18], weight decay [29] and weight elimination [41], and the \"complexity te rm\" used in minimum enco ding methods [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "T his is an approximation developed by Schwar z and ot hers [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14460436,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fca98082fa9ff8e9dbae9922491ae54976a0ccef",
            "isKey": true,
            "numCitedBy": 525,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors introduce an index of resolvability that is proved to bound the rate of convergence of minimum complexity density estimators as well as the information-theoretic redundancy of the corresponding total description length. The results on the index of resolvability demonstrate the statistical effectiveness of the minimum description-length principle as a method of inference. The minimum complexity estimator converges to true density nearly as fast as an estimator based on prior knowledge of the true subclass of densities. Interpretations and basic properties of minimum complexity estimators are discussed. Some regression and classification problems that can be examined from the minimum description-length framework are considered. >"
            },
            "slug": "Minimum-complexity-density-estimation-Barron-Cover",
            "title": {
                "fragments": [],
                "text": "Minimum complexity density estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "An index of resolvability is proved to bound the rate of convergence of minimum complexity density estimators as well as the information-theoretic redundancy of the corresponding total description length to demonstrate the statistical effectiveness of the minimum description-length principle as a method of inference."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "I nt r o d u ction Back-propagati on [32] is a p opular sche me for t raining feed-for ward connec t ionist networks."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19356,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15696344,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1d8ff72a970d03c37e776f1222051f3fd6c617c",
            "isKey": false,
            "numCitedBy": 382,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Unknown-Attribute-Values-in-Induction-Quinlan",
            "title": {
                "fragments": [],
                "text": "Unknown Attribute Values in Induction"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 57437891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5bf6f01402e1648b7d1e6c9200ede6cb1af30123",
            "isKey": false,
            "numCitedBy": 4579,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probabilistic-reasoning-in-intelligent-systems-Pearl",
            "title": {
                "fragments": [],
                "text": "Probabilistic reasoning in intelligent systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39201543"
                        ],
                        "name": "J. Berger",
                        "slug": "J.-Berger",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Berger",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Berger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 112
                            }
                        ],
                        "text": "There is considerable lit erat ur e on how to choose a prior to minimize the ass umptions implicit in the prior [3]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 106
                            }
                        ],
                        "text": "T he opt imality prop erties of Bayesian methods hold because t he methods are \"normative\" or \"rationa l\" [3] [15]; any ot her approach not approximating t hem should not perform as well on average."
                    },
                    "intents": []
                }
            ],
            "corpusId": 120366929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9dd05b69d6906fff6ea6c4ba3609a6d97c9b8a3",
            "isKey": false,
            "numCitedBy": 7326,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory. The text assumes a knowledge of basic probability theory and some advanced calculus is also required."
            },
            "slug": "Statistical-Decision-Theory-and-Bayesian-Analysis-Berger",
            "title": {
                "fragments": [],
                "text": "Statistical Decision Theory and Bayesian Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 87
                            }
                        ],
                        "text": "In some simple non-network mod els- such as distributions from the exponent ial family [3], class probability t rees, and Bayesian networks on discrete vari ables [6]-the post erior can be dealt with exac tly so the approximat ion using I(w) is not necessary."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 87
                            }
                        ],
                        "text": "Since a ap pears as a facto r of S(w) it is a scaling quantity, so a suitable prior is [3] Pr (a) <X l/a."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 152
                            }
                        ],
                        "text": "Noti ce that this is the prior we should have used for smoo thness originally since smoothness is a magnit ude (a quantity where scale is imp ortan t ) [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 105
                            }
                        ],
                        "text": "T he opt imality prop erties of Bayesian methods hold because the methods are \"normative\" or \"rationa l\" [3, 15]; any other approach not approximating them should not perform as well on average."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 59
                            }
                        ],
                        "text": "T his prior distribu tion on S(w) is from the Gamma family [3], so a priori we expec t S(w) to have mean Iwl/2a and standard deviation ~/(V2a) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 86
                            }
                        ],
                        "text": "We evaluate these integrals by using standard moments of the mult ivar iate Gau ssian [3], and by approximating U(w) in the vicini ty of w using the second-order Tay lor expansion , which in vector notation is as follows:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 109
                            }
                        ],
                        "text": "There is considerable literatur e on how to choose a prior to minimize the assumptions implicit in the prior [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 51
                            }
                        ],
                        "text": "T hese play a central part in stat ist ical theory [1 , 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 87
                            }
                        ],
                        "text": "Here we frame t he probabilistic component of back-prop agation in a Bayesian conte xt [6, 3, 7, 27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stati sti cal Decision Theory and Bayesian Analysis (New York, Springer-Verlag"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian Back-Propagation"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "Involving \"nuisance\" parameters like a in the search process is an extra complication (compare wit h [23])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "MacKay overcomes t his poor choice of prior by making a a \"hyperparameter\" and setting it using a Bayesian max imum a post eriori approach ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 175
                            }
                        ],
                        "text": "This sect ion shows how to take a penalty term such as quadratic smoothness and construct a pr ior th at requires no set t ing of additional hyperparameters or decay fact ors [29, 41, 23]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 145
                            }
                        ],
                        "text": "T his has been done because of the similarity of the Bayesian log-posterior wit h pe nalized least squares, ridge regression, and regularization [18, 23]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "See [6] where a number of different priors are discussed for t rees, and [23] where several priors are t ried and compared for a single network learning probl em ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 168
                            }
                        ],
                        "text": "The various techn iques such as smoothing , pe na lized least-squar es, and cross validation applied to th ese syste ms can t hen be cast in a network fram ework ; see MacKay [23] for a Bayesian int erpret ati on."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 169
                            }
                        ],
                        "text": "The various techn iques such as smoothing , penalized least-squares, and cross validation applied to th ese systems can then be cast in a network framework ; see MacKay [23] for a Bayesian int erpret ation."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Practi cal Bayesian Fram ework for Backprop Netwo rks"
            },
            "venue": {
                "fragments": [],
                "text": "sub mitted to Neural Computation (199 1) ."
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "Onl y when we have a really large sample (for instan ce, when uniform convergence bounds tell us the sample is large enough [14]) can we be sure that w is a sufficient ly good est imate so that no other minima need be considered ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 28
                            }
                        ],
                        "text": "Uniform convergence methods [4, 14], the basis for earlier computational learn ing theory, approximate Bayesian methods when the sample size is large [7, section 4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "See also [14] for a discussion in t he uni form convergence framework."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Decision Theoret ic Generalization of the PAC Learning Model and It s Application to Some Feed-forwa rd Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Information and Control, to ap pear (1991) ."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "Uniform convergence methods [4] [14], the basis for earlier computational learn ing theory, approx imate Bayesian methods when the sa mple size is larg e [7, sectio n 4.2]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "W hat Size Net Gives Valid Gener alization?"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 163
                            }
                        ],
                        "text": "In some simple non-network mod els- such as distributions from the exponent ial family [3], class probability t rees, and Bayesian networks on discrete vari ables [6]-the post erior can be dealt with exac tly so the approximat ion using I(w) is not necessary."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "out-of-samp le predict ion when learning classification t rees [6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 117
                            }
                        ],
                        "text": "as a useful met aphor , and their imp lementation is somewhat simp ler than a full Bayesian ap proach with averaging [6, 36]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "See [6] where a number of different priors are discussed for t rees, and [23] where several priors are t ried and compared for a single network learning probl em ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 121
                            }
                        ],
                        "text": "These kinds of priors, when well chosen, can improve the out -of-sample accuracy of t ree algorit hms by several percent [6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 87
                            }
                        ],
                        "text": "Here we frame t he probabilistic component of back-prop agation in a Bayesian conte xt [6, 3, 7, 27]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learn ing Classification Trees"
            },
            "venue": {
                "fragments": [],
                "text": "Technica l Report FIA-91-30 , RIACS and NASA Ames Research Center , Moffett F ield, CA, 1991; submit ted to Proceedings of the Thi rd Int ernational Works hop on Artificial Intelligence and Statistics."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "Several minimum enco ding app roac hes [40] [2] [33] are also exp lained in section 6.3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "Minimum encoding met hods such as MML [40] and MDL [33] are ofte n viewed as ap proximate Bayesian met hod s and are used 2See [121 for a mot ivat ed t uto rial introduct ion to th is widely used technique ."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochast ic Complexity Jou rna l of th e Royal Statistical Soci ety B"
            },
            "venue": {
                "fragments": [],
                "text": "Stochast ic Complexity Jou rna l of th e Royal Statistical Soci ety B"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "Several minimum enco ding app roac hes [40] [2] [33] are also exp lained in section 6.3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Min imum Complexity Densit y Es t imat ion"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Inf orm ation Th eory"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Uniform convergence methods [4] [14], the basis for earlier computational learn ing theory, approx imate Bayesian methods when the sa mple size is larg e [7, sectio n 4.2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "See also [14] for a discussion in t he uni form converg ence framework."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Decision Theoret ic Generalization of t he PA C Learning Mod el and It s Application to Some Feed-forwa rd Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Information and Control"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "Several minimum enco ding app roac hes [40] [2] [33] are also exp lained in section 6.3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "Minimum encoding met hods such as MML [40] and MDL [33] are ofte n viewed as ap proximate Bayesian met hod s and are used 2See [121 for a mot ivat ed t uto rial introduct ion to th is widely used technique ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation and Inferenc e by Compact Encoding"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Roy al Statistical Soci ety B"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 144
                            }
                        ],
                        "text": "(1) Choose an appropriate network st ruct ur e and size based on pr ior kn owledge about the applica tion (see, for instance, the discussion in [21] regarding choice of network) , and select a prior on the weights ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ge neralization and Network Design St rategies"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report CRG-T R-89-4, Deptartment of Computer Science, University of Toronto, Toronto, M5S lA4, Can ad a (1989)."
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Illustrat ion of Bayesian Inference in Norm al Data Models Using Gibbs Sampling Journ al of th e American Statistical A ssociation"
            },
            "venue": {
                "fragments": [],
                "text": "Illustrat ion of Bayesian Inference in Norm al Data Models Using Gibbs Sampling Journ al of th e American Statistical A ssociation"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Note on Gener aliza tion , Regul ar ization , and Ar chit ecture Select ion in Non -linear Learning Syst ems"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE W orkshop on N eural Networks for Sign al Processing"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 66
                            }
                        ],
                        "text": "Probabilistic interpret ati ons of networks are also discussed in [5, 26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Soft Competitive Adaption, (Doctoral dissertation"
            },
            "venue": {
                "fragments": [],
                "text": "Technica l Report CM U-CS -91-126 from the Schoo l of Computer Science"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 198
                            }
                        ],
                        "text": "+ exp L: i=l, ...,l W n ,iUi + w n ,o Some ot her act ivation fun ctions that have been used are indep end ent Gau ssians found in radial basis, exponent ial, and variou s trigonometric fun cti ons [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "T he Stone-Weierstrass T heore m and Its Application to Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on N eural Networks"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 105
                            }
                        ],
                        "text": "T he opt imality prop erties of Bayesian methods hold because the methods are \"normative\" or \"rationa l\" [3, 15]; any other approach not approximating them should not perform as well on average."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Framework for Comparin g Al ternative Formalisms for P lau sible Reasoning"
            },
            "venue": {
                "fragments": [],
                "text": "pages 210-214 in Fifth Nat ional Conference on Artificia l Int elligence, P hilade lphia , PA (1986) ."
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "2 Regression networks Another statist ical model that has a simple network representation is linear regression [18], where th e mean of the response var iable is usually given by y = L w;fi(x ) i=I,."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 145
                            }
                        ],
                        "text": "T his has been done because of the similarity of the Bayesian log-posterior wit h pe nalized least squares, ridge regression, and regularization [18, 23]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 120
                            }
                        ],
                        "text": "A prior for a network is related to the notions of \"penalty te rm\" or \"regularizer\" used in the statist ics literatur e [18], weight decay [29] and weight elimination [41], and the \"complexity te rm\" used in minimum enco ding methods [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalised Additive Models (London"
            },
            "venue": {
                "fragments": [],
                "text": "Cha pman and Hall,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 207
                            }
                        ],
                        "text": "3The mod el space does not have to contain th e \"t rue\" mod el and no prior is needed . as a useful met aphor , and their imp lementation is somew hat simp ler than a full Bayesian ap proach wit h averaging [6] [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learn ing Classification Tr ees Technica l Report FIA-91-30 , R IACS a nd NASA Am es Research Center"
            },
            "venue": {
                "fragments": [],
                "text": "bmit ted to Proceedings of the Thi rd Int ern ational Works hop on Artificial Intelligen ce and Statistics"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "Individual second derivatives can be determined using a number of methods [9]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 50
                            }
                        ],
                        "text": "Calculation of second derivatives is described in [9]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ca lcula ting Second Derivati ves on Feedforward Networks"
            },
            "venue": {
                "fragments": [],
                "text": "sub mitted (1991)."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Superv ised Lea rn ing of P robability Distributions by Neural Networ ks"
            },
            "venue": {
                "fragments": [],
                "text": "N eural Info rmation Processinq Sys tems (NIP S)"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of Dependencies Based on Emp iri cal Data"
            },
            "venue": {
                "fragments": [],
                "text": "Estimation of Dependencies Based on Emp iri cal Data"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Object ive Bayesianism and Geom etry"
            },
            "venue": {
                "fragments": [],
                "text": "Maximum En tropy and Bayesian Method s, edited by P. F. Fougere (Norwell, MA , Kluwer, 1990)."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "When a = 1, these include Zellner 's maximal data informat ion pr ior [42], which can be used to yield most of the t extb ook \"noninformat ive\" priors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Met hods and Entropy in Econo mics and Econometrics"
            },
            "venue": {
                "fragments": [],
                "text": "Maximum Entropy and Bayesian Methods, edite d by W. T. Grandy, Jr. and L. Schlick, (Norwell, MA , Kluwer, 1990)."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ge neralization and Network Design St rat egies"
            },
            "venue": {
                "fragments": [],
                "text": "Ge neralization and Network Design St rat egies"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Met hods a nd Entropy in E cono mics and Econom etrics"
            },
            "venue": {
                "fragments": [],
                "text": "Maximum Entropy and Bayesian Methods"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochast ic Complexity , \" Jou rna l of the"
            },
            "venue": {
                "fragments": [],
                "text": "Royal Statistical Soci ety B"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 211
                            }
                        ],
                        "text": "3The mod el space does not have to contain th e \"t rue\" mod el and no prior is needed . as a useful met aphor , and their imp lementation is somew hat simp ler than a full Bayesian ap proach wit h averaging [6] [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hierarchical Bayesian An alysis Using Mont e Carlo Integration: Computing Po sterior Distributions When Ther e Are Man y P ossible Models"
            },
            "venue": {
                "fragments": [],
                "text": "Th e Statist ician"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic Reasoning in Intelligent Systems (Los Altos, CA, Morga n Kauffman, 1988).  Bayesian Back-Propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "The statistical mechanics idea of an ensemble of networks [22] corresponds to the Bayesian post erior."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A St atistical Approach t o Learning a nd Gen eralization in Layered Neural Network s"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '89: S econd Work shop on Computation al Learning Th eory"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 131
                            }
                        ],
                        "text": "This is very different to analyze mathematically than the noisy cases above, and we do not consider this here (see, for instan ce, [16, 27])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unifying Bounds on the Sample Comp lexity of Bayesian Learning Using In formation Theory and the VC Dimension"
            },
            "venue": {
                "fragments": [],
                "text": "COLT'91: Workshop on Computational Leam ing Th eory (San Mateo, CA , Morgan Kaufmann, 1991).  642  Wray 1. Buntine and Andreas S. Weigend"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 66
                            }
                        ],
                        "text": "Probabilistic interpret ati ons of networks are also discussed in [5, 26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": "A similar act ivat ion function , the Softmax function [5] , firs t uses the exponential fun ction to ensur e positivity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "P robabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "F . Fougelman -Sou lie and J . Heraul t , editors, Neuro-computing: Algorithms, Archit ectures and Application s (New York , Springer-Verlag , 1989)."
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Wi ld , Nonl in ear-R egr-ession"
            },
            "venue": {
                "fragments": [],
                "text": "Wi ld , Nonl in ear-R egr-ession"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "Notion s such as the \"t rue\" subject ive probab ility functi on [13], the \"correct\" likelih ood funct ion derived from th e addit ive energy fun ct ion [22], and the use of probabili st ic mod eling in t he conte xt of \"unrealiza ble\" models [35J (when ass ump t ion (I) fails so the search space\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Unified Fra mewor k for Connectionist Sys t ems"
            },
            "venue": {
                "fragments": [],
                "text": "Bi ological Cybernetics"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 289
                            }
                        ],
                        "text": "\u2026the network in coherent ways, an expe rt can view the knowledge represented by the network, and we can more readily assign meaningful priors or som e other initialization for t he network weight s (for instance, a non-probabili stic scheme for initialization of weight s is describ ed in [38])."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Re fine ment of Approximate Domain Theories by Knowledge-b ased Neural Network s"
            },
            "venue": {
                "fragments": [],
                "text": "Eighth National Conference on Ar-tificial Intelligence"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Object ive Bayesianism and Geom etry,\" in Maximum En tropy and Bayesian Method s"
            },
            "venue": {
                "fragments": [],
                "text": "Object ive Bayesianism and Geom etry,\" in Maximum En tropy and Bayesian Method s"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 82
                            }
                        ],
                        "text": "These cost functions differ from maximum likelihood methods for network tr aining [8, 11] in that they introduce the important prior te rm."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A New Error Cr iterion for Posterior P robability Estimation with Neural Nets, pages"
            },
            "venue": {
                "fragments": [],
                "text": "Intern ational Joint Conference on Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 117
                            }
                        ],
                        "text": "as a useful met aphor , and their imp lementation is somewhat simp ler than a full Bayesian ap proach with averaging [6, 36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hierarchical Bayesian An alysis Using Monte Carlo Integration: Computing Po sterior Distributions When There Are Many Possible Models"
            },
            "venue": {
                "fragments": [],
                "text": "Th e Statist ician , 36 (1987) 211-219."
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "Some other act ivation fun ctions that have been used are indep end ent Gau ssians found in radial basis, exponent ial, and various trigonometric functi ons [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "T he Stone-Weierstrass T heorem and Its Application to Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on N eural Networks , 1(4) (1990) 290- 295."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalised Pe rformance of Bayes Optimal Classificat ion Algorit hm for Learning a Perceptron"
            },
            "venue": {
                "fragments": [],
                "text": "COLT'91: 1991 Workshop on Computational Learning Th eory"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exp eriment s on Learning by Back-pro paga t ion"
            },
            "venue": {
                "fragments": [],
                "text": "Exp eriment s on Learning by Back-pro paga t ion"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "T hese and other approximations are discussed in [30] ."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Statistics (New York"
            },
            "venue": {
                "fragments": [],
                "text": "Wil ey,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "Notions such as the \"t rue\" subject ive probab ility function [13], the \"correct\" likelihood funct ion derived from th e addit ive energy funct ion [22], and the use of probabili st ic mod eling in the context of \"unrealizable\" models [35J (when ass umpt ion (I) fails so the search space does not contain the \"true\" model) are only par ti ally consiste nt wit h a Bayesian approach ."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Unified Fra mewor k for Connectionist Sys tems"
            },
            "venue": {
                "fragments": [],
                "text": "Bi ological Cybernetics, 59 (1988) 109-120."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "MacKay overcomes t his poor choice of prior by making a a \"hyperparameter\" and setting it using a Bayesian max imum a post eriori approach ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 175
                            }
                        ],
                        "text": "The various techn iques such as smoothing , pe na lized least-squar es, and cross validation applied to th ese syste ms can t hen be cast in a network fram ework ; see MacKay [23] for a Bayesian int erpret ati on."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Practi cal Bayesian Fr am ework for Backprop Netwo rks"
            },
            "venue": {
                "fragments": [],
                "text": "A Practi cal Bayesian Fr am ework for Backprop Netwo rks"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 110
                            }
                        ],
                        "text": "T he opt imality prop erties of Bayesian methods hold because t he methods are \"normative\" or \"rationa l\" [3] [15]; any ot her approach not approximating t hem should not perform as well on average."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Fr amework for Comparin g Al ternativ e Formalisms for P lau sible Reasoning"
            },
            "venue": {
                "fragments": [],
                "text": "Fifth Nat ional Confe rence on Artificia l Int elligence, P hilade lphia"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ca lcula ti ng Second Derivati ves on Feed forward Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Ca lcula ti ng Second Derivati ves on Feed forward Networks"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Structural Learning Algorithm with Forget ti ng of Link Weights Technical Report TR-90-7 , E lect rotechnical Lab ora tory"
            },
            "venue": {
                "fragments": [],
                "text": "International Joint Conference on N eural N etworks"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "The form s given for generaliza t ion err or, equations (11) and (13) , are Bayesian version s of those in [25]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Note on Generaliza tion , Regular ization , and Ar chitecture Select ion in Non -linear Learning Syst ems"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE Workshop on Neural Networks for Signal Processing (Los Alamitos, CA, IEEE Computer Society , 1991)."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 182
                            }
                        ],
                        "text": "Logistic networks generalize the sigmoid activation function t o produce a family of simple networks that reproduce logistic stat ist ical model s for discriminat ion and regression [24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "Then an exponent ial distribution [24] on (x,y) can be (1) ot herwise. for y = 1, defined in te rms of t hese features, where ai ,y are some paramet ers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gen eralised Linear Models"
            },
            "venue": {
                "fragments": [],
                "text": "Gen eralised Linear Models"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "Vapnik [39] has also suggested using the Lap lace distribution as an erro r model when the experimental cond it ions may vary with maximal uncertainty, bu t the standard deviat ion is st ill ind ependent of x ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of Dependencies Based on Emp iri cal Data (New York"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Equivalence P roofs for Multi-layer Perceptron Class ifiers and the Bayesian Discrimination Function"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1990 Connectionist Models Summer School, edite d by David S. Touretzky, Jeffrey L. Elman, Terrence J. Sejnowski, and Geoffrey E . Hinton (San Mateo, CA , Morgan Kaufmann, 1990)."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Illustrat ion of Bayesian Inference in Norm al Data Models Using Gibbs Sampling"
            },
            "venue": {
                "fragments": [],
                "text": "Journ al of the American Statistical A ssociation, 8 5(412) (1990) 972-985."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ad vanced Econom etri cs"
            },
            "venue": {
                "fragments": [],
                "text": "Ad vanced Econom etri cs"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "Probabilistic int erpret ati ons of networks are also discussed in [5] [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Soft Competitive Adaption, (Doctoral dissertation, Carnegie Mellon Univers ity , 1991) ; available as Technica l Report CM U-CS -91-126 from t he Schoo l of Computer Science"
            },
            "venue": {
                "fragments": [],
                "text": "Soft Competitive Adaption, (Doctoral dissertation, Carnegie Mellon Univers ity , 1991) ; available as Technica l Report CM U-CS -91-126 from t he Schoo l of Computer Science"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "Statist ical mechanics t heory has been developed in t he context of training a perceptron in a nois e-free environment to estimate the generalization error [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learn ing Curves in Large Neu ral Network s"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '91: W orkshop on Computational Learning Th eory"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unifying Bounds on t he Sample Comp lexity of Bayesian Learning Using In formation Theor y and the VC Dimension"
            },
            "venue": {
                "fragments": [],
                "text": "COLT'91: W orkshop on Computatio nal Leam ing Th eory"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A T heo ry of Learning Class ifica tion Rules"
            },
            "venue": {
                "fragments": [],
                "text": "A T heo ry of Learning Class ifica tion Rules"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stone - Weierstrass T heorem and Its Application to Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on N eural Networks"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Opt imal Br ain Dam age"
            },
            "venue": {
                "fragments": [],
                "text": "589 in Ad vances in Neural Inf orm ation Processing Systems 2 (NIPS *89)"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Equivalence P roofs for Multi-layer Per ceptron Class ifiers and t he Bayesian Discrimi nation Function"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1990 Connectionist Models Summer School"
            },
            "year": 1990
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 25,
            "methodology": 16
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 72,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/Bayesian-Back-Propagation-Buntine-Weigend/c83684f6207697c12850db423fd9747572cf1784?sort=total-citations"
}