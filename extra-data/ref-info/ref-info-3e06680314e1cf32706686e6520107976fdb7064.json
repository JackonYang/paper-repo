{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 205119351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77fdd39ab366b65a617015a72fe8dc9d0b394d64",
            "isKey": false,
            "numCitedBy": 716,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-nonparametric-regression:-Multilayer-White",
            "title": {
                "fragments": [],
                "text": "Connectionist nonparametric regression: Multilayer feedforward networks can learn arbitrary mappings"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15659829,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "25406e6733a698bfc4ac836f8e74f458e75dad4f",
            "isKey": false,
            "numCitedBy": 1696,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 < \u220a 1/8. We show that if m O(W/\u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 \u220a of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than (W/\u220a) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 \u220a fraction of the future test examples."
            },
            "slug": "What-Size-Net-Gives-Valid-Generalization-Baum-Haussler",
            "title": {
                "fragments": [],
                "text": "What Size Net Gives Valid Generalization?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if m O(W/ \u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 2 \u220a of future test examples drawn from the same distribution."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145159381"
                        ],
                        "name": "Jenq-Neng Hwang",
                        "slug": "Jenq-Neng-Hwang",
                        "structuredName": {
                            "firstName": "Jenq-Neng",
                            "lastName": "Hwang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jenq-Neng Hwang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "28184513"
                        ],
                        "name": "J. J. Choi",
                        "slug": "J.-J.-Choi",
                        "structuredName": {
                            "firstName": "Jai",
                            "lastName": "Choi",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3235479"
                        ],
                        "name": "Seho Oh",
                        "slug": "Seho-Oh",
                        "structuredName": {
                            "firstName": "Seho",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seho Oh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47398186"
                        ],
                        "name": "R. Marks",
                        "slug": "R.-Marks",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Marks",
                            "middleNames": [
                                "J."
                            ],
                            "suffix": "Jr"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Marks"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 26254146,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0bd6e0c494ca277aeaa5ef00da41da5b2600b9c",
            "isKey": false,
            "numCitedBy": 174,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach is presented for query-based neural network learning. A layered perceptron partially trained for binary classification is considered. The single-output neuron is trained to be either a zero or a one. A test decision is made by thresholding the output at, for example, one-half. The set of inputs that produce an output of one-half forms the classification boundary. The authors adopted an inversion algorithm for the neural network that allows generation of this boundary. For each boundary point, the classification gradient can be generated. The gradient provides a useful measure of the steepness of the multidimensional decision surfaces. Conjugate input pairs are generated using the boundary point and gradient information and presented to an oracle for proper classification. These data are used to refine further the classification boundary, thereby increasing the classification accuracy. The result can be a significant reduction in the training set cardinality in comparison with, for example, randomly generated data points. An application example to power system security assessment is given."
            },
            "slug": "Query-based-learning-applied-to-partially-trained-Hwang-Choi",
            "title": {
                "fragments": [],
                "text": "Query-based learning applied to partially trained multilayer perceptrons"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "An approach is presented for query-based neural network learning that combines a layered perceptron partially trained for binary classification with an inversion algorithm for the neural network that allows generation of this boundary."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145159381"
                        ],
                        "name": "Jenq-Neng Hwang",
                        "slug": "Jenq-Neng-Hwang",
                        "structuredName": {
                            "firstName": "Jenq-Neng",
                            "lastName": "Hwang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jenq-Neng Hwang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "28184513"
                        ],
                        "name": "J. J. Choi",
                        "slug": "J.-J.-Choi",
                        "structuredName": {
                            "firstName": "Jai",
                            "lastName": "Choi",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3235479"
                        ],
                        "name": "Seho Oh",
                        "slug": "Seho-Oh",
                        "structuredName": {
                            "firstName": "Seho",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seho Oh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47398186"
                        ],
                        "name": "R. Marks",
                        "slug": "R.-Marks",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Marks",
                            "middleNames": [
                                "J."
                            ],
                            "suffix": "Jr"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Marks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17933436,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "129afe6d9b3ec6a2a0f6fac95ddbb085619cc4be",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel approach to query-based neural network learning is presented. A layered perceptron partially trained for binary classification is considered. The single-output neuron is trained to be either a 0 or a 1. A test decision is made by thresholding the output at, for example, 1/2. The set of inputs that produce an output of 1/2 forms the classification boundary. For each boundary point, the classification gradient can be generated. The gradient provides a useful measure of the sharpness of the multidimensional decision surfaces. Conjugate input pair locations are generated using the boundary point and gradient information and are presented to the oracle for proper classification. These new data are used to further refine the classification boundary, thereby increasing the classification accuracy. The result can be a significant reduction in the training set cardinality in comparison with, for example, randomly generated data points. An application example to power security assessment is given"
            },
            "slug": "Query-learning-based-on-boundary-search-and-of-Hwang-Choi",
            "title": {
                "fragments": [],
                "text": "Query learning based on boundary search and gradient computation of trained multilayer perceptrons"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A novel approach to query-based neural network learning is presented and a layered perceptron partially trained for binary classification is considered, resulting in a significant reduction in the training set cardinality in comparison with randomly generated data points."
            },
            "venue": {
                "fragments": [],
                "text": "1990 IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145468098"
                        ],
                        "name": "M. M\u00f8ller",
                        "slug": "M.-M\u00f8ller",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "M\u00f8ller",
                            "middleNames": [
                                "Fodslette"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. M\u00f8ller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9963836,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0631a99f68cb0c159b15f1cbbaa894bc9f5a738",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel algorithm combining the good properties of offline and online algorithms is introduced. The efficiency of supervised learning algorithms on small-scale problems does not necessarily scale up to large-scale problems. The redundancy of large training sets is reflected as redundancy gradient vectors in the network. Accumulating these gradient vectors implies redundant computations. In order to avoid these redundant computations a learning algorithm has to be able to update weights independently of the size of the training set. The stochastic learning algorithm proposed, the stochastic scaled conjugate gradient (SSCG) algorithm, has this property. Experimentally, it is shown that SSCG converges faster than the online backpropagation algorithm on the nettalk problem.<<ETX>>"
            },
            "slug": "Supervised-learning-on-large-redundant-training-M\u00f8ller",
            "title": {
                "fragments": [],
                "text": "Supervised learning on large redundant training sets"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A novel algorithm combining the good properties of offline and online algorithms is introduced, the stochastic scaled conjugate gradient (SSCG), and it is shown that SSCG converges faster than the online backpropagation algorithm on the nettalk problem."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing II Proceedings of the 1992 IEEE Workshop"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 43711678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "656a33c1db546da8490d6eba259e2a849d73a001",
            "isKey": false,
            "numCitedBy": 1012,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "The premise of this article is that learning procedures used to train artificial neural networks are inherently statistical techniques. It follows that statistical theory can provide considerable insight into the properties, advantages, and disadvantages of different network learning methods. We review concepts and analytical results from the literatures of mathematical statistics, econometrics, systems identification, and optimization theory relevant to the analysis of learning in artificial neural networks. Because of the considerable variety of available learning procedures and necessary limitations of space, we cannot provide a comprehensive treatment. Our focus is primarily on learning procedures for feedforward networks. However, many of the concepts and issues arising in this framework are also quite broadly relevant to other network learning paradigms. In addition to providing useful insights, the material reviewed here suggests some potentially useful new training methods for artificial neural networks."
            },
            "slug": "Learning-in-Artificial-Neural-Networks:-A-White",
            "title": {
                "fragments": [],
                "text": "Learning in Artificial Neural Networks: A Statistical Perspective"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Concepts and analytical results from the literatures of mathematical statistics, econometrics, systems identification, and optimization theory relevant to the analysis of learning in artificial neural networks are reviewed."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19460515,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a7404527c3a6aa542ea183da9c821efda05a2afc",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm which trains networks using examples and queries is proposed. In a query, the algorithm supplies a y and is told t(y) by an oracle. Queries appear to be available in practice for most problems of interest, e.g. by appeal to a human expert. The author's algorithm is proved to PAC learn in polynomial time the class of target functions defined by layered, depth two, threshold nets having n inputs connected to k hidden threshold units connected to one or more output units, provided k=/<4. While target functions and input distributions can be described for which the algorithm will fail for larger k, it appears likely to work well in practice. Tests of a variant of the algorithm have consistently and rapidly learned random nets of this type. Computational efficiency figures are given. The algorithm can also be proved to learn intersections of k half-spaces in R(n) in time polynomial in both n and k. A variant of the algorithm can learn arbitrary depth layered threshold networks with n inputs and k units in the first hidden layer in time polynomial in the larger of n and k but exponential in the smaller of the two."
            },
            "slug": "Neural-net-algorithms-that-learn-in-polynomial-time-Baum",
            "title": {
                "fragments": [],
                "text": "Neural net algorithms that learn in polynomial time from examples and queries"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The author's algorithm is proved to PAC learn in polynomial time the class of target functions defined by layered, depth two, threshold nets having n inputs connected to k hidden threshold units connected to one or more output units, provided k=/<4."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6530745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7abda1941534d3bb558dd959025d67f1df526303",
            "isKey": false,
            "numCitedBy": 792,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Three Bayesian ideas are presented for supervised adaptive classifiers. First, it is argued that the output of a classifier should be obtained by marginalizing over the posterior distribution of the parameters; a simple approximation to this integral is proposed and demonstrated. This involves a \"moderation\" of the most probable classifier's outputs, and yields improved performance. Second, it is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems. This framework successfully chooses the magnitude of weight decay terms, and ranks solutions found using different numbers of hidden units. Third, an information-based data selection criterion is derived and demonstrated within this framework."
            },
            "slug": "The-Evidence-Framework-Applied-to-Classification-Mackay",
            "title": {
                "fragments": [],
                "text": "The Evidence Framework Applied to Classification Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems and an information-based data selection criterion is derived and demonstrated within this framework."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16523475,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15e2989c299b63efc42f0b93bf63848a92b88c63",
            "isKey": false,
            "numCitedBy": 1420,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We have created a network that allocates a new computational unit whenever an unusual pattern is presented to the network. This network forms compact representations, yet learns easily and rapidly. The network can be used at any time in the learning process and the learning patterns do not have to be repeated. The units in this network respond to only a local region of the space of input values. The network learns by allocating new units and adjusting the parameters of existing units. If the network performs poorly on a presented pattern, then a new unit is allocated that corrects the response to the presented pattern. If the network performs well on a presented pattern, then the network parameters are updated using standard LMS gradient descent. We have obtained good results with our resource-allocating network (RAN). For predicting the Mackey-Glass chaotic time series, RAN learns much faster than do those using backpropagation networks and uses a comparable number of synapses."
            },
            "slug": "A-Resource-Allocating-Network-for-Function-Platt",
            "title": {
                "fragments": [],
                "text": "A Resource-Allocating Network for Function Interpolation"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A network that allocates a new computational unit whenever an unusual pattern is presented to the network, which learns much faster than do those using backpropagation networks and uses a comparable number of synapses."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1967384"
                        ],
                        "name": "S. Yakowitz",
                        "slug": "S.-Yakowitz",
                        "structuredName": {
                            "firstName": "Sidney",
                            "lastName": "Yakowitz",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Yakowitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49413400"
                        ],
                        "name": "E. Lugosi",
                        "slug": "E.-Lugosi",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Lugosi",
                            "middleNames": [],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Lugosi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13461947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "495cd6dff3a0d8c7f5d6a9ebf104ce2c0d223027",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A search for the global minimum of a function is proposed; the search is on the basis of sequential noisy measurements. Because no unimodality assumptions are made, stochastic approximation and other well-known methods are not directly applicable. The search plan is shown to be convergent in probability to a set of minimizers. This study was motivated by investigations into machine learning. This setting is explained, and the methodology is applied to create an adaptively improving strategy for 8-puzzle problems."
            },
            "slug": "Random-Search-in-the-Presence-of-Noise,-with-to-Yakowitz-Lugosi",
            "title": {
                "fragments": [],
                "text": "Random Search in the Presence of Noise, with Application to Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A search for the global minimum of a function is proposed; the search is on the basis of sequential noisy measurements and the search plan is shown to be convergent in probability to a set of minimizers."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2976268"
                        ],
                        "name": "D. Cohn",
                        "slug": "D.-Cohn",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cohn",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cohn"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117404966,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47a3c910af789bdadaae591b7fd6fdc06d5cd8da",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Local-Approach-to-Optimal-Queries-Cohn",
            "title": {
                "fragments": [],
                "text": "A Local Approach to Optimal Queries"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398965769"
                        ],
                        "name": "Y. Abu-Mostafa",
                        "slug": "Y.-Abu-Mostafa",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Abu-Mostafa",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Abu-Mostafa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17945746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f027c678076d7f2fd817f081079a334466449b1",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "When feasible, learning is a very attractive alternative to explicit programming. This is particularly true in areas where the problems do not lend themselves to systematic programming, such as pattern recognition in natural environments. The feasibility of learning an unknown function from examples depends on two questions: 1. Do the examples convey enough information to determine the function? 2. Is there a speedy way of constructing the function from the examples? These questions contrast the roles of information and complexity in learning. While the two roles share some ground, they are conceptually and technically different. In the common language of learning, the information question is that of generalization and the complexity question is that of scaling. The work of Vapnik and Chervonenkis (1971) provides the key tools for dealing with the information issue. In this review, we develop the main ideas of this framework and discuss how complexity fits in."
            },
            "slug": "The-Vapnik-Chervonenkis-Dimension:-Information-in-Abu-Mostafa",
            "title": {
                "fragments": [],
                "text": "The Vapnik-Chervonenkis Dimension: Information versus Complexity in Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The work of Vapnik and Chervonenkis (1971) provides the key tools for dealing with the information issue and the main ideas are developed and how complexity fits in are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113911099"
                        ],
                        "name": "R. Rivest",
                        "slug": "R.-Rivest",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rivest",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rivest"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8567973,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c49deca33488cfec0df2c03f37a343d93c80d7c",
            "isKey": false,
            "numCitedBy": 823,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Training-a-3-node-neural-network-is-NP-complete-Blum-Rivest",
            "title": {
                "fragments": [],
                "text": "Training a 3-node neural network is NP-complete"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14814125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c83684f6207697c12850db423fd9747572cf1784",
            "isKey": false,
            "numCitedBy": 376,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Connectionist feed-forward networks, t rained with backpropagat ion, can be used both for nonlinear regression and for (discrete one-of-C ) classification. This paper presents approximate Bayesian meth ods to statistical components of back-propagat ion: choosing a cost funct ion and penalty term (interpreted as a form of prior probability), pruning insignifican t weights, est imat ing the uncertainty of weights, predict ing for new pat terns (\"out -of-sample\") , est imating the uncertainty in the choice of this predict ion (\"erro r bars\" ), estimating the generalizat ion erro r, comparing different network st ructures, and handling missing values in the t raining patterns. These methods extend some heurist ic techniques suggested in the literature, and in most cases require a small addit ional facto r in comput at ion during back-propagat ion, or computation once back-pro pagat ion has finished."
            },
            "slug": "Bayesian-Back-Propagation-Buntine-Weigend",
            "title": {
                "fragments": [],
                "text": "Bayesian Back-Propagation"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15819455,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2046412fecff64e095cc5190b69172055afd2094",
            "isKey": false,
            "numCitedBy": 1202,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning can be made more efficient if we can actively select particularly salient data points. Within a Bayesian learning framework, objective functions are discussed that measure the expected informativeness of candidate measurements. Three alternative specifications of what we want to gain information about lead to three different criteria for data selection. All these criteria depend on the assumption that the hypothesis space is correct, which may prove to be their main weakness."
            },
            "slug": "Information-Based-Objective-Functions-for-Active-Mackay",
            "title": {
                "fragments": [],
                "text": "Information-Based Objective Functions for Active Data Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "Within a Bayesian learning framework, objective functions are discussed that measure the expected informativeness of candidate measurements that depend on the assumption that the hypothesis space is correct."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109973472"
                        ],
                        "name": "Subutai Ahmad",
                        "slug": "Subutai-Ahmad",
                        "structuredName": {
                            "firstName": "Subutai",
                            "lastName": "Ahmad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subutai Ahmad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12927221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36a13cee240c33c952310132508aafb9917af46f",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This report explores the problem of dynamically computing visual relations in connectionist systems. It concentrates on the task of learning whether three clumps of points in a 256x256 image form an equilateral triangle. We argue that feed-forward networks for solving this task would not scale well to images of this size. One reason for this is that local information does not contribute to the solution: it is necessary to compute relational information such as the distances between points. Our solution implements a mechanism for dynamically extracting the locations of the point clusters. It consists of an efficient focus of attention mechanism and a cluster detection scheme. The focus of attention mechanism allows the system to select any circular portion of the image in constant time. The cluster detector directs the focus of attention to clusters in the image. These two mechanisms are used to sequentially extract the relevant coordinates. With this new representation (locations of the points) very few training examples are required to learn the correct function. The resulting network is also very compact: the number of required weights is proportional to the number of input pixels."
            },
            "slug": "A-Network-for-Extracting-the-Locations-of-Point-1-Ahmad-Omohundro",
            "title": {
                "fragments": [],
                "text": "A Network for Extracting the Locations of Point Clusters Using Selective Attention 1"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This report concentrates on the task of learning whether three clumps of points in a 256x256 image form an equilateral triangle and implements a mechanism for dynamically extracting the locations of the point clusters through an efficient focus of attention mechanism and a cluster detection scheme."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2020074"
                        ],
                        "name": "A. Lapedes",
                        "slug": "A.-Lapedes",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Lapedes",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lapedes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2542113"
                        ],
                        "name": "R. Farber",
                        "slug": "R.-Farber",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Farber",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Farber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18474528,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d7de94252e5040a38ebaaf535841d3500791c79",
            "isKey": false,
            "numCitedBy": 373,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "There is presently great interest in the abilities of neural networks to mimic \"qualitative reasoning\" by manipulating neural incodings of symbols. Less work has been performed on using neural networks to process floating point numbers and it is sometimes stated that neural networks are somehow inherently inaccurate and therefore best suited for \"fuzzy\" qualitative reasoning. Nevertheless, the potential speed of massively parallel operations make neural net \"number crunching\" an interesting topic to explore. In this paper we discuss some of our work in which we demonstrate that for certain applications neural networks can achieve significantly higher numerical accuracy than more conventional techniques. In particular, prediction of future values of a chaotic time series can be performed with exceptionally high accuracy. We analyze how a neural net is able to do this, and in the process show that a large class of functions from Rn \u2192 Rm may be accurately approximated by a backpropagation neural net with just two \"hidden\" layers. The network uses this functional approximation to perform either interpolation (signal processing applications) or extrapolation (symbol processing applications). Neural nets therefore use quite familiar methods to perform their tasks. The geometrical viewpoint advocated here seems to be a useful approach to analyzing neural network operation and relates neural networks to well studied topics in functional approximation."
            },
            "slug": "How-Neural-Nets-Work-Lapedes-Farber",
            "title": {
                "fragments": [],
                "text": "How Neural Nets Work"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper demonstrates that for certain applications neural networks can achieve significantly higher numerical accuracy than more conventional techniques, and shows that prediction of future values of a chaotic time series can be performed with exceptionally high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705299"
                        ],
                        "name": "L. Atlas",
                        "slug": "L.-Atlas",
                        "structuredName": {
                            "firstName": "Les",
                            "lastName": "Atlas",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Atlas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2976268"
                        ],
                        "name": "D. Cohn",
                        "slug": "D.-Cohn",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cohn",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cohn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762656"
                        ],
                        "name": "R. Ladner",
                        "slug": "R.-Ladner",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Ladner",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ladner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18986608,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3c19172a59922fa7cbed8f09f30ed3a1ea74b4e",
            "isKey": false,
            "numCitedBy": 261,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "\"Selective sampling\" is a form of directed search that can greatly increase the ability of a connectionist network to generalize accurately. Based on information from previous batches of samples, a network may be trained on data selectively sampled from regions in the domain that are unknown. This is realizable in cases when the distribution is known, or when the cost of drawing points from the target distribution is negligible compared to the cost of labeling them with the proper classification. The approach is justified by its applicability to the problem of training a network for power system security analysis. The benefits of selective sampling are studied analytically, and the results are confirmed experimentally."
            },
            "slug": "Training-Connectionist-Networks-with-Queries-and-Atlas-Cohn",
            "title": {
                "fragments": [],
                "text": "Training Connectionist Networks with Queries and Selective Sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The approach is justified by its applicability to the problem of training a network for power system security analysis and the benefits are studied analytically, and the results are confirmed experimentally."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151194700"
                        ],
                        "name": "H. M\u00fcller",
                        "slug": "H.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Hans-Georg",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. M\u00fcller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120945515,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "df0a1a8e1a524b46fd398afe75b6ba945e0ee352",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Optimal-designs-for-nonparametric-kernel-regression-M\u00fcller",
            "title": {
                "fragments": [],
                "text": "Optimal designs for nonparametric kernel regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145666307"
                        ],
                        "name": "G. Mitchison",
                        "slug": "G.-Mitchison",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Mitchison",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mitchison"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13531615,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2eadcb6fb0c285ef361fcec080979dc336e1df79",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "I describe a local synaptic learning rule that can be used to remove the effects of certain types of systematic temporal variation in the inputs to a unit. According to this rule, changes in synaptic weight result from a conjunction of short-term temporal changes in the inputs and the output. Formally, This is like the differential rule proposed by Klopf (1986) and Kosko (1986), except for a change of sign, which gives it an anti-Hebbian character. By itself this rule is insufficient. A weight conservation condition is needed to prevent the weights from collapsing to zero, and some further constraintimplemented here by a biasing termto select particular sets of weights from the subspace of those which give minimal variation. As an example, I show that this rule will generate center-surround receptive fields that remove temporally varying linear gradients from the inputs."
            },
            "slug": "Removing-Time-Variation-with-the-Anti-Hebbian-Mitchison",
            "title": {
                "fragments": [],
                "text": "Removing Time Variation with the Anti-Hebbian Differential Synapse"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A local synaptic learning rule is described that can be used to remove the effects of certain types of systematic temporal variation in the inputs to a unit and will generate center-surround receptive fields that remove temporally varying linear gradients from the inputs."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2964655"
                        ],
                        "name": "M. Stinchcombe",
                        "slug": "M.-Stinchcombe",
                        "structuredName": {
                            "firstName": "Maxwell",
                            "lastName": "Stinchcombe",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stinchcombe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2757547,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f22f6972e66bdd2e769fa64b0df0a13063c0c101",
            "isKey": false,
            "numCitedBy": 17356,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multilayer-feedforward-networks-are-universal-Hornik-Stinchcombe",
            "title": {
                "fragments": [],
                "text": "Multilayer feedforward networks are universal approximators"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740486"
                        ],
                        "name": "W. Gasarch",
                        "slug": "W.-Gasarch",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gasarch",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gasarch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2653810"
                        ],
                        "name": "M. Pleszkoch",
                        "slug": "M.-Pleszkoch",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Pleszkoch",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pleszkoch"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31830785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "429f19576637f203e4f2f86cb6da63d845eddbbe",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-via-queries-to-an-oracle-Gasarch-Pleszkoch",
            "title": {
                "fragments": [],
                "text": "Learning via queries to an oracle"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '89"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2578247"
                        ],
                        "name": "S. Kurtz",
                        "slug": "S.-Kurtz",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Kurtz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kurtz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152471016"
                        ],
                        "name": "Carl H. Smith",
                        "slug": "Carl-H.-Smith",
                        "structuredName": {
                            "firstName": "Carl H.",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl H. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8796029,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "c5f71a9603ae2816b57c86afb2a0fee4d1f00b15",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning algorithms that input examples of some phenomenon and produce conjectured explanations of the phenomenon are examined. It is argued that there are two sources for the explanations that occur as outputs of the learning process: 1 use the input data to guide a search through some (possibly very complex) space of explanations, or"
            },
            "slug": "On-the-role-of-search-for-learning-Kurtz-Smith",
            "title": {
                "fragments": [],
                "text": "On the role of search for learning"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '89"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2223248"
                        ],
                        "name": "J. Faraway",
                        "slug": "J.-Faraway",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Faraway",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Faraway"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 116208428,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8850ca8df6b9e2b800d06a54f69de94f27e14fb7",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of sequentially selecting the design points for a nonparametric regression is considered. It is desired to estimate some unknown function over some known region. Observations may be taken at a design point chosen on the basis of past observations. An adaptive method is described which both selects the local amount of smoothing required for the regression estimate and proposes the location of the next best design point. The method developed has a wide application in the estimation of surfaces where no parametric model is known or appropriate."
            },
            "slug": "Sequential-Design-for-the-Nonparametric-Regression-Faraway",
            "title": {
                "fragments": [],
                "text": "Sequential Design for the Nonparametric Regression of Curves and Surfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An adaptive method is described which both selects the local amount of smoothing required for the regression estimate and proposes the location of the next best design point for a nonparametric regression."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144932833"
                        ],
                        "name": "J. S. Judd",
                        "slug": "J.-S.-Judd",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Judd",
                            "middleNames": [
                                "Stephen"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. S. Judd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 28930642,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8d2e8e01960b196b21248d212e416742d9cfa5c",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-complexity-of-loading-shallow-neural-Judd",
            "title": {
                "fragments": [],
                "text": "On the complexity of loading shallow neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "J. Complex."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12837541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5fc0c7afc6bb27fb3752eae0ea5869413b1259b7",
            "isKey": false,
            "numCitedBy": 1646,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learned using it in a reasonable (polynomial) number of steps. Although inherent algorithmic complexity appears to set serious limits to the range of concepts that can be learned, we show that there are some important nontrivial classes of propositional concepts that can be learned in a realistic sense."
            },
            "slug": "A-theory-of-the-learnable-Valiant",
            "title": {
                "fragments": [],
                "text": "A theory of the learnable"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper regards learning as the phenomenon of knowledge acquisition in the absence of explicit programming, and gives a precise methodology for studying this phenomenon from a computational viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18251470,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "445ad69010658097fc317f7b83f1198179eebae8",
            "isKey": false,
            "numCitedBy": 1840,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper develops the separating capacities of families of nonlinear decision surfaces by a direct application of a theorem in classical combinatorial geometry. It is shown that a family of surfaces having d degrees of freedom has a natural separating capacity of 2d pattern vectors, thus extending and unifying results of Winder and others on the pattern-separating capacity of hyperplanes. Applying these ideas to the vertices of a binary n-cube yields bounds on the number of spherically, quadratically, and, in general, nonlinearly separable Boolean functions of n variables. It is shown that the set of all surfaces which separate a dichotomy of an infinite, random, separable set of pattern vectors can be characterized, on the average, by a subset of only 2d extreme pattern vectors. In addition, the problem of generalizing the classifications on a labeled set of pattern points to the classification of a new point is defined, and it is found that the probability of ambiguous generalization is large unless the number of training patterns exceeds the capacity of the set of separating surfaces."
            },
            "slug": "Geometrical-and-Statistical-Properties-of-Systems-Cover",
            "title": {
                "fragments": [],
                "text": "Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that a family of surfaces having d degrees of freedom has a natural separating capacity of 2d pattern vectors, thus extending and unifying results of Winder and others on the pattern-separating capacity of hyperplanes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Electron. Comput."
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688713"
                        ],
                        "name": "L. Hellerstein",
                        "slug": "L.-Hellerstein",
                        "structuredName": {
                            "firstName": "Lisa",
                            "lastName": "Hellerstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Hellerstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729473"
                        ],
                        "name": "M. Karpinski",
                        "slug": "M.-Karpinski",
                        "structuredName": {
                            "firstName": "Marek",
                            "lastName": "Karpinski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Karpinski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5454144,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "6d0a62c7696bcecce88e2e16c8e118810d7b8da8",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-read-once-formulas-using-membership-Hellerstein-Karpinski",
            "title": {
                "fragments": [],
                "text": "Learning read-once formulas using membership queries"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '89"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1460135553"
                        ],
                        "name": "Manel. Cooray-Wijesinha",
                        "slug": "Manel.-Cooray-Wijesinha",
                        "structuredName": {
                            "firstName": "Manel.",
                            "lastName": "Cooray-Wijesinha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manel. Cooray-Wijesinha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69453516"
                        ],
                        "name": "A. I. Khuri",
                        "slug": "A.-I.-Khuri",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Khuri",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. I. Khuri"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123200076,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bebf3f6c6095c1fc4c5b217752dd33996c7b4e4f",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The construction of Doptimal designs for multiresponse experiments was consideredby Fedorov (1972, Ch. 5). His algorithm required that the variance-covariance matrix, \u00a3, of the responses be known. This is rarely the case in practice. The primary objective of this paper isto develop a sequential procedure for the construction of multiresponse designs when \u00a3 is not known. Several numerical examples are given to illustrate this procedure"
            },
            "slug": "The-sequential-generation-of-multiresponse-designs-Cooray-Wijesinha-Khuri",
            "title": {
                "fragments": [],
                "text": "The sequential generation of multiresponse d-optimal designs when the variance-covariance matrix is not known"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083386048"
                        ],
                        "name": "Farmer",
                        "slug": "Farmer",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Farmer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Farmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30111504"
                        ],
                        "name": "Sidorowich",
                        "slug": "Sidorowich",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Sidorowich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sidorowich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 44464211,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38918f38a875bc2ede6e6865552bcf736c67dc95",
            "isKey": false,
            "numCitedBy": 1792,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a forecasting technique for chaotic data. After embedding a time series in a state space using delay coordinates, we ''learn'' the induced nonlinear mapping using local approximation. This allows us to make short-term predictions of the future behavior of a time series, using information based only on past values. We present an error estimate for this technique, and demonstrate its effectiveness by applying it to several examples, including data from the Mackey-Glass delay differential equation, Rayleigh-Benard convection, and Taylor-Couette flow."
            },
            "slug": "Predicting-chaotic-time-series.-Farmer-Sidorowich",
            "title": {
                "fragments": [],
                "text": "Predicting chaotic time series."
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An error estimate is presented for this forecasting technique for chaotic data, and its effectiveness is demonstrated by applying it to several examples, including data from the Mackey-Glass delay differential equation, Rayleigh-Benard convection, and Taylor-Couette flow."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review letters"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70851852"
                        ],
                        "name": "E. Shoesmith",
                        "slug": "E.-Shoesmith",
                        "structuredName": {
                            "firstName": "Eddie",
                            "lastName": "Shoesmith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Shoesmith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145948621"
                        ],
                        "name": "G. Box",
                        "slug": "G.-Box",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Box",
                            "middleNames": [
                                "E.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Box"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40159987"
                        ],
                        "name": "N. Draper",
                        "slug": "N.-Draper",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Draper",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Draper"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117364770,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5d090074d5fa968d852e4468820cb683040d075a",
            "isKey": false,
            "numCitedBy": 4626,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction to Response Surface Methodology. The Use of Graduating Functions. Least Squares for Response Surface Work. Factorial Designs at Two Levels. Blocking and Fractionating 2 k Factorial Designs. The Use of Steepest Ascent to Achieve System Improvement. Fitting Second--Order Models. Adequacy of Estimation and the Use of Transformation. Exploration of Maxima and Ridge Systems with Second--Order Response Surfaces. Occurrence and Elucidation of Ridge Systems, I. Occurrence and Elucidation of Ridge Systems, II. Links Between Emprirical and Theoretical Models. Design Aspects of Variance, Bias, and Lack of Fit. Variance----Optimal Designs. Practical Choice of a Response Surface Design. Subject Index. Index."
            },
            "slug": "Empirical-Model\u2010Building-and-Response-Surfaces-Shoesmith-Box",
            "title": {
                "fragments": [],
                "text": "Empirical Model\u2010Building and Response Surfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This work discusses the use of Graduating Functions, design Aspects of Variance, Bias, and Lack of Fit, and Practical Choice of a Response Surface Design in relation to Second--Order Response Surfaces."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40325722"
                        ],
                        "name": "V. Fedorov",
                        "slug": "V.-Fedorov",
                        "structuredName": {
                            "firstName": "Valerii",
                            "lastName": "Fedorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Fedorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2636336"
                        ],
                        "name": "W. J. Studden",
                        "slug": "W.-J.-Studden",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Studden",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. J. Studden"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98701042"
                        ],
                        "name": "E. Klimko",
                        "slug": "E.-Klimko",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Klimko",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Klimko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121909870,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "65c1a71c5307492782177a9799bef2cdf539ea00",
            "isKey": false,
            "numCitedBy": 2567,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-Of-Optimal-Experiments-Fedorov-Studden",
            "title": {
                "fragments": [],
                "text": "Theory Of Optimal Experiments"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49936898"
                        ],
                        "name": "R. H. Myers",
                        "slug": "R.-H.-Myers",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Myers",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. H. Myers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69453516"
                        ],
                        "name": "A. I. Khuri",
                        "slug": "A.-I.-Khuri",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Khuri",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. I. Khuri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144957896"
                        ],
                        "name": "W. H. Carter",
                        "slug": "W.-H.-Carter",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Carter",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. H. Carter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121978666,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "3abce03e37e223df2ded1aada4e1b2957fe25229",
            "isKey": false,
            "numCitedBy": 451,
            "numCiting": 167,
            "paperAbstract": {
                "fragments": [],
                "text": "Response sarfxe methodology (RSM) is a collection of tools developed in the 1950s for the purpose of determining optimum operating conditions in applications in the chemical industry. This article reviews the progrrss of RSM in the general areas of experimental design and analysis and indicates how its role has been affected by advanccs in other fields of applied statistics. Current areas of research in RSM are highlighted. and areas for future research are discussed."
            },
            "slug": "Response-surface-methodology:-1966\u20131988-Myers-Khuri",
            "title": {
                "fragments": [],
                "text": "Response surface methodology: 1966\u20131988"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This article reviews the progrrss of RSM in the general areas of experimental design and analysis and indicates how its role has been affected by advanccs in other fields of applied statistics."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101681617"
                        ],
                        "name": "P. Billingsley",
                        "slug": "P.-Billingsley",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Billingsley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Billingsley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122274264,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1d4e9d95dd060a84e9054047e0314261949a7804",
            "isKey": false,
            "numCitedBy": 2179,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Probability. Measure. Integration. Random Variables and Expected Values. Convergence of Distributions. Derivatives and Conditional Probability. Stochastic Processes. Appendix. Notes on the Problems. Bibliography. List of Symbols. Index."
            },
            "slug": "Probability-and-Measure-Billingsley",
            "title": {
                "fragments": [],
                "text": "Probability and Measure"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48107116"
                        ],
                        "name": "M. Mackey",
                        "slug": "M.-Mackey",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mackey",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mackey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145548576"
                        ],
                        "name": "L. Glass",
                        "slug": "L.-Glass",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Glass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Glass"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42039623,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "e39c17da0a3a0e7f709ef3e785c912df5cf386df",
            "isKey": false,
            "numCitedBy": 3643,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "First-order nonlinear differential-delay equations describing physiological control systems are studied. The equations display a broad diversity of dynamical behavior including limit cycle oscillations, with a variety of wave forms, and apparently aperiodic or \"chaotic\" solutions. These results are discussed in relation to dynamical respiratory and hematopoietic diseases."
            },
            "slug": "Oscillation-and-chaos-in-physiological-control-Mackey-Glass",
            "title": {
                "fragments": [],
                "text": "Oscillation and chaos in physiological control systems."
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "First-order nonlinear differential-delay equations describing physiological control systems displaying a broad diversity of dynamical behavior including limit cycle oscillations, with a variety of wave forms, and apparently aperiodic or \"chaotic\" solutions are studied."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46753437"
                        ],
                        "name": "U. Vazirani",
                        "slug": "U.-Vazirani",
                        "structuredName": {
                            "firstName": "Umesh",
                            "lastName": "Vazirani",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Vazirani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125169301,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09d2b2bafb6682aa8314b1188f4d6db3d58e90ea",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Vapnik-Chervonenkis-Dimension-Kearns-Vazirani",
            "title": {
                "fragments": [],
                "text": "The Vapnik-Chervonenkis Dimension"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256718"
                        ],
                        "name": "W. Press",
                        "slug": "W.-Press",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Press",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Press"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48590121"
                        ],
                        "name": "S. Teukolsky",
                        "slug": "S.-Teukolsky",
                        "structuredName": {
                            "firstName": "Saul",
                            "lastName": "Teukolsky",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Teukolsky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124525506,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "493cd5e15d6b8726ce27ed0595f1993c27525670",
            "isKey": false,
            "numCitedBy": 17356,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Numerical-recipes-Press-Teukolsky",
            "title": {
                "fragments": [],
                "text": "Numerical recipes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393852197"
                        ],
                        "name": "Bartle",
                        "slug": "Bartle",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Bartle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bartle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145212438"
                        ],
                        "name": "R. Gardner",
                        "slug": "R.-Gardner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gardner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gardner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123574280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1433b5c1a9088e35b93bc939612ec980773c2da6",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Elements-Of-Integration-Bartle-Gardner",
            "title": {
                "fragments": [],
                "text": "The Elements Of Integration"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 47367503,
            "fieldsOfStudy": [],
            "id": "2914e7802172a2348f8529f02e16cc51b7554f7b",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training a 3-node neural network is NP-complete"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '88"
            },
            "year": 1988
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 39,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Selecting-concise-training-sets-from-clean-data-Plutowski-White/3e06680314e1cf32706686e6520107976fdb7064?sort=total-citations"
}