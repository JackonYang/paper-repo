{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145820949"
                        ],
                        "name": "T. Pavlidis",
                        "slug": "T.-Pavlidis",
                        "structuredName": {
                            "firstName": "Theodosios",
                            "lastName": "Pavlidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pavlidis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120068581,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6839013b5dd6bcc30d6aeef615c5f034103af090",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-vectorizer-and-feature-extractor-for-document-Pavlidis",
            "title": {
                "fragments": [],
                "text": "A vectorizer and feature extractor for document recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3126111"
                        ],
                        "name": "C. Shih",
                        "slug": "C.-Shih",
                        "structuredName": {
                            "firstName": "Ching-chuan",
                            "lastName": "Shih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Shih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62580154,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b9c3e383ab0a16c34951fdae3738a84bc344a19",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A typical office document or an engineering drawing consists of regions of text, graphics, and halftone images. Developing algorithms for automating the input of such documents is the goal of this research. The scope of this paper is the design of algorithms to process raster-oriented binary images of paper-based graphics to obtain vector-oriented description files. The image is preprocessed to suppress noise and other digitization artifacts. The graphical components are then represented as a union of maximal squares using the maximal square moving algorithm. The pointers describing the connectivity of the maximal squares are analyzed to generate a linked list of squares. Straight line segments, curves, junctions, and large areas are then identified after extensive processing of this linked list. The output of the algorithm is a graphics description file which is then used as input to a graphics recognition system."
            },
            "slug": "Generation-Of-A-Line-Description-File-For-Graphics-Shih-Kasturi",
            "title": {
                "fragments": [],
                "text": "Generation Of A Line Description File For Graphics Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The design of algorithms to process raster-oriented binary images of paper-based graphics to obtain vector-oriented description files to automating the input of such documents is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Defense, Security, and Sensing"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47260288"
                        ],
                        "name": "L. Fletcher",
                        "slug": "L.-Fletcher",
                        "structuredName": {
                            "firstName": "Lloyd",
                            "lastName": "Fletcher",
                            "middleNames": [
                                "Alan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Fletcher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57137574,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb38a99da634e3501ccfe42eac4ca56ff3941fa9",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "An automated system for document analysis is extremely desirable. A digitized image consisting of a mixture of text and graphics should be segmented in order to more efficiently represent both the areas of text and graphics. This paper describes the development and implementation of a new algorithm for automated text string separation which is relatively independent of changes in text font style and size, and of string orientation. The algorithm does not explicitly recognize individual characters. The principal components of the algorithm are the generation of connected components and the application of the Hough transform in order to logically group together components into character strings which may then be separated from the graphics. The algorithm outputs two images, one containing text strings, and the other graphics. These images may then be processed by suitable character recognition and graphics recognition systems. The performance of the algorithm, both in terms of its effectiveness and computational efficiency, was evaluated using several test images. The results of the evaluations are described. The superior performance of this algorithm compared to other techniques is clear from the evaluations."
            },
            "slug": "Segmentation-Of-Binary-Images-Into-Text-Strings-And-Fletcher-Kasturi",
            "title": {
                "fragments": [],
                "text": "Segmentation Of Binary Images Into Text Strings And Graphics"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The development and implementation of a new algorithm for automated text string separation which is relatively independent of changes in text font style and size, and of string orientation is described."
            },
            "venue": {
                "fragments": [],
                "text": "Other Conferences"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34896449"
                        ],
                        "name": "R. Casey",
                        "slug": "R.-Casey",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Casey",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Casey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69485492"
                        ],
                        "name": "D. R. Ferguson",
                        "slug": "D.-R.-Ferguson",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ferguson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. R. Ferguson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29472483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45bdf2e5368886fd77f7542d797f58909ce3a169",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The automatic reading of optically scanned forms consists of two major components: extraction of the data image from the form and interpretation of the image as coded alphanumerics. The second component is also known as optical character recognition, or OCR. We have implemented a method for entry of a wide variety of forms that contain machine-printed data and that are often produced in business environments. The function, called Intelligent Forms Processing (IFP), accepts conventional forms that call for information to be printed in designated blank areas, but in which the information may exceed boundaries due to poor registration during printing. The human eye easily accommodates data that impinge on form boundaries or on background text; however, the same powers of discrimination applied to machine processing pose a technical challenge. The IFP system uses a setup phase to create a model of each form that is to be read. Scanned forms containing data are compared against the matching form model. Special algorithms are employed to extract data fields while removing background printing (e.g., form lines) intersecting the data. The extracted data images are interpreted by an OCR process that reads typical monospace fonts. New fonts may be added easily in a separate design mode. If the data are alphabetic, a lexicon may be assembled to define the possible entries."
            },
            "slug": "Intelligent-Forms-Processing-Casey-Ferguson",
            "title": {
                "fragments": [],
                "text": "Intelligent Forms Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "The automatic reading of optically scanned forms consists of two major components: extraction of the data image from the form and interpretation of the image as coded alphanumerics, also known as optical character recognition, or OCR."
            },
            "venue": {
                "fragments": [],
                "text": "IBM Syst. J."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2775803"
                        ],
                        "name": "A. Pizano",
                        "slug": "A.-Pizano",
                        "structuredName": {
                            "firstName": "Arturo",
                            "lastName": "Pizano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pizano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118278778"
                        ],
                        "name": "May-Inn Tan",
                        "slug": "May-Inn-Tan",
                        "structuredName": {
                            "firstName": "May-Inn",
                            "lastName": "Tan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "May-Inn Tan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7827912"
                        ],
                        "name": "Naoto Gambo",
                        "slug": "Naoto-Gambo",
                        "structuredName": {
                            "firstName": "Naoto",
                            "lastName": "Gambo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naoto Gambo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 37925986,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa9cfb776a79460645f77703c96a0e3430b5a8c1",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A pattern recognition system is described that classifies digitized images of business forms according to a predefined set of templates. The process involves a training phase, where images of the template forms are scanned, analyzed and stored in a data dictionary; and a recognition phase, during which scanned form images are compared to templates in a dictionary to determine their class membership. The system has been tested under a variety of conditions and its performance has been proven to be satisfactory.<<ETX>>"
            },
            "slug": "A-business-form-recognition-system-Pizano-Tan",
            "title": {
                "fragments": [],
                "text": "A business form recognition system"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A pattern recognition system is described that classifies digitized images of business forms according to a predefined set of templates and its performance has been proven to be satisfactory."
            },
            "venue": {
                "fragments": [],
                "text": "[1991] Proceedings The Fifteenth Annual International Computer Software & Applications Conference"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118761599,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57a53139d6ce1328f2d8084b98b64940265398ba",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Circuit diagrams are characterised by symbols, connections and text. The task considered in this paper is the automatic extraction of a description from a circuit diagram. The system presented in this paper consists of two major parts, one for the line interpretation and one for the text interpretation. A decision tree containing the a priori knowledge about the form of symbols controls the interpretation of lines while the text interpretation is controlled by both the results of the line interpretation and a set of finite state automata defining the denotations."
            },
            "slug": "Automatic-Interpretation-of-Lines-and-Text-in-Bunke",
            "title": {
                "fragments": [],
                "text": "Automatic Interpretation of Lines and Text in Circuit Diagrams"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A decision tree containing the a priori knowledge about the form of symbols controls the interpretation of lines while the text interpretation is controlled by both the results of the line interpretation and a set of finite state automata defining the denotations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158184"
                        ],
                        "name": "M. Karima",
                        "slug": "M.-Karima",
                        "structuredName": {
                            "firstName": "Medhat",
                            "lastName": "Karima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Karima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081849"
                        ],
                        "name": "Kuldip S. Sadhal",
                        "slug": "Kuldip-S.-Sadhal",
                        "structuredName": {
                            "firstName": "Kuldip",
                            "lastName": "Sadhal",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kuldip S. Sadhal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145433970"
                        ],
                        "name": "Tim O. McNeil",
                        "slug": "Tim-O.-McNeil",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "McNeil",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim O. McNeil"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17031684,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a2725e1977579f07783603329aa24e21b32b0c3",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "A Canadian feasibility study shows that the key to reliable and cost-effective automatic reading of drawings is detailed planning of an operator-assisted system."
            },
            "slug": "From-Paper-Drawings-to-Computer-Aided-Design-Karima-Sadhal",
            "title": {
                "fragments": [],
                "text": "From Paper Drawings to Computer-Aided Design"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A Canadian feasibility study shows that the key to reliable and cost-effective automatic reading of drawings is detailed planning of an operator-assisted system."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Computer Graphics and Applications"
            },
            "year": 1985
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 7,
        "totalPages": 1
    },
    "page_url": "https://www.semanticscholar.org/paper/Extracting-line-features-from-images-of-business-Pizano/0c247977a3ad35620f9e929252fc8935deae4205?sort=total-citations"
}