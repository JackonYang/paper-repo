{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 48
                            }
                        ],
                        "text": "Also, transfer learning or multi-task learning (Caruana 1997) was proposed and developed so that labeled training sets from different but related tasks could be grouped together in order to improve performance for all these tasks, compared to solving them separately."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 45998148,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "161ffb54a3fdf0715b198bb57bd22f910242eb49",
            "isKey": false,
            "numCitedBy": 3252,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems."
            },
            "slug": "Multitask-Learning-Caruana",
            "title": {
                "fragments": [],
                "text": "Multitask Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Prior work on MTL is reviewed, new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals is presented, and new results for MTL with k-nearest neighbor and kernel regression are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143626983"
                        ],
                        "name": "B. Bakker",
                        "slug": "B.-Bakker",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Bakker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Bakker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790356"
                        ],
                        "name": "T. Heskes",
                        "slug": "T.-Heskes",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Heskes",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Heskes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 79
                            }
                        ],
                        "text": "Moreover, class or task descriptions (features) have been investigated before (Bakker & Heskes 2003; Bonilla, Agakov, & Williams 2007; Tsochantaridis et al. 2005), but never in a setting where a task had no training data at all."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10436583,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a43d7b8e5e1bcb7c3fbf82164cfc9d12737176e8",
            "isKey": false,
            "numCitedBy": 594,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling a collection of similar regression or classification tasks can be improved by making the tasks 'learn from each other'. In machine learning, this subject is approached through 'multitask learning', where parallel tasks are modeled as multiple outputs of the same network. In multilevel analysis this is generally implemented through the mixed-effects linear model where a distinction is made between 'fixed effects', which are the same for all tasks, and 'random effects', which may vary between tasks. In the present article we will adopt a Bayesian approach in which some of the model parameters are shared (the same for all tasks) and others more loosely connected through a joint prior distribution that can be learned from the data. We seek in this way to combine the best parts of both the statistical multilevel approach and the neural network machinery. The standard assumption expressed in both approaches is that each task can learn equally well from any other task. In this article we extend the model by allowing more differentiation in the similarities between tasks. One such extension is to make the prior mean depend on higher-level task characteristics. More unsupervised clustering of tasks is obtained if we go from a single Gaussian prior to a mixture of Gaussians. This can be further generalized to a mixture of experts architecture with the gates depending on task characteristics. All three extensions are demonstrated through application both on an artificial data set and on two real-world problems, one a school problem and the other involving single-copy newspaper sales."
            },
            "slug": "Task-Clustering-and-Gating-for-Bayesian-Multitask-Bakker-Heskes",
            "title": {
                "fragments": [],
                "text": "Task Clustering and Gating for Bayesian Multitask Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A Bayesian approach is adopted in which some of the model parameters are shared and others more loosely connected through a joint prior distribution that can be learned from the data to combine the best parts of both the statistical multilevel approach and the neural network machinery."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765700"
                        ],
                        "name": "Ioannis Tsochantaridis",
                        "slug": "Ioannis-Tsochantaridis",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Tsochantaridis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ioannis Tsochantaridis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783941"
                        ],
                        "name": "Y. Altun",
                        "slug": "Y.-Altun",
                        "structuredName": {
                            "firstName": "Yasemin",
                            "lastName": "Altun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Altun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 135
                            }
                        ],
                        "text": "Moreover, class or task descriptions (features) have been investigated before (Bakker & Heskes 2003; Bonilla, Agakov, & Williams 2007; Tsochantaridis et al. 2005), but never in a setting where a task had no training data at all."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17671150,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc97e7dbb821a4edfb5151bff4352655eedca9ee",
            "isKey": false,
            "numCitedBy": 2247,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing flexible and powerful input representations, this paper addresses the complementary issue of designing classification algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classification problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach."
            },
            "slug": "Large-Margin-Methods-for-Structured-and-Output-Tsochantaridis-Joachims",
            "title": {
                "fragments": [],
                "text": "Large Margin Methods for Structured and Interdependent Output Variables"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper proposes to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation and presents a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2979876"
                        ],
                        "name": "R. Raina",
                        "slug": "R.-Raina",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Raina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Raina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078284037"
                        ],
                        "name": "Alexis Battle",
                        "slug": "Alexis-Battle",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Battle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexis Battle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409971380"
                        ],
                        "name": "Ben Packer",
                        "slug": "Ben-Packer",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Packer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Packer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 36
                            }
                        ],
                        "text": "More recently, self-taught learning (Raina et al. 2007) algorithms"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 37
                            }
                        ],
                        "text": "More recently, self-taught learning (Raina et al. 2007) algorithms\nCopyright c\u00a9 2008, Association for the Advancement of Artificial Intelligence (www.aaai.org)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6692382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3852f0113fcf8a3913c55ae92393ae6ccde347e",
            "isKey": false,
            "numCitedBy": 1610,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new machine learning framework called \"self-taught learning\" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation."
            },
            "slug": "Self-taught-learning:-transfer-learning-from-data-Raina-Battle",
            "title": {
                "fragments": [],
                "text": "Self-taught learning: transfer learning from unlabeled data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data to form a succinct input representation and significantly improve classification performance."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 118
                            }
                        ],
                        "text": "Recognizing that labeled data is often scarce but unlabeled data usually is not, semi-supervised learning algorithms (Chapelle, Scho\u0308lkopf, & Zien 2006) were proposed to make the most out of that situation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3869071,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba04a332de10e844752794c25c2dee35483d4ca3",
            "isKey": false,
            "numCitedBy": 1293,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Semi-supervised learning uses both labeled and unlabeled data to perform an otherwise supervised learning or unsupervised learning task. In the former case, there is a distinction between inductive semi-supervised learning and transductive learning. In inductive semi-supervised learning, the learner has both labeled training data {(xi, yi)}i=1 iid \u223c p(x, y) and unlabeled training data {xi} i=l+1 iid \u223c p(x), and learns a predictor f : X 7\u2192 Y, f \u2208 F where F is the hypothesis space. Here x \u2208 X is an input instance, y \u2208 Y its target label (discrete for classification or continuous for regression), p(x, y) the unknown joint distribution and p(x) its marginal, and typically l u. The goal is to learn a predictor that predicts future test data better than the predictor learned from the labeled training data alone. In transductive learning, the setting is the same except that one is solely interested in the predictions on the unlabeled training data {xi} i=l+1, without any intention to generalize to future test data. In the latter case, an unsupervised learning task is enhanced by labeled data. For example, in semi-supervised clustering (a.k.a. constrained clustering) one may have a few must-links (two instances must be in the same cluster) and cannot-links (two instances cannot be in the same cluster) in addition to the unlabeled instances to be clustered; in semi-supervised dimensionality reduction one might have the target low-dimensional coordinates on a few instances. This entry will focus on the former case of learning a predictor."
            },
            "slug": "Semi-Supervised-Learning-Zhu",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This entry will focus on the former case of learning a predictor, which is to learn a predictor that predicts future test data better than the predictor learned from the labeled training data alone."
            },
            "venue": {
                "fragments": [],
                "text": "Encyclopedia of Machine Learning"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2864754"
                        ],
                        "name": "J. Ghosn",
                        "slug": "J.-Ghosn",
                        "structuredName": {
                            "firstName": "Joumana",
                            "lastName": "Ghosn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ghosn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5208032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6d0f67ba57dca682cadfc802f7e30747aec7f19",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Biasing the hypothesis space of a learner has been shown to improve generalisation performances. Methods for achieving this goal are proposed, that range from deriving and introducing a bias into a learner to automatically learning the bias. In the latter case, most methods learn the bias by simultaneously training several related tasks derived from the same domain and imposing constraints on their parameters. We extend some of the ideas presented in this field and describe a new model that parametrizes the parameters of each task as a function of an affine manifold defined in parameter space and a point lying on the manifold. An analysis of variance on a class of learning tasks is performed that shows some significantly improved performances when using the model."
            },
            "slug": "Bias-learning,-knowledge-sharing-Ghosn-Bengio",
            "title": {
                "fragments": [],
                "text": "Bias learning, knowledge sharing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new model is described that parametrizes the parameters of each task as a function of an affine manifold defined in parameter space and a point lying on the manifold and shows some significantly improved performances when using the model."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 20
                            }
                        ],
                        "text": "Life-long learning (Thrun 1996) addresses a similar problem where a sequence of tasks must be learned one after the other."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1016169,
            "fieldsOfStudy": [
                "Computer Science",
                "Education"
            ],
            "id": "371c9dc680e916f79d9c78fcf6c894a2dd299095",
            "isKey": false,
            "numCitedBy": 685,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates learning in a lifelong context. Lifelong learning addresses situations in which a learner faces a whole stream of learning tasks. Such scenarios provide the opportunity to transfer knowledge across multiple learning tasks, in order to generalize more accurately from less training data. In this paper, several different approaches to lifelong learning are described, and applied in an object recognition domain. It is shown that across the board, lifelong learning approaches generalize consistently more accurately from less training data, by their ability to transfer knowledge across learning tasks."
            },
            "slug": "Is-Learning-The-n-th-Thing-Any-Easier-Than-Learning-Thrun",
            "title": {
                "fragments": [],
                "text": "Is Learning The n-th Thing Any Easier Than Learning The First?"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that across the board, lifelong learning approaches generalize consistently more accurately from less training data, by their ability to transfer knowledge across learning tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66191239"
                        ],
                        "name": "Bernhard Schlkopf",
                        "slug": "Bernhard-Schlkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Schlkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bernhard Schlkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281542"
                        ],
                        "name": "A. Zien",
                        "slug": "A.-Zien",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Zien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zien"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 141
                            }
                        ],
                        "text": "Recognizing that labeled data is often scarce but unlabeled data usually is not, semi-supervised learning algorithms (Chapelle, Scho\u0308lkopf, & Zien 2006) were proposed to make the most out of that situation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60860751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ee8a371fc5adc5469435020a52fb815f3b57a71",
            "isKey": false,
            "numCitedBy": 2539,
            "numCiting": 473,
            "paperAbstract": {
                "fragments": [],
                "text": "In the field of machine learning, semi-supervised learning (SSL) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). Interest in SSL has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. This first comprehensive overview of SSL presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research. Semi-Supervised Learning first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. The core of the book is the presentation of SSL methods, organized according to algorithmic strategies. After an examination of generative models, the book describes algorithms that implement the low-density separation assumption, graph-based methods, and algorithms that perform two-step learning. The book then discusses SSL applications and offers guidelines for SSL practitioners by analyzing the results of extensive benchmark experiments. Finally, the book looks at interesting directions for SSL research. The book closes with a discussion of the relationship between semi-supervised learning and transduction. Adaptive Computation and Machine Learning series"
            },
            "slug": "Semi-Supervised-Learning-Chapelle-Schlkopf",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This first comprehensive overview of semi-supervised learning presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2647414"
                        ],
                        "name": "T. Rogers",
                        "slug": "T.-Rogers",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Rogers",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Rogers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49031498"
                        ],
                        "name": "Ruichen Qian",
                        "slug": "Ruichen-Qian",
                        "structuredName": {
                            "firstName": "Ruichen",
                            "lastName": "Qian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruichen Qian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2347339"
                        ],
                        "name": "C. Kalish",
                        "slug": "C.-Kalish",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Kalish",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kalish"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 197
                            }
                        ],
                        "text": "Moreover, most of the stimuli humans receive are not labeled by an expert, which would mean that humans are able to leverage unlabeled stimuli, a fact that has been validated by experimental data (Zhu et al. 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5516805,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c5c744b1609f473c882bcbd751eb13a72f13e7bc",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore the connections between machine learning and human learning in one form of semi-supervised classification. 22 human subjects completed a novel 2-class categorization task in which they were first taught to categorize a single labeled example from each category, and subsequently were asked to categorize, without feedback, a large set of additional items. Stimuli were visually complex and unrecognizable shapes. The unlabeled examples were sampled from a bimodal distribution with modes appearing either to the left (left-shift condition) or right (right-shift condition) of the two labeled examples. Results showed that, although initial decision boundaries were near the middle of the two labeled examples, after exposure to the unlabeled examples, they shifted in different directions in the two groups. In this respect, the human behavior conformed well to the predictions of a Gaussian mixture model for semi-supervised learning. The human behavior differed from model predictions in other interesting respects, suggesting some fruitful avenues for future inquiry."
            },
            "slug": "Humans-Perform-Semi-Supervised-Classification-Too-Zhu-Rogers",
            "title": {
                "fragments": [],
                "text": "Humans Perform Semi-Supervised Classification Too"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Results showed that, although initial decision boundaries were near the middle of the two labeled examples, after exposure to the unlabeled examples, they shifted in different directions in the two groups, suggesting some fruitful avenues for future inquiry."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41224113"
                        ],
                        "name": "E. Miller",
                        "slug": "E.-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 19
                            }
                        ],
                        "text": "One-shot learning (Miller 2002) attempts to solve the problem of recognizing a particular object from only one labeled example, also by using labeled data from other categories of related objects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2527791,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13295994fca8120869508ea8315373eb62d6f8f1",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": "Human beings exhibit rapid learning when presented with a small number of images of a new object. A person can identify an object under a wide variety of visual conditions after having seen only a single example of that object. This ability can be partly explained by the application of previously learned statistical knowledge to a new setting. This thesis presents an approach to acquiring knowledge in one setting and using it in another. Specifically, we develop probability densities over common image changes. Given a single image of a new object and a model of change learned from a different object, we form a model of the new object that can be used for synthesis, classification, and other visual tasks. \nWe start by modeling spatial changes. We develop a framework for learning statistical knowledge of spatial transformations in one task and using that knowledge in a new task. By sharing a probability density over spatial transformations learned from a sample of handwritten letters, we develop a handwritten digit classifier that achieves 88.6% accuracy using only a single hand-picked training example from each class. \nThe classification scheme includes a new algorithm, congealing , for the joint alignment of a set of images using an entropy minimization criterion. We investigate properties of this algorithm and compare it to other methods of addressing spatial variability in images. We illustrate its application to binary images, gray-scale images, and a set of 3-D neonatal magnetic resonance brain volumes. \nNext, we extend the method of change modeling from spatial transformations to color transformations. By measuring statistically common joint color changes of a scene in an office environment, and then applying standard statistical techniques such as principal components analysis, we develop a probabilistic model of color change. We show that these color changes, which we call color flows, can be shared effectively between certain types of scenes. That is, a probability density over color change developed by observing one scene can provide useful information about the variability of another scene. We demonstrate a variety of applications including image synthesis, image matching, and shadow detection. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)"
            },
            "slug": "Learning-from-one-example-in-machine-vision-by-Miller",
            "title": {
                "fragments": [],
                "text": "Learning from one example in machine vision by sharing probability densities"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A framework for learning statistical knowledge of spatial transformations in one task and using that knowledge in a new task is developed and a probabilistic model of color change is developed, which can be shared effectively between certain types of scenes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405347571"
                        ],
                        "name": "Pierre-Jean L'Heureux",
                        "slug": "Pierre-Jean-L'Heureux",
                        "structuredName": {
                            "firstName": "Pierre-Jean",
                            "lastName": "L'Heureux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre-Jean L'Heureux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50358810"
                        ],
                        "name": "S. Yue",
                        "slug": "S.-Yue",
                        "structuredName": {
                            "firstName": "Shi",
                            "lastName": "Yue",
                            "middleNames": [
                                "Yi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Yue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14109100,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "867e055da3d50d98698be9cda4b762a66e2e05fb",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Building a QSAR model of a new biological target for which few screening data are available is a statistical challenge. However, the new target may be part of a bigger family, for which we have more screening data. Collaborative filtering or, more generally, multi-task learning, is a machine learning approach that improves the generalization performance of an algorithm by using information from related tasks as an inductive bias. We use collaborative filtering techniques for building predictive models that link multiple targets to multiple examples. The more commonalities between the targets, the better the multi-target model that can be built. We show an example of a multi-target neural network that can use family information to produce a predictive model of an undersampled target. We evaluate JRank, a kernel-based method designed for collaborative filtering. We show their performance on compound prioritization for an HTS campaign and the underlying shared representation between targets. JRank outperformed the neural network both in the single- and multi-target models."
            },
            "slug": "Collaborative-Filtering-on-a-Family-of-Biological-Erhan-L'Heureux",
            "title": {
                "fragments": [],
                "text": "Collaborative Filtering on a Family of Biological Targets"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work shows an example of a multi-target neural network that can use family information to produce a predictive model of an undersampled target, and evaluates JRank, a kernel-based method designed for collaborative filtering."
            },
            "venue": {
                "fragments": [],
                "text": "J. Chem. Inf. Model."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30561807"
                        ],
                        "name": "Edwin V. Bonilla",
                        "slug": "Edwin-V.-Bonilla",
                        "structuredName": {
                            "firstName": "Edwin",
                            "lastName": "Bonilla",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edwin V. Bonilla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3288675"
                        ],
                        "name": "F. Agakov",
                        "slug": "F.-Agakov",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Agakov",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Agakov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Moreover, class or task descriptions (features) have been investigated before (Bakker & Heskes 2003;  Bonilla, Agakov, & Williams 2007;  Tsochantaridis et al. 2005), but never in a setting where a task had no training data at all."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8873859,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5a3a6e596c7a7eece2a147da33a6f8479af360f",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we are concerned with multitask learning when task-specific features are available. We describe two ways of achieving this using Gaussian process predictors: in the first method, the data from all tasks is combined into one dataset, making use of the task-specific features. In the second method we train specific predictors for each reference task, and then combine their predictions using a gating network. We demonstrate these methods on a compiler performance prediction problem, where a task is defined as predicting the speed-up obtained when applying a sequence of code transformations to a given program."
            },
            "slug": "Kernel-Multi-task-Learning-using-Task-specific-Bonilla-Agakov",
            "title": {
                "fragments": [],
                "text": "Kernel Multi-task Learning using Task-specific Features"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "Two ways of achieving multitask learning when task-specific features are available using Gaussian process predictors are described, one of which is to train specific predictors for each reference task, and then combine their predictions using a gating network."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3177568"
                        ],
                        "name": "J. Q. Candela",
                        "slug": "J.-Q.-Candela",
                        "structuredName": {
                            "firstName": "Joaquin",
                            "lastName": "Candela",
                            "middleNames": [
                                "Qui\u00f1onero"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Q. Candela"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50095217"
                        ],
                        "name": "Fabian H Sinz",
                        "slug": "Fabian-H-Sinz",
                        "structuredName": {
                            "firstName": "Fabian",
                            "lastName": "Sinz",
                            "middleNames": [
                                "H"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabian H Sinz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7259908,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e5afed6eb2ef1e360618b7b1d545b7616d750b8",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This Chapter presents the PASCAL Evaluating Predictive Uncertainty Challenge, introduces the contributed Chapters by the participants who obtained outstanding results, and provides a discussion with some lessons to be learnt. The Challenge was set up to evaluate the ability of Machine Learning algorithms to provide good \u201cprobabilistic predictions\u201d, rather than just the usual \u201cpoint predictions\u201d with no measure of uncertainty, in regression and classification problems. Parti-cipants had to compete on a number of regression and classification tasks, and were evaluated by both traditional losses that only take into account point predictions and losses we proposed that evaluate the quality of the probabilistic predictions."
            },
            "slug": "Evaluating-Predictive-Uncertainty-Challenge-Candela-Rasmussen",
            "title": {
                "fragments": [],
                "text": "Evaluating Predictive Uncertainty Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This Chapter presents the PASCAL Evaluating Predictive Uncertainty Challenge, introduces the contributed Chapters by the participants who obtained outstanding results, and provides a discussion with some lessons to be learnt."
            },
            "venue": {
                "fragments": [],
                "text": "MLCW"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2299972"
                        ],
                        "name": "I. Biederman",
                        "slug": "I.-Biederman",
                        "structuredName": {
                            "firstName": "Irving",
                            "lastName": "Biederman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Biederman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, the number of objects that a human is able to discriminate has been estimated to up to 30,000 (Bierderman 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8054340,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b37258659bcdbc380b1e6c4e22cce9ea06397a1",
            "isKey": false,
            "numCitedBy": 5632,
            "numCiting": 129,
            "paperAbstract": {
                "fragments": [],
                "text": "The perceptual recognition of objects is conceptualized to be a process in which the image of the input is segmented at regions of deep concavity into an arrangement of simple geometric components, such as blocks, cylinders, wedges, and cones. The fundamental assumption of the proposed theory, recognition-by-components (RBC), is that a modest set of generalized-cone components, called geons (N \u00a3 36), can be derived from contrasts of five readily detectable properties of edges in a two-dimensiona l image: curvature, collinearity, symmetry, parallelism, and cotermination. The detection of these properties is generally invariant over viewing position an$ image quality and consequently allows robust object perception when the image is projected from a novel viewpoint or is degraded. RBC thus provides a principled account of the heretofore undecided relation between the classic principles of perceptual organization and pattern recognition: The constraints toward regularization (Pragnanz) characterize not the complete object but the object's components. Representational power derives from an allowance of free combinations of the geons. A Principle of Componential Recovery can account for the major phenomena of object recognition: If an arrangement of two or three geons can be recovered from the input, objects can be quickly recognized even when they are occluded, novel, rotated in depth, or extensively degraded. The results from experiments on the perception of briefly presented pictures by human observers provide empirical support for the theory. Any single object can project an infinity of image configurations to the retina. The orientation of the object to the viewer can vary continuously, each giving rise to a different two-dimensional projection. The object can be occluded by other objects or texture fields, as when viewed behind foliage. The object need not be presented as a full-colored textured image but instead can be a simplified line drawing. Moreover, the object can even be missing some of its parts or be a novel exemplar of its particular category. But it is only with rare exceptions that an image fails to be rapidly and readily classified, either as an instance of a familiar object category or as an instance that cannot be so classified (itself a form of classification)."
            },
            "slug": "Recognition-by-components:-a-theory-of-human-image-Biederman",
            "title": {
                "fragments": [],
                "text": "Recognition-by-components: a theory of human image understanding."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Recognition-by-components (RBC) provides a principled account of the heretofore undecided relation between the classic principles of perceptual organization and pattern recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2957934"
                        ],
                        "name": "Y. Moses",
                        "slug": "Y.-Moses",
                        "structuredName": {
                            "firstName": "Yael",
                            "lastName": "Moses",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Moses"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743045"
                        ],
                        "name": "S. Ullman",
                        "slug": "S.-Ullman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Ullman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ullman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2331213"
                        ],
                        "name": "S. Edelman",
                        "slug": "S.-Edelman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Edelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Edelman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "There is also empirical evidence that humans are able to recognize objects (such as faces) with high accuracy ( Moses, Ullman, & Edelman 1996 ) even after having been exposed to only one instance of that object (one-shot learning)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8411547,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "13f752e5ec3b6187014e97ae5d719500f648f41d",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "An image of a face depends not only on its shape, but also on the viewpoint, illumination conditions, and facial expression. A face recognition system must overcome the changes in face appearance induced by these factors. Two related questions were investigated: the capacity of the human visual system to generalize the recognition of faces to novel images, and the level at which this generalization occurs. This problem was approached by comparing the identification and generalization capacity for upright and inverted faces. For upright faces, remarkably good generalization to novel conditions was found. For inverted faces, the generalization to novel views was significantly worse for both new illumination and viewpoint, although the performance on the training images was similar to that on the upright condition. The results indicate that at least some of the processes that support generalization across viewpoint and illumination are neither universal (because subjects did not generalize as easily for inverted faces as for upright ones) nor strictly object specific (because in upright faces nearly perfect generalization was possible from a single view, by itself insufficient for building a complete object-specific model). It is proposed that generalization in face recognition occurs at an intermediate level that is applicable to a class of objects, and that at this level upright and inverted faces initially constitute distinct object classes."
            },
            "slug": "Generalization-to-Novel-Images-in-Upright-and-Faces-Moses-Ullman",
            "title": {
                "fragments": [],
                "text": "Generalization to Novel Images in Upright and Inverted Faces"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is proposed that generalization in face recognition occurs at an intermediate level that is applicable to a class of objects, and that at this level upright and inverted faces initially constitute distinct object classes."
            },
            "venue": {
                "fragments": [],
                "text": "Perception"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34568108"
                        ],
                        "name": "A. Schein",
                        "slug": "A.-Schein",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Schein",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729548"
                        ],
                        "name": "Alexandrin Popescul",
                        "slug": "Alexandrin-Popescul",
                        "structuredName": {
                            "firstName": "Alexandrin",
                            "lastName": "Popescul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandrin Popescul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1412391493"
                        ],
                        "name": "L. Ungar",
                        "slug": "L.-Ungar",
                        "structuredName": {
                            "firstName": "Lyle",
                            "lastName": "Ungar",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ungar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766638"
                        ],
                        "name": "D. Pennock",
                        "slug": "D.-Pennock",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pennock",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pennock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 78
                            }
                        ],
                        "text": "In the context of recommender systems, it is known as the \u201ccold start\u201d problem (Schein et al. 2002) where a task corresponds to an object to recommend."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9021966,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01dd5af05830e96211e5445b65c96abccf8d0445",
            "isKey": false,
            "numCitedBy": 1755,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed a method for recommending items that combines content and collaborative data under a single probabilistic framework. We benchmark our algorithm against a na\u00efve Bayes classifier on the cold-start problem, where we wish to recommend items that no one in the community has yet rated. We systematically explore three testing methodologies using a publicly available data set, and explain how these methods apply to specific real-world applications. We advocate heuristic recommenders when benchmarking to give competent baseline performance. We introduce a new performance metric, the CROC curve, and demonstrate empirically that the various components of our testing strategy combine to obtain deeper understanding of the performance characteristics of recommender systems. Though the emphasis of our testing is on cold-start recommending, our methods for recommending and evaluation are general."
            },
            "slug": "Methods-and-metrics-for-cold-start-recommendations-Schein-Popescul",
            "title": {
                "fragments": [],
                "text": "Methods and metrics for cold-start recommendations"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A method for recommending items that combines content and collaborative data under a single probabilistic framework is developed, and it is demonstrated empirically that the various components of the testing strategy combine to obtain deeper understanding of the performance characteristics of recommender systems."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '02"
            },
            "year": 2002
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 12
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 16,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Zero-data-Learning-of-New-Tasks-Larochelle-Erhan/fabb43426ead447305c9186f54d5124863730e47?sort=total-citations"
}