{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3106437"
                        ],
                        "name": "Alessandro Raganato",
                        "slug": "Alessandro-Raganato",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Raganato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alessandro Raganato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143675545"
                        ],
                        "name": "J. Tiedemann",
                        "slug": "J.-Tiedemann",
                        "structuredName": {
                            "firstName": "J\u00f6rg",
                            "lastName": "Tiedemann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tiedemann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 161
                            }
                        ],
                        "text": ", 2017) overtaking recurrent models as the primary architectures for many NLP tasks, analyzing attention has become another common strategy for interpretability (Raganato and Tiedemann, 2018a; Clark et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53596423,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94238dead40b12735d79ed63e29ead70730261a2",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "The attention mechanism is a successful technique in modern NLP, especially in tasks like machine translation. The recently proposed network architecture of the Transformer is based entirely on attention mechanisms and achieves new state of the art results in neural machine translation, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the model and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the attention mechanism in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about syntax while higher layers tend to encode more semantics."
            },
            "slug": "An-Analysis-of-Encoder-Representations-in-Machine-Raganato-Tiedemann",
            "title": {
                "fragments": [],
                "text": "An Analysis of Encoder Representations in Transformer-Based Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work investigates the information that is learned by the attention mechanism in Transformer models with different translation quality, and sheds light on the relative strengths and weaknesses of the various encoder representations."
            },
            "venue": {
                "fragments": [],
                "text": "BlackboxNLP@EMNLP"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056908"
                        ],
                        "name": "Jesse Vig",
                        "slug": "Jesse-Vig",
                        "structuredName": {
                            "firstName": "Jesse",
                            "lastName": "Vig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jesse Vig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083259"
                        ],
                        "name": "Yonatan Belinkov",
                        "slug": "Yonatan-Belinkov",
                        "structuredName": {
                            "firstName": "Yonatan",
                            "lastName": "Belinkov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yonatan Belinkov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 92
                            }
                        ],
                        "text": "Previous studies have uncovered specific attention heads that learn particular dependencies (Vig and Belinkov, 2019; Clark et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 77
                            }
                        ],
                        "text": "Earlier layers of a BERT model can capture particular linguistic information (Clark et al., 2019; Vig and Belinkov, 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 184486755,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a039ea239e37f53a2cb60c68e0a1967994353166",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads."
            },
            "slug": "Analyzing-the-Structure-of-Attention-in-a-Language-Vig-Belinkov",
            "title": {
                "fragments": [],
                "text": "Analyzing the Structure of Attention in a Transformer Language Model"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is found that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers, and the deepest layers of the model capture the most distant relationships."
            },
            "venue": {
                "fragments": [],
                "text": "BlackboxNLP@ACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144906624"
                        ],
                        "name": "Alex Wang",
                        "slug": "Alex-Wang",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50286460"
                        ],
                        "name": "Amanpreet Singh",
                        "slug": "Amanpreet-Singh",
                        "structuredName": {
                            "firstName": "Amanpreet",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amanpreet Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38614754"
                        ],
                        "name": "Julian Michael",
                        "slug": "Julian-Michael",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Michael",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julian Michael"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145783676"
                        ],
                        "name": "Felix Hill",
                        "slug": "Felix-Hill",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Hill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Felix Hill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39455775"
                        ],
                        "name": "Omer Levy",
                        "slug": "Omer-Levy",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3644767"
                        ],
                        "name": "Samuel R. Bowman",
                        "slug": "Samuel-R.-Bowman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Bowman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel R. Bowman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5034059,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93b8da28d006415866bf48f9a6e06b5242129195",
            "isKey": false,
            "numCitedBy": 2635,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions."
            },
            "slug": "GLUE:-A-Multi-Task-Benchmark-and-Analysis-Platform-Wang-Singh",
            "title": {
                "fragments": [],
                "text": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models, which favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks."
            },
            "venue": {
                "fragments": [],
                "text": "BlackboxNLP@EMNLP"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056908"
                        ],
                        "name": "Jesse Vig",
                        "slug": "Jesse-Vig",
                        "structuredName": {
                            "firstName": "Jesse",
                            "lastName": "Vig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jesse Vig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 34
                            }
                        ],
                        "text": "fast and interactive for the user (Vig, 2019)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 189762556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0de0a44b859a3719d11834479112314b4caba669",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The Transformer is a sequence model that forgoes traditional recurrent architectures in favor of a fully attention-based approach. Besides improving performance, an advantage of using attention is that it can also help to interpret a model by showing how the model assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior."
            },
            "slug": "A-Multiscale-Visualization-of-Attention-in-the-Vig",
            "title": {
                "fragments": [],
                "text": "A Multiscale Visualization of Attention in the Transformer Model"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism in the Transformer model is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39172707"
                        ],
                        "name": "Jacob Devlin",
                        "slug": "Jacob-Devlin",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Devlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Devlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744179"
                        ],
                        "name": "Ming-Wei Chang",
                        "slug": "Ming-Wei-Chang",
                        "structuredName": {
                            "firstName": "Ming-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Wei Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 70
                            }
                        ],
                        "text": "Over the past year, similar tests have primarily been applied to BERT (Devlin et al., 2019) and its derivatives (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 56
                            }
                        ],
                        "text": "These efforts focus on selecting a model, such as BERT (Devlin et al., 2019), and exploring the Transformer\u2019s contextual embeddings and attentions across layers to determine whether and where it learns to represent linguistic features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 71
                            }
                        ],
                        "text": "Over the past year, similar tests have primarily been applied to BERT (Devlin et al., 2019) and its derivatives (e.g., Sanh et al., 2019; Liu et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 92
                            }
                        ],
                        "text": "We show here how some of their insights are easily accessible through the EXBERT interface (Devlin et al., 2019) for the case-sensitive BERT-base model, which has 12 layers and 12 heads per layer."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52967399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "isKey": true,
            "numCitedBy": 33754,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
            },
            "slug": "BERT:-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang",
            "title": {
                "fragments": [],
                "text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A new language representation model, BERT, designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2879705"
                        ],
                        "name": "Hendrik Strobelt",
                        "slug": "Hendrik-Strobelt",
                        "structuredName": {
                            "firstName": "Hendrik",
                            "lastName": "Strobelt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hendrik Strobelt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3159346"
                        ],
                        "name": "Sebastian Gehrmann",
                        "slug": "Sebastian-Gehrmann",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Gehrmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Gehrmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143758236"
                        ],
                        "name": "H. Pfister",
                        "slug": "H.-Pfister",
                        "structuredName": {
                            "firstName": "Hanspeter",
                            "lastName": "Pfister",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Pfister"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2531268"
                        ],
                        "name": "Alexander M. Rush",
                        "slug": "Alexander-M.-Rush",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Rush",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander M. Rush"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 22
                            }
                        ],
                        "text": "Searching Inspired by Strobelt et al. (2017, 2018), EXBERT performs a nearest neighbor search of embeddings on a reference corpus as follows."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 229
                            }
                        ],
                        "text": "One way to identify information is to probe the representations by using them as features in classifiers for linguistic tasks, or by identifying contexts that lead to similar patterns (Tenney et al., 2019b; Conneau et al., 2018; Strobelt et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 76
                            }
                        ],
                        "text": "Representations, on the other hand, cannot be easily visualized footnoteSee Strobelt et al. (2017) for a discussion why heat-maps are not an appropriate visualization of hidden states. but they can be understood by searching for similar representations in an annotated corpus."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 25127323,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a798c13da2d500dd76c4ff76f18ded43df0d59bc",
            "isKey": false,
            "numCitedBy": 272,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks, and in particular long short-term memory (LSTM) networks, are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In this work, we present LSTMVis, a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. The tool allows users to select a hypothesis input range to focus on local state changes, to match these states changes to similar patterns in a large data set, and to align these results with structural annotations from their domain. We show several use cases of the tool for analyzing specific hidden state properties on dataset containing nesting, phrase structure, and chord progressions, and demonstrate how the tool can be used to isolate patterns for further statistical analysis. We characterize the domain, the different stakeholders, and their goals and tasks. Long-term usage data after putting the tool online revealed great interest in the machine learning community."
            },
            "slug": "LSTMVis:-A-Tool-for-Visual-Analysis-of-Hidden-State-Strobelt-Gehrmann",
            "title": {
                "fragments": [],
                "text": "LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents LSTMVis, a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics, and describes the domain, the different stakeholders, and their goals and tasks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Visualization and Computer Graphics"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144358401"
                        ],
                        "name": "Kevin Clark",
                        "slug": "Kevin-Clark",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3030219"
                        ],
                        "name": "Urvashi Khandelwal",
                        "slug": "Urvashi-Khandelwal",
                        "structuredName": {
                            "firstName": "Urvashi",
                            "lastName": "Khandelwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Urvashi Khandelwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39455775"
                        ],
                        "name": "Omer Levy",
                        "slug": "Omer-Levy",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 226
                            }
                        ],
                        "text": "With Transformers (Vaswani et al., 2017) overtaking recurrent models as the primary architectures for many NLP tasks, analyzing attention has become another common strategy for interpretabil-\nity (Raganato and Tiedemann, 2018a; Clark et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 117
                            }
                        ],
                        "text": "Previous studies have uncovered specific attention heads that learn particular dependencies (Vig and Belinkov, 2019; Clark et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 78
                            }
                        ],
                        "text": "Earlier layers of a BERT model can capture particular linguistic information (Clark et al., 2019; Vig and Belinkov, 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Clark et al. (2019) showed that individual heads recognize standard Part of Speech (POS) and Dependency (DEP) relationships (e.g., Objects of the Preposition (POBJ) and Determinants (DET)) with high fidelity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 71
                            }
                        ],
                        "text": "We demonstrate that EXBERT can replicate insights from the analysis by Clark et al. (2019) and easily extend it to other models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 76
                            }
                        ],
                        "text": "These tokens often receive very high attention and act as a null operation (Clark et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Clark et al. (2019) performed an extensive analysis to determine which heads in a base sized BERT Transformer model learned which dependencies."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 161
                            }
                        ],
                        "text": ", 2017) overtaking recurrent models as the primary architectures for many NLP tasks, analyzing attention has become another common strategy for interpretability (Raganato and Tiedemann, 2018a; Clark et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 184486746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "95a251513853c6032bdecebd4b74e15795662986",
            "isKey": true,
            "numCitedBy": 755,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT\u2019s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT\u2019s attention."
            },
            "slug": "What-Does-BERT-Look-at-An-Analysis-of-BERT\u2019s-Clark-Khandelwal",
            "title": {
                "fragments": [],
                "text": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that certain attention heads correspond well to linguistic notions of syntax and coreference, and an attention-based probing classifier is proposed and used to demonstrate that substantial syntactic information is captured in BERT\u2019s attention."
            },
            "venue": {
                "fragments": [],
                "text": "BlackboxNLP@ACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2844898"
                        ],
                        "name": "N. Keskar",
                        "slug": "N.-Keskar",
                        "structuredName": {
                            "firstName": "Nitish",
                            "lastName": "Keskar",
                            "middleNames": [
                                "Shirish"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Keskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143775536"
                        ],
                        "name": "Bryan McCann",
                        "slug": "Bryan-McCann",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "McCann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan McCann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697944"
                        ],
                        "name": "L. Varshney",
                        "slug": "L.-Varshney",
                        "structuredName": {
                            "firstName": "Lav",
                            "lastName": "Varshney",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Varshney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2228109"
                        ],
                        "name": "Caiming Xiong",
                        "slug": "Caiming-Xiong",
                        "structuredName": {
                            "firstName": "Caiming",
                            "lastName": "Xiong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Caiming Xiong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 202573071,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75acc731bdd2b626edc74672a30da3bc51010ae8",
            "isKey": false,
            "numCitedBy": 496,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release CTRL, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-specific behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow CTRL to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of CTRL at this https URL."
            },
            "slug": "CTRL:-A-Conditional-Transformer-Language-Model-for-Keskar-McCann",
            "title": {
                "fragments": [],
                "text": "CTRL: A Conditional Transformer Language Model for Controllable Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "CTRL is released, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-specific behavior, providing more explicit control over text generation."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50335211"
                        ],
                        "name": "Thomas Wolf",
                        "slug": "Thomas-Wolf",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1380459402"
                        ],
                        "name": "Lysandre Debut",
                        "slug": "Lysandre-Debut",
                        "structuredName": {
                            "firstName": "Lysandre",
                            "lastName": "Debut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lysandre Debut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51918868"
                        ],
                        "name": "Victor Sanh",
                        "slug": "Victor-Sanh",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Sanh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Sanh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40811585"
                        ],
                        "name": "Julien Chaumond",
                        "slug": "Julien-Chaumond",
                        "structuredName": {
                            "firstName": "Julien",
                            "lastName": "Chaumond",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julien Chaumond"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40899333"
                        ],
                        "name": "Clement Delangue",
                        "slug": "Clement-Delangue",
                        "structuredName": {
                            "firstName": "Clement",
                            "lastName": "Delangue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Clement Delangue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1382164294"
                        ],
                        "name": "Anthony Moi",
                        "slug": "Anthony-Moi",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Moi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anthony Moi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1382164165"
                        ],
                        "name": "Pierric Cistac",
                        "slug": "Pierric-Cistac",
                        "structuredName": {
                            "firstName": "Pierric",
                            "lastName": "Cistac",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierric Cistac"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1382164170"
                        ],
                        "name": "T. Rault",
                        "slug": "T.-Rault",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Rault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Rault"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2185329"
                        ],
                        "name": "R\u00e9mi Louf",
                        "slug": "R\u00e9mi-Louf",
                        "structuredName": {
                            "firstName": "R\u00e9mi",
                            "lastName": "Louf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9mi Louf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97662964"
                        ],
                        "name": "Morgan Funtowicz",
                        "slug": "Morgan-Funtowicz",
                        "structuredName": {
                            "firstName": "Morgan",
                            "lastName": "Funtowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Morgan Funtowicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 61
                            }
                        ],
                        "text": "EXBERT runs Huggingface\u2019s unified API for Transformer models (Wolf et al., 2019) which allows any Transformer model from that API to take full advantage of the Attention View."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 204796444,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbde0f942a2541437e3edd55941ddafba6c5adb0",
            "isKey": false,
            "numCitedBy": 203,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent advances in modern Natural Language Processing (NLP) research have been dominated by the combination of Transfer Learning methods with large-scale Transformer language models. With them came a paradigm shift in NLP with the starting point for training a model on a downstream task moving from a blank specific model to a general-purpose pretrained architecture. Still, creating these general-purpose models remains an expensive and time-consuming process restricting the use of these methods to a small sub-set of the wider NLP community. In this paper, we present Transformers, a library for state-of-the-art NLP, making these developments available to the community by gathering state-of-the-art general-purpose pretrained models under a unified API together with an ecosystem of libraries, examples, tutorials and scripts targeting many downstream NLP tasks. Transformers features carefully crafted model implementations and highperformance pretrained weights for two main deep learning frameworks, PyTorch and TensorFlow, while supporting all the necessary tools to analyze, evaluate and use these models in downstream tasks such as text/token classification, questions answering and language generation among others. Transformers has gained significant organic traction and adoption among both the researcher and practitioner communities. We are committed at Hugging Face to pursue the efforts to develop Transformers with the ambition of creating the standard library for building NLP systems."
            },
            "slug": "Transformers-:-State-ofthe-art-Natural-Language-Wolf-Debut",
            "title": {
                "fragments": [],
                "text": "Transformers : State-ofthe-art Natural Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Transformers is presented, a library for state-of-the-art NLP, making these developments available to the community by gathering state of theart general-purpose pretrained models under a unified API together with an ecosystem of libraries, examples, tutorials and scripts targeting many downstream NLP tasks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6117577"
                        ],
                        "name": "Ian Tenney",
                        "slug": "Ian-Tenney",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Tenney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian Tenney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2465658"
                        ],
                        "name": "Patrick Xia",
                        "slug": "Patrick-Xia",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Xia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrick Xia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108381400"
                        ],
                        "name": "Berlin Chen",
                        "slug": "Berlin-Chen",
                        "structuredName": {
                            "firstName": "Berlin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Berlin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144906624"
                        ],
                        "name": "Alex Wang",
                        "slug": "Alex-Wang",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48926630"
                        ],
                        "name": "Adam Poliak",
                        "slug": "Adam-Poliak",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Poliak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Poliak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145534175"
                        ],
                        "name": "R. Thomas McCoy",
                        "slug": "R.-Thomas-McCoy",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "McCoy",
                            "middleNames": [
                                "Thomas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Thomas McCoy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8756748"
                        ],
                        "name": "Najoung Kim",
                        "slug": "Najoung-Kim",
                        "structuredName": {
                            "firstName": "Najoung",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Najoung Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7536576"
                        ],
                        "name": "Benjamin Van Durme",
                        "slug": "Benjamin-Van-Durme",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Durme",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Van Durme"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3644767"
                        ],
                        "name": "Samuel R. Bowman",
                        "slug": "Samuel-R.-Bowman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Bowman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel R. Bowman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143790066"
                        ],
                        "name": "Dipanjan Das",
                        "slug": "Dipanjan-Das",
                        "structuredName": {
                            "firstName": "Dipanjan",
                            "lastName": "Das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dipanjan Das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2949185"
                        ],
                        "name": "Ellie Pavlick",
                        "slug": "Ellie-Pavlick",
                        "structuredName": {
                            "firstName": "Ellie",
                            "lastName": "Pavlick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ellie Pavlick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 165
                            }
                        ],
                        "text": "Researchers have developed suites of probing techniques, agnostic to the underlying model, that can capture these properties across many different linguistic tasks (Tenney et al., 2019b; Conneau et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 185
                            }
                        ],
                        "text": "One way to identify information is to probe the representations by using them as features in classifiers for linguistic tasks, or by identifying contexts that lead to similar patterns (Tenney et al., 2019b; Conneau et al., 2018; Strobelt et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 10
                            }
                        ],
                        "text": "Moreover, Tenney et al. (2019a) demonstrated that linguistic information is very localized within the representations in different layers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 108300988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2587eddd57bc4ba286d91b27c185083f16f40ee",
            "isKey": false,
            "numCitedBy": 496,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline."
            },
            "slug": "What-do-you-learn-from-context-Probing-for-sentence-Tenney-Xia",
            "title": {
                "fragments": [],
                "text": "What do you learn from context? Probing for sentence structure in contextualized word representations"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel edge probing task design is introduced and a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline are constructed to investigate how sentence structure is encoded across a range of syntactic, semantic, local, and long-range phenomena."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46235299"
                        ],
                        "name": "Elena Voita",
                        "slug": "Elena-Voita",
                        "structuredName": {
                            "firstName": "Elena",
                            "lastName": "Voita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elena Voita"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144251066"
                        ],
                        "name": "David Talbot",
                        "slug": "David-Talbot",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Talbot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Talbot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157158"
                        ],
                        "name": "F. Moiseev",
                        "slug": "F.-Moiseev",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Moiseev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Moiseev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082372"
                        ],
                        "name": "Rico Sennrich",
                        "slug": "Rico-Sennrich",
                        "structuredName": {
                            "firstName": "Rico",
                            "lastName": "Sennrich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rico Sennrich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144889265"
                        ],
                        "name": "Ivan Titov",
                        "slug": "Ivan-Titov",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Titov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ivan Titov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 486,
                                "start": 158
                            }
                        ],
                        "text": "Experiments for these analyses are often supported by open-source repositories that implement the newest architectures and thus enable rapid experimentation [Wolf et al., 2019]. We similarly need flexible evaluation frameworks for Transformer models that allow the community to test hypotheses rapidly. Toward that end, visualization tools can offer concise summaries of useful information and allow interaction with large models. Attention visualizations such as BertViz by Vig [2019] have taken large steps toward these goals by making exploration of BERT\u2019s attention fast and interactive for the user."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 162183964,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "07a64686ce8e43ac475a8d820a8a9f1d87989583",
            "isKey": false,
            "numCitedBy": 438,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU."
            },
            "slug": "Analyzing-Multi-Head-Self-Attention:-Specialized-Do-Voita-Talbot",
            "title": {
                "fragments": [],
                "text": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is found that the most important and confident heads play consistent and often linguistically-interpretable roles and when pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, it is observed that specialized heads are last to be pruned."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49837811"
                        ],
                        "name": "Sarthak Jain",
                        "slug": "Sarthak-Jain",
                        "structuredName": {
                            "firstName": "Sarthak",
                            "lastName": "Jain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sarthak Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1912476"
                        ],
                        "name": "Byron C. Wallace",
                        "slug": "Byron-C.-Wallace",
                        "structuredName": {
                            "firstName": "Byron",
                            "lastName": "Wallace",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Byron C. Wallace"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 183
                            }
                        ],
                        "text": "However, interpreting attention patterns without understanding the attended-to embeddings, or relying on attention alone as a faithful explanation, can lead to faulty interpretations (Brunner et al., 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Li et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 67855860,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f",
            "isKey": false,
            "numCitedBy": 624,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful \u201cexplanations\u201d for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do."
            },
            "slug": "Attention-is-not-Explanation-Jain-Wallace",
            "title": {
                "fragments": [],
                "text": "Attention is not Explanation"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work performs extensive experiments across a variety of NLP tasks to assess the degree to which attention weights provide meaningful \u201cexplanations\u201d for predictions, and finds that they largely do not."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2879705"
                        ],
                        "name": "Hendrik Strobelt",
                        "slug": "Hendrik-Strobelt",
                        "structuredName": {
                            "firstName": "Hendrik",
                            "lastName": "Strobelt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hendrik Strobelt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3159346"
                        ],
                        "name": "Sebastian Gehrmann",
                        "slug": "Sebastian-Gehrmann",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Gehrmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Gehrmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144075157"
                        ],
                        "name": "Michael Behrisch",
                        "slug": "Michael-Behrisch",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Behrisch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Behrisch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2912842"
                        ],
                        "name": "Adam Perer",
                        "slug": "Adam-Perer",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Perer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Perer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143758236"
                        ],
                        "name": "H. Pfister",
                        "slug": "H.-Pfister",
                        "structuredName": {
                            "firstName": "Hanspeter",
                            "lastName": "Pfister",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Pfister"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2531268"
                        ],
                        "name": "Alexander M. Rush",
                        "slug": "Alexander-M.-Rush",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Rush",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander M. Rush"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 22
                            }
                        ],
                        "text": "Searching Inspired by Strobelt et al. (2017, 2018), EXBERT performs a nearest neighbor search of embeddings on a reference corpus as follows."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13754931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1abec8d03b8d1435ae920edea9b437be913a30ca",
            "isKey": false,
            "numCitedBy": 146,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural sequence-to-sequence models have proven to be accurate and robust for many sequence prediction tasks, and have become the standard approach for automatic translation of text. The models work with a five-stage blackbox pipeline that begins with encoding a source sequence to a vector space and then decoding out to a new target sequence. This process is now standard, but like many deep learning methods remains quite difficult to understand or debug. In this work, we present a visual analysis tool that allows interaction and \u201cwhat if\u201d-style exploration of trained sequence-to-sequence models through each stage of the translation process. The aim is to identify which patterns have been learned, to detect model errors, and to probe the model with counterfactual scenario. We demonstrate the utility of our tool through several real-world sequence-to-sequence use cases on large-scale models."
            },
            "slug": "Seq2seq-Vis:-A-Visual-Debugging-Tool-for-Models-Strobelt-Gehrmann",
            "title": {
                "fragments": [],
                "text": "Seq2seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work presents a visual analysis tool that allows interaction and \u201cwhat if\u201d-style exploration of trained sequence-to-sequence models through each stage of the translation process, to identify which patterns have been learned, to detect model errors, and to probe the model with counterfactual scenario."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Visualization and Computer Graphics"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38909097"
                        ],
                        "name": "Alec Radford",
                        "slug": "Alec-Radford",
                        "structuredName": {
                            "firstName": "Alec",
                            "lastName": "Radford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alec Radford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49387725"
                        ],
                        "name": "Jeff Wu",
                        "slug": "Jeff-Wu",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48422824"
                        ],
                        "name": "Rewon Child",
                        "slug": "Rewon-Child",
                        "structuredName": {
                            "firstName": "Rewon",
                            "lastName": "Child",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rewon Child"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150970919"
                        ],
                        "name": "D. Luan",
                        "slug": "D.-Luan",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Luan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Luan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2698777"
                        ],
                        "name": "Dario Amodei",
                        "slug": "Dario-Amodei",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Amodei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dario Amodei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 109
                            }
                        ],
                        "text": "Some showed that Transformer models, similar to recurrent models, can\n1For autoregressive models like GPT-2 (Radford et al., 2019), this matrix is triangular since attention cannot point toward unseen tokens.\neffectively encode syntactic properties in their representations (Raganato and Tiedemann,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 94
                            }
                        ],
                        "text": "The following experiments use the smaller configuration of GPT-2 with 12 layers and 12 heads (Radford et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 37
                            }
                        ],
                        "text": "For autoregressive models like GPT-2 (Radford et al., 2019), this matrix is triangular since attention cannot point toward unseen tokens."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 160025533,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "isKey": true,
            "numCitedBy": 6284,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations."
            },
            "slug": "Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu",
            "title": {
                "fragments": [],
                "text": "Language Models are Unsupervised Multitask Learners"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is demonstrated that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText, suggesting a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51918868"
                        ],
                        "name": "Victor Sanh",
                        "slug": "Victor-Sanh",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Sanh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Sanh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1380459402"
                        ],
                        "name": "Lysandre Debut",
                        "slug": "Lysandre-Debut",
                        "structuredName": {
                            "firstName": "Lysandre",
                            "lastName": "Debut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lysandre Debut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40811585"
                        ],
                        "name": "Julien Chaumond",
                        "slug": "Julien-Chaumond",
                        "structuredName": {
                            "firstName": "Julien",
                            "lastName": "Chaumond",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julien Chaumond"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50335211"
                        ],
                        "name": "Thomas Wolf",
                        "slug": "Thomas-Wolf",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Wolf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 119
                            }
                        ],
                        "text": "Over the past year, similar tests have primarily been applied to BERT (Devlin et al., 2019) and its derivatives (e.g., Sanh et al., 2019; Liu et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 203626972,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a54b56af24bb4873ed0163b77df63b92bd018ddc",
            "isKey": false,
            "numCitedBy": 2086,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study."
            },
            "slug": "DistilBERT,-a-distilled-version-of-BERT:-smaller,-Sanh-Debut",
            "title": {
                "fragments": [],
                "text": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can be fine-tuned with good performances on a wide range of tasks like its larger counterparts, and introduces a triple loss combining language modeling, distillation and cosine-distance losses."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6117577"
                        ],
                        "name": "Ian Tenney",
                        "slug": "Ian-Tenney",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Tenney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian Tenney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143790066"
                        ],
                        "name": "Dipanjan Das",
                        "slug": "Dipanjan-Das",
                        "structuredName": {
                            "firstName": "Dipanjan",
                            "lastName": "Das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dipanjan Das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2949185"
                        ],
                        "name": "Ellie Pavlick",
                        "slug": "Ellie-Pavlick",
                        "structuredName": {
                            "firstName": "Ellie",
                            "lastName": "Pavlick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ellie Pavlick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 165
                            }
                        ],
                        "text": "Researchers have developed suites of probing techniques, agnostic to the underlying model, that can capture these properties across many different linguistic tasks (Tenney et al., 2019b; Conneau et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 185
                            }
                        ],
                        "text": "One way to identify information is to probe the representations by using them as features in classifiers for linguistic tasks, or by identifying contexts that lead to similar patterns (Tenney et al., 2019b; Conneau et al., 2018; Strobelt et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 10
                            }
                        ],
                        "text": "Moreover, Tenney et al. (2019a) demonstrated that linguistic information is very localized within the representations in different layers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 155092004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97906df07855b029b7aae7c2a1c6c5e8df1d531c",
            "isKey": false,
            "numCitedBy": 678,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations."
            },
            "slug": "BERT-Rediscovers-the-Classical-NLP-Pipeline-Tenney-Das",
            "title": {
                "fragments": [],
                "text": "BERT Rediscovers the Classical NLP Pipeline"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work finds that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35823986"
                        ],
                        "name": "Sarah Wiegreffe",
                        "slug": "Sarah-Wiegreffe",
                        "structuredName": {
                            "firstName": "Sarah",
                            "lastName": "Wiegreffe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sarah Wiegreffe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1826312"
                        ],
                        "name": "Yuval Pinter",
                        "slug": "Yuval-Pinter",
                        "structuredName": {
                            "firstName": "Yuval",
                            "lastName": "Pinter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuval Pinter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 230
                            }
                        ],
                        "text": "However, interpreting attention patterns without understanding the attended-to embeddings, or relying on attention alone as a faithful explanation, can lead to faulty interpretations (Brunner et al., 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Li et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 199552244,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce177672b00ddf46e4906157a7e997ca9338b8b9",
            "isKey": false,
            "numCitedBy": 421,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model\u2019s prediction, and consequently reach insights regarding the model\u2019s decision-making process. A recent paper claims that \u2018Attention is not Explanation\u2019 (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one\u2019s definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don\u2019t perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability."
            },
            "slug": "Attention-is-not-not-Explanation-Wiegreffe-Pinter",
            "title": {
                "fragments": [],
                "text": "Attention is not not Explanation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that even when reliable adversarial distributions can be found, they don\u2019t perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144906624"
                        ],
                        "name": "Alex Wang",
                        "slug": "Alex-Wang",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100984698"
                        ],
                        "name": "Yada Pruksachatkun",
                        "slug": "Yada-Pruksachatkun",
                        "structuredName": {
                            "firstName": "Yada",
                            "lastName": "Pruksachatkun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yada Pruksachatkun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10666396"
                        ],
                        "name": "Nikita Nangia",
                        "slug": "Nikita-Nangia",
                        "structuredName": {
                            "firstName": "Nikita",
                            "lastName": "Nangia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nikita Nangia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50286460"
                        ],
                        "name": "Amanpreet Singh",
                        "slug": "Amanpreet-Singh",
                        "structuredName": {
                            "firstName": "Amanpreet",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amanpreet Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38614754"
                        ],
                        "name": "Julian Michael",
                        "slug": "Julian-Michael",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Michael",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julian Michael"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145783676"
                        ],
                        "name": "Felix Hill",
                        "slug": "Felix-Hill",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Hill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Felix Hill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39455775"
                        ],
                        "name": "Omer Levy",
                        "slug": "Omer-Levy",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3644767"
                        ],
                        "name": "Samuel R. Bowman",
                        "slug": "Samuel-R.-Bowman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Bowman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel R. Bowman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 143424870,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9f6ada77448664b71128bb19df15765336974a6",
            "isKey": false,
            "numCitedBy": 823,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at this http URL."
            },
            "slug": "SuperGLUE:-A-Stickier-Benchmark-for-General-Purpose-Wang-Pruksachatkun",
            "title": {
                "fragments": [],
                "text": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new benchmark styled after GLUE is presented, a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard are presented."
            },
            "venue": {
                "fragments": [],
                "text": "NeurIPS"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40348417"
                        ],
                        "name": "Ashish Vaswani",
                        "slug": "Ashish-Vaswani",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Vaswani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Vaswani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3877127"
                        ],
                        "name": "Niki Parmar",
                        "slug": "Niki-Parmar",
                        "structuredName": {
                            "firstName": "Niki",
                            "lastName": "Parmar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Niki Parmar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39328010"
                        ],
                        "name": "Jakob Uszkoreit",
                        "slug": "Jakob-Uszkoreit",
                        "structuredName": {
                            "firstName": "Jakob",
                            "lastName": "Uszkoreit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakob Uszkoreit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145024664"
                        ],
                        "name": "Llion Jones",
                        "slug": "Llion-Jones",
                        "structuredName": {
                            "firstName": "Llion",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llion Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19177000"
                        ],
                        "name": "Aidan N. Gomez",
                        "slug": "Aidan-N.-Gomez",
                        "structuredName": {
                            "firstName": "Aidan",
                            "lastName": "Gomez",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aidan N. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3443442"
                        ],
                        "name": "Illia Polosukhin",
                        "slug": "Illia-Polosukhin",
                        "structuredName": {
                            "firstName": "Illia",
                            "lastName": "Polosukhin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Illia Polosukhin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 19
                            }
                        ],
                        "text": "With Transformers (Vaswani et al., 2017) overtaking recurrent models as the primary architectures for many NLP tasks, analyzing attention has become another common strategy for interpretabil-\nity (Raganato and Tiedemann, 2018a; Clark et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 41
                            }
                        ],
                        "text": "If these tokens are split further by the Transformer\u2019s tokenization scheme, each word-piece receives the metadata of its parent token."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 18
                            }
                        ],
                        "text": "With Transformers (Vaswani et al., 2017) overtaking recurrent models as the primary architectures for many NLP tasks, analyzing attention has become another common strategy for interpretability (Raganato and Tiedemann, 2018a; Clark et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 96
                            }
                        ],
                        "text": "These efforts focus on selecting a model, such as BERT (Devlin et al., 2019), and exploring the Transformer\u2019s contextual embeddings and attentions across layers to determine whether and where it learns to represent linguistic features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 91
                            }
                        ],
                        "text": "We now use EXBERT to explore the problem of gender bias and co-reference in autoregressive Transformers (Zhao et al., 2018), a problem inherent in the training data that infects the model\u2019s understanding of language (Font and Costa-juss\u00e0, 2019)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 44
                            }
                        ],
                        "text": "The Transformer architecture, as defined by Vaswani et al. (2017), relies on multiple sequential applications of self attention layers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13756489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "isKey": true,
            "numCitedBy": 35157,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            "slug": "Attention-is-All-you-Need-Vaswani-Shazeer",
            "title": {
                "fragments": [],
                "text": "Attention is All you Need"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely is proposed, which generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083259"
                        ],
                        "name": "Yonatan Belinkov",
                        "slug": "Yonatan-Belinkov",
                        "structuredName": {
                            "firstName": "Yonatan",
                            "lastName": "Belinkov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yonatan Belinkov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145898106"
                        ],
                        "name": "James R. Glass",
                        "slug": "James-R.-Glass",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Glass",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James R. Glass"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 113
                            }
                        ],
                        "text": "The analysis of learned contextual representation in neural networks has been a widely investigated topic in NLP (Belinkov and Glass, 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 122
                            }
                        ],
                        "text": "Uncovering this information plays a vital role in understanding and interpreting the learned structure of neural networks (Belinkov and Glass, 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 107
                            }
                        ],
                        "text": "With Transformers (Vaswani et al., 2017) overtaking recurrent models as the primary architectures for many NLP tasks, analyzing attention has become another common strategy for interpretabil-\nity (Raganato and Tiedemann, 2018a; Clark et al., 2019)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 56657817,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "668f42a4d4094f0a66d402a16087e14269b31a1f",
            "isKey": false,
            "numCitedBy": 305,
            "numCiting": 217,
            "paperAbstract": {
                "fragments": [],
                "text": "The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work."
            },
            "slug": "Analysis-Methods-in-Neural-Language-Processing:-A-Belinkov-Glass",
            "title": {
                "fragments": [],
                "text": "Analysis Methods in Neural Language Processing: A Survey"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Analysis methods in neural language processing are reviewed, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work."
            },
            "venue": {
                "fragments": [],
                "text": "TACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3375440"
                        ],
                        "name": "Stephen Merity",
                        "slug": "Stephen-Merity",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Merity",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Merity"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2228109"
                        ],
                        "name": "Caiming Xiong",
                        "slug": "Caiming-Xiong",
                        "structuredName": {
                            "firstName": "Caiming",
                            "lastName": "Xiong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Caiming Xiong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40518045"
                        ],
                        "name": "James Bradbury",
                        "slug": "James-Bradbury",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bradbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Bradbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 74
                            }
                        ],
                        "text": "By searching for the token \u201cin\u201d across a subset of the \u201cWikipedia\u201d corpus (Merity et al., 2016), we confirm that many"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 71
                            }
                        ],
                        "text": "By searching for the token \u201cin\u201d across a subset of the \u201cWikipedia\u201d corpus (Merity et al., 2016), we confirm that many\n3http://www.gutenberg.org/ebooks/55\nother annotated sentences exhibit this pattern."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16299141,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "efbd381493bb9636f489b965a2034d529cd56bcd",
            "isKey": false,
            "numCitedBy": 1042,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus."
            },
            "slug": "Pointer-Sentinel-Mixture-Models-Merity-Xiong",
            "title": {
                "fragments": [],
                "text": "Pointer Sentinel Mixture Models"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank while using far fewer parameters than a standard softmax LSTM and the freely available WikiText corpus is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2480903"
                        ],
                        "name": "A. Conneau",
                        "slug": "A.-Conneau",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Conneau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Conneau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067996"
                        ],
                        "name": "Germ\u00e1n Kruszewski",
                        "slug": "Germ\u00e1n-Kruszewski",
                        "structuredName": {
                            "firstName": "Germ\u00e1n",
                            "lastName": "Kruszewski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Germ\u00e1n Kruszewski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830914"
                        ],
                        "name": "Guillaume Lample",
                        "slug": "Guillaume-Lample",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Lample",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Lample"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2934336"
                        ],
                        "name": "Lo\u00efc Barrault",
                        "slug": "Lo\u00efc-Barrault",
                        "structuredName": {
                            "firstName": "Lo\u00efc",
                            "lastName": "Barrault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lo\u00efc Barrault"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 187
                            }
                        ],
                        "text": "Researchers have developed suites of probing techniques, agnostic to the underlying model, that can capture these properties across many different linguistic tasks (Tenney et al., 2019b; Conneau et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 207
                            }
                        ],
                        "text": "One way to identify information is to probe the representations by using them as features in classifiers for linguistic tasks, or by identifying contexts that lead to similar patterns (Tenney et al., 2019b; Conneau et al., 2018; Strobelt et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 24461982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c41516420ddbd0f29e010ca259a74c1fc2da0466",
            "isKey": false,
            "numCitedBy": 545,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. \u201cDownstream\u201d tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods."
            },
            "slug": "What-you-can-cram-into-a-single-$&!#*-vector:-for-Conneau-Kruszewski",
            "title": {
                "fragments": [],
                "text": "What you can cram into a single vector: Probing sentence embeddings for linguistic properties"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "10 probing tasks designed to capture simple linguistic features of sentences are introduced and used to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of bothencoders and training methods."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11323179"
                        ],
                        "name": "Yinhan Liu",
                        "slug": "Yinhan-Liu",
                        "structuredName": {
                            "firstName": "Yinhan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yinhan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40511414"
                        ],
                        "name": "Myle Ott",
                        "slug": "Myle-Ott",
                        "structuredName": {
                            "firstName": "Myle",
                            "lastName": "Ott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Myle Ott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39589154"
                        ],
                        "name": "Naman Goyal",
                        "slug": "Naman-Goyal",
                        "structuredName": {
                            "firstName": "Naman",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naman Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3048577"
                        ],
                        "name": "Jingfei Du",
                        "slug": "Jingfei-Du",
                        "structuredName": {
                            "firstName": "Jingfei",
                            "lastName": "Du",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jingfei Du"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144863691"
                        ],
                        "name": "Mandar Joshi",
                        "slug": "Mandar-Joshi",
                        "structuredName": {
                            "firstName": "Mandar",
                            "lastName": "Joshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mandar Joshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50536468"
                        ],
                        "name": "Danqi Chen",
                        "slug": "Danqi-Chen",
                        "structuredName": {
                            "firstName": "Danqi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danqi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39455775"
                        ],
                        "name": "Omer Levy",
                        "slug": "Omer-Levy",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35084211"
                        ],
                        "name": "M. Lewis",
                        "slug": "M.-Lewis",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Lewis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759422"
                        ],
                        "name": "Veselin Stoyanov",
                        "slug": "Veselin-Stoyanov",
                        "structuredName": {
                            "firstName": "Veselin",
                            "lastName": "Stoyanov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Veselin Stoyanov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 138
                            }
                        ],
                        "text": "Over the past year, similar tests have primarily been applied to BERT (Devlin et al., 2019) and its derivatives (e.g., Sanh et al., 2019; Liu et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 198953378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
            "isKey": false,
            "numCitedBy": 7267,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code."
            },
            "slug": "RoBERTa:-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott",
            "title": {
                "fragments": [],
                "text": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is found that BERT was significantly undertrained, and can match or exceed the performance of every model published after it, and the best model achieves state-of-the-art results on GLUE, RACE and SQuAD."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082372"
                        ],
                        "name": "Rico Sennrich",
                        "slug": "Rico-Sennrich",
                        "structuredName": {
                            "firstName": "Rico",
                            "lastName": "Sennrich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rico Sennrich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259100"
                        ],
                        "name": "B. Haddow",
                        "slug": "B.-Haddow",
                        "structuredName": {
                            "firstName": "Barry",
                            "lastName": "Haddow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Haddow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2539211"
                        ],
                        "name": "Alexandra Birch",
                        "slug": "Alexandra-Birch",
                        "structuredName": {
                            "firstName": "Alexandra",
                            "lastName": "Birch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandra Birch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "bene\ufb01t from transfer learning [Devlin et al.,2019]. BERT introduces special tokens into the typical training process in addition to the input tokens that are extracted using Byte-Pair Encoding (BPE) [Sennrich et al., 2015]. The architecture requires every input to start with a \u201c[CLS]\u201d and end with a \u201c[SEP]\u201d token and uses a technique called Masked Language Modeling (MLM) to develop its language model [Devlin et al.,20"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1114678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1af68821518f03568f913ab03fc02080247a27ff",
            "isKey": false,
            "numCitedBy": 4794,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively."
            },
            "slug": "Neural-Machine-Translation-of-Rare-Words-with-Units-Sennrich-Haddow",
            "title": {
                "fragments": [],
                "text": "Neural Machine Translation of Rare Words with Subword Units"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper introduces a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units, and empirically shows that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.3 BLEU."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31125953"
                        ],
                        "name": "Joel Escud\u00e9 Font",
                        "slug": "Joel-Escud\u00e9-Font",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Font",
                            "middleNames": [
                                "Escud\u00e9"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joel Escud\u00e9 Font"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398996347"
                        ],
                        "name": "M. Costa-juss\u00e0",
                        "slug": "M.-Costa-juss\u00e0",
                        "structuredName": {
                            "firstName": "Marta",
                            "lastName": "Costa-juss\u00e0",
                            "middleNames": [
                                "Ruiz"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Costa-juss\u00e0"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 100
                            }
                        ],
                        "text": ", 2018), a problem inherent in the training data that infects the model\u2019s understanding of language (Font and Costa-juss\u00e0, 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57761084,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50154080ccbaec1a3b4ba401bebd94b80225d21a",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural machine translation has significantly pushed forward the quality of the field. However, there are remaining big issues with the output translations and one of them is fairness. Neural models are trained on large text corpora which contain biases and stereotypes. As a consequence, models inherit these social biases. Recent methods have shown results in reducing gender bias in other natural language processing tools such as word embeddings. We take advantage of the fact that word embeddings are used in neural machine translation to propose a method to equalize gender biases in neural machine translation using these representations. Specifically, we propose, experiment and analyze the integration of two debiasing techniques over GloVe embeddings in the Transformer translation architecture. We evaluate our proposed system on the WMT English-Spanish benchmark task, showing gains up to one BLEU point. As for the gender bias evaluation, we generate a test set of occupations and we show that our proposed system learns to equalize existing biases from the baseline system."
            },
            "slug": "Equalizing-Gender-Bias-in-Neural-Machine-with-Word-Font-Costa-juss\u00e0",
            "title": {
                "fragments": [],
                "text": "Equalizing Gender Bias in Neural Machine Translation with Word Embeddings Techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes, experiment and analyzes the integration of two debiasing techniques over GloVe embeddings in the Transformer translation architecture, and shows that the proposed system learns to equalize existing biases from the baseline system."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48569393"
                        ],
                        "name": "Xintong Li",
                        "slug": "Xintong-Li",
                        "structuredName": {
                            "firstName": "Xintong",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xintong Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2123036664"
                        ],
                        "name": "Guanlin Li",
                        "slug": "Guanlin-Li",
                        "structuredName": {
                            "firstName": "Guanlin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guanlin Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2978364"
                        ],
                        "name": "Lemao Liu",
                        "slug": "Lemao-Liu",
                        "structuredName": {
                            "firstName": "Lemao",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lemao Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144444550"
                        ],
                        "name": "M. Meng",
                        "slug": "M.-Meng",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Meng",
                            "middleNames": [
                                "Q.-H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34720053"
                        ],
                        "name": "Shuming Shi",
                        "slug": "Shuming-Shi",
                        "structuredName": {
                            "firstName": "Shuming",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuming Shi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 258
                            }
                        ],
                        "text": "However, interpreting attention patterns without understanding the attended-to embeddings, or relying on attention alone as a faithful explanation, can lead to faulty interpretations (Brunner et al., 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Li et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 196176486,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1c7c5ab9ffce84ee3681011f09def462074f1ca",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Prior researches suggest that neural machine translation (NMT) captures word alignment through its attention mechanism, however, this paper finds attention may almost fail to capture word alignment for some NMT models. This paper thereby proposes two methods to induce word alignment which are general and agnostic to specific NMT models. Experiments show that both methods induce much better word alignment than attention. This paper further visualizes the translation through the word alignment induced by NMT. In particular, it analyzes the effect of alignment errors on translation errors at word level and its quantitative analysis over many testing examples consistently demonstrate that alignment errors are likely to lead to translation errors measured by different metrics."
            },
            "slug": "On-the-Word-Alignment-from-Neural-Machine-Li-Li",
            "title": {
                "fragments": [],
                "text": "On the Word Alignment from Neural Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper proposes two methods to induce word alignment which are general and agnostic to specific NMT models and experiments show that both methods induce much better word alignment than attention."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2536245"
                        ],
                        "name": "D. Mare\u010dek",
                        "slug": "D.-Mare\u010dek",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mare\u010dek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mare\u010dek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2436967"
                        ],
                        "name": "Rudolf Rosa",
                        "slug": "Rudolf-Rosa",
                        "structuredName": {
                            "firstName": "Rudolf",
                            "lastName": "Rosa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rudolf Rosa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 69
                            }
                        ],
                        "text": "189 effectively encode syntactic properties in their representations (Raganato and Tiedemann, 2018b; Mare\u010dek and Rosa, 2018)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53583865,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49400b3a3ea01772e321e3e010b7b891c3d6cb88",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This is a work in progress about extracting the sentence tree structures from the encoder\u2019s self-attention weights, when translating into another language using the Transformer neural network architecture. We visualize the structures and discuss their characteristics with respect to the existing syntactic theories and annotations."
            },
            "slug": "Extracting-Syntactic-Trees-from-Transformer-Encoder-Mare\u010dek-Rosa",
            "title": {
                "fragments": [],
                "text": "Extracting Syntactic Trees from Transformer Encoder Self-Attentions"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This is a work in progress about extracting the sentence tree structures from the encoder\u2019s self-attention weights, when translating into another language using the Transformer neural network architecture."
            },
            "venue": {
                "fragments": [],
                "text": "BlackboxNLP@EMNLP"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33524946"
                        ],
                        "name": "Jieyu Zhao",
                        "slug": "Jieyu-Zhao",
                        "structuredName": {
                            "firstName": "Jieyu",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jieyu Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785372925"
                        ],
                        "name": "Tianlu Wang",
                        "slug": "Tianlu-Wang",
                        "structuredName": {
                            "firstName": "Tianlu",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tianlu Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064210"
                        ],
                        "name": "Mark Yatskar",
                        "slug": "Mark-Yatskar",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Yatskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Yatskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2004053"
                        ],
                        "name": "Vicente Ordonez",
                        "slug": "Vicente-Ordonez",
                        "structuredName": {
                            "firstName": "Vicente",
                            "lastName": "Ordonez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vicente Ordonez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2782886"
                        ],
                        "name": "Kai-Wei Chang",
                        "slug": "Kai-Wei-Chang",
                        "structuredName": {
                            "firstName": "Kai-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai-Wei Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 96
                            }
                        ],
                        "text": "These efforts focus on selecting a model, such as BERT (Devlin et al., 2019), and exploring the Transformer\u2019s contextual embeddings and attentions across layers to determine whether and where it learns to represent linguistic features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 41
                            }
                        ],
                        "text": "If these tokens are split further by the Transformer\u2019s tokenization scheme, each word-piece receives the metadata of its parent token."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 105
                            }
                        ],
                        "text": "We now use EXBERT to explore the problem of gender bias and co-reference in autoregressive Transformers (Zhao et al., 2018), a problem inherent in the training data that infects the model\u2019s understanding of language (Font and Costa-juss\u00e0, 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 5
                            }
                        ],
                        "text": "With Transformers (Vaswani et al., 2017) overtaking recurrent models as the primary architectures for many NLP tasks, analyzing attention has become another common strategy for interpretabil-\nity (Raganato and Tiedemann, 2018a; Clark et al., 2019)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4952494,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0be19fd9896e5d40222c690cc3ff553adc7c0e27",
            "isKey": true,
            "numCitedBy": 356,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we introduce a new benchmark for co-reference resolution focused on gender bias, WinoBias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets."
            },
            "slug": "Gender-Bias-in-Coreference-Resolution:-Evaluation-Zhao-Wang",
            "title": {
                "fragments": [],
                "text": "Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A data-augmentation approach is demonstrated that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by rule-based, feature-rich, and neural coreference systems in WinoBias without significantly affecting their performance on existing datasets."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL-HLT"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2089067"
                        ],
                        "name": "Yoav Goldberg",
                        "slug": "Yoav-Goldberg",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Goldberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoav Goldberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 58007068,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "efeab0dcdb4c1cce5e537e57745d84774be99b9a",
            "isKey": false,
            "numCitedBy": 314,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) \"coloreless green ideas\" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases."
            },
            "slug": "Assessing-BERT's-Syntactic-Abilities-Goldberg",
            "title": {
                "fragments": [],
                "text": "Assessing BERT's Syntactic Abilities"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "The extent to which the recently introduced BERT model captures English syntactic phenomena is assessed, using naturally-occurring subject-verb agreement stimuli; \"coloreless green ideas\" subject- Verb Agreement stimuli; and manually crafted stimuli for subject- verb agreement and reflexive anaphora phenomena."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115354049"
                        ],
                        "name": "Jeff Johnson",
                        "slug": "Jeff-Johnson",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3271933"
                        ],
                        "name": "M. Douze",
                        "slug": "M.-Douze",
                        "structuredName": {
                            "firstName": "Matthijs",
                            "lastName": "Douze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Douze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681054"
                        ],
                        "name": "H. J\u00e9gou",
                        "slug": "H.-J\u00e9gou",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "J\u00e9gou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. J\u00e9gou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 151
                            }
                        ],
                        "text": "The model then processes this corpus, and its embeddings E(l) are stored at every layer l and indexed for a Cosine Similarity (CS) search using faiss (Johnson et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 926364,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2cbb8de53759e75411bc528518947a3094fbce3a",
            "isKey": false,
            "numCitedBy": 1258,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Similarity search finds application in database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data parallel tasks such as distance computation, prior approaches in this domain are bottlenecked by algorithms that expose less parallelism, such as <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq1-2921572.gif\"/></alternatives></inline-formula>-min selection, or make poor use of the memory hierarchy. We propose a novel design for <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq2-2921572.gif\"/></alternatives></inline-formula>-selection. We apply it in different similarity search scenarios, by optimizing brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation operates at up to 55 percent of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5 \u00d7 faster than prior GPU state of the art. It enables the construction of a high accuracy <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq3-2921572.gif\"/></alternatives></inline-formula>-NN graph on 95 million images from the <sc>Yfcc100M</sc> dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility."
            },
            "slug": "Billion-Scale-Similarity-Search-with-GPUs-Johnson-Douze",
            "title": {
                "fragments": [],
                "text": "Billion-Scale Similarity Search with GPUs"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper proposes a novel design for an inline-formula that enables the construction of a high accuracy, brute-force, approximate and compressed-domain search based on product quantization, and applies it in different similarity search scenarios."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Big Data"
            },
            "year": 2021
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38094934"
                        ],
                        "name": "Gino Brunner",
                        "slug": "Gino-Brunner",
                        "structuredName": {
                            "firstName": "Gino",
                            "lastName": "Brunner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gino Brunner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40457423"
                        ],
                        "name": "Yang Liu",
                        "slug": "Yang-Liu",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150973452"
                        ],
                        "name": "Damian Pascual",
                        "slug": "Damian-Pascual",
                        "structuredName": {
                            "firstName": "Damian",
                            "lastName": "Pascual",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Damian Pascual"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143944934"
                        ],
                        "name": "Oliver Richter",
                        "slug": "Oliver-Richter",
                        "structuredName": {
                            "firstName": "Oliver",
                            "lastName": "Richter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oliver Richter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716440"
                        ],
                        "name": "Roger Wattenhofer",
                        "slug": "Roger-Wattenhofer",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Wattenhofer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roger Wattenhofer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 184
                            }
                        ],
                        "text": "However, interpreting attention patterns without understanding the attended-to embeddings, or relying on attention alone as a faithful explanation, can lead to faulty interpretations (Brunner et al., 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Li et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 199543755,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d88b671af49e477e3a0e85014fb853b6d3bd363",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Explainability of deep learning systems is a vital requirement for many applications. However, it is still an unsolved problem. Recent self-attention based models for natural language processing, such as the Transformer or BERT, offer hope of greater explainability by providing attention maps that can be directly inspected. Nevertheless, by just looking at the attention maps one often overlooks that the attention is not over words but over hidden embeddings, which themselves can be mixed representations of multiple embeddings. We investigate to what extent the implicit assumption made in many recent papers - that hidden embeddings at all layers still correspond to the underlying words - is justified. We quantify how much embeddings are mixed based on a gradient based attribution method and find that already after the first layer less than 50% of the embedding is attributed to the underlying word, declining thereafter to a median contribution of 7.5% in the last layer. While throughout the layers the underlying word remains as the one contributing most to the embedding, we argue that attention visualizations are misleading and should be treated with care when explaining the underlying deep learning system."
            },
            "slug": "On-the-Validity-of-Self-Attention-as-Explanation-in-Brunner-Liu",
            "title": {
                "fragments": [],
                "text": "On the Validity of Self-Attention as Explanation in Transformer Models"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is argued that attention visualizations are misleading and should be treated with care when explaining the underlying deep learning system."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "URL https://arxiv"
            },
            "venue": {
                "fragments": [],
                "text": "org/abs/1908.04626."
            },
            "year": 2019
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 96
                            }
                        ],
                        "text": "EXBERT accomplishes this by first tokenizing, normalizing, and labeling the sentence with spaCy (Honnibal and Montani, 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing"
            },
            "venue": {
                "fragments": [],
                "text": "To appear."
            },
            "year": 2017
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 61
                            }
                        ],
                        "text": "EXBERT runs Huggingface\u2019s unified API for Transformer models (Wolf et al., 2019) which allows any Transformer model from that API to take full advantage of the Attention View."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 157
                            }
                        ],
                        "text": "Experiments for these analyses are often supported by open-source repositories that implement the newest architectures and thus enable rapid experimentation [Wolf et al., 2019]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Transformers: Stateof-the-art natural language processing, 2019"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2019
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 14
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 35,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/exBERT:-A-Visual-Analysis-Tool-to-Explore-Learned-Hoover-Strobelt/327d7e55d64cb34d55bd3a3fe58233c238a312cd?sort=total-citations"
}