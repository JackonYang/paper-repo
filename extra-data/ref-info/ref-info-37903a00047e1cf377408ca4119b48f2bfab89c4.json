{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721328"
                        ],
                        "name": "Sameer Kiran Antani",
                        "slug": "Sameer-Kiran-Antani",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Antani",
                            "middleNames": [
                                "Kiran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Kiran Antani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821130"
                        ],
                        "name": "David J. Crandall",
                        "slug": "David-J.-Crandall",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Crandall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Crandall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 2
                            }
                        ],
                        "text": ", [2,11]) we used the term segmentation to refer to the binarization problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5272396,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aefca18101764f904edbca6ace7991045f1392e3",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite advances in the archiving of digital video, we are still unable to efficiently search and retrieve the portions that interest us. Video indexing by shot segmentation has been a proposed solution and several research efforts are seen in the literature. Shot segmentation alone cannot solve the problem of content based access to video. Recognition of text in video has been proposed as an additional feature. Several research efforts are found in the literature for text extraction from complex images and video with applications for video indexing. We present an update of our system for detection and extraction of an unconstrained variety of text from general purpose video. The text detection results from a variety of methods are fused and each single text instance is segmented to enable it for OCR. Problems in segmenting text from video are similar to those faced in detection and localization phases. Video has low resolution and the text often has poor contrast with a changing background. The proposed system applies a variety of methods and takes advantage of the temporal redundancy in video resulting in good text segmentation."
            },
            "slug": "Robust-extraction-of-text-in-video-Antani-Crandall",
            "title": {
                "fragments": [],
                "text": "Robust extraction of text in video"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An update of the system for detection and extraction of an unconstrained variety of text from general purpose video and takes advantage of the temporal redundancy in video resulting in good text segmentation is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2907696"
                        ],
                        "name": "J. Shim",
                        "slug": "J.-Shim",
                        "structuredName": {
                            "firstName": "Jae",
                            "lastName": "Shim",
                            "middleNames": [
                                "Chang"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145253787"
                        ],
                        "name": "C. Dorai",
                        "slug": "C.-Dorai",
                        "structuredName": {
                            "firstName": "Chitra",
                            "lastName": "Dorai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dorai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70029967"
                        ],
                        "name": "R. Bolle",
                        "slug": "R.-Bolle",
                        "structuredName": {
                            "firstName": "Ruud",
                            "lastName": "Bolle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bolle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "[41,42] propose a method to detect and binarize caption text in video."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12062439,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1eb854ce0539b6fd18dbba942f52a80a735f5c8e",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient content-based retrieval of image and video databases is an important application due to rapid proliferation of digital video data on the Internet and corporate intranets. Text either embedded or superimposed within video frames is very useful for describing the contents of the frames, as it enables both keyword and free-text based search, automatic video logging, and video cataloging. We have developed a scheme for automatically extracting text from digital images and videos for content annotation and retrieval. We present our approach to robust text extraction from video frames, which can handle complex image backgrounds, deal with different font sizes, font styles, and font appearances such as normal and inverse video. Our algorithm results in segmented characters that can be directly processed by an OCR system to produce ASCII text. Results from our experiments with over 5000 frames obtained from twelve MPEG video streams demonstrate the good performance of our system in terms of text identification accuracy and computational efficiency."
            },
            "slug": "Automatic-text-extraction-from-video-for-annotation-Shim-Dorai",
            "title": {
                "fragments": [],
                "text": "Automatic text extraction from video for content-based annotation and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has developed a scheme for automatically extracting text from digital images and videos for content annotation and retrieval that results in segmented characters that can be directly processed by an OCR system to produce ASCII text."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110062608"
                        ],
                        "name": "Toshio Sato",
                        "slug": "Toshio-Sato",
                        "structuredName": {
                            "firstName": "Toshio",
                            "lastName": "Sato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshio Sato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2816639"
                        ],
                        "name": "Ellen K. Hughes",
                        "slug": "Ellen-K.-Hughes",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Hughes",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ellen K. Hughes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116645471"
                        ],
                        "name": "Michael A. Smith",
                        "slug": "Michael-A.-Smith",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael A. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[39] performs simultaneous localization and binarization of caption text in video frames."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43395565,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67c4ed0ef1c978defe1c44868029790aaad21752",
            "isKey": false,
            "numCitedBy": 275,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Video OCR is a technique that can greatly help to locate topics of interest in a large digital news video archive via the automatic extraction and reading of captions and annotations. News captions generally provide vital search information about the video being presented, the names of people and places or descriptions of objects. In this paper, two difficult problems of character recognition for videos are addressed: low resolution characters and extremely complex backgrounds. We apply an interpolation filter, multi-frame integration and a combination of four filters to solve these problems. Segmenting characters is done by a recognition-based segmentation method and intermediate character recognition results are used to improve the segmentation. The overall recognition results are good enough for use in news indexing. Performing video OCR on news video and combining its results with other video understanding techniques will improve the overall understanding of the news video content."
            },
            "slug": "Video-OCR-for-digital-news-archive-Sato-Kanade",
            "title": {
                "fragments": [],
                "text": "Video OCR for digital news archive"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper applies an interpolation filter, multi-frame integration and a combination of four filters to solve the problems of character recognition for videos: low resolution characters and extremely complex backgrounds."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1998 IEEE International Workshop on Content-Based Access of Image and Video Database"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2124754"
                        ],
                        "name": "U. Gargi",
                        "slug": "U.-Gargi",
                        "structuredName": {
                            "firstName": "Ullas",
                            "lastName": "Gargi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Gargi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721328"
                        ],
                        "name": "Sameer Kiran Antani",
                        "slug": "Sameer-Kiran-Antani",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Antani",
                            "middleNames": [
                                "Kiran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Kiran Antani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "a color stroke-based approach [10] (Algorithm 1), an intensity edge-based approach [19] (Algorithm 2), a color clustering approach [29] (Algorithm 3), and two texturebased approaches [40] (Algorithm 4) and [5,6] (Algorithm 5)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[10] describe an algorithm for locating horizontal text strings in video frames."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30086203,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "7e3ed958f25974dab09b445fbd6f297852b761fc",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Like shot changes, the presence of text in digital video is an important event that can be used to index digital video and provide extremely useful semantic information about the scene content. The special characteristics of digital video compared to document images both require and allow new robust approaches to recognition of text in video. We discuss the characteristics and special challenges of text in video and present a strategy of detecting, localizing, and segmenting text from video data for the text indexing problem. Preliminary results from our approach are presented."
            },
            "slug": "Indexing-text-events-in-digital-video-databases-Gargi-Antani",
            "title": {
                "fragments": [],
                "text": "Indexing text events in digital video databases"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The characteristics and special challenges of text in video are discussed and a strategy of detecting, localizing, and segmenting text from video data for the text indexing problem is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3272081"
                        ],
                        "name": "O. Kia",
                        "slug": "O.-Kia",
                        "structuredName": {
                            "firstName": "Omid",
                            "lastName": "Kia",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "A recent extension to this work [24] adds a post-processing step to correct tracking results when text grows or shrinks slightly from frame to frame."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15485643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8f5c282dc11937d29183b955dc3e4fbb677571b",
            "isKey": false,
            "numCitedBy": 652,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Text that appears in a scene or is graphically added to video can provide an important supplemental source of index information as well as clues for decoding the video's structure and for classification. In this work, we present algorithms for detecting and tracking text in digital video. Our system implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks. Our text tracking scheme consists of two modules: a sum of squared difference (SSD)-based module to find the initial position and a contour-based module to refine the position. Experiments conducted with a variety of video sources show that our scheme can detect and track text robustly."
            },
            "slug": "Automatic-text-detection-and-tracking-in-digital-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "Automatic text detection and tracking in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents algorithms for detecting and tracking text in digital video that implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48764203"
                        ],
                        "name": "Victor Wu",
                        "slug": "Victor-Wu",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31338632"
                        ],
                        "name": "E. Riseman",
                        "slug": "E.-Riseman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Riseman",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riseman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 20
                            }
                        ],
                        "text": "Several researchers [13,45,48] have attempted recognition from images and video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[48] describe a scheme for finding and binarizing text in images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 208945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3420ab835c1af02071364b1f4e0f69abf733d88c",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "There are many applications in which the automatic detection and recognition of text embedded in images is useful. These applications include digad libraries, multimedia systems, and Geographical Information Systems. When machine generated text is prdnted against clean backgrounds, it can be converted to a computer readable form (ASCII) using current Optical Character Recognition (OCR) technology. However, text is often printed against shaded or textured backgrounds or is embedded in images. Examples include maps, advertisements, photographs, videos and stock certificates. Current document segmentation and recognition technologies cannot handle these situafons well. In this paper, a four-step system which automaticnlly detects and extracts text in images i& proposed. First, a texture segmentation scheme is used to focus attention on regions where text may occur. Second, strokes are extracted from the segmented text regions. Using reasonable heuristics on text strings such as height similarity, spacing and alignment, the extracted strokes are then processed to form rectangular boxes surrounding the corresponding ttzt strings. To detect text over a wide range of font sizes, the above steps are first applied to a pyramid of images generated from the input image, and then the boxes formed at each resolution level of the pyramid are fused at the image in the original resolution level. Third, text is extracted by cleaning up the background and binarizing the detected ted strings. Finally, better text bounding boxes are generated by srsiny the binarized text as strokes. Text is then cleaned and binarized from these new boxes, and can then be passed through a commercial OCR engine for recognition if the text is of an OCR-recognizable font. The system is stable, robust, and works well on imayes (with or without structured layouts) from a wide van\u2019ety of sources, including digitized video frames, photographs, *This material is based on work supported in part by the National Science Foundation, Library of Congress and Department of Commerce under cooperative agreement number EEC9209623, in part by the United States Patent and mademark Office and Defense Advanced Research Projects Agency/IT0 under ARPA order number D468, issued by ESC/AXS contract number F19628-96-C-0235, in part by the National Science Foundation under grant number IF&9619117 and in part by NSF Multimedia CDA-9502639. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsors. Prrmission to make digital/hard copies ofall or part oflhis material for personal or clrrssroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title ofthe publication and its date appear, and notice is given that copyright is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission and/or fe DL 97 Philadelphia PA, USA Copyright 1997 AChi 0-89791~868-1197/7..$3.50 newspapers, advertisements, stock certifimtes, and personal checks. All parameters remain the same for-all the experiments."
            },
            "slug": "Finding-text-in-images-Wu-Manmatha",
            "title": {
                "fragments": [],
                "text": "Finding text in images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A four-step system which automaticnlly detects and extracts text in images is proposed and works well on imayes (with or without structured layouts) from a wide range of sources, including digitized video frames, photographs, and personal checks."
            },
            "venue": {
                "fragments": [],
                "text": "DL '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 9
                            }
                        ],
                        "text": "Lienhart [27,28,26] locates and recognizes text in video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 828474,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4da3825130a082e8a2707925ef926a72b860dc24",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient indexing and retrieval of digital video is an importantaspect of video databases. One powerful index for retrieval is the text appearing in them. It enables content- based browsing. We present our methods for automatic segmentation and recognition of text in digital videos. The algorithms we propose make use of typical characteristics of text in videos in order to enable and enhance segmentation and recognition performance. Especially the inter-frame dependencies of the characters provide new possibilities for their refinement. Then, a straightforward indexing and retrieval scheme is introduced. It is used in the experiments to demonstrate that the proposed text segmentation and text recognition algorithms are suitable for indexing and retrieval of relevant video scenes in and from a video data base. Our experimental results are very encouraging and suggest that these algorithms can be used in video retrieval applications as well as to recognize higher semantics in video."
            },
            "slug": "Indexing-and-retrieval-of-digital-video-sequences-Lienhart",
            "title": {
                "fragments": [],
                "text": "Indexing and retrieval of digital video sequences based on automatic text recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed text segmentation and text recognition algorithms are suitable for indexing and retrieval of relevant video scenes in and from a video data base and suggest that they can be used in video retrieval applications as well as to recognize higher semantics in video."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '96"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 9
                            }
                        ],
                        "text": "Lienhart [27,28,26] locates and recognizes text in video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8416045,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0dd1def5778f24c2c5a5f1c114846326e8f86123",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient indexing and retrieval of digital video is an important aspect of video databases. One powerful index for retrieval is the text appearing in them. It enables content- based browsing. We present our methods for automatic segmentation and recognition of text in digital videos. The algorithms we propose make use of typical characteristics of text in videos in order to enable and enhance segmentation and recognition performance. Especially the inter-frame dependencies of the characters provide new possibilities for their refinement. Then, a straightforward indexing and retrieval scheme is introduced. It is used in the experiments to demonstrate that the proposed text segmentation and text recognition algorithms are suitable for indexing and retrieval of relevant video scenes in and from a video data base. Our experimental results are very encouraging and suggest that these algorithms can be used in video retrieval applications as well as to recognize higher semantics in video."
            },
            "slug": "Automatic-text-recognition-for-video-indexing-Lienhart",
            "title": {
                "fragments": [],
                "text": "Automatic text recognition for video indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed text segmentation and text recognition algorithms are suitable for indexing and retrieval of relevant video scenes in and from a video data base and suggest that they can be used in video retrieval applications as well as to recognize higher semantics in video."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '96"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2898481"
                        ],
                        "name": "Wei-Song Qi",
                        "slug": "Wei-Song-Qi",
                        "structuredName": {
                            "firstName": "Wei-Song",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Song Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3062639"
                        ],
                        "name": "L. Gu",
                        "slug": "L.-Gu",
                        "structuredName": {
                            "firstName": "Lie",
                            "lastName": "Gu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152631223"
                        ],
                        "name": "Hao Jiang",
                        "slug": "Hao-Jiang",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772671"
                        ],
                        "name": "Xiangrong Chen",
                        "slug": "Xiangrong-Chen",
                        "structuredName": {
                            "firstName": "Xiangrong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangrong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15472062,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9bab6c1c2270e91e94d393e82d86fa1e05d2b61",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a system developed for content-based broadcast news video browsing for home users. There are three main factors that distinguish our work from other similar ones. First, we have integrated the image and audio analysis results in identifying news segments. Second, we use the video OCR technology to detect text from frames, which provides a good source of textual information for story classification when transcripts and close captions are not available. Finally, natural language processing (NLP) technologies are used to perform automated categorization of news stories based on the texts obtained from close caption or video OCR process. Based on these video structure and content analysis technologies, we have developed two advanced video browsers for home users: intelligent highlight player and HTML-based video browser."
            },
            "slug": "Integrating-visual,-audio-and-text-analysis-for-Qi-Gu",
            "title": {
                "fragments": [],
                "text": "Integrating visual, audio and text analysis for news video"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Two advanced video browsers for home users are developed: intelligent highlight player and HTML-based video browser that perform automated categorization of news stories based on the texts obtained from close caption or video OCR process."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083891040"
                        ],
                        "name": "F. Stuber",
                        "slug": "F.-Stuber",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Stuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Stuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 9
                            }
                        ],
                        "text": "Lienhart [27,28,26] locates and recognizes text in video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14147742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "778a307aa0cf8b2ed273b9089cb9aa8210f49f24",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits. The algorithms we propose make use of typical characteristics of text in videos in order to enhance segmentation and, consequently, recognition performance. As a result, we get segmented characters from video pictures. These can be parsed by any OCR software. The recognition results of multiple instances of the same character throughout subsequent frames are combined to enhance recognition result and to compute the final output. We have tested our segmentation algorithms in a series of experiments with video clips recorded from television and achieved good segmentation results."
            },
            "slug": "Automatic-text-recognition-in-digital-videos-Lienhart-Stuber",
            "title": {
                "fragments": [],
                "text": "Automatic text recognition in digital videos"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits are developed."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "[22,23] address detection and tracking of moving text in video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 33710622,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4bec9e387021ad057dfd99cfc9b1afada4148933",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of automatically tracking moving text in digital videos. Our scheme consists of two separate processes: monitoring which detects the new text line entering a frame, and tracking which uses prediction techniques to reconcile the text from frame to frame. Temporal consistency allows one to monitor periodically and reduce the computation complexity. The tracking process uses a rapid prediction/search scheme to update the position of the text blocks between frames. We provide details of the implementation and results for text moving in the scene and text which moves as a result of camera motion."
            },
            "slug": "Automatic-text-tracking-in-digital-videos-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "Automatic text tracking in digital videos"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This work addresses the problem of automatically tracking moving text in digital videos by consisting of two separate processes: monitoring which detects the new text line entering a frame, and tracking which uses prediction techniques to reconcile the text from frame to frame."
            },
            "venue": {
                "fragments": [],
                "text": "1998 IEEE Second Workshop on Multimedia Signal Processing (Cat. No.98EX175)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5196787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f565f502ad1acb81c5659b051c04683a34ed138f",
            "isKey": false,
            "numCitedBy": 576,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic text location (without character recognition capabilities) deals with extracting image regions that contain text only. The images of these regions can then be fed to an optical character recognition module or highlighted for users. This is very useful in a number of applications such as database indexing and converting paper documents to their electronic versions. The performance of our automatic text location algorithm is shown in several applications. Compared with some traditional text location methods, our method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "slug": "Automatic-text-location-in-images-and-video-frames-Jain-Yu",
            "title": {
                "fragments": [],
                "text": "Automatic text location in images and video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Compared with some traditional text location methods, this method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2242803"
                        ],
                        "name": "L. Agnihotri",
                        "slug": "L.-Agnihotri",
                        "structuredName": {
                            "firstName": "Lalitha",
                            "lastName": "Agnihotri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Agnihotri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1667502629"
                        ],
                        "name": "N. Dimitrova",
                        "slug": "N.-Dimitrova",
                        "structuredName": {
                            "firstName": "Natasa",
                            "lastName": "Dimitrova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dimitrova"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 112
                            }
                        ],
                        "text": "In each case we summarize the approach and highlight any noteworthy contributions, assumptions, or limitations:\nAgnihotri and Dimitrova [1] detect and binarize horizontal white, yellow, and black caption text in video frames."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 136
                            }
                        ],
                        "text": "In each case we summarize the approach and highlight any noteworthy contributions, assumptions, or limitations: Agnihotri and Dimitrova [1] detect and binarize horizontal white, yellow, and black caption text in video frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60735577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3b2c6b7bfaf9ab0d44c7103585fa0c81f60f3b9",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Textual information brings important semantic clues in video content analysis. We describe a method for detection and representation of text in video segments. The method consists of seven steps: channel separation, image enhancement, edge detection, edge filtering, character detection, text box detection, and text line detection. Our results show that this method can be applied to English as well as non-English text (such as Korean) with precision and recall of 85%."
            },
            "slug": "Text-detection-for-video-analysis-Agnihotri-Dimitrova",
            "title": {
                "fragments": [],
                "text": "Text detection for video analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This work describes a method for detection and representation of text in video segments that can be applied to English as well as non-English text (such as Korean) with precision and recall of 85%."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Workshop on Content-Based Access of Image and Video Libraries (CBAIVL'99)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143772136"
                        ],
                        "name": "O. Hori",
                        "slug": "O.-Hori",
                        "structuredName": {
                            "firstName": "Osamu",
                            "lastName": "Hori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Hori"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 20
                            }
                        ],
                        "text": "Several researchers [13,45,48] have attempted recognition from images and video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26032059,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "117dea69032c9bf51ee10ff3bbad9b7b4d9e638b",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents a method to precisely extract only video character portions from a video text rectangle region in order to make a readable image for OCR. In conventional methods, gray image binarization processing with a given threshold is employed to extract high-intensity video character regions. A video has a complex background with various kinds of intensity so that appropriate thresholds are not always obtained. The proposed method extracts reliable high-intensity regions in a video text region and then expands them in order to make the whole video character regions. The experiments show this new method to be superior to the conventional methods."
            },
            "slug": "A-video-text-extraction-method-for-character-Hori",
            "title": {
                "fragments": [],
                "text": "A video text extraction method for character recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "The proposed method extracts reliable high-intensity regions in a video text region and then expands them in order to make the whole video character regions to be superior to the conventional methods."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2124754"
                        ],
                        "name": "U. Gargi",
                        "slug": "U.-Gargi",
                        "structuredName": {
                            "firstName": "Ullas",
                            "lastName": "Gargi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Gargi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821130"
                        ],
                        "name": "David J. Crandall",
                        "slug": "David-J.-Crandall",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Crandall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Crandall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721328"
                        ],
                        "name": "Sameer Kiran Antani",
                        "slug": "Sameer-Kiran-Antani",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Antani",
                            "middleNames": [
                                "Kiran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Kiran Antani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2372810"
                        ],
                        "name": "T. Gandhi",
                        "slug": "T.-Gandhi",
                        "structuredName": {
                            "firstName": "Tarak",
                            "lastName": "Gandhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gandhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115338247"
                        ],
                        "name": "Ryan Keener",
                        "slug": "Ryan-Keener",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Keener",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Keener"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57425516,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d784187382befa5cb50b62f10ddb87feb487102",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Video indexing is an important problem that has occupied recent research efforts. The text appearing in video can provide semantic information about the scene content. Detecting and recognizing text events can provide indices into the video for content based querying. We describe a system for detecting, tracking, and extracting artificial and scene text in MPEG-1 video. Preliminary results are presented."
            },
            "slug": "A-system-for-automatic-text-detection-in-video-Gargi-Crandall",
            "title": {
                "fragments": [],
                "text": "A system for automatic text detection in video"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A system for detecting, tracking, and extracting artificial and scene text in MPEG-1 video to provide indices into the video for content based querying is described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3314902"
                        ],
                        "name": "Edward K. Wong",
                        "slug": "Edward-K.-Wong",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Wong",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward K. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50133585"
                        ],
                        "name": "Minya Chen",
                        "slug": "Minya-Chen",
                        "structuredName": {
                            "firstName": "Minya",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minya Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "Wong [47] locate and binarize text in video frames."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44601365,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b84b397a325afb12f768080705f5bb7b719397f0",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We report on the development and implementation of a new algorithm for extracting text in digitized color video. The algorithm operates by locating potential text line segments from horizontal scan lines. Detected text line segments are expanded or combined with text line segments from adjacent scan lines to form larger text blocks, which are then subject to filtering and refinement. Text pixels within text blocks are then found using bi-color clustering and connected-component analysis. Morphological contour smoothing and morphological resolution enhancement algorithms are then applied to the detected binary texts to enhance their visual quality. The implemented algorithm has fast execution time and is effective in detecting text in difficult cases, such as scenes with highly textured background, and scenes with small text. A variety of color images digitized from broadcast television are used to test the algorithm and excellent performance result was obtained."
            },
            "slug": "A-robust-algorithm-for-text-extraction-in-color-Wong-Chen",
            "title": {
                "fragments": [],
                "text": "A robust algorithm for text extraction in color video"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The implemented algorithm has fast execution time and is effective in detecting text in difficult cases, such as scenes with highly textured background, and scenes with small text."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No.00TH8532)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055230387"
                        ],
                        "name": "K. Jeong",
                        "slug": "K.-Jeong",
                        "structuredName": {
                            "firstName": "Ki-Young",
                            "lastName": "Jeong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jeong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715155"
                        ],
                        "name": "Eun Yi Kim",
                        "slug": "Eun-Yi-Kim",
                        "structuredName": {
                            "firstName": "Eun",
                            "lastName": "Kim",
                            "middleNames": [
                                "Yi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eun Yi Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70261685"
                        ],
                        "name": "Hang-Joon Kim",
                        "slug": "Hang-Joon-Kim",
                        "structuredName": {
                            "firstName": "Hang-Joon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang-Joon Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[17] apply neural networks to find text captions in Korean news broadcasts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9424151,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de034f320270091bcf7e80400ad5bf5340b3a46a",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The retrieval of video clips from multimedia databases has been increasingly spotlighted. Texts in videos include useful information for automatic annotation or indexing. Text location is the first step for recognizing the textual information. This paper proposes a neural network-based text location method for news video indexing. Text can be characterized by texture, location, alignment, and font size. The proposed method classifies text pixels and non-text pixels using a network that operates as a set of texture discrimination filters. We find and locate text regions using histogram analysis after removing errors in the classification results. Experimental results show that the proposed method is effective at locating texts."
            },
            "slug": "Neural-network-based-text-location-for-news-video-Jeong-Jung",
            "title": {
                "fragments": [],
                "text": "Neural network-based text location for news video indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A neural network-based text location method for news video indexing using a network that operates as a set of texture discrimination filters and results show that the proposed method is effective at locating texts."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2673919"
                        ],
                        "name": "V. Mariano",
                        "slug": "V.-Mariano",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Mariano",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Mariano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 195348929,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "1d4a5e70874d828bbd6c16300511fa2744f0b2fd",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper a method is proposed for locating horizontal, uniform-colored text in video frames. It was observed that when a row of pixels across such a text region is clustered in perceptually uniform L*a*b* color space, the pixels of one of these clusters would belong to the text strokes. These pixels would appeal as a line of short streaks on the row since a typical text region has many vertical and diagonal strokes. The proposed method examines every third row of the the image and checks whether this row passes through a horizontal text region. For a given row R, the pixels of R are hierarchically clustered in L*a*b* space and each cluster is tested whether similar-colored pixels in R's vicinity are possibly part of a text region. Candidate text blocks are marked by heuristics using information about the cluster's line of shell streaks. The detected text blocks are fused with the text regions. The method was tested on key frames of several video sequences and was able to locate a wide variety of text."
            },
            "slug": "Locating-uniform-colored-text-in-video-frames-Mariano-Kasturi",
            "title": {
                "fragments": [],
                "text": "Locating uniform-colored text in video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed method examines every third row of the the image and checks whether this row passes through a horizontal text region, and detects text blocks marked by heuristics using information about the cluster's line of shell streaks."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107824420"
                        ],
                        "name": "C. Garcia",
                        "slug": "C.-Garcia",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Garcia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Garcia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3346037"
                        ],
                        "name": "X. Apostolidis",
                        "slug": "X.-Apostolidis",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Apostolidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Apostolidis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 80
                            }
                        ],
                        "text": "The more periodic of the two polarities is chosen using the method presented in [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "Garcia and Apostolidis [9] locate and binarize horizontal text in color images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46620452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57930a675de539c59bc33f56d9894c999d264f72",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Text is a very powerful index in content-based image and video indexing. We propose a new text detection and segmentation algorithm that is especially designed for being applied to color images with complicated background. Our goal is to minimize the number of false alarms and to binarize efficiently the detected text areas so that they can be processed by standard OCR software. First, potential areas of text are detected by enhancement and clustering processes, considering most of constraints related to the texture of words. Then, classification and binarization of potential text areas are achieved in a single scheme performing color quantization and characters periodicity analysis. We report a high rate of good detection results with very few false alarms and reliable text binarization."
            },
            "slug": "Text-detection-and-segmentation-in-complex-color-Garcia-Apostolidis",
            "title": {
                "fragments": [],
                "text": "Text detection and segmentation in complex color images"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A new text detection and segmentation algorithm that is especially designed for being applied to color images with complicated background and to binarize efficiently the detected text areas so that they can be processed by standard OCR software."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.00CH37100)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108487560"
                        ],
                        "name": "J. C. Lee",
                        "slug": "J.-C.-Lee",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lee",
                            "middleNames": [
                                "Chung-Mong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. C. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2257741"
                        ],
                        "name": "A. Kankanhalli",
                        "slug": "A.-Kankanhalli",
                        "structuredName": {
                            "firstName": "Atreyi",
                            "lastName": "Kankanhalli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kankanhalli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "[21,20] use a combined detection and binarization approach."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11365167,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "07a0d7a8938fa04cad3a1afc28c3dcaf1724bde8",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed a generalized alphanumeric character extraction algorithm that can efficiently and accurately locate and extract characters from complex scene images. A scene image may be complex due to the following reasons: (1) the characters are embedded in an image with other objects, such as structural bars, company logos and smears; (2) the characters may be painted or printed in any color including white, and the background color may differ only slightly from that of the characters; (3) the font, size and format of the characters may be different; and (4) the lighting may be uneven. The main contribution of this research is that it permits the quick and accurate extraction of characters in a complex scene. A coarse search technique is used to locate potential characters, and then a fine grouping technique is used to extract characters accurately. Several additional techniques in the postprocessing phase eliminate spurious as well as overlapping characters. Experimental results of segmenting characters written on cargo container surfaces show that the system is feasible under real-life constraints. The program has been installed as part of a vision system which verifies container codes on vehicles passing through the Port of Singapore."
            },
            "slug": "Automatic-Extraction-of-Characters-in-Complex-Scene-Lee-Kankanhalli",
            "title": {
                "fragments": [],
                "text": "Automatic Extraction of Characters in Complex Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A generalized alphanumeric character extraction algorithm that can efficiently and accurately locate and extract characters from complex scene images is developed that permits the quick and accurate extraction of characters in a complex scene."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472153"
                        ],
                        "name": "S. Messelodi",
                        "slug": "S.-Messelodi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Messelodi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Messelodi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2389886"
                        ],
                        "name": "C. M. Modena",
                        "slug": "C.-M.-Modena",
                        "structuredName": {
                            "firstName": "Carla",
                            "lastName": "Modena",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. M. Modena"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13597946,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55f6886a7a70c800aa3c899d16e784a611dc9ba2",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-identification-and-skew-estimation-of-in-Messelodi-Modena",
            "title": {
                "fragments": [],
                "text": "Automatic identification and skew estimation of text lines in real scene images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34602043"
                        ],
                        "name": "L. Winger",
                        "slug": "L.-Winger",
                        "structuredName": {
                            "firstName": "Lowell",
                            "lastName": "Winger",
                            "middleNames": [
                                "LeRoy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Winger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143794424"
                        ],
                        "name": "M. Jernigan",
                        "slug": "M.-Jernigan",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Jernigan",
                            "middleNames": [
                                "Ed"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jernigan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115466358"
                        ],
                        "name": "J. Robinson",
                        "slug": "J.-Robinson",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Robinson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Robinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[46] performs binarization of lowcontrast scene text using a modification of Niblack\u2019s Multiple and Variable Thresholding scheme [34]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5218832,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8283a9fefa9e3690cc2d64308c6bb61f55623371",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We are developing a portable text-to-speech system for the vision impaired. The input image is acquired with a lightweight CCD camera that may be poorly focused and aimed, and perhaps taken under inadequate and uneven illumination. We therefore require efficient and effective thresholding and segmentation methods which are robust with respect to character contrast, font, size, and format. In this paper, we present a fast thresholding scheme which combines a local variance measure with a logical stroke-width method. An efficient post- thresholding segmentation scheme utilizing Fisher's linear discriminant to distinguish noise and character components functions as an effective pre-processing step for the application of commercial segmentation and character recognition methods. The performance of this fast new method compared favorably with other methods for the extraction of characters from uncontrolled illumination, omnifont scene images. We demonstrate the suitability of this method for use in an automated portable reader through a software implementation running on a laptop 486 computer in our prototype device."
            },
            "slug": "Character-segmentation-and-thresholding-in-scene-Winger-Jernigan",
            "title": {
                "fragments": [],
                "text": "Character segmentation and thresholding in low-contrast scene images"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper presents a fast thresholding scheme which combines a local variance measure with a logical stroke-width method and demonstrates the suitability of this method for use in an automated portable reader through a software implementation running on a laptop 486 computer in the prototype device."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693680"
                        ],
                        "name": "N. Chaddha",
                        "slug": "N.-Chaddha",
                        "structuredName": {
                            "firstName": "Navin",
                            "lastName": "Chaddha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Chaddha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153216710"
                        ],
                        "name": "R. Sharma",
                        "slug": "R.-Sharma",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075308385"
                        ],
                        "name": "A. Agrawal",
                        "slug": "A.-Agrawal",
                        "structuredName": {
                            "firstName": "Avneesh",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041740460"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 206
                            }
                        ],
                        "text": "a color stroke-based approach [10] (Algorithm 1), an intensity edge-based approach [19] (Algorithm 2), a color clustering approach [29] (Algorithm 3), and two texturebased approaches [40] (Algorithm 4) and [5,6] (Algorithm 5)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 2
                            }
                        ],
                        "text": ", [5]) use this term to refer to the text region segmentation problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 8
                            }
                        ],
                        "text": "Chaddha [5] analyzes JPEG images in the frequency domain to detect blocks with text-like texture."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "The DCT domain was used for text detection before in [5], but the application was constrained document images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58358347,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9754dba62f5a329c9dd64b7d091c7900f489f5f0",
            "isKey": true,
            "numCitedBy": 50,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Block based algorithms have found widespread use in image and video compression. However, popular algorithms such as JPEG, which are very effective in compressing continuous tone images, do not perform well with mixed-mode images which have a substantial text component. With a growing number of applications where such images occur, e.g., color facsimile, digital libraries and educational videos, there are advantages in being able to classify each block as being text or continuous tone. With such a classification, different compression parameters or even algorithms may be employed for the two kinds of data to obtain high compression with minimal loss in visual quality. In this paper we analyze and compare four methods for block classification in mixed mode images, namely variance, absolute-deviation, edge, and DCT based methods. Our evaluation of each scheme is based on the accuracy of segmentation, robustness across different types of images and sensitivity to the threshold used for segmentation. Our results show that DCT based segmentation offers the best accuracy and robustness. Another advantage of DCT is that it is compatible with standards like JPEG, MPEG and H.261.<<ETX>>"
            },
            "slug": "Text-segmentation-in-mixed-mode-images-Chaddha-Sharma",
            "title": {
                "fragments": [],
                "text": "Text segmentation in mixed-mode images"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This paper analyzes and compares four methods for block classification in mixed mode images, namely variance, absolute-deviation, edge, and DCT based methods, and shows that DCTbased segmentation offers the best accuracy and robustness."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 28th Asilomar Conference on Signals, Systems and Computers"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144851973"
                        ],
                        "name": "M. Kamel",
                        "slug": "M.-Kamel",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Kamel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kamel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1500387699"
                        ],
                        "name": "A. Zhao",
                        "slug": "A.-Zhao",
                        "structuredName": {
                            "firstName": "Aiguo",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zhao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 33047782,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b786be0f6e167d58a9da67eca864113efbf6b01",
            "isKey": false,
            "numCitedBy": 221,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The extraction of binary character/graphics images from gray-scale document images with background pictures, shadows, highlight, smear, and smudge is a common critical image processing operation, particularly for document image analysis, optical character recognition, check image processing, image transmission, and videoconferencing. After a brief review of previous work with emphasis on five published extraction techniques, viz., a global thresholding technique, YDH technique, a nonlinear adaptive technique, an integrated function technique, and a local contrast technique, this paper presents two new extraction techniques: a logical level technique and a mask-based subtraction technique. With experiments on images of a typical check and a poor-quality text document, this paper systematically evaluates and analyses both new and published techniques with respect to six aspects, viz., speed, memory requirement, stroke width restriction, parameter number, parameter setting, and human subjective evaluation of result images. Experiments and evaluations have shown that one new technique is superior to the rest, suggesting its suitability for high-speed low-cost applications."
            },
            "slug": "Extraction-of-Binary-Character/Graphics-Images-from-Kamel-Zhao",
            "title": {
                "fragments": [],
                "text": "Extraction of Binary Character/Graphics Images from Grayscale Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents two new extraction techniques: a logical level technique and a mask-based subtraction technique, suggesting its suitability for high-speed low-cost applications."
            },
            "venue": {
                "fragments": [],
                "text": "CVGIP Graph. Model. Image Process."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1925316"
                        ],
                        "name": "M. Pilu",
                        "slug": "M.-Pilu",
                        "structuredName": {
                            "firstName": "Maurizio",
                            "lastName": "Pilu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pilu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "This idea was inspired by papers by Nakajima et al. [33], who used motion vectors to detect moving objects in MPEG video, and by Pilu [36], who used them to detect camera motion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "[33], who used motion vectors to detect moving objects in MPEG video, and by Pilu [36], who used them to detect camera motion."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1543016,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "69a84b03133a766cf0f60f6bab2d76b2fb0c93d0",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a simple and effective method to determine global camera motion using raw MPEG-1 motion vectors information obtained straight form real MPEG-1 streams such as those of the new HITACHI MP-EG1A digital camcorder. The simple approach we have experimented with robustly fits a global affine optic flow model to the motion vectors. Other more robust methods are also proposed. In order to cope with the group-of-frames (GOF) discontinuity of the MPEG stream, B frames are used backward to determine the 'missing link' to a previous GOF thereby ensuring continuity of the motion estimation across a reasonable number of frames. As a tested, we have applied the method to the image mosaicing problem, for which interesting results have been obtained. Although several other methods exists to perform camera motion estimation, the approach presented here is particularly interesting because exploits 'free' information present in MPEG streams and bypass the highly expensive correlation process."
            },
            "slug": "Using-raw-MPEG-motion-vectors-to-determine-global-Pilu",
            "title": {
                "fragments": [],
                "text": "Using raw MPEG motion vectors to determine global camera motion"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a simple and effective method to determine global camera motion using raw MPEG-1 motion vectors information obtained straight form real MPEG- 1 streams such as those of the new HITACHI MP-EG1A digital camcorder."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2848535"
                        ],
                        "name": "Joan L. Mitchell",
                        "slug": "Joan-L.-Mitchell",
                        "structuredName": {
                            "firstName": "Joan",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joan L. Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2926874"
                        ],
                        "name": "W. B. Pennebaker",
                        "slug": "W.-B.-Pennebaker",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Pennebaker",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. B. Pennebaker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46768116"
                        ],
                        "name": "C. E. Fogg",
                        "slug": "C.-E.-Fogg",
                        "structuredName": {
                            "firstName": "Chad",
                            "lastName": "Fogg",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. E. Fogg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50338638"
                        ],
                        "name": "D. LeGall",
                        "slug": "D.-LeGall",
                        "structuredName": {
                            "firstName": "Didier",
                            "lastName": "LeGall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. LeGall"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "MPEG encoders generally use a search window of 32 pixels in each direction during motion compensation searches [31]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "For details on the MPEG video standard, the reader is referred to [31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26370542,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce8f2abf14c6e3b67adb9e7062cad24fda2e6cab",
            "isKey": false,
            "numCitedBy": 188,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nFor all those interested in high definition television, multimedia, and image compression, this unique reference will be an essential tool. It provides the first comprehensive introduction to this field, incorporating material ranging from basic concerns of newcomers to the field through sophisticated reviews of cutting edge technical issues. Written by acknowledged experts in the field, MPEG Video Compression Standard offers important benefits to readers including detailed information on MPEG modes of operation, signaling conventions, and structure of MPEG compressed data. Each section of the book is labeled by level of technical difficulty, allowing less technical readers to skip higher level sections and still gain a broad understanding of the subject while guiding advanced readers to the in-depth coverage they require. With its comprehensive coverage of MPEG video compression, this important book meets the needs of those working to develop the standard as well as those who use MPEG in their work. Electrical engineers, multimedia producers, computer scientists, as well as all those interested in this fast growing field will find MPEG Video Compression Standard essential in their work."
            },
            "slug": "MPEG-Video-Compression-Standard-Mitchell-Pennebaker",
            "title": {
                "fragments": [],
                "text": "MPEG Video Compression Standard"
            },
            "venue": {
                "fragments": [],
                "text": "Springer US"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49024862"
                        ],
                        "name": "Jincheng Huang",
                        "slug": "Jincheng-Huang",
                        "structuredName": {
                            "firstName": "Jincheng",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jincheng Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109171705"
                        ],
                        "name": "Zhu Liu",
                        "slug": "Zhu-Liu",
                        "structuredName": {
                            "firstName": "Zhu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhu Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37015240"
                        ],
                        "name": "Yao Wang",
                        "slug": "Yao-Wang",
                        "structuredName": {
                            "firstName": "Yao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yao Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144838569"
                        ],
                        "name": "Yu Chen",
                        "slug": "Yu-Chen",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3314902"
                        ],
                        "name": "Edward K. Wong",
                        "slug": "Edward-K.-Wong",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Wong",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward K. Wong"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14690949,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e3ad9c88813445f8b7f0904601d7d91dac990ae",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Along with the advances in multimedia and Internet technology, a huge amount of data, including digital video and audio, are generated daily. Tools for the efficient indexing and retrieval of such data are indispensable. With multi-modal information present in the data, effective integration is necessary and is still a challenging problem. In this paper, we present four different methods for integrating audio and visual information for video classification based on a hidden Markov model (HMM): direct concatenation, product HMM, two-stage HMM, and integration by neural network. Our results have shown significant improvements over using a single modality."
            },
            "slug": "Integration-of-multimodal-features-for-video-scene-Huang-Liu",
            "title": {
                "fragments": [],
                "text": "Integration of multimodal features for video scene classification based on HMM"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Four different methods for integrating audio and visual information for video classification based on a hidden Markov model (HMM) are presented: direct concatenation, product HMM, two-stage H MM, and integration by neural network."
            },
            "venue": {
                "fragments": [],
                "text": "1999 IEEE Third Workshop on Multimedia Signal Processing (Cat. No.99TH8451)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767238"
                        ],
                        "name": "Y. Nakajima",
                        "slug": "Y.-Nakajima",
                        "structuredName": {
                            "firstName": "Yasuyuki",
                            "lastName": "Nakajima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Nakajima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144415144"
                        ],
                        "name": "A. Yoneyama",
                        "slug": "A.-Yoneyama",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Yoneyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yoneyama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796620"
                        ],
                        "name": "H. Yanagihara",
                        "slug": "H.-Yanagihara",
                        "structuredName": {
                            "firstName": "Hiromasa",
                            "lastName": "Yanagihara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yanagihara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769639"
                        ],
                        "name": "M. Sugano",
                        "slug": "M.-Sugano",
                        "structuredName": {
                            "firstName": "Masaru",
                            "lastName": "Sugano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sugano"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[33], who used motion vectors to detect moving objects in MPEG video, and by Pilu [36], who used them to detect camera motion."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58750105,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcbac7ea4875d3e56886d4f1a4423bd3af30b2ef",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method of moving object detection directly from MPEG coded data. Since motion information in MPEG coded data is determined in terms of coding efficiency point of view, it does not always provide real motion information of objects. We use a wide variety of coding information including motion vectors and DCT coefficients to estimate real object motion. Since such information can be directly obtained from coded bitstream, very fast operation can be expected. Moving objects are detected basically analyzing motion vectors and spatio-temporal correlation of motion in P-, and B-pictures. Moving objects are also detected in intra macroblocks by analyzing coding characteristics of intra macroblocks in P- and B-pictures and by investigating temporal motion continuity in I-pictures. The simulation results show that successful moving object detection has been performed on macroblock level using several test sequences. Since proposed method is very simple and requires much less computational power than the conventional object detection methods, it has a significant advantage as motion analysis tool."
            },
            "slug": "Moving-object-detection-from-MPEG-coded-data-Nakajima-Yoneyama",
            "title": {
                "fragments": [],
                "text": "Moving-object detection from MPEG coded data"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Simulation results show that successful moving object detection has been performed on macroblock level using several test sequences and proposed method has a significant advantage as motion analysis tool."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780258"
                        ],
                        "name": "Jisheng Liang",
                        "slug": "Jisheng-Liang",
                        "structuredName": {
                            "firstName": "Jisheng",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jisheng Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1365695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e44015ab8d9ba89ac004421e3c56bcbeecbd0a47",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A performance evaluation protocol for the layout analysis is discussed in this paper. In the University of Washington English Document Image Database-III, there are 1600 English document images that come with manually edited ground truth of entity bounding boxes. These bounding boxes enclose text and non-text zones, text-lines, and words. We describe a performance metric for the comparison of the detected entities and the ground truth in terms of their bounding boxes. The Document Attribute Format Specification is used as the standard data representation. The protocol is intended to serve as a model for using the UW-III database to evaluate the document analysis algorithms. A set of layout analysis algorithms which detect different entities have been tested based on the data set and the performance metric. The evaluation results are presented in this paper."
            },
            "slug": "Performance-evaluation-of-document-layout-analysis-Liang-Phillips",
            "title": {
                "fragments": [],
                "text": "Performance evaluation of document layout analysis algorithms on the UW data set"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A performance metric is described for the comparison of the detected entities and the ground truth in terms of their bounding boxes and the evaluation results are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2476750"
                        ],
                        "name": "B. C. Tom",
                        "slug": "B.-C.-Tom",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Tom",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. C. Tom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144842935"
                        ],
                        "name": "A. Katsaggelos",
                        "slug": "A.-Katsaggelos",
                        "structuredName": {
                            "firstName": "Aggelos",
                            "lastName": "Katsaggelos",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Katsaggelos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "For example, there has been some work in resolution enhancement using multi-frame integration of moving objects [44]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16333626,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "10730607187d4d5020aa39a65976b398db893fef",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an iterative algorithm for enhancing the resolution of monochrome and color image sequences. Various approaches toward motion estimation are investigated and compared. Improving the spatial resolution of an image sequence critically depends upon the accuracy of the motion estimator. The problem is complicated by the fact that the motion field is prone to significant errors since the original high-resolution images are not available. Improved motion estimates may be obtained by using a more robust and accurate motion estimator, such as a pel-recursive scheme instead of block matching, in processing color image sequences, there is the added advantage of having more flexibility in how the final motion estimates are obtained, and further improvement in the accuracy of the motion field is therefore possible. This is because there are three different intensity fields (channels) conveying the same motion information. In this paper, the choice of which motion estimator to use versus how the final estimates are obtained is weighed to see which issue is more critical in improving the estimated high-resolution sequences. Toward this end, an iterative algorithm is proposed, and two sets of experiments are presented. First, several different experiments using the same motion estimator but three different data fusion approaches to merge the individual motion fields were performed. Second, estimated high-resolution images using the block matching estimator were compared to those obtained by employing a pel-recursive scheme. Experiments were performed on a real color image sequence, and performance was measured by the peak signal to noise ratio (PSNR)."
            },
            "slug": "Resolution-enhancement-of-monochrome-and-color-Tom-Katsaggelos",
            "title": {
                "fragments": [],
                "text": "Resolution enhancement of monochrome and color video using motion compensation"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "An iterative algorithm for enhancing the resolution of monochrome and color image sequences and several different experiments using the same motion estimator but three different data fusion approaches to merge the individual motion fields were performed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708785"
                        ],
                        "name": "J. Ohya",
                        "slug": "J.-Ohya",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Ohya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ohya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2019875"
                        ],
                        "name": "A. Shio",
                        "slug": "A.-Shio",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Shio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49052113"
                        ],
                        "name": "S. Akamatsu",
                        "slug": "S.-Akamatsu",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Akamatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Akamatsu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[35] use a combined detection/binarization stage and OCR to extract characters from scene images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1565945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e94d1ff801fce49eea8d8aa51a477b130ca755de",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "An effective algorithm for character recognition in scene images is studied. Scene images are segmented into regions by an image segmentation method based on adaptive thresholding. Character candidate regions are detected by observing gray-level differences between adjacent regions. To ensure extraction of multisegment characters as well as single-segment characters, character pattern candidates are obtained by associating the detected regions according to their positions and gray levels. A character recognition process selects patterns with high similarities by calculating the similarities between character pattern candidates and the standard patterns in a dictionary and then comparing the similarities to the thresholds. A relaxational approach to determine character patterns updates the similarities by evaluating the interactions between categories of patterns, and finally character patterns and their recognition results are obtained. Highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting. >"
            },
            "slug": "Recognizing-Characters-in-Scene-Images-Ohya-Shio",
            "title": {
                "fragments": [],
                "text": "Recognizing Characters in Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "An effective algorithm for character recognition in scene images is studied and highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145573734"
                        ],
                        "name": "F. Lebourgeois",
                        "slug": "F.-Lebourgeois",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Lebourgeois",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Lebourgeois"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29939979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a8754ab68589b8e893d6962eb92c56300ecb764",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents a general robust OCR system designed for practical use and suited to unconstrained gray-level images grabbed from a CCD camera. The system works with minimum assumptions on font, text location, size, color and the background scene. The text blocks localization in complex scenes using a specific filter which enhances any text from the background without binarization. A special stage is designed to separate characters, even touched by using gray-level information. The authors also extract gray-level features which make the algorithm more reliable, in particular under poor printing conditions or bad contrast digitization."
            },
            "slug": "Robust-multifont-OCR-system-from-gray-level-images-Lebourgeois",
            "title": {
                "fragments": [],
                "text": "Robust multifont OCR system from gray level images"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The paper presents a general robust OCR system designed for practical use and suited to unconstrained gray-level images grabbed from a CCD camera, with minimum assumptions on font, text location, size, color and the background scene."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2229252"
                        ],
                        "name": "R. Dugad",
                        "slug": "R.-Dugad",
                        "structuredName": {
                            "firstName": "Rakesh",
                            "lastName": "Dugad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dugad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237406"
                        ],
                        "name": "N. Ahuja",
                        "slug": "N.-Ahuja",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Ahuja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ahuja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "Note that subsampling can be performed efficiently in the DCT domain (see [8])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20678335,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "085ef3520d192860780833803b02198b52119d2a",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a video frame or image in terms of its 8/spl times/8 block-DCT coefficients we wish to obtain a downsized (lower resolution) or upsized (higher resolution) version of this frame also in terms of 8/spl times/8 block -DCT coefficients. We propose an algorithm for achieving this directly in the compressed domain which is computationally much faster, produces visually sharper images and gives significant improvements in PSNR (typically 4 dB better compared to other compressed domain methods based on bilinear interpolation). The downsampling and upsampling schemes combined together preserve all the low-frequency DCT coefficients of the original signal. This implies tremendous savings for coding the difference between the original (unsampled image) and its prediction (the upsampled image). This is desirable for many applications based on scalable encoding of video."
            },
            "slug": "A-fast-scheme-for-altering-resolution-in-the-domain-Dugad-Ahuja",
            "title": {
                "fragments": [],
                "text": "A fast scheme for altering resolution in the compressed domain"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An algorithm for achieving this directly in the compressed domain which is computationally much faster, produces visually sharper images and gives significant improvements in PSNR (typically 4 dB better compared to other compressed domain methods based on bilinear interpolation)."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48189908"
                        ],
                        "name": "Yasuhiko Watanabe",
                        "slug": "Yasuhiko-Watanabe",
                        "structuredName": {
                            "firstName": "Yasuhiko",
                            "lastName": "Watanabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yasuhiko Watanabe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090132835"
                        ],
                        "name": "Yoshihiro Okada",
                        "slug": "Yoshihiro-Okada",
                        "structuredName": {
                            "firstName": "Yoshihiro",
                            "lastName": "Okada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshihiro Okada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3329332"
                        ],
                        "name": "Kengo Kaneji",
                        "slug": "Kengo-Kaneji",
                        "structuredName": {
                            "firstName": "Kengo",
                            "lastName": "Kaneji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kengo Kaneji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060093076"
                        ],
                        "name": "Yoshitaka Sakamoto",
                        "slug": "Yoshitaka-Sakamoto",
                        "structuredName": {
                            "firstName": "Yoshitaka",
                            "lastName": "Sakamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshitaka Sakamoto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 20
                            }
                        ],
                        "text": "Several researchers [13,45,48] have attempted recognition from images and video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 45791318,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "db1f20150ea535439745fd78204109bd5a990a46",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Using TV newscasts and newspaper together can enable more effective communication. TV newscasts typically report events clearly and intuitively with speech and image information, but without much detail. In contrast, newspapers usually report the same events in greater detail but primarily use text information. However, using TV newscasts and newspapers together without aligning the newspaper articles with the TV news reports is difficult. To solve this problem, we propose an alignment method that extracts text from TV captions and newspaper articles. We also propose a method for extracting a newspaper article and its follow-up articles. With these methods, we've developed a system for browsing and retrieving newspaper articles and TV news reports."
            },
            "slug": "TIVA-Applications:-Retrieving-Related-TV-News-and-Watanabe-Okada",
            "title": {
                "fragments": [],
                "text": "TIVA Applications: Retrieving Related TV News Reports and Newspaper Articles"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An alignment method that extracts text from TV captions and newspaper articles and a method for extracting a newspaper article and its follow-up articles is proposed, and a system for browsing and retrieving newspaperarticles and TV news reports is developed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Intell. Syst."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405374270"
                        ],
                        "name": "M. Schaar-Mitrea",
                        "slug": "M.-Schaar-Mitrea",
                        "structuredName": {
                            "firstName": "Mihaela",
                            "lastName": "Schaar-Mitrea",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schaar-Mitrea"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62607491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4155a9423e46ed278ee1acde33c1a0e3054170a5",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The diversity in TV images has augmented with the increased application of computer graphics. In this paper we study z coding system that supports both the lossless coding of such graphics data and regular lossy video compression. The lossless coding techniques are based on runlength and arithmetical coding. For video compression, we introduce a simple block predictive coding technique featuring individual pixel access, so that it enables a gradual shift from lossless coding of graphics to the lossy coding of video. An overall bit rate control completes the system. Computer simulations show a very high quality with a compression factor between 2-3.\u00a9 (1998) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only."
            },
            "slug": "Compression-of-mixed-video-and-graphics-images-for-Schaar-Mitrea",
            "title": {
                "fragments": [],
                "text": "Compression of mixed video and graphics images for TV systems"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "Z coding system that supports both the lossless coding of such graphics data and regular lossy video compression is studied and a simple block predictive coding technique featuring individual pixel access is introduced, so that it enables a gradual shift from lossed coding of graphics to the lossy coding of video."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082032608"
                        ],
                        "name": "David Mihalcik",
                        "slug": "David-Mihalcik",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mihalcik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Mihalcik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "Datasets A and B were ground-truthed by hand using the ViPER tool [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 25521649,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "442644532f74506d4b34052871366ba5afc21f5d",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We outline a reconfigurable video performance evaluation resource (ViPER), which provides an interface for ground truth generation, metrics for evaluation and tools for visualization of video analysis results. A key component is that the approach provides the basic infrastructure, and allows users to configure data generation and evaluation. Although ViPER can be used for any type of data, we focus on applications which require video content."
            },
            "slug": "Tools-and-techniques-for-video-performance-Doermann-Mihalcik",
            "title": {
                "fragments": [],
                "text": "Tools and techniques for video performance evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A reconfigurable video performance evaluation resource (ViPER), which provides an interface for ground truth generation, metrics for evaluation and tools for visualization of video analysis results, is outlined."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141915"
                        ],
                        "name": "W. Niblack",
                        "slug": "W.-Niblack",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Niblack",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Niblack"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "[46] performs binarization of lowcontrast scene text using a modification of Niblack\u2019s Multiple and Variable Thresholding scheme [34]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60929037,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "43678765df1d0b4594f7a49298cf27d75e174787",
            "isKey": false,
            "numCitedBy": 1417,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-introduction-to-digital-image-processing-Niblack",
            "title": {
                "fragments": [],
                "text": "An introduction to digital image processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145061328"
                        ],
                        "name": "R. Gonz\u00e1lez",
                        "slug": "R.-Gonz\u00e1lez",
                        "structuredName": {
                            "firstName": "Rafael",
                            "lastName": "Gonz\u00e1lez",
                            "middleNames": [
                                "Corsino"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gonz\u00e1lez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32377259"
                        ],
                        "name": "R. Woods",
                        "slug": "R.-Woods",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Woods",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Woods"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "Next, grayscale histogram stretching [12] is performed on each text region."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7472395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72ce0faa2d0be574f5cb88cfe6353a3ba40a08ae",
            "isKey": false,
            "numCitedBy": 7123,
            "numCiting": 171,
            "paperAbstract": {
                "fragments": [],
                "text": "2. A sampled (but not quantized) image, whose brightness before quantization, in each spatial position, can take values in the range [0mV;250mV], has the linear histogram of its brightness represented (approximately) in Fig. 2.a). After the quantization, the histogram of the resulting digital image is the one in Fig. 2.b). Then, most likely, the quantizer that was used is: a) an 8 bit uniform quantizer; b) a 2 bit optimal quantizer; c) a 4 bit optimal quantizer; d) a 2 bit uniform quantizer."
            },
            "slug": "Digital-Image-Processing-Gonz\u00e1lez-Woods",
            "title": {
                "fragments": [],
                "text": "Digital Image Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "A sampled (but not quantized) image, whose brightness before quantization, in each spatial position, can take values in the range [0mV;250mV], has the linear histogram of its brightness represented (approximately) in Fig.2."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055691681"
                        ],
                        "name": "P. Zhu",
                        "slug": "P.-Zhu",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Zhu",
                            "middleNames": [
                                "Fei"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2863224"
                        ],
                        "name": "P. Chirlian",
                        "slug": "P.-Chirlian",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Chirlian",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Chirlian"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 38820796,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "a71e3bc0ad37e8ec1f68bf3281eda89e2edb4e4c",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a nonlinear algorithm for critical point detection (CPD) of 2D digital shapes. The algorithm eliminates the problems arising from curvature approximation and Gaussian filtering in the existing algorithms. Based on the definition of \"critical level,\" we establish a set of criteria for the design of an effective CPD algorithm for the first time. By quantifying the critical level to the modified area confined by three consecutive \"pseudocritical points,\" a simple but very effective algorithm is developed. The comparison of our experimental results with those of many other CPD algorithms shows that the proposed algorithm is superior in that it provides a sequence of figures at every detail level, and each has a smaller integral error than the others with the same number of critical points. The experimental results on shapes with various complexities also show the algorithm is reliable and robust with regard to noise. >"
            },
            "slug": "On-Critical-Point-Detection-of-Digital-Shapes-Zhu-Chirlian",
            "title": {
                "fragments": [],
                "text": "On Critical Point Detection of Digital Shapes"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A nonlinear algorithm for critical point detection (CPD) of 2D digital shapes by quantifying the critical level to the modified area confined by three consecutive \"pseudocritical points\" is developed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50112753"
                        ],
                        "name": "Seong-Whan Lee",
                        "slug": "Seong-Whan-Lee",
                        "structuredName": {
                            "firstName": "Seong-Whan",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seong-Whan Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70076794"
                        ],
                        "name": "Young-Joon Kim",
                        "slug": "Young-Joon-Kim",
                        "structuredName": {
                            "firstName": "Young-Joon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Young-Joon Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "[21,20] use a combined detection and binarization approach."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "We are exploring an alternative approach for binarizing very small text based on topographical analysis [21]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62313190,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f4fc9b065e4c816e826971ff875bc643d6c0fcf",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Direct-Extraction-of-Topographic-Features-for-Lee-Kim",
            "title": {
                "fragments": [],
                "text": "Direct Extraction of Topographic Features for"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "Optical character recognition (OCR) in the context of document images has been extensively studied [32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "In particular, OCR for document images has been studied extensively [32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 620082,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce3b569e18670f6c10e61aa9a8bda7c30fd37411",
            "isKey": false,
            "numCitedBy": 554,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "The contributions to document image analysis of 99 papers published in the IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) are clustered, summarized, interpolated, interpreted, and evaluated."
            },
            "slug": "Twenty-Years-of-Document-Image-Analysis-in-PAMI-Nagy",
            "title": {
                "fragments": [],
                "text": "Twenty Years of Document Image Analysis in PAMI"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The contributions to document image analysis of 99 papers published in the IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) are clustered, summarized, interpolated, interpreted, and evaluated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 206
                            }
                        ],
                        "text": "a color stroke-based approach [10] (Algorithm 1), an intensity edge-based approach [19] (Algorithm 2), a color clustering approach [29] (Algorithm 3), and two texturebased approaches [40] (Algorithm 4) and [5,6] (Algorithm 5)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 149
                            }
                        ],
                        "text": "Temporal averaging of text regions also tends to smooth the background, increasing contrast and eliminating text-like strokes in the background (see [6] for an example)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 79
                            }
                        ],
                        "text": "In earlier work, we modified and enhanced Chaddha\u2019s algorithm for use in video [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "In some cases, any histogram-based thresholding scheme will fail (see [6] for an example)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extraction of unconstrained caption text from general-purpose video"
            },
            "venue": {
                "fragments": [],
                "text": "Master\u2019s thesis,"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For details on the MPEG video standard, the reader is referred to [ 31 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "MPEG encoders generally use a search window of 32 pixels in each direction during motion compensation searches [ 31 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61075136,
            "fieldsOfStudy": [],
            "id": "a979f5874a86a240c1a0673b17cd0143383e169b",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "MPEG Video: Compression Standard"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721328"
                        ],
                        "name": "Sameer Kiran Antani",
                        "slug": "Sameer-Kiran-Antani",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Antani",
                            "middleNames": [
                                "Kiran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Kiran Antani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[3] performs text detection in video using a decision fusion approach."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 96
                            }
                        ],
                        "text": "One possibility is to combine outputs of multiple detection algorithms to produce better output [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 64187558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6aeb3e2ccc1fbb277f24ba449063ce1d4c241d83",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Reliable-Extraction-of-Text-from-Video-Antani",
            "title": {
                "fragments": [],
                "text": "Reliable Extraction of Text from Video"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3087848"
                        ],
                        "name": "R. Dubes",
                        "slug": "R.-Dubes",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Dubes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dubes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "We have tried using color clustering [15] to separate text strokes from the background."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29535089,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa4bddbd10eafd8e1b54338517eedfee408f03ae",
            "isKey": false,
            "numCitedBy": 10558,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Algorithms-for-Clustering-Data-Jain-Dubes",
            "title": {
                "fragments": [],
                "text": "Algorithms for Clustering Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "Jain and Yu [16] presents a method to locate text in pseudo-color images, color images, and video frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 34993677,
            "fieldsOfStudy": [],
            "id": "73bf7b2fdb26498c05896d99fee4b8f3608c3bd6",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic text location in images and video frames"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extraction of unconstrained caption text from general-purpose video. Master's thesis"
            },
            "venue": {
                "fragments": [],
                "text": "Extraction of unconstrained caption text from general-purpose video. Master's thesis"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "He is currently a Research Scientist with Eastman Kodak Company"
            },
            "venue": {
                "fragments": [],
                "text": "He is currently a Research Scientist with Eastman Kodak Company"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "Li and Doermann\u2019s work [23] represents the state-of-the-art in text tracking."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "[22,23] address detection and tracking of moving text in video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Text extraction and recognition in digital video"
            },
            "venue": {
                "fragments": [],
                "text": "IAPR Int. Workshop on Document Analysis Systems,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Messelodi and Modena [30] present a system for binarizing text from book cover images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic identification and skew estimation of text lines in real scene images. Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "Color clustering in L*a*b* space [37] is performed on individual scan lines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "After the preprocessing steps described above, the text region is converted into the perceptually-uniform L*a*b* color space [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Digital image processing, 2nd edn"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "We have presented a similar evaluation in [4]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evaluation of methods for extraction of text from video"
            },
            "venue": {
                "fragments": [],
                "text": "IAPR Int. Workshop on Document Analysis Systems,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 131
                            }
                        ],
                        "text": "a color stroke-based approach [10] (Algorithm 1), an intensity edge-based approach [19] (Algorithm 2), a color clustering approach [29] (Algorithm 3), and two texturebased approaches [40] (Algorithm 4) and [5,6] (Algorithm 5)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "Mariano [29] performs simultaneous detection and binarization of horizontal text regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Locating uniformcolored text in video"
            },
            "venue": {
                "fragments": [],
                "text": "frames. In: Proc. Int. Conf. on Pattern Recognition,"
            },
            "year": 2000
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 24,
            "methodology": 20,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 53,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/Extraction-of-special-effects-caption-text-events-Crandall-Antani/37903a00047e1cf377408ca4119b48f2bfab89c4?sort=total-citations"
}