{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2339397"
                        ],
                        "name": "Michele Banko",
                        "slug": "Michele-Banko",
                        "structuredName": {
                            "firstName": "Michele",
                            "lastName": "Banko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michele Banko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6983197,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c8898cda9a1f13607e24306f6f64f20e0ff2ae7",
            "isKey": false,
            "numCitedBy": 409,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional Information Extraction (IE) takes a relation name and hand-tagged examples of that relation as input. Open IE is a relationindependent extraction paradigm that is tailored to massive and heterogeneous corpora such as the Web. An Open IE system extracts a diverse set of relational tuples from text without any relation-specific input. How is Open IE possible? We analyze a sample of English sentences to demonstrate that numerous relationships are expressed using a compact set of relation-independent lexico-syntactic patterns, which can be learned by an Open IE system. What are the tradeoffs between Open IE and traditional IE? We consider this question in the context of two tasks. First, when the number of relations is massive, and the relations themselves are not pre-specified, we argue that Open IE is necessary. We then present a new model for Open IE called O-CRF and show that it achieves increased precision and nearly double the recall than the model employed by TEXTRUNNER, the previous stateof-the-art Open IE system. Second, when the number of target relations is small, and their names are known in advance, we show that O-CRF is able to match the precision of a traditional extraction system, though at substantially lower recall. Finally, we show how to combine the two types of systems into a hybrid that achieves higher precision than a traditional extractor, with comparable recall."
            },
            "slug": "The-Tradeoffs-Between-Open-and-Traditional-Relation-Banko-Etzioni",
            "title": {
                "fragments": [],
                "text": "The Tradeoffs Between Open and Traditional Relation Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new model for Open IE called O-CRF is presented and it is shown that it achieves increased precision and nearly double the recall than the model employed by TEXTRUNNER, the previous stateof-the-art Open IE system."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47861681"
                        ],
                        "name": "Binyamin Rosenfeld",
                        "slug": "Binyamin-Rosenfeld",
                        "structuredName": {
                            "firstName": "Binyamin",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Binyamin Rosenfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145864794"
                        ],
                        "name": "Ronen Feldman",
                        "slug": "Ronen-Feldman",
                        "structuredName": {
                            "firstName": "Ronen",
                            "lastName": "Feldman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronen Feldman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "URES [71] also leveraged high-quality output from baseline KnowItAll to train a pattern-learning algorithm that increased the number of facts extracted while maintaining the same precision of the baseline system."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Inspired by KnowItAll, the URES Web IE system [71], also utilized high-quality output from baseline KnowItAll to automatically supervise the learning of relation-specific extraction patterns with success."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 279977,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8235935ce1d7e58d45fa63f114bdc98a91746ecb",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Most information extraction systems either use hand written extraction patterns or use a machine learning algorithm that is trained on a manually annotated corpus. Both of these approaches require massive human effort and hence prevent information extraction from becoming more widely applicable. In this paper we present URES (Unsupervised Relation Extraction System), which extracts relations from the Web in a totally unsupervised way. It takes as input the descriptions of the target relations, which include the names of the predicates, the types of their attributes, and several seed instances of the relations. Then the system downloads from the Web a large collection of pages that are likely to contain instances of the target relations. From those pages, utilizing the known seed instances, the system learns the relation patterns, which are then used for extraction. We present several experiments in which we learn patterns and extract instances of a set of several common IE relations, comparing several pattern learning and filtering setups. We demonstrate that using simple noun phrase tagger is sufficient as a base for accurate patterns. However, having a named entity recognizer, which is able to recognize the types of the relation attributes significantly, enhances the extraction performance. We also compare our approach with KnowItAll's fixed generic patterns."
            },
            "slug": "URES-:-an-Unsupervised-Web-Relation-Extraction-Rosenfeld-Feldman",
            "title": {
                "fragments": [],
                "text": "URES : an Unsupervised Web Relation Extraction System"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "URES (Unsupervised Relation Extraction System), which extracts relations from the Web in a totally unsupervised way and demonstrates that using simple noun phrase tagger is sufficient as a base for accurate patterns and compares the approach with KnowItAll's fixed generic patterns."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714612"
                        ],
                        "name": "S. Sekine",
                        "slug": "S.-Sekine",
                        "structuredName": {
                            "firstName": "Satoshi",
                            "lastName": "Sekine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sekine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This year, Sekine [Sekine, 2006] proposed a paradigm for \u201con-demand information extraction,\u201d which aims to eliminate customization involved with adapting IE systems to new topics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 648239,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a4d81c67da140f88de5d8625906a9be17b3af40",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "At present, adapting an Information Extraction system to new topics is an expensive and slow process, requiring some knowledge engineering for each new topic. We propose a new paradigm of Information Extraction which operates 'on demand' in response to a user's query. On-demand Information Extraction (ODIE) aims to completely eliminate the customization effort. Given a user's query, the system will automatically create patterns to extract salient relations in the text of the topic, and build tables from the extracted information using paraphrase discovery technology. It relies on recent advances in pattern discovery, paraphrase discovery, and extended named entity tagging. We report on experimental results in which the system created useful tables for many topics, demonstrating the feasibility of this approach."
            },
            "slug": "On-Demand-Information-Extraction-Sekine",
            "title": {
                "fragments": [],
                "text": "On-Demand Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "On-demand Information Extraction (ODIE) aims to completely eliminate the customization effort, and is reported on on experimental results in which the system created useful tables for many topics, demonstrating the feasibility of this approach."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725561"
                        ],
                        "name": "Michael J. Cafarella",
                        "slug": "Michael-J.-Cafarella",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Cafarella",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Cafarella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2339397"
                        ],
                        "name": "Michele Banko",
                        "slug": "Michele-Banko",
                        "structuredName": {
                            "firstName": "Michele",
                            "lastName": "Banko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michele Banko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "An algorithm for computing answers to qualified list queries using TextRunner was presented in [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "TextRunner\u2019s tuple index enables relational Web search [15] \u2013 search through a large entity-relationship graph that is automatically derived from the text of Web pages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7142575,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cdb0efae2bad7e09832950423785c1da3299054e",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Facts are naturally organized in terms of entities, classes, and their relationships as in an entity-relationship diagram or a semantic network. Search engines have eschewed such structures because, in the past, their creation and processing have not been practical at Web scale. This paper introduces the extraction graph, a textual approximation to an entity-relationship graph, which is automatically extracted from Web pages. The extraction graph is an intermediate representation that is more informative than a mere page-hyperlink graph but far easier to construct than a semantic network. The paper also introduces TextRunner, a search engine that utilizes this representation to answer complex relational queries that are dicult to answer using today\u2019s search engines or Web Information Extraction (IE) systems. The paper compares TextRunner to a state-of-the-art IE system on list searches, and nds that TextRunner is 40% more precise, with 11% better recall than the IE system. Our experiments, computed over a 90-million page corpus and a 227-million node extraction graph, show how TextRunner will scale to billions of pages."
            },
            "slug": "Relational-Web-Search-Cafarella-Banko",
            "title": {
                "fragments": [],
                "text": "Relational Web Search"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The extraction graph is a textual approximation to an entity-relationship graph, which is automatically extracted from Web pages, and TextRunner, a search engine that utilizes this representation to answer complex relational queries that are difficult to answer using today\u2019s search engines or Web Information Extraction systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725561"
                        ],
                        "name": "Michael J. Cafarella",
                        "slug": "Michael-J.-Cafarella",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Cafarella",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Cafarella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145612610"
                        ],
                        "name": "Doug Downey",
                        "slug": "Doug-Downey",
                        "structuredName": {
                            "firstName": "Doug",
                            "lastName": "Downey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doug Downey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16409804,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe8dc921ebe4f85969f4181c50959fa0dc552476",
            "isKey": false,
            "numCitedBy": 146,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Numerous NLP applications rely on search-engine queries, both to extract information from and to compute statistics over the Web corpus. But search engines often limit the number of available queries. As a result, query-intensive NLP applications such as Information Extraction (IE) distribute their query load over several days, making IE a slow, offline process.This paper introduces a novel architecture for IE that obviates queries to commercial search engines. The architecture is embodied in a system called KnowItNow that performs high-precision IE in minutes instead of days. We compare KnowItNow experimentally with the previously-published KnowItAll system, and quantify the tradeoff between recall and speed. KnowItNow's extraction rate is two to three orders of magnitude higher than KnowItAll's."
            },
            "slug": "KnowItNow:-Fast,-Scalable-Information-Extraction-Cafarella-Downey",
            "title": {
                "fragments": [],
                "text": "KnowItNow: Fast, Scalable Information Extraction from the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel architecture for IE that obviates queries to commercial search engines is introduced, embodied in a system called KnowItNow that performs high-precision IE in minutes instead of days, and the tradeoff between recall and speed is quantified."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2293353"
                        ],
                        "name": "Stefan Schoenmackers",
                        "slug": "Stefan-Schoenmackers",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Schoenmackers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Schoenmackers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 220250,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf3ba53a5030b8dd6ec65101b6f5a9b8e4d06f80",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Most Web-based Q/A systems work by finding pages that contain an explicit answer to a question. These systems are helpless if the answer has to be inferred from multiple sentences, possibly on different pages. To solve this problem, we introduce the Holmes system, which utilizes textual inference (TI) over tuples extracted from text. \n \nWhereas previous work on TI (e.g., the literature on textual entailment) has been applied to paragraph-sized texts, Holmes utilizes knowledge-based model construction to scale TI to a corpus of 117 million Web pages. Given only a few minutes, Holmes doubles recall for example queries in three disparate domains (geography, business, and nutrition). Importantly, Holmes's runtime is linear in the size of its input corpus due to a surprising property of many textual relations in the Web corpus---they are \"approximately\" functional in a well-defined sense."
            },
            "slug": "Scaling-Textual-Inference-to-the-Web-Schoenmackers-Etzioni",
            "title": {
                "fragments": [],
                "text": "Scaling Textual Inference to the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The Holmes system, which utilizes textual inference over tuples extracted from text to scale TI to a corpus of 117 million Web pages, and its runtime is linear in the size of its input corpus."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725561"
                        ],
                        "name": "Michael J. Cafarella",
                        "slug": "Michael-J.-Cafarella",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Cafarella",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Cafarella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145612610"
                        ],
                        "name": "Doug Downey",
                        "slug": "Doug-Downey",
                        "structuredName": {
                            "firstName": "Doug",
                            "lastName": "Downey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doug Downey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36445704"
                        ],
                        "name": "Ana-Maria Popescu",
                        "slug": "Ana-Maria-Popescu",
                        "structuredName": {
                            "firstName": "Ana-Maria",
                            "lastName": "Popescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ana-Maria Popescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3296031"
                        ],
                        "name": "Tal Shaked",
                        "slug": "Tal-Shaked",
                        "structuredName": {
                            "firstName": "Tal",
                            "lastName": "Shaked",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tal Shaked"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3321874"
                        ],
                        "name": "A. Yates",
                        "slug": "A.-Yates",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Yates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yates"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7162988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "421151fa75e40dd86414215abf29d9f2c052a2e1",
            "isKey": false,
            "numCitedBy": 1229,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Unsupervised-named-entity-extraction-from-the-Web:-Etzioni-Cafarella",
            "title": {
                "fragments": [],
                "text": "Unsupervised named-entity extraction from the Web: An experimental study"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3321874"
                        ],
                        "name": "A. Yates",
                        "slug": "A.-Yates",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Yates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yates"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3125641,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9bf587f7ec5ce93fb0f9b93d0db6cca7989bb0b0",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The task of identifying synonymous relations and objects, or Synonym Resolution (SR), is critical for high-quality information extraction. The bulk of previous SR work assumed strong domain knowledge or hand-tagged training examples. This paper investigates SR in the context of unsupervised information extraction, where neither is available. The paper presents a scalable, fully-implemented system for SR that runs in O(KN log N) time in the number of extractions N and the maximum number of synonyms per word, K. The system, called RESOLVER, introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them. Given two million assertions extracted from the Web, RESOLVER resolves objects with 78% precision and an estimated 68% recall and resolves relations with 90% precision and 35% recall."
            },
            "slug": "Unsupervised-Resolution-of-Objects-and-Relations-on-Yates-Etzioni",
            "title": {
                "fragments": [],
                "text": "Unsupervised Resolution of Objects and Relations on the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A scalable, fully-implemented system for SR that runs in O(KN log N) time in the number of extractions N and the maximum number of synonyms per word, K, and introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36445704"
                        ],
                        "name": "Ana-Maria Popescu",
                        "slug": "Ana-Maria-Popescu",
                        "structuredName": {
                            "firstName": "Ana-Maria",
                            "lastName": "Popescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ana-Maria Popescu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53725248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9adc8d3e2afa0a631ba2f54b8781f7ed81a760a7",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 119,
            "paperAbstract": {
                "fragments": [],
                "text": "In the past few years the World Wide Web has emerged as an important source of data, much of it in the form of unstructured text. This thesis describes an extensible model for information extraction that takes advantage of the unique characteristics of Web text and leverages existent search engine technology in order to ensure the quality of the extracted information. The key features of our approach are the use of lexico-syntactic patterns, Web-scale statistics and unsupervised or semi-supervised learning methods. Our information extraction model has been instantiated and extended in order to solve a set of diverse information extraction tasks: subclass and related class extraction, relation property learning, the acquisition of salient product features and corresponding user opinions from customer reviews and finally, the mining of commonsense information from the Web for the benefit of integrated AI systems."
            },
            "slug": "Information-extraction-from-unstructured-web-text-Etzioni-Popescu",
            "title": {
                "fragments": [],
                "text": "Information extraction from unstructured web text"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This thesis describes an extensible model for information extraction that takes advantage of the unique characteristics of Web text and leverages existent search engine technology in order to ensure the quality of the extracted information."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3321874"
                        ],
                        "name": "A. Yates",
                        "slug": "A.-Yates",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Yates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yates"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "A study by Yates [97] empirically quantified the importance of synonym resolution, the task of identifying synonymous entity and relation names."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 104
                            }
                        ],
                        "text": "We expect future work to improve both \nthe precision and recall of Open IE (for example, see Downey8 and Yates24)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 21871089,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8d92c3adf2418ed970f17ed1272d3ca6bf45581",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "Web Information Extraction (WIE) systems have recently been able to extract massive quantities of relational data from online text. This has opened the possibility of achieving an elusive goal in Artificial Intelligence (AI): broad-coverage domain knowledge. AI systems depend to a great extent on having knowledge about the domains in which they operate, and such knowledge is typically expensive to enter into the system. Furthermore, the knowledge must be entered for every different domain in which an application is to operate. The Web contains knowledge about all kinds of different domains, but in a format that is not readily usable by AI systems. WIE promises to bridge the gap between the Web and AI. \nNatural Language Processing is an example of an area in AI in which knowledge can make a dramatic difference in the performance of an application. Understanding or interpreting language depends on the ability to understand the words used in a domain. The meanings, usages, and syntactic properties of words, and the relative frequency with which certain words are used, are necessary pieces of information for effective language processing, and much of this information can be extracted from text. In one case study, this thesis examines methods for using extracted information in improving a particular kind of language processing tool, a parser. \nBefore information extraction can become broadly useful, however, more research must be done to improve the quality of the extracted information. A number of factors affect the quality, including correctness, importance or relevance, and the sophistication of meaning representation. The second case study in this thesis investigates a method for resolving synonyms in extracted information. This technique changes the meaning representation of extractions from one that relates words or names to one that relates entities to one another."
            },
            "slug": "Information-extraction-from-the-web:-techniques-and-Etzioni-Yates",
            "title": {
                "fragments": [],
                "text": "Information extraction from the web: techniques and applications"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This thesis examines methods for using extracted information in improving a particular kind of language processing tool, a parser, and investigates a method for resolving synonyms in extracted information."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2339397"
                        ],
                        "name": "Michele Banko",
                        "slug": "Michele-Banko",
                        "structuredName": {
                            "firstName": "Michele",
                            "lastName": "Banko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michele Banko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18301907,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a1336dfa2f4c2df080990df2eff6e2bb3389238",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The increasing availability of electronic text has made it possible to acquire information using a variety of techniques that leverage the expertise of both humans and machines. In particular, the field of Information Extraction (IE), in which knowledge is extracted automatically from text, has shown promise for large-scale knowledge acquisition. While IE systems can uncover assertions about individual entities with an increasing level of sophistication,alltext understanding -- the formation of a coherent theory from a textual corpus -- involves representation and learning abilities not currently achievable by today's IE systems. Compared to individual relational assertions outputted by IE systems, a theory includes coherent knowledge of abstract concepts and the relationships among them. We believe that the ability to fully discover the richness of knowledge present within large, unstructured and heterogeneous corpora will require a lifelong learning process in which earlier learned knowledge is used to guide subsequent learning. This paper introduces Alice, a lifelong learning agent whose goal is to automatically discovera collection of concepts, facts and generalizations that describe a particular topic of interest directly from a large volume of Web text. Building upon recent advances in unsupervised information extraction, we demonstrate that Alice can iteratively discover new concepts and compose general domain knowledge with a precision of 78%."
            },
            "slug": "Strategies-for-lifelong-knowledge-extraction-from-Banko-Etzioni",
            "title": {
                "fragments": [],
                "text": "Strategies for lifelong knowledge extraction from the web"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Alice, a lifelong learning agent whose goal is to automatically discover a collection of concepts, facts and generalizations that describe a particular topic of interest directly from a large volume of Web text, is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "K-CAP '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16677640,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29c99d263b5e05aae6bb96f004f025dcc9b5caae",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Information extraction IE is the problem of lling out pre de ned structured sum maries from text documents We are in terested in performing IE in non traditional domains where much of the text is often ungrammatical such as electronic bulletin board posts and Web pages We suggest that the best approach is one that takes into ac count many di erent kinds of information and argue for the suitability of a multistrat egy approach We describe learners for IE drawn from three separate machine learning paradigms rote memorization term space text classi cation and relational rule induc tion By building regression models mapping from learner con dence to probability of cor rectness and combining probabilities appro priately it is possible to improve extraction accuracy over that achieved by any individ ual learner We describe three di erent mul tistrategy approaches Experiments on two IE domains a collection of electronic seminar announcements from a university computer science department and a set of newswire ar ticles describing corporate acquisitions from the Reuters collection demonstrate the e ec tiveness of all three approaches"
            },
            "slug": "Multistrategy-Learning-for-Information-Extraction-Freitag",
            "title": {
                "fragments": [],
                "text": "Multistrategy Learning for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is possible to improve extraction accuracy over that achieved by any individ ual learner by building regression models mapping from learner con dence to probability of cor rectness and combining probabilities."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144557047"
                        ],
                        "name": "M. Craven",
                        "slug": "M.-Craven",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Craven",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Craven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2922396"
                        ],
                        "name": "Dan DiPasquo",
                        "slug": "Dan-DiPasquo",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "DiPasquo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan DiPasquo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682522"
                        ],
                        "name": "Se\u00e1n Slattery",
                        "slug": "Se\u00e1n-Slattery",
                        "structuredName": {
                            "firstName": "Se\u00e1n",
                            "lastName": "Slattery",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Se\u00e1n Slattery"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Automation The first step in automating IE was moving from knowledge-based IE systems to trainable systems that took as input hand-tagged instances [Riloff, 1996] or document segments [Craven et al., 1999] and automatically learned domain-specific extraction patterns."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5303928,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10dfa97d41f17755257f3d653f4808a30fd7481b",
            "isKey": false,
            "numCitedBy": 526,
            "numCiting": 106,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-to-construct-knowledge-bases-from-the-Wide-Craven-DiPasquo",
            "title": {
                "fragments": [],
                "text": "Learning to construct knowledge bases from the World Wide Web"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110920850"
                        ],
                        "name": "Fei Wu",
                        "slug": "Fei-Wu",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2566295"
                        ],
                        "name": "Raphael Hoffmann",
                        "slug": "Raphael-Hoffmann",
                        "structuredName": {
                            "firstName": "Raphael",
                            "lastName": "Hoffmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raphael Hoffmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Daniel S. Weld (weld@cs.washington.edu) is the Thomas j. cable/Wrf \nProfessor of computer science and Engineering at the university of Washington, seattle."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "Wu, Hoffmann and Weld [95] also demonstrated they could improve the recall of a highprecision extraction algorithm by applying shrinkage, a technique that enables a system to find additional training examples within a corpus when the initial set of training instances is sparse."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "Weld, d., \nWu, f., adar, E., amershi, s., fogarty, j., hoffmann, r., Patel, k. and skinner, m. intelligence in Wikipedia. \nin Proceedings of the 23rd Conference on Artificial Intelligence (2008)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "Etzioni, o., cafarella, \nm., downey, d., kok, s., Popescu, a., shaked, T., soderland, s., Weld, d. and yates, a. unsupervised \nnamed-entity extraction from the Web: an experimental study."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7781746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4bd8e9c69c5270905a1c0eb1a87fca7a92944cb",
            "isKey": true,
            "numCitedBy": 159,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Not only is Wikipedia a comprehensive source of quality information, it has several kinds of internal structure (e.g., relational summaries known as infoboxes), which enable self-supervised information extraction. While previous efforts at extraction from Wikipedia achieve high precision and recall on well-populated classes of articles, they fail in a larger number of cases, largely because incomplete articles and infrequent use of infoboxes lead to insufficient training data. This paper presents three novel techniques for increasing recall from Wikipedia's long tail of sparse classes: (1) shrinkage over an automatically-learned subsumption taxonomy, (2) a retraining technique for improving the training data, and (3) supplementing results by extracting from the broader Web. Our experiments compare design variations and show that, used in concert, these techniques increase recall by a factor of 1.76 to 8.71 while maintaining or increasing precision."
            },
            "slug": "Information-extraction-from-Wikipedia:-moving-down-Wu-Hoffmann",
            "title": {
                "fragments": [],
                "text": "Information extraction from Wikipedia: moving down the long tail"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Three novel techniques for increasing recall from Wikipedia's long tail of sparse classes are presented: shrinkage over an automatically-learned subsumption taxonomy, a retraining technique for improving the training data, and supplementing results by extracting from the broader Web."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145864794"
                        ],
                        "name": "Ronen Feldman",
                        "slug": "Ronen-Feldman",
                        "structuredName": {
                            "firstName": "Ronen",
                            "lastName": "Feldman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronen Feldman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47861681"
                        ],
                        "name": "Binyamin Rosenfeld",
                        "slug": "Binyamin-Rosenfeld",
                        "structuredName": {
                            "firstName": "Binyamin",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Binyamin Rosenfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2333457,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "501428daffd5d70d1305582ddec7a93dae1f704e",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Web extraction systems attempt to use the immense amount of unlabeled text in the Web in order to create large lists of entities and relations. Unlike traditional Information Extraction methods, the Web extraction systems do not label every mention of the target entity or relation, instead focusing on extracting as many different instances as possible while keeping the precision of the resulting list reasonably high. SRES is a self-supervised Web relation extraction system that learns powerful extraction patterns from unlabeled text, using short descriptions of the target relations and their attributes. SRES automatically generates the training data needed for its pattern-learning component. The performance of SRES is further enhanced by classifying its output instances using the properties of the instances and the patterns. The features we use for classification and the trained classification model are independent from the target relation, which we demonstrate in a series of experiments. We also compare the performance of SRES to the performance of the state-of-the-art KnowItAll system, and to the performance of its pattern learning component, which learns simpler pattern language than SRES."
            },
            "slug": "Self-supervised-relation-extraction-from-the-Web-Feldman-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "Self-supervised Relation Extraction from the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "SRES is a self-supervised Web relation extraction system that learns powerful extraction patterns from unlabeled text, using short descriptions of the target relations and their attributes and the properties of the instances and the patterns."
            },
            "venue": {
                "fragments": [],
                "text": "ISMIS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759772"
                        ],
                        "name": "Hoifung Poon",
                        "slug": "Hoifung-Poon",
                        "structuredName": {
                            "firstName": "Hoifung",
                            "lastName": "Poon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hoifung Poon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740213"
                        ],
                        "name": "Pedro M. Domingos",
                        "slug": "Pedro-M.-Domingos",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Domingos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro M. Domingos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 658845,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9afc2cb61a4da0aa29fa9f40889d21ff67157c7a",
            "isKey": false,
            "numCitedBy": 289,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of information extraction is to extract database records from text or semi-structured sources. Traditionally, information extraction proceeds by first segmenting each candidate record separately, and then merging records that refer to the same entities. While computationally efficient, this approach is suboptimal, because it ignores the fact that segmenting one candidate record can help to segment similar ones. For example, resolving a well-segmented field with a less-clear one can disambiguate the latter's boundaries. In this paper we propose a joint approach to information extraction, where segmentation of all records and entity resolution are performed together in a single integrated inference process. While a number of previous authors have taken steps in this direction (eg., Pasula et al. (2003), Wellner et al. (2004)), to our knowledge this is the first fully joint approach. In experiments on the CiteSeer and Cora citation matching datasets, joint inference improved accuracy, and our approach outperformed previous ones. Further, by using Markov logic and the existing algorithms for it, our solution consisted mainly of writing the appropriate logical formulas, and required much less engineering than previous ones."
            },
            "slug": "Joint-Inference-in-Information-Extraction-Poon-Domingos",
            "title": {
                "fragments": [],
                "text": "Joint Inference in Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper proposes a joint approach to information extraction, where segmentation of all records and entity resolution are performed together in a single integrated inference process, and is believed to be the first fully joint approach."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685296"
                        ],
                        "name": "Eugene Agichtein",
                        "slug": "Eugene-Agichtein",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Agichtein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Agichtein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684012"
                        ],
                        "name": "L. Gravano",
                        "slug": "L.-Gravano",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Gravano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gravano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7579604,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cee045e890270abae65455667b292db355d53728",
            "isKey": false,
            "numCitedBy": 1365,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Text documents often contain valuable structured data that is hidden Yin regular English sentences. This data is best exploited infavailable as arelational table that we could use for answering precise queries or running data mining tasks.We explore a technique for extracting such tables from document collections that requires only a handful of training examples from users. These examples are used to generate extraction patterns, that in turn result in new tuples being extracted from the document collection.We build on this idea and present our Snowball system. Snowball introduces novel strategies for generating patterns and extracting tuples from plain-text documents.At each iteration of the extraction process, Snowball evaluates the quality of these patterns and tuples without human intervention,and keeps only the most reliable ones for the next iteration. In this paper we also develop a scalable evaluation methodology and metrics for our task, and present a thorough experimental evaluation of Snowball and comparable techniques over a collection of more than 300,000 newspaper documents."
            },
            "slug": "Snowball:-extracting-relations-from-large-Agichtein-Gravano",
            "title": {
                "fragments": [],
                "text": "Snowball: extracting relations from large plain-text collections"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper develops a scalable evaluation methodology and metrics for the task, and presents a thorough experimental evaluation of Snowball and comparable techniques over a collection of more than 300,000 newspaper documents."
            },
            "venue": {
                "fragments": [],
                "text": "DL '00"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145612610"
                        ],
                        "name": "Doug Downey",
                        "slug": "Doug-Downey",
                        "structuredName": {
                            "firstName": "Doug",
                            "lastName": "Downey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doug Downey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The model was shown to estimate far more accurate probabilities for IE than noisyor and pointwise mutual information based methods [Downey et al., 2005]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Redundancy-Based Assessor: The Assessor assigns a probability to each retained tuple based on a probabilistic model of redundancy in text introduced in [Downey et al., 2005]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5311461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "310cd6a39b0539193561148cd9897b1953fa8b28",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Unsupervised Information Extraction (UIE) is the task of extracting knowledge from text without using hand-tagged training examples. A fundamental problem for both UIE and supervised IE is assessing the probability that extracted information is correct. In massive corpora such as the Web, the same extraction is found repeatedly in different documents. How does this redundancy impact the probability of correctness? \n \nThis paper introduces a combinatorial \"balls-andurns\" model that computes the impact of sample size, redundancy, and corroboration from multiple distinct extraction rules on the probability that an extraction is correct. We describe methods for estimating the model's parameters in practice and demonstrate experimentally that for UIE the model's log likelihoods are 15 times better, on average, than those obtained by Pointwise Mutual Information (PMI) and the noisy-or model used in previous work. For supervised IE, the model's performance is comparable to that of Support Vector Machines, and Logistic Regression."
            },
            "slug": "A-Probabilistic-Model-of-Redundancy-in-Information-Downey-Etzioni",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Model of Redundancy in Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A combinatorial \"balls-andurns\" model is introduced that computes the impact of sample size, redundancy, and corroboration from multiple distinct extraction rules on the probability that an extraction is correct."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145612610"
                        ],
                        "name": "Doug Downey",
                        "slug": "Doug-Downey",
                        "structuredName": {
                            "firstName": "Doug",
                            "lastName": "Downey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doug Downey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50452701"
                        ],
                        "name": "M. Broadhead",
                        "slug": "M.-Broadhead",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Broadhead",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Broadhead"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Recent work by Downey [29] demonstrated the ability to recognize complex entities in web text where the set of entity classes is unknown and the names are often difficult to recognize (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "While unit features are sufficient for high precision open extraction, the use of a Web-based named-entity recognizer such as [29] could further improve the quality of the output."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "These include integration with a Web-scale named-entity recognizer such as Lex [29], developing the ability to resolve underspecified entities (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7902420,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d54f4215dbdf272820f080b8fc2cbba99bd634e7",
            "isKey": false,
            "numCitedBy": 178,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Named Entity Recognition (NER) is the task of locating and classifying names in text. In previous work, NER was limited to a small number of pre-defined entity classes (e.g., people, locations, and organizations). However, NER on the Web is a far more challenging problem. Complex names (e.g., film or book titles) can be very difficult to pick out precisely from text. Further, the Web contains a wide variety of entity classes, which are not known in advance. Thus, hand-tagging examples of each entity class is impractical. \n \nThis paper investigates a novel approach to the first step in Web NER: locating complex named entities in Web text. Our key observation is that named entities can be viewed as a species of multiword units, which can be detected by accumulating n-gram statistics over the Web corpus. We show that this statistical method's F1 score is 50% higher than that of supervised techniques including Conditional Random Fields (CRFs) and Conditional Markov Models (CMMs) when applied to complex names. The method also outperforms CMMs and CRFs by 117% on entity classes absent from the training data. Finally, our method outperforms a semi-supervised CRF by 73%."
            },
            "slug": "Locating-Complex-Named-Entities-in-Web-Text-Downey-Broadhead",
            "title": {
                "fragments": [],
                "text": "Locating Complex Named Entities in Web Text"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper investigates a novel approach to the first step in Web NER: locating complex named entities in Web text and shows that named entities can be viewed as a species of multiword units, which can be detected by accumulating n-gram statistics over the Web corpus."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144621026"
                        ],
                        "name": "R. Snow",
                        "slug": "R.-Snow",
                        "structuredName": {
                            "firstName": "Rion",
                            "lastName": "Snow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Snow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746807"
                        ],
                        "name": "Dan Jurafsky",
                        "slug": "Dan-Jurafsky",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Jurafsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u201d The identification of these patterns by hand inspired a body of work in which this initial set of extraction patterns is used to seed a bootstrapping process that automatically acquires additional patterns for is-a or part-whole relations [33, 39, 81]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1854720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e703e928bc07900527c368db2428d0d5c57148c2",
            "isKey": false,
            "numCitedBy": 800,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic taxonomies such as WordNet provide a rich source of knowledge for natural language processing applications, but are expensive to build, maintain, and extend. Motivated by the problem of automatically constructing and extending such taxonomies, in this paper we present a new algorithm for automatically learning hypernym (is-a) relations from text. Our method generalizes earlier work that had relied on using small numbers of hand-crafted regular expression patterns to identify hypernym pairs. Using \"dependency path\" features extracted from parse trees, we introduce a general-purpose formalization and generalization of these patterns. Given a training set of text containing known hypernym pairs, our algorithm automatically extracts useful dependency paths and applies them to new corpora to identify novel pairs. On our evaluation task (determining whether two nouns in a news article participate in a hypernym relationship), our automatically extracted database of hypernyms attains both higher precision and higher recall than WordNet."
            },
            "slug": "Learning-Syntactic-Patterns-for-Automatic-Hypernym-Snow-Jurafsky",
            "title": {
                "fragments": [],
                "text": "Learning Syntactic Patterns for Automatic Hypernym Discovery"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper presents a new algorithm for automatically learning hypernym (is-a) relations from text, using \"dependency path\" features extracted from parse trees and introduces a general-purpose formalization and generalization of these patterns."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Freitag [35] used linear regression to model the relationship between the confidence of several inductive learning algorithms \u2013 rote learning, Naive Bayes, grammatical inference and a relational rule learning \u2013 and the probability that a prediction is correct."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8618254,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58de638505046e7de5fe7cc0660b4c6d79247488",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of learning to perform information extraction in domains where linguistic processing is problematic, such as Usenet posts, email, and finger plan files. In place of syntactic and semantic information, other sources of information can be used, such as term frequency, typography, formatting, and mark-up. We describe four learning approaches to this problem, each drawn from a different paradigm: a rote learner, a term-space learner based on Naive Bayes, an approach using grammatical induction, and a relational rule learner. Experiments on 14 information extraction problems defined over four diverse document collections demonstrate the effectiveness of these approaches. Finally, we describe a multistrategy approach which combines these learners and yields performance competitive with or better than the best of them. This technique is modular and flexible, and could find application in other machine learning problems."
            },
            "slug": "Machine-Learning-for-Information-Extraction-in-Freitag",
            "title": {
                "fragments": [],
                "text": "Machine Learning for Information Extraction in Informal Domains"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A multistrategy approach which combines these learners and yields performance competitive with or better than the best of them is described, which is modular and flexible, and could find application in other machine learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145612610"
                        ],
                        "name": "Doug Downey",
                        "slug": "Doug-Downey",
                        "structuredName": {
                            "firstName": "Doug",
                            "lastName": "Downey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doug Downey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2293353"
                        ],
                        "name": "Stefan Schoenmackers",
                        "slug": "Stefan-Schoenmackers",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Schoenmackers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Schoenmackers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 86987,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b214871c7780f1e1030da595af8715bd2962d811",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Even in a massive corpus such as the Web, a substantial fraction of extractions appear infrequently. This paper shows how to assess the correctness of sparse extractions by utilizing unsupervised language models. The REALM system, which combines HMMbased and n-gram-based language models, ranks candidate extractions by the likelihood that they are correct. Our experiments show that REALM reduces extraction error by 39%, on average, when compared with previous work. Because REALM pre-computes language models based on its corpus and does not require any hand-tagged seeds, it is far more scalable than approaches that learn models for each individual relation from handtagged data. Thus, REALM is ideally suited for open information extraction where the relations of interest are not specified in advance and their number is potentially vast."
            },
            "slug": "Sparse-Information-Extraction:-Unsupervised-Models-Downey-Schoenmackers",
            "title": {
                "fragments": [],
                "text": "Sparse Information Extraction: Unsupervised Language Models to the Rescue"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This paper shows how to assess the correctness of sparse extractions by utilizing unsupervised language models in the REALM system, which combines HMMbased and n-gram-based language models and reduces extraction error by 39%, on average, when compared with previous work."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786259"
                        ],
                        "name": "S. Brin",
                        "slug": "S.-Brin",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Brin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Brin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We measured precision and recall for ten relations that have often been the focus of study of previously published work in IE, specifically that of Dipre [11], Snowball [1], KnowItAll [33], Yago [85, 86] and Bunescu\u2019s SSK relation extraction algorithm [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Brin [11], Agichtein and Gravano [1], Riloff and Jones [70], Pasca et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6075461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "92575a3c554353a27b2c0263ad7f8487d9102301",
            "isKey": false,
            "numCitedBy": 1235,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The World Wide Web is a vast resource for information. At the same time it is extremely distributed. A particular type of data such as restaurant lists may be scattered across thousands of independent information sources in many different formats. In this paper, we consider the problem of extracting a relation for such a data type from all of these sources automatically. We present a technique which exploits the duality between sets of patterns and relations to grow the target relation starting from a small sample. To test our technique we use it to extract a relation of (author,title) pairs from the World Wide Web."
            },
            "slug": "Extracting-Patterns-and-Relations-from-the-World-Brin",
            "title": {
                "fragments": [],
                "text": "Extracting Patterns and Relations from the World Wide Web"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents a technique which exploits the duality between sets of patterns and relations to grow the target relation starting from a small sample and uses it to extract a relation of (author,title) pairs from the World Wide Web."
            },
            "venue": {
                "fragments": [],
                "text": "WebDB"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8359747,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22fb3b3b2bdf768dd435eedfc5ef5155d3e56b1a",
            "isKey": false,
            "numCitedBy": 1071,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "A wealth of on-line text information can be made available to automatic processing by information extraction (IE) systems. Each IE application needs a separate set of rules tuned to the domain and writing style. WHISK helps to overcome this knowledge-engineering bottleneck by learning text extraction rules automatically.WHISK is designed to handle text styles ranging from highly structured to free text, including text that is neither rigidly formatted nor composed of grammatical sentences. Such semi-structured text has largely been beyond the scope of previous systems. When used in conjunction with a syntactic analyzer and semantic tagging, WHISK can also handle extraction from free text such as news stories."
            },
            "slug": "Learning-Information-Extraction-Rules-for-and-Free-Soderland",
            "title": {
                "fragments": [],
                "text": "Learning Information Extraction Rules for Semi-Structured and Free Text"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "WHISK is designed to handle text styles ranging from highly structured to free text, including text that is neither rigidly formatted nor composed of grammatical sentences, and can also handle extraction from free text such as news stories."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2469966"
                        ],
                        "name": "R. Girju",
                        "slug": "R.-Girju",
                        "structuredName": {
                            "firstName": "Roxana",
                            "lastName": "Girju",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Girju"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3154735"
                        ],
                        "name": "A. Badulescu",
                        "slug": "A.-Badulescu",
                        "structuredName": {
                            "firstName": "Adriana",
                            "lastName": "Badulescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Badulescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40497400"
                        ],
                        "name": "D. Moldovan",
                        "slug": "D.-Moldovan",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Moldovan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Moldovan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u201d The identification of these patterns by hand inspired a body of work in which this initial set of extraction patterns is used to seed a bootstrapping process that automatically acquires additional patterns for is-a or part-whole relations [33, 39, 81]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6219536,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "966f28e435e6a5315e3d7918d3cdf2269f7fa3c8",
            "isKey": false,
            "numCitedBy": 304,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "An important problem in knowledge discovery from text is the automatic extraction of semantic relations. This paper presents a supervised, semantically intensive, domain independent approach for the automatic detection of part-whole relations in text. First an algorithm is described that identifies lexico-syntactic patterns that encode part-whole relations. A difficulty is that these patterns also encode other semantic relations, and a learning method is necessary to discriminate whether or not a pattern contains a part-whole relation. A large set of training examples have been annotated and fed into a specialized learning system that learns classification rules. The rules are learned through an iterative semantic specialization (ISS) method applied to noun phrase constituents. Classification rules have been generated this way for different patterns such as genitives, noun compounds, and noun phrases containing prepositional phrases to extract part-whole relations from them. The applicability of these rules has been tested on a test corpus obtaining an overall average precision of 80.95% and recall of 75.91%. The results demonstrate the importance of word sense disambiguation for this task. They also demonstrate that different lexico-syntactic patterns encode different semantic information and should be treated separately in the sense that different clarification rules apply to different patterns."
            },
            "slug": "Automatic-Discovery-of-Part-Whole-Relations-Girju-Badulescu",
            "title": {
                "fragments": [],
                "text": "Automatic Discovery of Part-Whole Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This paper presents a supervised, semantically intensive, domain independent approach for the automatic detection of part-whole relations in text and demonstrates the importance of word sense disambiguation for this task."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1983124"
                        ],
                        "name": "Cody C. T. Kwok",
                        "slug": "Cody-C.-T.-Kwok",
                        "structuredName": {
                            "firstName": "Cody",
                            "lastName": "Kwok",
                            "middleNames": [
                                "C.",
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cody C. T. Kwok"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Redundancy-based techniques for extraction were first explored by the Mulder [51] and AskMSR [31] question answering systems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52096276,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7bdb08efd640311ad18466a80498c78267f886ca",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The wealth of information on the web makes it an attractive resource for seeking quick answers to simple, factual questions such as \\who was the rst American in space?\" or \\what is the second tallest mountain in the world?\" Yet today's most advanced web search services (e.g., Google and AskJeeves) make it surprisingly tedious to locate answers to such questions. In this paper, we extend question-answering techniques, rst studied in the information retrieval literature, to the web and experimentally evaluate their performance. First we introduce Mulder, which we believe to be the rst general-purpose, fully-automated questionanswering system available on the web. Second, we describe Mulder's architecture, which relies on multiple search-engine queries, natural-language parsing, and a novel voting procedure to yield reliable answers coupled with high recall. Finally, we compare Mulder's performance to that of Google and AskJeeves on questions drawn from the TREC-8 question track. We nd that Mulder's recall is more than a factor of three higher than that of AskJeeves. In addition, we nd that Google requires 6.6 times as much user e ort to achieve the same level of recall as Mulder."
            },
            "slug": "Scaling-question-answering-to-the-Web-Kwok-Etzioni",
            "title": {
                "fragments": [],
                "text": "Scaling question answering to the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Mulder is introduced, which is believed to be the first general-purpose, fully-automated questionanswering system available on the web, and its architecture is described, which relies on multiple search-engine queries, natural-language parsing, and a novel voting procedure to yield reliable answers coupled with high recall."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741453"
                        ],
                        "name": "A. Culotta",
                        "slug": "A.-Culotta",
                        "structuredName": {
                            "firstName": "Aron",
                            "lastName": "Culotta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Culotta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To obtain the probability at each position of a linear-chain CRF, the constrained forward-backward technique described in [22] is used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6715557,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2ca689c4eb4284ec10f27cc8c6e5bae73569140",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Information extraction techniques automatically create structured databases from unstructured data sources, such as the Web or newswire documents. Despite the successes of these systems, accuracy will always be imperfect. For many reasons, it is highly desirable to accurately estimate the confidence the system has in the correctness of each extracted field. The information extraction system we evaluate is based on a linear-chain conditional random field (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a Markov model. We implement several techniques to estimate the confidence of both extracted fields and entire multi-field records, obtaining an average precision of 98% for retrieving correct fields and 87% for multi-field records."
            },
            "slug": "Confidence-Estimation-for-Information-Extraction-Culotta-McCallum",
            "title": {
                "fragments": [],
                "text": "Confidence Estimation for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work evaluates a information extraction system based on a linear-chain conditional random field (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a Markov model."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 176
                            }
                        ],
                        "text": "Previous work has noted that distinguished relations, such as hypernymy (is-a) and meronymy (part-whole), are often expressed using a small number of lexico-syntactic patterns [41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15763200,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "dbfd191afbbc8317577cbc44afe7156df546e143",
            "isKey": false,
            "numCitedBy": 3647,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested."
            },
            "slug": "Automatic-Acquisition-of-Hyponyms-from-Large-Text-Hearst",
            "title": {
                "fragments": [],
                "text": "Automatic Acquisition of Hyponyms from Large Text Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest are identified."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Information extraction technology has been developed for a variety of text collections, ranging from domain-specific corpora [36, 63, 70, 82] to newspaper articles [1, 78, 99] to the general-purpose Brown Corpus [75]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60585486,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca198cc81878fd036c7b97ee10441f1d09839f65",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "An enormous amount of knowledge is needed to infer the meaning of unrestricted natural language. The problem can be reduced to a manageable size by restricting attention to a specific {\\em domain}, which is a corpus of texts together with a predefined set of {\\em concepts} that are of interest to that domain. Two widely different domains are used to illustrate this domain-specific approach. One domain is a collection of Wall Street Journal articles in which the target concept is management succession events: identifying persons moving into corporate management positions or moving out. A second domain is a collection of hospital discharge summaries in which the target concepts are various classes of diagnosis or symptom. The goal of an information extraction system is to identify references to the concept of interest for a particular domain. A key knowledge source for this purpose is a set of text analysis rules based on the vocabulary, semantic classes, and writing style peculiar to the domain. This thesis presents CRYSTAL, an implemented system that automatically induces domain-specific text analysis rules from training examples. CRYSTAL learns rules that approach the performance of hand-coded rules, are robust in the face of noise and inadequate features, and require only a modest amount of training data. CRYSTAL belongs to a class of machine learning algorithms called covering algorithms, and presents a novel control strategy with time and space complexities that are independent of the number of features. CRYSTAL navigates efficiently through an extremely large space of possible rules. CRYSTAL also demonstrates that expressive rule representation is essential for high performance, robust text analysis rules. While simple rules are adequate to capture the most salient regularities in the training data, high performance can only be achieved when rules are expressive enough to reflect the subtlety and variability of unrestricted natural language."
            },
            "slug": "Learning-text-analysis-rules-for-domain-specific-Soderland",
            "title": {
                "fragments": [],
                "text": "Learning text analysis rules for domain-specific natural language processing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This thesis presents CRYSTAL, an implemented system that automatically induces domain-specific text analysis rules from training examples that approach the performance of hand-coded rules, are robust in the face of noise and inadequate features, and require only a modest amount of training data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2550658"
                        ],
                        "name": "Yusuke Shinyama",
                        "slug": "Yusuke-Shinyama",
                        "structuredName": {
                            "firstName": "Yusuke",
                            "lastName": "Shinyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yusuke Shinyama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714612"
                        ],
                        "name": "S. Sekine",
                        "slug": "S.-Sekine",
                        "structuredName": {
                            "firstName": "Satoshi",
                            "lastName": "Sekine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sekine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8186401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84364e5660d0db7fe4654febb1e8aba0399835b5",
            "isKey": false,
            "numCitedBy": 283,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We are trying to extend the boundary of Information Extraction (IE) systems. Existing IE systems require a lot of time and human effort to tune for a new scenario. Preemptive Information Extraction is an attempt to automatically create all feasible IE systems in advance without human intervention. We propose a technique called Unrestricted Relation Discovery that discovers all possible relations from texts and presents them as tables. We present a preliminary system that obtains reasonably good results."
            },
            "slug": "Preemptive-Information-Extraction-using-Relation-Shinyama-Sekine",
            "title": {
                "fragments": [],
                "text": "Preemptive Information Extraction using Unrestricted Relation Discovery"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A technique called Unrestricted Relation Discovery is proposed that discovers all possible relations from texts and presents them as tables in order to extend the boundary of Information Extraction systems."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064774858"
                        ],
                        "name": "Deepak Ravichandran",
                        "slug": "Deepak-Ravichandran",
                        "structuredName": {
                            "firstName": "Deepak",
                            "lastName": "Ravichandran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deepak Ravichandran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144547315"
                        ],
                        "name": "E. Hovy",
                        "slug": "E.-Hovy",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Hovy",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hovy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 100
                            }
                        ],
                        "text": "DIPRE [Brin, 1998], SNOWBALL [Agichtein and Gravano, 2000], and Webbased question answering systems [Ravichandran and Hovy, 2002] further reduced manual labor needed for relationspecific text extraction by requiring only a small set of tagged seed instances or a few hand-crafted extraction patterns, per relation, to launch the training process."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 226541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fbd0e0ad4e06902b10b6a157b9db92df577720f1",
            "isKey": false,
            "numCitedBy": 933,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we explore the power of surface text patterns for open-domain question answering systems. In order to obtain an optimal set of patterns, we have developed a method for learning such patterns automatically. A tagged corpus is built from the Internet in a bootstrapping process by providing a few hand-crafted examples of each question type to Altavista. Patterns are then automatically extracted from the returned documents and standardized. We calculate the precision of each pattern, and the average precision for each question type. These patterns are then applied to find answers to new questions. Using the TREC-10 question set, we report results for two cases: answers determined from the TREC-10 corpus and from the web."
            },
            "slug": "Learning-surface-text-patterns-for-a-Question-Ravichandran-Hovy",
            "title": {
                "fragments": [],
                "text": "Learning surface text patterns for a Question Answering System"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper has developed a method for learning an optimal set of surface text patterns automatically from a tagged corpus, and calculates the precision of each pattern, and the average precision for each question type."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2169401916"
                        ],
                        "name": "J.-T. Kim",
                        "slug": "J.-T.-Kim",
                        "structuredName": {
                            "firstName": "J.-T.",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J.-T. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40497400"
                        ],
                        "name": "D. Moldovan",
                        "slug": "D.-Moldovan",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Moldovan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Moldovan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59002598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e80b34a55aa56578f9a4f27ea207f8c42c93a378",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "A knowledge acquisition tool to extract semantic patterns for a memory-based information retrieval system is presented. The major goal of this tool is to facilitate the construction of a large knowledge base of semantic patterns. The system acquires semantic patterns from texts with a small amount of user interaction. It acquires new phrasal patterns from the input text, maps each element of the pattern to a meaning frame, generalizes the acquired pattern, and merges it into the current knowledge base. Interaction with the user is introduced at some decision points, where the ambiguity cannot be resolved automatically without other pieces of predefined knowledge. The acquisition process is described in detail, and a preliminary experimental result is discussed.<<ETX>>"
            },
            "slug": "Acquisition-of-semantic-patterns-for-information-Kim-Moldovan",
            "title": {
                "fragments": [],
                "text": "Acquisition of semantic patterns for information extraction from corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A knowledge acquisition tool to extract semantic patterns for a memory-based information retrieval system is presented to facilitate the construction of a large knowledge base of semantic patterns."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 9th IEEE Conference on Artificial Intelligence for Applications"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724629"
                        ],
                        "name": "Marius Pasca",
                        "slug": "Marius-Pasca",
                        "structuredName": {
                            "firstName": "Marius",
                            "lastName": "Pasca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marius Pasca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744846"
                        ],
                        "name": "Jeffrey P. Bigham",
                        "slug": "Jeffrey-P.-Bigham",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Bigham",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey P. Bigham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2049693"
                        ],
                        "name": "Andrei Lifchits",
                        "slug": "Andrei-Lifchits",
                        "structuredName": {
                            "firstName": "Andrei",
                            "lastName": "Lifchits",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrei Lifchits"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777385"
                        ],
                        "name": "Alpa Jain",
                        "slug": "Alpa-Jain",
                        "structuredName": {
                            "firstName": "Alpa",
                            "lastName": "Jain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alpa Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[62], and Bunescu and Mooney [12] sought to reduce the amount of manual labor necessary to perform relationspecific extraction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Modern IE systems [33, 62, 86] have demonstrated the ability to extract a large number of facts from the Web."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Pasca [62] extracted 1 million instances of the BornOnDate relation from 100 million Web pages with a precision of around 90%."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Two recent Web IE systems [12, 62], demonstrated the ability to extract relations in the weakly-supervised setting using only 8-10 training examples per relation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17121460,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "708355e97d0a1d9e074261cd6e4ec6cdf2d55031",
            "isKey": true,
            "numCitedBy": 110,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In a new approach to large-scale extraction of facts from unstructured text, distributional similarities become an integral part of both the iterative acquisition of high-coverage contextual extraction patterns, and the validation and ranking of candidate facts. The evaluation measures the quality and coverage of facts extracted from one hundred million Web documents, starting from ten seed facts and using no additional knowledge, lexicons or complex tools."
            },
            "slug": "Names-and-Similarities-on-the-Web:-Fact-Extraction-Pasca-Lin",
            "title": {
                "fragments": [],
                "text": "Names and Similarities on the Web: Fact Extraction in the Fast Lane"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "In a new approach to large-scale extraction of facts from unstructured text, distributional similarities become an integral part of both the iterative acquisition of high-coverage contextual extraction patterns and the validation and ranking of candidate facts."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1983124"
                        ],
                        "name": "Cody C. T. Kwok",
                        "slug": "Cody-C.-T.-Kwok",
                        "structuredName": {
                            "firstName": "Cody",
                            "lastName": "Kwok",
                            "middleNames": [
                                "C.",
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cody C. T. Kwok"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5456456,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "016e9cc85c658c6a69710b4c617609ad2a5d3a74",
            "isKey": false,
            "numCitedBy": 359,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "The wealth of information on the web makes it an attractive resource for seeking quick answers to simple, factual questions such as &quote;who was the first American in space?&quote; or &quote;what is the second tallest mountain in the world?&quote; Yet today's most advanced web search services (e.g., Google and AskJeeves) make it surprisingly tedious to locate answers to such questions. In this paper, we extend question-answering techniques, first studied in the information retrieval literature, to the web and experimentally evaluate their performance.First we introduce Mulder, which we believe to be the first general-purpose, fully-automated question-answering system available on the web. Second, we describe Mulder's architecture, which relies on multiple search-engine queries, natural-language parsing, and a novel voting procedure to yield reliable answers coupled with high recall. Finally, we compare Mulder's performance to that of Google and AskJeeves on questions drawn from the TREC-8 question answering track. We find that Mulder's recall is more than a factor of three higher than that of AskJeeves. In addition, we find that Google requires 6.6 times as much user effort to achieve the same level of recall as Mulder."
            },
            "slug": "Scaling-question-answering-to-the-web-Kwok-Etzioni",
            "title": {
                "fragments": [],
                "text": "Scaling question answering to the web"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Mulder is introduced, which is believed to be the first general-purpose, fully-automated question-answering system available on the web, and its architecture is described, which relies on multiple search-engine queries, natural-language parsing, and a novel voting procedure to yield reliable answers coupled with high recall."
            },
            "venue": {
                "fragments": [],
                "text": "TOIS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691993"
                        ],
                        "name": "E. Riloff",
                        "slug": "E.-Riloff",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Riloff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riloff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15894892,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "acec622ca4fb7e01a56116522d35ded149969d0a",
            "isKey": false,
            "numCitedBy": 762,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Many corpus-based natural language processing systems rely on text corpora that have been manually annotated with syntactic or semantic tags. In particular, all previous dictionary construction systems for information extraction have used an annotated training corpus or some form of annotated input. We have developed a system called AutoSlog-TS that creates dictionaries of extraction patterns using only untagged text. AutoSlog-TS is based on the AutoSlog system, which generated extraction patterns using annotated text and a set of heuristic rules. By adapting AutoSlog and combining it with statistical techniques, we eliminated its dependency on tagged text. In experiments with the MUG-4 terrorism domain, AutoSlog-TS created a dictionary of extraction patterns that performed comparably to a dictionary created by AutoSlog, using only preclassified texts as input."
            },
            "slug": "Automatically-Generating-Extraction-Patterns-from-Riloff",
            "title": {
                "fragments": [],
                "text": "Automatically Generating Extraction Patterns from Untagged Text"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has developed a system called AutoSlog-TS that creates dictionaries of extraction patterns using only untagged text, and in experiments with the MUG-4 terrorism domain, created a dictionary of extraction pattern that performed comparably to a dictionary created by autoSlog, using only preclassified texts as input."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI, Vol. 2"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145262164"
                        ],
                        "name": "David Fisher",
                        "slug": "David-Fisher",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fisher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Fisher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51080653"
                        ],
                        "name": "J. Aseltine",
                        "slug": "J.-Aseltine",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Aseltine",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Aseltine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1925215"
                        ],
                        "name": "W. Lehnert",
                        "slug": "W.-Lehnert",
                        "structuredName": {
                            "firstName": "Wendy",
                            "lastName": "Lehnert",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Lehnert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9168228,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8245f6099f547008522ebbe6fb813d8132085746",
            "isKey": false,
            "numCitedBy": 409,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the central knowledge sources of an information extraction (IE) system IS a dictionary of linguistic patterns that can be used to identify references to relevant information in a text Automatic creation of conceptual dictionaries is important for portability and scalability of an IE system This paper describes CRYSTAL, a system which automatically induces a dictionary of \"concept-node definitions\" sufficient to identify relevant information from a training corpus Each of these concept-node definitions is generalized as far as possible without producing errors, so that a minimum number of dictionary entries cover the positive training instances Because it tests the accuracy of each proposed definition, CRYSTAL can often surpass human intuitions in creating reliable extraction rules."
            },
            "slug": "CRYSTAL:-Inducing-a-Conceptual-Dictionary-Soderland-Fisher",
            "title": {
                "fragments": [],
                "text": "CRYSTAL: Inducing a Conceptual Dictionary"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "CRYSTAL is described, a system which automatically induces a dictionary of \"concept-node definitions\" sufficient to identify relevant information from a training corpus that can often surpass human intuitions in creating reliable extraction rules."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682136"
                        ],
                        "name": "Fuchun Peng",
                        "slug": "Fuchun-Peng",
                        "structuredName": {
                            "firstName": "Fuchun",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fuchun Peng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The set of features used by O-nb and O-crf is similar to those used by other state-of-theart relation extraction systems [12, 63] with a few exceptions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Keeping pace with progress in machine learning, a diverse set of learning algorithms has been applied to the task of IE, including support vector machines [13, 21, 99], hidden Markov models [36], conditional random fields [63, 23] and Markov logic networks [40]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Information extraction technology has been developed for a variety of text collections, ranging from domain-specific corpora [36, 63, 70, 82] to newspaper articles [1, 78, 99] to the general-purpose Brown Corpus [75]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13169886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ed5f7ff2ea4bfae8f93b17546a71631880277d0",
            "isKey": false,
            "numCitedBy": 335,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance. This paper employs Conditional Random Fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers. The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration. This paper makes an empirical exploration of several factors, including variations on Gaussian, exponential and hyperbolic-L1 priors for improved regularization, and several classes of features and Markov order. On a standard benchmark data set, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares even more favorably against HMMs."
            },
            "slug": "Accurate-Information-Extraction-from-Research-using-Peng-McCallum",
            "title": {
                "fragments": [],
                "text": "Accurate Information Extraction from Research Papers using Conditional Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "New state-of-the-art performance is achieved on a standard benchmark data set, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2339397"
                        ],
                        "name": "Michele Banko",
                        "slug": "Michele-Banko",
                        "structuredName": {
                            "firstName": "Michele",
                            "lastName": "Banko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michele Banko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145022783"
                        ],
                        "name": "E. Brill",
                        "slug": "E.-Brill",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145580839"
                        ],
                        "name": "Jimmy J. Lin",
                        "slug": "Jimmy-J.-Lin",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Lin",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy J. Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067947449"
                        ],
                        "name": "Andrew Y. Ng",
                        "slug": "Andrew-Y.-Ng",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Ng",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Y. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Redundancy-based techniques for extraction were first explored by the Mulder [51] and AskMSR [31] question answering systems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9391992,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8f9537a6cfb1ba2501c1c6ac3b114c274534095",
            "isKey": false,
            "numCitedBy": 338,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a question answering system that is designed to capitalize on the tremendous amount of data that is now available online. Most question answering systems use a wide variety of linguistic resources. We focus instead on the redundancy available in large corpora as an important resource. We use this redundancy to simplify the query rewrites that we need to use, and to support answer mining from returned snippets. Our system performs quite well given the simplicity of the techniques being utilized. Experimental results show that question answering accuracy can be greatly improved by analyzing more and more matching passages. Simple passage ranking and n-gram extraction techniques work well in our system making it efficient to use with many backend retrieval engines."
            },
            "slug": "Web-question-answering:-is-more-always-better-Dumais-Banko",
            "title": {
                "fragments": [],
                "text": "Web question answering: is more always better?"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This paper describes a question answering system that is designed to capitalize on the tremendous amount of data that is now available online, and uses the redundancy available in large corpora as an important resource to simplify the query rewrites and support answer mining from returned snippets."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '02"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1893601"
                        ],
                        "name": "Siqing Du",
                        "slug": "Siqing-Du",
                        "structuredName": {
                            "firstName": "Siqing",
                            "lastName": "Du",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siqing Du"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1921916"
                        ],
                        "name": "D. Metzler",
                        "slug": "D.-Metzler",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Metzler",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Metzler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 44823534,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d83252162d5362cf39b79e6936a314bc362ae24",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a natural language system that extracts entity relationship diagram components from natural language database design documents. The system is a fully integrated composite of existing, publicly available components including a parser, WordNet and Google web corpus search facilities, and a novel rule-based tuple-extraction process. The system differs from previous approaches in being fully automatic (as opposed to approaches requiring human disambiguation or other interaction) and in providing a higher level of performance than previously reported results."
            },
            "slug": "An-Automated-Multi-component-Approach-to-Extracting-Du-Metzler",
            "title": {
                "fragments": [],
                "text": "An Automated Multi-component Approach to Extracting Entity Relationships from Database Requirement Specification Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "This paper describes a natural language system that extracts entity relationship diagram components from natural language database design documents using a novel rule-based tuple-extraction process and a fully integrated composite of existing, publicly available components."
            },
            "venue": {
                "fragments": [],
                "text": "NLDB"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741453"
                        ],
                        "name": "A. Culotta",
                        "slug": "A.-Culotta",
                        "structuredName": {
                            "firstName": "Aron",
                            "lastName": "Culotta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Culotta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38765847"
                        ],
                        "name": "Jonathan Betz",
                        "slug": "Jonathan-Betz",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Betz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Betz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Linear-chain CRFs have been applied to a variety of sequential text processing tasks including named-entity recognition, part-ofspeech tagging, word segmentation, semantic role identification, and traditional forms of relation extraction [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Keeping pace with progress in machine learning, a diverse set of learning algorithms has been applied to the task of IE, including support vector machines [13, 21, 99], hidden Markov models [36], conditional random fields [63, 23] and Markov logic networks [40]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 311673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "92d520c905ba682734e0d1f4999e19428b27eb3e",
            "isKey": false,
            "numCitedBy": 210,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In order for relation extraction systems to obtain human-level performance, they must be able to incorporate relational patterns inherent in the data (for example, that one's sister is likely one's mother's daughter, or that children are likely to attend the same college as their parents). Hand-coding such knowledge can be time-consuming and inadequate. Additionally, there may exist many interesting, unknown relational patterns that both improve extraction performance and provide insight into text. We describe a probabilistic extraction model that provides mutual benefits to both \"top-down\" relational pattern discovery and \"bottom-up\" relation extraction."
            },
            "slug": "Integrating-Probabilistic-Extraction-Models-and-to-Culotta-McCallum",
            "title": {
                "fragments": [],
                "text": "Integrating Probabilistic Extraction Models and Data Mining to Discover Relations and Patterns in Text"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A probabilistic extraction model is described that provides mutual benefits to both \"top-down\" relational pattern discovery and \"bottom-up\" relation extraction."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691993"
                        ],
                        "name": "E. Riloff",
                        "slug": "E.-Riloff",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Riloff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riloff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144494993"
                        ],
                        "name": "R. Jones",
                        "slug": "R.-Jones",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 125
                            }
                        ],
                        "text": "Information extraction technology has been developed for a variety of text collections, ranging from domain-specific corpora [36, 63, 70, 82] to newspaper articles [1, 78, 99] to the general-purpose Brown Corpus [75]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 55
                            }
                        ],
                        "text": "Brin [11], Agichtein and Gravano [1], Riloff and Jones [70], Pasca et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1053009,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41e936981f5a2d55bfec0143e9a15e23ad96436b",
            "isKey": false,
            "numCitedBy": 890,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Information extraction systems usually require two dictionaries: a semantic lexicon and a dictionary of extraction patterns for the domain. We present a multilevel bootstrapping algorithm that generates both the semantic lexicon and extraction patterns simultaneously. As input, our technique requires only unannotated training texts and a handful of seed words for a category. We use a mutual bootstrapping technique to alternately select the best extraction pattern for the category and bootstrap its extractions into the semantic lexicon, which is the basis for selecting the next extraction pattern. To make this approach more robust, we add a second level of bootstrapping (metabootstrapping) that retains only the most reliable lexicon entries produced by mutual bootstrapping and then restarts the process. We evaluated this multilevel bootstrapping technique on a collection of corporate web pages and a corpus of terrorism news articles. The algorithm produced high-quality dictionaries for several semantic categories."
            },
            "slug": "Learning-Dictionaries-for-Information-Extraction-by-Riloff-Jones",
            "title": {
                "fragments": [],
                "text": "Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A multilevel bootstrapping algorithm is presented that generates both the semantic lexicon and extraction patterns simultaneously simultaneously and produces high-quality dictionaries for several semantic categories."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679784"
                        ],
                        "name": "Fabian M. Suchanek",
                        "slug": "Fabian-M.-Suchanek",
                        "structuredName": {
                            "firstName": "Fabian",
                            "lastName": "Suchanek",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabian M. Suchanek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686448"
                        ],
                        "name": "Gjergji Kasneci",
                        "slug": "Gjergji-Kasneci",
                        "structuredName": {
                            "firstName": "Gjergji",
                            "lastName": "Kasneci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gjergji Kasneci"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751591"
                        ],
                        "name": "G. Weikum",
                        "slug": "G.-Weikum",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Weikum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Weikum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We measured precision and recall for ten relations that have often been the focus of study of previously published work in IE, specifically that of Dipre [11], Snowball [1], KnowItAll [33], Yago [85, 86] and Bunescu\u2019s SSK relation extraction algorithm [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207163173,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "00a3f6924f90fcd77e6e7e6534b957a75d0ced07",
            "isKey": false,
            "numCitedBy": 3480,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "We present YAGO, a light-weight and extensible ontology with high coverage and quality. YAGO builds on entities and relations and currently contains more than 1 million entities and 5 million facts. This includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as HASONEPRIZE). The facts have been automatically extracted from Wikipedia and unified with WordNet, using a carefully designed combination of rule-based and heuristic methods described in this paper. The resulting knowledge base is a major step beyond WordNet: in quality by adding knowledge about individuals like persons, organizations, products, etc. with their semantic relationships - and in quantity by increasing the number of facts by more than an order of magnitude. Our empirical evaluation of fact correctness shows an accuracy of about 95%. YAGO is based on a logically clean model, which is decidable, extensible, and compatible with RDFS. Finally, we show how YAGO can be further extended by state-of-the-art information extraction techniques."
            },
            "slug": "Yago:-a-core-of-semantic-knowledge-Suchanek-Kasneci",
            "title": {
                "fragments": [],
                "text": "Yago: a core of semantic knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "YAGO builds on entities and relations and currently contains more than 1 million entities and 5 million facts, which includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as HASONEPRIZE)."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2404386"
                        ],
                        "name": "Lenhart K. Schubert",
                        "slug": "Lenhart-K.-Schubert",
                        "structuredName": {
                            "firstName": "Lenhart",
                            "lastName": "Schubert",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lenhart K. Schubert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2002629,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb0968ab69b8655d442f17cdfe317ce3bfebd789",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "As one attack on the \"knowledge acquisition bottleneck\", we are attempting to exploit a largely untapped source of general knowledge in texts, lying at a level beneath the explicit assertional content. This knowledge consists of relationships implied to be possible in the world, or, under certain conditions, implied to be normal or commonplace in the world. The goal of the work reported is to derive such general world knowledge (initially, from Penn Tree-bank corpora) in two stages: first, we derive general \"possibilistic\" propositions from noun phrases and clauses; then we try to derive stronger generalizations, based on the nature and statistical distribution of the possibilistic claims obtained in the first phase. Here we report preliminary results of the first phase, which indicate the feasibility of our project, and its likely limitations."
            },
            "slug": "Can-we-derive-general-world-knowledge-from-texts-Schubert",
            "title": {
                "fragments": [],
                "text": "Can we derive general world knowledge from texts"
            },
            "tldr": {
                "abstractSimilarityScore": 33,
                "text": "Preliminary results of the first phase are reported, which indicate the feasibility of the project, and its likely limitations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679784"
                        ],
                        "name": "Fabian M. Suchanek",
                        "slug": "Fabian-M.-Suchanek",
                        "structuredName": {
                            "firstName": "Fabian",
                            "lastName": "Suchanek",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabian M. Suchanek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686448"
                        ],
                        "name": "Gjergji Kasneci",
                        "slug": "Gjergji-Kasneci",
                        "structuredName": {
                            "firstName": "Gjergji",
                            "lastName": "Kasneci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gjergji Kasneci"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751591"
                        ],
                        "name": "G. Weikum",
                        "slug": "G.-Weikum",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Weikum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Weikum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We measured precision and recall for ten relations that have often been the focus of study of previously published work in IE, specifically that of Dipre [11], Snowball [1], KnowItAll [33], Yago [85, 86] and Bunescu\u2019s SSK relation extraction algorithm [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "These observations have sparked several extraction efforts [64, 86, 96] that focus on mining these semi-structured resources as opposed to the unstructured text of Wikipedia articles."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Yago [86] is another system that uses properties specific to Wikipedia in order to extract a large set of relational data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "These statistics reflect measurements taken in July 2008, as reported in [86]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Modern IE systems [33, 62, 86] have demonstrated the ability to extract a large number of facts from the Web."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 205702077,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09f6fa1869be4d3d9188d1313061602038cb97d4",
            "isKey": true,
            "numCitedBy": 869,
            "numCiting": 188,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "YAGO:-A-Large-Ontology-from-Wikipedia-and-WordNet-Suchanek-Kasneci",
            "title": {
                "fragments": [],
                "text": "YAGO: A Large Ontology from Wikipedia and WordNet"
            },
            "venue": {
                "fragments": [],
                "text": "J. Web Semant."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683562"
                        ],
                        "name": "Preslav Nakov",
                        "slug": "Preslav-Nakov",
                        "structuredName": {
                            "firstName": "Preslav",
                            "lastName": "Nakov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Preslav Nakov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This observation has been exploited with success in a variety of areas including ambiguity resolution [4, 57, 91], language modeling [47] and thesaurus construction [24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 552136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f92bcebde247b1c51036e208e43703b9fb165b2a",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has shown that very large corpora can act as training data for NLP algorithms even without explicit labels. In this paper we show how the use of surface features and paraphrases in queries against search engines can be used to infer labels for structural ambiguity resolution tasks. Using unsupervised algorithms, we achieve 84% precision on PP-attachment and 80% on noun compound coordination."
            },
            "slug": "Using-the-Web-as-an-Implicit-Training-Set:-to-Nakov-Hearst",
            "title": {
                "fragments": [],
                "text": "Using the Web as an Implicit Training Set: Application to Structural Ambiguity Resolution"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper shows how the use of surface features and paraphrases in queries against search engines can be used to infer labels for structural ambiguity resolution tasks using unsupervised algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2971806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3f9a848dca0a80ef64987a9dd511ee6b7e19cd1",
            "isKey": false,
            "numCitedBy": 565,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose an unsupervised method for discovering inference rules from text, such as \"X is author of Y \u2248 X wrote Y\", \"X solved Y \u2248 X found a solution to Y\", and \"X caused Y \u2248 Y is triggered by X\". Inference rules are extremely important in many fields such as natural language processing, information retrieval, and artificial intelligence in general. Our algorithm is based on an extended version of Harris' Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus."
            },
            "slug": "DIRT-@SBT@discovery-of-inference-rules-from-text-Lin-Pantel",
            "title": {
                "fragments": [],
                "text": "DIRT @SBT@discovery of inference rules from text"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This paper proposes an unsupervised method for discovering inference rules from text, based on an extended version of Harris' Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3139133"
                        ],
                        "name": "Razvan C. Bunescu",
                        "slug": "Razvan-C.-Bunescu",
                        "structuredName": {
                            "firstName": "Razvan",
                            "lastName": "Bunescu",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Razvan C. Bunescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3179848,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8dff21517f7ac744089a260dbc3e2f48649e3119",
            "isKey": false,
            "numCitedBy": 305,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new approach to relation extraction that requires only a handful of training examples. Given a few pairs of named entities known to exhibit or not exhibit a particular relation, bags of sentences containing the pairs are extracted from the web. We extend an existing relation extraction method to handle this weaker form of supervision, and present experimental results demonstrating that our approach can reliably extract relations from web documents."
            },
            "slug": "Learning-to-Extract-Relations-from-the-Web-using-Bunescu-Mooney",
            "title": {
                "fragments": [],
                "text": "Learning to Extract Relations from the Web using Minimal Supervision"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "An existing relation extraction method is extended to handle this weaker form of supervision, and experimental results demonstrate that the approach can reliably extract relations from web documents."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143694777"
                        ],
                        "name": "Frank Keller",
                        "slug": "Frank-Keller",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Keller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Keller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1992275"
                        ],
                        "name": "Maria Lapata",
                        "slug": "Maria-Lapata",
                        "structuredName": {
                            "firstName": "Maria",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maria Lapata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4349038"
                        ],
                        "name": "Olga Ourioupina",
                        "slug": "Olga-Ourioupina",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Ourioupina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olga Ourioupina"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This observation has been exploited with success in a variety of areas including ambiguity resolution [4, 57, 91], language modeling [47] and thesaurus construction [24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1625546,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79883c30922037c93392ddbbecc6fd35674a6a1c",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows that the web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the web by querying a search engine. We evaluate this method by demonstrating that web frequencies and correlate with frequencies obtained from a carefully edited, balanced corpus. We also perform a task-based evaluation, showing that web frequencies can reliably predict human plausibility judgments."
            },
            "slug": "Using-the-Web-to-Overcome-Data-Sparseness-Keller-Lapata",
            "title": {
                "fragments": [],
                "text": "Using the Web to Overcome Data Sparseness"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "It is shown that the web can be employed to obtain frequencies for bigrams that are unseen in a given corpus by demonstrating that web frequencies and correlate with frequencies obtained from a carefully edited, balanced corpus."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2339397"
                        ],
                        "name": "Michele Banko",
                        "slug": "Michele-Banko",
                        "structuredName": {
                            "firstName": "Michele",
                            "lastName": "Banko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michele Banko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145022783"
                        ],
                        "name": "E. Brill",
                        "slug": "E.-Brill",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This observation has been exploited with success in a variety of areas including ambiguity resolution [4, 57, 91], language modeling [47] and thesaurus construction [24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6645623,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7628b62d64d2e5c33a13a5a473bc41b2391c1ebc",
            "isKey": false,
            "numCitedBy": 693,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "The amount of readily available on-line text has reached hundreds of billions of words and continues to grow. Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less. In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used. We are fortunate that for this particular application, correctly labeled training data is free. Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost."
            },
            "slug": "Scaling-to-Very-Very-Large-Corpora-for-Natural-Banko-Brill",
            "title": {
                "fragments": [],
                "text": "Scaling to Very Very Large Corpora for Natural Language Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper examines methods for effectively exploiting very large corpora when labeled data comes at a cost, and evaluates the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambigsuation."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A handful of approaches to knowledge acquisition [21, 54, 75] have tried to extract relationships by analyzing the full structure and meaning (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The Dirt system [54] used the Minipar parser to produce tuples relating two entities in a single sentence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Preferring speed at first, we began with the extraction heuristics previously implemented as part of the Dirt system [54], which used the speedy Minipar parser to identify textual paraphrases."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12363172,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b4317b8a4490c84301907a61f5b8ebb26ab8828d",
            "isKey": false,
            "numCitedBy": 627,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the main challenges in question-answering is the potential mismatch between the expressions in questions and the expressions in texts. While humans appear to use inference rules such as \u2018X writes Y\u2019 implies \u2018X is the author of Y\u2019 in answering questions, such rules are generally unavailable to question-answering systems due to the inherent difficulty in constructing them. In this paper, we present an unsupervised algorithm for discovering inference rules from text. Our algorithm is based on an extended version of Harris\u2019 Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus. Essentially, if two paths tend to link the same set of words, we hypothesize that their meanings are similar. We use examples to show that our system discovers many inference rules easily missed by humans."
            },
            "slug": "Discovery-of-inference-rules-for-question-answering-Lin-Pantel",
            "title": {
                "fragments": [],
                "text": "Discovery of inference rules for question-answering"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents an unsupervised algorithm for discovering inference rules from text based on an extended version of Harris\u2019 Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar."
            },
            "venue": {
                "fragments": [],
                "text": "Natural Language Engineering"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144135485"
                        ],
                        "name": "Tom. Mitchell",
                        "slug": "Tom.-Mitchell",
                        "structuredName": {
                            "firstName": "Tom.",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom. Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207228399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "278841ab0cb24c1abcb75e363aeed1fa741c8cc4",
            "isKey": false,
            "numCitedBy": 5471,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of using a large unlabeled sample to boost performance of a learning algorit,hrn when only a small set of labeled examples is available. In particular, we consider a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views. For example, the description of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks t,hat point to that page. We assume that either view of the example would be sufficient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples. Specifically, the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algorithm\u2019s predictions on new unlabeled examples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice. *This research was supported in part by the DARPA HPKB program under contract F30602-97-1-0215 and by NSF National Young investigator grant CCR-9357793. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. TO copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. COLT 98 Madison WI USA Copyright ACM 1998 l-58113-057--0/98/ 7...%5.00 92 Tom Mitchell School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-3891 mitchell+@cs.cmu.edu"
            },
            "slug": "Combining-labeled-and-unlabeled-data-with-Blum-Mitchell",
            "title": {
                "fragments": [],
                "text": "Combining labeled and unlabeled data with co-training"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A PAC-style analysis is provided for a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views, to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2013145"
                        ],
                        "name": "Jinxiu Chen",
                        "slug": "Jinxiu-Chen",
                        "structuredName": {
                            "firstName": "Jinxiu",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinxiu Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719916"
                        ],
                        "name": "D. Ji",
                        "slug": "D.-Ji",
                        "structuredName": {
                            "firstName": "Dong-Hong",
                            "lastName": "Ji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2715551"
                        ],
                        "name": "Zheng-Yu Niu",
                        "slug": "Zheng-Yu-Niu",
                        "structuredName": {
                            "firstName": "Zheng-Yu",
                            "lastName": "Niu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zheng-Yu Niu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1973525,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b46f401b17ce3151cfee660561588335bef019dd",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Shortage of manually labeled data is an obstacle to supervised relation extraction methods. In this paper we investigate a graph based semi-supervised learning algorithm, a label propagation (LP) algorithm, for relation extraction. It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph, and tries to obtain a labeling function to satisfy two constraints: 1) it should be fixed on the labeled nodes, 2) it should be smooth on the whole graph. Experiment results on the ACE corpus showed that this LP algorithm achieves better performance than SVM when only very few labeled examples are available, and it also performs better than bootstrapping for the relation extraction task."
            },
            "slug": "Relation-Extraction-Using-Label-Propagation-Based-Chen-Ji",
            "title": {
                "fragments": [],
                "text": "Relation Extraction Using Label Propagation Based Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This paper investigates a graph based semi-supervised learning algorithm, a label propagation (LP) algorithm, for relation extraction that represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph, and tries to obtain a labeling function to satisfy two constraints."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110920850"
                        ],
                        "name": "Fei Wu",
                        "slug": "Fei-Wu",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1893204,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57faa0e5f99442d1723d2c5ccb70b1461987a7ed",
            "isKey": false,
            "numCitedBy": 432,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Berners-Lee's compelling vision of a Semantic Web is hindered by a chicken-and-egg problem, which can be best solved by a bootstrapping method - creating enough structured data to motivate the development of applications. This paper argues that autonomously \"Semantifying Wikipedia\" is the best way to solve the problem. We choose Wikipedia as an initial data source, because it is comprehensive, not too large, high-quality, and contains enough manually-derived structure to bootstrap an autonomous, self-supervised process. We identify several types of structures which can be automatically enhanced in Wikipedia (e.g., link structure, taxonomic data, infoboxes, etc.), and we describea prototype implementation of a self-supervised, machine learning system which realizes our vision. Preliminary experiments demonstrate the high precision of our system's extracted data - in one case equaling that of humans."
            },
            "slug": "Autonomously-semantifying-wikipedia-Wu-Weld",
            "title": {
                "fragments": [],
                "text": "Autonomously semantifying wikipedia"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper argues that autonomously \"Semantifying Wikipedia\" is the best way to solve the chicken-and-egg problem, and describes a prototype implementation of a self-supervised, machine learning system which realizes the vision."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118239775"
                        ],
                        "name": "Jing Jiang",
                        "slug": "Jing-Jiang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736467"
                        ],
                        "name": "ChengXiang Zhai",
                        "slug": "ChengXiang-Zhai",
                        "structuredName": {
                            "firstName": "ChengXiang",
                            "lastName": "Zhai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "ChengXiang Zhai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17069935,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c99ae181e2e051f9d813ad69fc440d373e1341a9",
            "isKey": false,
            "numCitedBy": 190,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Relation extraction is the task of finding semantic relations between entities from text. The state-of-the-art methods for relation extraction are mostly based on statistical learning, and thus all have to deal with feature selection, which can significantly affect the classification performance. In this paper, we systematically explore a large space of features for relation extraction and evaluate the effectiveness of different feature subspaces. We present a general definition of feature spaces based on a graphic representation of relation instances, and explore three different representations of relation instances and features of different complexities within this framework. Our experiments show that using only basic unit features is generally sufficient to achieve state-of-the-art performance, while overinclusion of complex features may hurt the performance. A combination of features of different levels of complexity and from different sentence representations, coupled with task-oriented feature pruning, gives the best performance."
            },
            "slug": "A-Systematic-Exploration-of-the-Feature-Space-for-Jiang-Zhai",
            "title": {
                "fragments": [],
                "text": "A Systematic Exploration of the Feature Space for Relation Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper systematically explore a large space of features for relation extraction and evaluates the effectiveness of different feature subspaces, and presents a general definition of feature spaces based on a graphic representation of relation instances."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741453"
                        ],
                        "name": "A. Culotta",
                        "slug": "A.-Culotta",
                        "structuredName": {
                            "firstName": "Aron",
                            "lastName": "Culotta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Culotta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144431938"
                        ],
                        "name": "Jeffrey Scott Sorensen",
                        "slug": "Jeffrey-Scott-Sorensen",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Sorensen",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey Scott Sorensen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Several approaches have employed support-vector machines tuned with natural language-oriented kernels to classify pairs of entities [13, 21, 99]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A handful of approaches to knowledge acquisition [21, 54, 75] have tried to extract relationships by analyzing the full structure and meaning (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Keeping pace with progress in machine learning, a diverse set of learning algorithms has been applied to the task of IE, including support vector machines [13, 21, 99], hidden Markov models [36], conditional random fields [63, 23] and Markov logic networks [40]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7395989,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70a2fcfc4e78e8d6db23bf2922f18dd73162b644",
            "isKey": false,
            "numCitedBy": 865,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences. Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles. We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a \"bag-of-words\" kernel."
            },
            "slug": "Dependency-Tree-Kernels-for-Relation-Extraction-Culotta-Sorensen",
            "title": {
                "fragments": [],
                "text": "Dependency Tree Kernels for Relation Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This work extends previous work on tree kernels to estimate the similarity between the dependency trees of sentences, and uses this kernel within a Support Vector Machine to detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2404386"
                        ],
                        "name": "Lenhart K. Schubert",
                        "slug": "Lenhart-K.-Schubert",
                        "structuredName": {
                            "firstName": "Lenhart",
                            "lastName": "Schubert",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lenhart K. Schubert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49488601"
                        ],
                        "name": "Matthew H Tong",
                        "slug": "Matthew-H-Tong",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Tong",
                            "middleNames": [
                                "H"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew H Tong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A handful of approaches to knowledge acquisition [21, 54, 75] have tried to extract relationships by analyzing the full structure and meaning (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Knext [75] was developed to glean \u201cgeneral world knowledge\u201d from English text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Information extraction technology has been developed for a variety of text collections, ranging from domain-specific corpora [36, 63, 70, 82] to newspaper articles [1, 78, 99] to the general-purpose Brown Corpus [75]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14512374,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50449f7d4810c79cc3f89a161df2244420a5b48f",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We have been developing techniques for extracting general world knowledge from miscellaneous texts by a process of approximate interpretation and abstraction, focusing initially on the Brown corpus. We apply interpretive rules to clausal patterns and patterns of modification, and concurrently abstract general \"possibilistic\" propositions from the resulting formulas. Two examples are \"A person may believe a proposition\", and \"Children may live with relatives\". Our methods currently yield over 117,000 such propositions (of variable quality) for the Brown corpus (more than 2 per sentence). We report here on our efforts to evaluate these results with a judging scheme aimed at determining how many of these propositions pass muster as \"reasonable general claims\" about the world in the opinion of human judges. We find that nearly 60% of the extracted propositions are favorably judged according to our scheme by any given judge. The percentage unanimously judged to be reasonable claims by multiple judges is lower, but still sufficiently high to suggest that our techniques may be of some use in tackling the long-standing \"knowledge acquisition bottleneck\" in AI."
            },
            "slug": "Extracting-and-evaluating-general-world-knowledge-Schubert-Tong",
            "title": {
                "fragments": [],
                "text": "Extracting and evaluating general world knowledge from the Brown Corpus"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "Evaluated techniques for extracting general world knowledge from miscellaneous texts by a process of approximate interpretation and abstraction find that nearly 60% of the extracted propositions are favorably judged according to the scheme by any given judge."
            },
            "venue": {
                "fragments": [],
                "text": "HLT-NAACL 2003"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1887557"
                        ],
                        "name": "Georgios Sigletos",
                        "slug": "Georgios-Sigletos",
                        "structuredName": {
                            "firstName": "Georgios",
                            "lastName": "Sigletos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Georgios Sigletos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4738873"
                        ],
                        "name": "G. Paliouras",
                        "slug": "G.-Paliouras",
                        "structuredName": {
                            "firstName": "Georgios",
                            "lastName": "Paliouras",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Paliouras"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2124138"
                        ],
                        "name": "C. Spyropoulos",
                        "slug": "C.-Spyropoulos",
                        "structuredName": {
                            "firstName": "Constantine",
                            "lastName": "Spyropoulos",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Spyropoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799843"
                        ],
                        "name": "M. Hatzopoulos",
                        "slug": "M.-Hatzopoulos",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Hatzopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hatzopoulos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 17
                            }
                        ],
                        "text": "Previous studies [88, 100, 79] have shown that the probabilities of each class value as estimated by each base-level algorithm are more effective features when training metalearners."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "The efficacy of ensemble-based methods for extraction was further investigated by [79], who experimented with combining the outputs of a rule-based learner, a Hidden Markov Model and a wrapper-induction algorithm in five different domains."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15659142,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6c913b3c852bae4f568bd30d4c26cb3762a1248",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "This article investigates the effectiveness of voting and stacked generalization -also known as stacking- in the context of information extraction (IE). A new stacking framework is proposed that accommodates well-known approaches for IE. The key idea is to perform cross-validation on the base-level data set, which consists of text documents annotated with relevant information, in order to create a meta-level data set that consists of feature vectors. A classifier is then trained using the new vectors. Therefore, base-level IE systems are combined with a common classifier at the meta-level. Various voting schemes are presented for comparing against stacking in various IE domains. Well known IE systems are employed at the base-level, together with a variety of classifiers at the meta-level. Results show that both voting and stacking work better when relying on probabilistic estimates by the base-level systems. Voting proved to be effective in most domains in the experiments. Stacking, on the other hand, proved to be consistently effective over all domains, doing comparably or better than voting and always better than the best base-level systems. Particular emphasis is also given to explaining the results obtained by voting and stacking at the meta-level, with respect to the varying degree of similarity in the output of the base-level systems."
            },
            "slug": "Combining-Information-Extraction-Systems-Using-and-Sigletos-Paliouras",
            "title": {
                "fragments": [],
                "text": "Combining Information Extraction Systems Using Voting and Stacked Generalization"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "Investigation of the effectiveness of voting and stacked generalization in the context of information extraction (IE) finds that both voting and stacking work better when relying on probabilistic estimates by the base-level systems."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3139133"
                        ],
                        "name": "Razvan C. Bunescu",
                        "slug": "Razvan-C.-Bunescu",
                        "structuredName": {
                            "firstName": "Razvan",
                            "lastName": "Bunescu",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Razvan C. Bunescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5165854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a8832216fa59867aab8bb98270763fc2de3d8d8",
            "isKey": false,
            "numCitedBy": 949,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel approach to relation extraction, based on the observation that the information required to assert a relationship between two named entities in the same sentence is typically captured by the shortest path between the two entities in the dependency graph. Experiments on extracting top-level relations from the ACE (Automated Content Extraction) newspaper corpus show that the new shortest path dependency kernel outperforms a recent approach based on dependency tree kernels."
            },
            "slug": "A-Shortest-Path-Dependency-Kernel-for-Relation-Bunescu-Mooney",
            "title": {
                "fragments": [],
                "text": "A Shortest Path Dependency Kernel for Relation Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Experiments on extracting top-level relations from the ACE (Automated Content Extraction) newspaper corpus show that the new shortest path dependency kernel outperforms a recent approach based on dependency tree kernels."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744313"
                        ],
                        "name": "L. Ramshaw",
                        "slug": "L.-Ramshaw",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Ramshaw",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ramshaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 108
                            }
                        ],
                        "text": "Tokens in the surrounding context are labeled using the BIO encoding widely-used for natural language tasks [65], where each token is labeled as B-X, I-X or O."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 725590,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9c71db75046473f0e3d3229950d7c84c09afd5e",
            "isKey": false,
            "numCitedBy": 1530,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Transformation-based learning, a technique introduced by Eric Brill (1993b), has been shown to do part-of-speech tagging with fairly high accuracy. This same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive \u201cbaseNP\u201d chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 93% for baseNP chunks (trained on 950K words) and 88% for somewhat more complex chunks that partition the sentence (trained on 200K words). Working in this new application and with larger template and training sets has also required some interesting adaptations to the transformation-based learning approach."
            },
            "slug": "Text-Chunking-using-Transformation-Based-Learning-Ramshaw-Marcus",
            "title": {
                "fragments": [],
                "text": "Text Chunking using Transformation-Based Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has shown that the transformation-based learning approach can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive \u201cbaseNP\u201d chunks."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This observation has been exploited with success in a variety of areas including ambiguity resolution [4, 57, 91], language modeling [47] and thesaurus construction [24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5867456,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f201774b00708aef88cc14c5cd4347a376b52142",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Ensemble methods are state of the art for many NLP tasks. Recent work by Banko and Brill (2001) suggests that this would not necessarily be true if very large training corpora were available. However, their results are limited by the simplicity of their evaluation task and individual classifiers.Our work explores ensemble efficacy for the more complex task of automatic thesaurus extraction on up to 300 million words. We examine our conflicting results in terms of the constraints on, and complexity of, different contextual representations, which contribute to the sparseness-and noise-induced bias behaviour of NLP systems on very large corpora."
            },
            "slug": "Ensemble-Methods-for-Automatic-Thesaurus-Extraction-Curran",
            "title": {
                "fragments": [],
                "text": "Ensemble Methods for Automatic Thesaurus Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work explores ensemble efficacy for the more complex task of automatic thesaurus extraction on up to 300 million words, and examines the constraints on, and complexity of, different contextual representations which contribute to the sparseness-and noise-induced bias behaviour of NLP systems on very large corpora."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748977"
                        ],
                        "name": "P. Cimiano",
                        "slug": "P.-Cimiano",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Cimiano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cimiano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789102"
                        ],
                        "name": "S. Handschuh",
                        "slug": "S.-Handschuh",
                        "structuredName": {
                            "firstName": "Siegfried",
                            "lastName": "Handschuh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Handschuh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752093"
                        ],
                        "name": "Steffen Staab",
                        "slug": "Steffen-Staab",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Staab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steffen Staab"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6755749,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bead0ccccd0ab1da5237ca7ea80b425745e94083",
            "isKey": false,
            "numCitedBy": 431,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The success of the Semantic Web depends on the availability of ontologies as well as on the proliferation of web pages annotated with metadata conforming to these ontologies. Thus, a crucial question is where to acquire these metadata from. In this paper wepropose PANKOW (Pattern-based Annotation through Knowledge on theWeb), a method which employs an unsupervised, pattern-based approach to categorize instances with regard to an ontology. The approach is evaluated against the manual annotations of two human subjects. The approach is implemented in OntoMat, an annotation tool for the Semantic Web and shows very promising results."
            },
            "slug": "Towards-the-self-annotating-web-Cimiano-Handschuh",
            "title": {
                "fragments": [],
                "text": "Towards the self-annotating web"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "PANKOW (Pattern-based Annotation through Knowledge on theWeb), a method which employs an unsupervised, pattern-based approach to categorize instances with regard to an ontology, is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064774858"
                        ],
                        "name": "Deepak Ravichandran",
                        "slug": "Deepak-Ravichandran",
                        "structuredName": {
                            "firstName": "Deepak",
                            "lastName": "Ravichandran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deepak Ravichandran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144547315"
                        ],
                        "name": "E. Hovy",
                        "slug": "E.-Hovy",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Hovy",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hovy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16179481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "647adf77449ce0d1730d54abdbca20bf0e75088e",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Although vast amounts of textual data are freely available, many NLP algorithms exploit only a minute percentage of it. In this paper, we study the challenges of working at the terascale. We present an algorithm, designed for the teraxale, for mining is-a relations that achieves similar performance to a state-of-the-art linguistically-rich method. We focus on the accuracy of these two systems as a function of processing time and corpus size."
            },
            "slug": "Towards-terascale-knowledge-acquisition-Pantel-Ravichandran",
            "title": {
                "fragments": [],
                "text": "Towards terascale knowledge acquisition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An algorithm is presented, designed for the teraxale, for mining is-a relations that achieves similar performance to a state-of-the-art linguistically-rich method, and focuses on the accuracy of these two systems as a function of processing time and corpus size."
            },
            "venue": {
                "fragments": [],
                "text": "COLING 2004"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Ratnaparkhi [66] assessed the cross-domain portability of a maximum-entropy parser and found that when trained on a corpus of Wall Street Journal (WSJ) news articles and tested on sections of the Brown corpus \u2014 a dataset comprised of fiction, magazines and journal articles \u2013 precision dropped by 6."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3231298,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6140a793a4554806eb39d15c018d8f782d2ac1e",
            "isKey": false,
            "numCitedBy": 287,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a machine learning system for parsing natural language that learns from manually parsed example sentences, and parses unseen data at state-of-the-art accuracies. Its machine learning technology, based on the maximum entropy framework, is highly reusable and not specific to the parsing problem, while the linguistic hints that it uses to learn can be specified concisely. It therefore requires a minimal amount of human effort and linguistic knowledge for its construction. In practice, the running time of the parser on a test sentence is linear with respect to the sentence length. We also demonstrate that the parser can train from other domains without modification to the modeling framework or the linguistic hints it uses to learn. Furthermore, this paper shows that research into rescoring the top 20 parses returned by the parser might yield accuracies dramatically higher than the state-of-the-art."
            },
            "slug": "Learning-to-Parse-Natural-Language-with-Maximum-Ratnaparkhi",
            "title": {
                "fragments": [],
                "text": "Learning to Parse Natural Language with Maximum Entropy Models"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A machine learning system for parsing natural language that learns from manually parsed example sentences, and parses unseen data at state-of-the-art accuracies, and it is demonstrated that the parser can train from other domains without modification to the modeling framework or the linguistic hints it uses to learn."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Keeping pace with progress in machine learning, a diverse set of learning algorithms has been applied to the task of IE, including support vector machines [13, 21, 99], hidden Markov models [36], conditional random fields [63, 23] and Markov logic networks [40]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Information extraction technology has been developed for a variety of text collections, ranging from domain-specific corpora [36, 63, 70, 82] to newspaper articles [1, 78, 99] to the general-purpose Brown Corpus [75]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15413758,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02c8a0bc8bab9920e6615cfacf1df2ab3f2b1f68",
            "isKey": false,
            "numCitedBy": 313,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent research has demonstrated the strong performance of hidden Markov models applied to information extraction\u2014the task of populating database slots with corresponding phrases from text documents. A remaining problem, however, is the selection of state-transition structure for the model. This paper demonstrates that extraction accuracy strongly depends on the selection of structure, and presents an algorithm for automatically finding good structures by stochastic optimization. Our algorithm begins with a simple model and then performs hill-climbing in the space of possible structures by splitting states and gauging performance on a validation set. Experimental results show that this technique finds HMM models that almost always out-perform a fixed model, and have superior average performance across tasks."
            },
            "slug": "Information-Extraction-with-HMM-Structures-Learned-Freitag-McCallum",
            "title": {
                "fragments": [],
                "text": "Information Extraction with HMM Structures Learned by Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper demonstrates that extraction accuracy strongly depends on the selection of structure, and presents an algorithm for automatically finding good structures by stochastic optimization, which finds HMM models that almost always out-perform a fixed model, and have superior average performance across tasks."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145044578"
                        ],
                        "name": "S. Auer",
                        "slug": "S.-Auer",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Auer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Auer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729154"
                        ],
                        "name": "C. Bizer",
                        "slug": "C.-Bizer",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Bizer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bizer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2051816"
                        ],
                        "name": "Georgi Kobilarov",
                        "slug": "Georgi-Kobilarov",
                        "structuredName": {
                            "firstName": "Georgi",
                            "lastName": "Kobilarov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Georgi Kobilarov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144568027"
                        ],
                        "name": "Jens Lehmann",
                        "slug": "Jens-Lehmann",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Lehmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jens Lehmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702661"
                        ],
                        "name": "Richard Cyganiak",
                        "slug": "Richard-Cyganiak",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Cyganiak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Cyganiak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804315"
                        ],
                        "name": "Z. Ives",
                        "slug": "Z.-Ives",
                        "structuredName": {
                            "firstName": "Zachary",
                            "lastName": "Ives",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Ives"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "DBpedia is a community effort designed to extract structured data from Wikipedia [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ") [ 2 ] B o r n I n ( E i n s t e i n , U l m ) [ 2 ] S e l l ( E i n s t e i n B r o s , b a g e l s ) [ 1 ] C o n t a i n ( o r a n g e s , V i t a m i n C ) [ 1 ] R a w T u p l e s ( X Y Z C o r p ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7278297,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2",
            "isKey": false,
            "numCitedBy": 4419,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human-andmachine-consumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data."
            },
            "slug": "DBpedia:-A-Nucleus-for-a-Web-of-Open-Data-Auer-Bizer",
            "title": {
                "fragments": [],
                "text": "DBpedia: A Nucleus for a Web of Open Data"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The extraction of the DBpedia datasets is described, and how the resulting information is published on the Web for human-andmachine-consumption and how DBpedia could serve as a nucleus for an emerging Web of open data."
            },
            "venue": {
                "fragments": [],
                "text": "ISWC/ASWC"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2600845,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b49db3ac26d96b6c5c081dc6c2cc24da93e633f1",
            "isKey": false,
            "numCitedBy": 554,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis demonstrates that several important kinds of natural language ambiguities can be resolved to state-of-the-art accuracies using a single statistical modeling technique based on the principle of maximum entropy. \nWe discuss the problems of sentence boundary detection, part-of-speech tagging, prepositional phrase attachment, natural language parsing, and text categorization under the maximum entropy framework. In practice, we have found that maximum entropy models offer the following advantages: \nState-of-the-art accuracy. The probability models for all of the tasks discussed perform at or near state-of-the-art accuracies, or outperform competing learning algorithms when trained and tested under similar conditions. Methods which outperform those presented here require much more supervision in the form of additional human involvement or additional supporting resources. \nKnowledge-poor features. The facts used to model the data, or features, are linguistically very simple, or \"knowledge-poor\", but yet succeed in approximating complex linguistic relationships. \nReusable software technology. The mathematics of the maximum entropy framework are essentially independent of any particular task, and a single software implementation can be used for all of the probability models in this thesis. \nThe experiments in this thesis suggest that experimenters can obtain state-of-the-art accuracies on a wide range of natural language tasks, with little task-specific effort, by using maximum entropy probability models."
            },
            "slug": "Maximum-entropy-models-for-natural-language-Ratnaparkhi-Marcus",
            "title": {
                "fragments": [],
                "text": "Maximum entropy models for natural language ambiguity resolution"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This thesis demonstrates that several important kinds of natural language ambiguities can be resolved to state-of-the-art accuracies using a single statistical modeling technique based on the principle of maximum entropy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36445704"
                        ],
                        "name": "Ana-Maria Popescu",
                        "slug": "Ana-Maria-Popescu",
                        "structuredName": {
                            "firstName": "Ana-Maria",
                            "lastName": "Popescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ana-Maria Popescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 631855,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "ff75055d4e47737702d3b550879d6128cec13233",
            "isKey": false,
            "numCitedBy": 2146,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Consumers are often forced to wade through many on-line reviews in order to make an informed product choice. This paper introduces Opine, an unsupervised information-extraction system which mines reviews in order to build a model of important product features, their evaluation by reviewers, and their relative quality across products.Compared to previous work, Opine achieves 22% higher precision (with only 3% lower recall) on the feature extraction task. Opine's novel use of relaxation labeling for finding the semantic orientation of words in context leads to strong performance on the tasks of finding opinion phrases and their polarity."
            },
            "slug": "Extracting-Product-Features-and-Opinions-from-Popescu-Etzioni",
            "title": {
                "fragments": [],
                "text": "Extracting Product Features and Opinions from Reviews"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Opine is introduced, an unsupervised information-extraction system which mines reviews in order to build a model of important product features, their evaluation by reviewers, and their relative quality across products."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48323507"
                        ],
                        "name": "Peter Clark",
                        "slug": "Peter-Clark",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143878270"
                        ],
                        "name": "P. Harrison",
                        "slug": "P.-Harrison",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Harrison",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Harrison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116794438"
                        ],
                        "name": "John A. Thompson",
                        "slug": "John-A.-Thompson",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Thompson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Thompson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[18], applied a parser to a textual corpus of 800,000 news articles and yielded 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9344818,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8686cedf619eba40b946d0ea34eb50cb12d49d94",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Our goal is to be able to answer questions about text that go beyond facts explicitly stated in the text, a task which inherently requires extracting a \"deep\" level of meaning from that text. Our approach treats meaning processing fundamentally as a modeling activity, in which a knowledge base of common-sense expectations guides interpretation of text, and text suggests which parts of the knowledge base might be relevant. In this paper, we describe our ongoing investigations to develop this approach into a usable method for meaning processing."
            },
            "slug": "A-knowledge-driven-approach-to-text-meaning-Clark-Harrison",
            "title": {
                "fragments": [],
                "text": "A knowledge-driven approach to text meaning processing"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper describes the ongoing investigations to develop the approach to meaning processing fundamentally as a modeling activity, in which a knowledge base of common-sense expectations guides interpretation of text, and text suggests which parts of the knowledge base might be relevant."
            },
            "venue": {
                "fragments": [],
                "text": "HLT-NAACL 2003"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801255"
                        ],
                        "name": "Simone Paolo Ponzetto",
                        "slug": "Simone-Paolo-Ponzetto",
                        "structuredName": {
                            "firstName": "Simone",
                            "lastName": "Ponzetto",
                            "middleNames": [
                                "Paolo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simone Paolo Ponzetto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31380436"
                        ],
                        "name": "M. Strube",
                        "slug": "M.-Strube",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Strube",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Strube"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These observations have sparked several extraction efforts [64, 86, 96] that focus on mining these semi-structured resources as opposed to the unstructured text of Wikipedia articles."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6860966,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22d258fa8f9acadfe513ee8b2842f2bc5db85149",
            "isKey": false,
            "numCitedBy": 520,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We take the category system in Wikipedia as a conceptual network. We label the semantic relations between categories using methods based on connectivity in the network and lexicosyntactic matching. As a result we are able to derive a large scale taxonomy containing a large amount of subsumption, i.e. isa, relations. We evaluate the quality of the created resource by comparing it with ResearchCyc, one of the largest manually annotated ontologies, as well as computing semantic similarity between words in benchmarking datasets."
            },
            "slug": "Deriving-a-Large-Scale-Taxonomy-from-Wikipedia-Ponzetto-Strube",
            "title": {
                "fragments": [],
                "text": "Deriving a Large-Scale Taxonomy from Wikipedia"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A large scale taxonomy containing a large amount of subsumption is derived using methods based on connectivity in the network and lexicosyntactic matching to label the semantic relations between categories in Wikipedia."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "WordNet [56], a broad-coverage catalog of English words, contains over 25,000 verbs and 150,000 nouns."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "While the exact number of relations in the English language is unknown, we can reasonably estimate the number to be tens of thousands according to existing lexical resources such as WordNet [56], VerbNet [46] and PropBank [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1671874,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "68c03788224000794d5491ab459be0b2a2c38677",
            "isKey": false,
            "numCitedBy": 13889,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4]."
            },
            "slug": "WordNet:-A-Lexical-Database-for-English-Miller",
            "title": {
                "fragments": [],
                "text": "WordNet: A Lexical Database for English"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "WordNet1 provides a more effective combination of traditional lexicographic information and modern computing, and is an online lexical database designed for use under program control."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 128
                            }
                        ],
                        "text": "phrases are reduced to their lexical heads using a set of head-finding rules developed and used widely by the parsing community [19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7901127,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3fc44ff7f37ec5585310666c183c65e0a0bb2446",
            "isKey": false,
            "numCitedBy": 2062,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models."
            },
            "slug": "Head-Driven-Statistical-Models-for-Natural-Language-Collins",
            "title": {
                "fragments": [],
                "text": "Head-Driven Statistical Models for Natural Language Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "Three statistical models for natural language parsing are described, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720988"
                        ],
                        "name": "Joakim Nivre",
                        "slug": "Joakim-Nivre",
                        "structuredName": {
                            "firstName": "Joakim",
                            "lastName": "Nivre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joakim Nivre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712813"
                        ],
                        "name": "Johan Hall",
                        "slug": "Johan-Hall",
                        "structuredName": {
                            "firstName": "Johan",
                            "lastName": "Hall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johan Hall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145446170"
                        ],
                        "name": "Jens Nilsson",
                        "slug": "Jens-Nilsson",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Nilsson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jens Nilsson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2649192"
                        ],
                        "name": "Atanas Chanev",
                        "slug": "Atanas-Chanev",
                        "structuredName": {
                            "firstName": "Atanas",
                            "lastName": "Chanev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Atanas Chanev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3348151"
                        ],
                        "name": "G. Eryigit",
                        "slug": "G.-Eryigit",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Eryigit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Eryigit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804668"
                        ],
                        "name": "Sandra K\u00fcbler",
                        "slug": "Sandra-K\u00fcbler",
                        "structuredName": {
                            "firstName": "Sandra",
                            "lastName": "K\u00fcbler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sandra K\u00fcbler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1915339"
                        ],
                        "name": "S. Marinov",
                        "slug": "S.-Marinov",
                        "structuredName": {
                            "firstName": "Svetoslav",
                            "lastName": "Marinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Marinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748039"
                        ],
                        "name": "E. Marsi",
                        "slug": "E.-Marsi",
                        "structuredName": {
                            "firstName": "Erwin",
                            "lastName": "Marsi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Marsi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9743340,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4546c7b4d2103523b104daddea6c7eaa1925cf12",
            "isKey": false,
            "numCitedBy": 556,
            "numCiting": 84,
            "paperAbstract": {
                "fragments": [],
                "text": "Parsing unrestricted text is useful for many language technology applications but requires parsing methods that are both robust and efficient. MaltParser is a language-independent system for data-driven dependency parsing that can be used to induce a parser for a new language from a treebank sample in a simple yet flexible manner. Experimental evaluation confirms that MaltParser can achieve robust, efficient and accurate parsing for a wide range of languages without language-specific enhancements and with rather limited amounts of training data."
            },
            "slug": "MaltParser:-A-Language-Independent-System-for-Nivre-Hall",
            "title": {
                "fragments": [],
                "text": "MaltParser: A Language-Independent System for Data-Driven Dependency Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental evaluation confirms that MaltParser can achieve robust, efficient and accurate parsing for a wide range of languages without language-specific enhancements and with rather limited amounts of training data."
            },
            "venue": {
                "fragments": [],
                "text": "Natural Language Engineering"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057788"
                        ],
                        "name": "M. Osborne",
                        "slug": "M.-Osborne",
                        "structuredName": {
                            "firstName": "Miles",
                            "lastName": "Osborne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Osborne"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1039363,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "46f0c02c9ee4f8e5336bfcae94c4cd3e40d92f3e",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Banko and Brill (2001) suggested that the development of very large training corpora may be more effective for progress in empirical Natural Language Processing than improving methods that use existing smaller training corpora.This work tests their claim by exploring whether a very large corpus can eliminate the sparseness problems associated with estimating unigram probabilities. We do this by empirically investigating the convergence behaviour of unigram probability estimates on a one billion word corpus. When using one billion words, as expected, we do find that many of our estimates do converge to their eventual value. However, we also find that for some words, no such convergence occurs. This leads us to conclude that simply relying upon large corpora is not in itself sufficient: we must pay attention to the statistical modelling as well."
            },
            "slug": "A-Very-Very-Large-Corpus-Doesn\u2019t-Always-Yield-Curran-Osborne",
            "title": {
                "fragments": [],
                "text": "A Very Very Large Corpus Doesn\u2019t Always Yield Reliable Estimates"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work empirically investigates the convergence behaviour of unigram probability estimates on a one billion word corpus and concludes that simply relying upon large corpora is not in itself sufficient: one must pay attention to the statistical modelling as well."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110920850"
                        ],
                        "name": "Fei Wu",
                        "slug": "Fei-Wu",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2630700"
                        ],
                        "name": "Eytan Adar",
                        "slug": "Eytan-Adar",
                        "structuredName": {
                            "firstName": "Eytan",
                            "lastName": "Adar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eytan Adar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719124"
                        ],
                        "name": "Saleema Amershi",
                        "slug": "Saleema-Amershi",
                        "structuredName": {
                            "firstName": "Saleema",
                            "lastName": "Amershi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Saleema Amershi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145504534"
                        ],
                        "name": "J. Fogarty",
                        "slug": "J.-Fogarty",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Fogarty",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Fogarty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2566295"
                        ],
                        "name": "Raphael Hoffmann",
                        "slug": "Raphael-Hoffmann",
                        "structuredName": {
                            "firstName": "Raphael",
                            "lastName": "Hoffmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raphael Hoffmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39699737"
                        ],
                        "name": "Kayur Patel",
                        "slug": "Kayur-Patel",
                        "structuredName": {
                            "firstName": "Kayur",
                            "lastName": "Patel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kayur Patel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38823289"
                        ],
                        "name": "Michael Skinner",
                        "slug": "Michael-Skinner",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Skinner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Skinner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The Intelligence in Wikipedia (IWP) project [92] was designed to exploit the fact that each article in Wikipedia corresponds to a primary object and that many articles contain infoboxes, tabular summaries of the most important attributes (and their values) for these objects."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9880228,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8932d95161208690deac3865eb5520e98f6c549d",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "The Intelligence in Wikipedia project at the University of Washington is combining self-supervised information extraction (IE) techniques with a mixed initiative interface designed to encourage communal content creation (CCC). Since IE and CCC are each powerful ways to produce large amounts of structured information, they have been studied extensively \u2014 but only in isolation. By combining the two methods in a virtuous feedback cycle, we aim for substantial synergy. While previous papers have described the details of individual aspects of our endeavor [25, 26, 24, 13], this report provides an overview of the project\u2019s progress and vision."
            },
            "slug": "Intelligence-in-Wikipedia-Weld-Wu",
            "title": {
                "fragments": [],
                "text": "Intelligence in Wikipedia"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "The Intelligence in Wikipedia project at the University of Washington is combining self-supervised information extraction techniques with a mixed initiative interface designed to encourage communal content creation to aim for substantial synergy."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793218"
                        ],
                        "name": "D. Gildea",
                        "slug": "D.-Gildea",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Gildea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gildea"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Gildea [38] witnessed similar results when studying a statistical chart parser using the same corpora, citing cross-domain losses 5."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 196105,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee7e21dd09949a5a53b39c13fca9cd3d55e2bc50",
            "isKey": false,
            "numCitedBy": 326,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank. While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might a ect parser performance, and how portable parsing models are across corpora. We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained. This leads us to a technique for pruning parameters to reduce the size of the parsing model."
            },
            "slug": "Corpus-Variation-and-Parser-Performance-Gildea",
            "title": {
                "fragments": [],
                "text": "Corpus Variation and Parser Performance"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work examines how other types of text might a ect parser performance, and how portable parsing models are across corpora by comparing results for the Brown and WSJ corpora, and considers which parts of the parser's probability model are particularly tuned to the corpus on which it was trained."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144105277"
                        ],
                        "name": "Wen-tau Yih",
                        "slug": "Wen-tau-Yih",
                        "structuredName": {
                            "firstName": "Wen-tau",
                            "lastName": "Yih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen-tau Yih"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Recent progress in the probabilistic inference and machine learning has made it possible to recognize named entities and relations simultaneously [72, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10048734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5aa70188f70d349580aed96c10a68f57dace2d33",
            "isKey": false,
            "numCitedBy": 419,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Given a collection of discrete random variables representing outcomes of learned local predictors in natural language. e.g.. named entities and relations. we seek an optimal global assignment to the variables in the presence of general (non-sequential) constraints. Examples of these constraints include the type of arguments a relation can take, and the mutual activity of different relations. etc. We develop a linear programing formulation for this problem and evaluate it in the context of simultaneously learning named entities and relations. Our approach allows us to efficiently incorporate domain and task specific constraints at decision time, resulting in significant improvements in the accuracy and the \"human-like\" quality of the inferences."
            },
            "slug": "A-Linear-Programming-Formulation-for-Global-in-Roth-Yih",
            "title": {
                "fragments": [],
                "text": "A Linear Programming Formulation for Global Inference in Natural Language Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work develops a linear programing formulation for this problem and evaluates it in the context of simultaneously learning named entities and relations to efficiently incorporate domain and task specific constraints at decision time, resulting in significant improvements in the accuracy and the \"human-like\" quality of the inferences."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2302447"
                        ],
                        "name": "N. Kambhatla",
                        "slug": "N.-Kambhatla",
                        "structuredName": {
                            "firstName": "Nanda",
                            "lastName": "Kambhatla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kambhatla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 62705866,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93e7e3d02d16f72cd071f69ccb302859ec441d3f",
            "isKey": false,
            "numCitedBy": 562,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Extracting semantic relationships between entities is challenging because of a paucity of annotated data and the errors induced by entity detection modules. We employ Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text. Our system obtained competitive results in the Automatic Content Extraction (ACE) evaluation. Here we present our general approach and describe our ACE results."
            },
            "slug": "Combining-lexical,-syntactic,-and-semantic-features-Kambhatla",
            "title": {
                "fragments": [],
                "text": "Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work employs Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text to obtain competitive results in the Automatic Content Extraction (ACE) evaluation."
            },
            "venue": {
                "fragments": [],
                "text": "ACL 2004"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144422314"
                        ],
                        "name": "Matthew Richardson",
                        "slug": "Matthew-Richardson",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Richardson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Richardson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740213"
                        ],
                        "name": "Pedro M. Domingos",
                        "slug": "Pedro-M.-Domingos",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Domingos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro M. Domingos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "01 according to a two-sample t-test, using the methodology for measuring the standard deviation of AUC given in [68]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12698795,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "950a3c89dacbc3e7ddcd43d7ff6f985697e41cdb",
            "isKey": false,
            "numCitedBy": 2804,
            "numCiting": 97,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a simple approach to combining first-order logic and probabilistic graphical models in a single representation. A Markov logic network (MLN) is a first-order knowledge base with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it specifies a ground Markov network containing one feature for each possible grounding of a first-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Weights are efficiently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. Optionally, additional clauses are learned using inductive logic programming techniques. Experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach."
            },
            "slug": "Markov-logic-networks-Richardson-Domingos",
            "title": {
                "fragments": [],
                "text": "Markov logic networks"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "Experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach to combining first-order logic and probabilistic graphical models in a single representation."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 119
                            }
                        ],
                        "text": "Data-driven taggers and chunkers for English have been shown to achieve a precision of up to 97% and 94%, respectively [89, 90], and are generally considered to be robust."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14835360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb42a490cf4f186d3383c92963817d100afd81e2",
            "isKey": false,
            "numCitedBy": 3438,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result."
            },
            "slug": "Feature-Rich-Part-of-Speech-Tagging-with-a-Cyclic-Toutanova-Klein",
            "title": {
                "fragments": [],
                "text": "Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A new part-of-speech tagger is presented that demonstrates the following ideas: explicit use of both preceding and following tag contexts via a dependency network representation, broad use of lexical features, and effective use of priors in conditional loglinear models."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403816758"
                        ],
                        "name": "O. Babko-Malaya",
                        "slug": "O.-Babko-Malaya",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Babko-Malaya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Babko-Malaya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145755155"
                        ],
                        "name": "Martha Palmer",
                        "slug": "Martha-Palmer",
                        "structuredName": {
                            "firstName": "Martha",
                            "lastName": "Palmer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martha Palmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702849"
                        ],
                        "name": "Nianwen Xue",
                        "slug": "Nianwen-Xue",
                        "structuredName": {
                            "firstName": "Nianwen",
                            "lastName": "Xue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nianwen Xue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714374"
                        ],
                        "name": "A. Joshi",
                        "slug": "A.-Joshi",
                        "structuredName": {
                            "firstName": "Aravind",
                            "lastName": "Joshi",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Joshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2695965"
                        ],
                        "name": "S. Kulick",
                        "slug": "S.-Kulick",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Kulick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kulick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "VerbNet [46] and PropBank [3], two verb lexicons for the English language, contain 5000 and 3600 semantically distinct entries, respectively."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "While the exact number of relations in the English language is unknown, we can reasonably estimate the number to be tens of thousands according to existing lexical resources such as WordNet [56], VerbNet [46] and PropBank [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6492314,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "e9239157a0559c3e60c60908e96c143ba6040671",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The PropBank project is creating a corpus of text annotated with information about basic semantic propositions. PropBank I (Kingsbury & Palmer, 2002) added a layer of predicateargument information, or semantic roles, to the syntactic structures of the English Penn Treebank. This paper presents an overview of the second phase of PropBank Annotation, PropBank II, which is being applied to English and Chinese, and includes (Neodavidsonian) eventuality variables, nominal references, sense tagging, and connections to the Penn Discourse Treebank (PDTB), a project for annotating discourse connectives and their arguments."
            },
            "slug": "Proposition-Bank-II:-Delving-Deeper-Babko-Malaya-Palmer",
            "title": {
                "fragments": [],
                "text": "Proposition Bank II: Delving Deeper"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An overview of the second phase of PropBank Annotation, PropBank II, which is being applied to English and Chinese, and includes (Neodavidsonian) eventuality variables, nominal references, sense tagging, and connections to the Penn Discourse Treebank (PDTB), a project for annotating discourse connectives and their arguments."
            },
            "venue": {
                "fragments": [],
                "text": "FCP@NAACL-HLT"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706729"
                        ],
                        "name": "G. Ngai",
                        "slug": "G.-Ngai",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Ngai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ngai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3264132,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52e16b234c8d15d20de0e892e190bb6af04576e3",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker: human rule writing and active learning using interactive real-time human annotation. Several novel variations on active learning are investigated, and underlying cost models for cross-modal machine learning comparison are presented and explored. Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment."
            },
            "slug": "Rule-Writing-or-Annotation:-Cost-efficient-Resource-Ngai-Yarowsky",
            "title": {
                "fragments": [],
                "text": "Rule Writing or Annotation: Cost-efficient Resource Usage for Base Noun Phrase Chunking"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9966171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34dc22dcbdf1e09fb48691ee1fc6fe4bb8f834c3",
            "isKey": false,
            "numCitedBy": 475,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Conditional Random Fields (CRFs) are undirected graphical models, a special case of which correspond to conditionally-trained finite state machines. A key advantage of CRFs is their great flexibility to include a wide variety of arbitrary, non-independent features of the input. Faced with this freedom, however, an important question remains: what features should be used? This paper presents an efficient feature induction method for CRFs. The method is founded on the principle of iteratively constructing feature conjunctions that would significantly increase conditional log-likelihood if added to the model. Automated feature induction enables not only improved accuracy and dramatic reduction in parameter count, but also the use of larger cliques, and more freedom to liberally hypothesize atomic input variables that may be relevant to a task. The method applies to linear-chain CRFs, as well as to more arbitrary CRF structures, such as Relational Markov Networks, where it corresponds to learning clique templates, and can also be understood as supervised structure learning. Experimental results on named entity extraction and noun phrase segmentation tasks are presented."
            },
            "slug": "Efficiently-Inducing-Features-of-Conditional-Random-McCallum",
            "title": {
                "fragments": [],
                "text": "Efficiently Inducing Features of Conditional Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper presents an efficient feature induction method for CRFs founded on the principle of iteratively constructing feature conjunctions that would significantly increase conditional log-likelihood if added to the model."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753415"
                        ],
                        "name": "M. Volk",
                        "slug": "M.-Volk",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Volk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Volk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This observation has been exploited with success in a variety of areas including ambiguity resolution [4, 57, 91], language modeling [47] and thesaurus construction [24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1218631,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "c83568b0bbf00e5471c23cb643be429b98725a44",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Finding the correct attachment site for prepositional phrases (PPs) is one of the hardest problems when parsing natural languages. An English sentence consisting of a subject, a verb, and a nominal object followed by a prepositional phrase is a priori ambiguous. The PP in sentence 1 is a noun attribute and needs to be attached to the noun, but the PP in 2 is an adverbial and thus part of the verb phrase."
            },
            "slug": "Exploiting-the-WWW-as-a-corpus-to-resolve-PP-Volk",
            "title": {
                "fragments": [],
                "text": "Exploiting the WWW as a corpus to resolve PP attachment ambiguities"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Finding the correct attachment site for prepositional phrases (PPs) is one of the hardest problems when parsing natural languages."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Minipar [53], a well-studied broad-coverage English parser, was shown to achieve recall of 81."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59702881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99328d4b34d1ac02252258a9437b8b2c1acdb92c",
            "isKey": false,
            "numCitedBy": 773,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we first present a dependency-based method for parser evaluation. We then use the method to evaluate a broad-coverage parser, called MINIPAR, with the SUSANNE corpus. The method allows us to evaluate not only the overall performance of the parser, but also its performance with respect to different grammatical relationships and phenomena. The evaluation results show that MINIPAR is able to cover about 79% of the dependency relationships in the SUSANNE corpus with about 89% precision."
            },
            "slug": "Dependency-Based-Evaluation-of-Minipar-Lin",
            "title": {
                "fragments": [],
                "text": "Dependency-Based Evaluation of Minipar"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A dependency-based method for parser evaluation is presented and a broad-coverage parser, called MINIPAR, is evaluated with the SUSANNE corpus."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757166"
                        ],
                        "name": "Kenji Sagae",
                        "slug": "Kenji-Sagae",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Sagae",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenji Sagae"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737901"
                        ],
                        "name": "Junichi Tsujii",
                        "slug": "Junichi-Tsujii",
                        "structuredName": {
                            "firstName": "Junichi",
                            "lastName": "Tsujii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junichi Tsujii"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "By comparison, while a statistical parser developed this year by Sagae and Tsujii [74] achieved the same accuracy with better recall (88."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 8836054,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "866e6d8d48ce516d853b44a225562ef48bb945b5",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Most data-driven dependency parsing approaches assume that sentence structure is represented as trees. Although trees have several desirable properties from both computational and linguistic perspectives, the structure of linguistic phenomena that goes beyond shallow syntax often cannot be fully captured by tree representations. We present a parsing approach that is nearly as simple as current data-driven transition-based dependency parsing frameworks, but outputs directed acyclic graphs (DAGs). We demonstrate the benefits of DAG parsing in two experiments where its advantages over dependency tree parsing can be clearly observed: predicate-argument analysis of English and syntactic analysis of Danish with a representation that includes long-distance dependencies and anaphoric reference links."
            },
            "slug": "Shift-Reduce-Dependency-DAG-Parsing-Sagae-Tsujii",
            "title": {
                "fragments": [],
                "text": "Shift-Reduce Dependency DAG Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents a parsing approach that is nearly as simple as current data-driven transition-based dependency parsing frameworks, but outputs directed acyclic graphs (DAGs)."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3207496"
                        ],
                        "name": "Y. Seginer",
                        "slug": "Y.-Seginer",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Seginer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Seginer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2862221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed9a3200d6d0850fc2404d7205f5cc1b458d06f8",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an incremental parser and an unsupervised learning algorithm for inducing this parser from plain text. The parser uses a representation for syntactic structure similar to dependency links which is well-suited for incremental parsing. In contrast to previous unsupervised parsers, the parser does not use part-of-speech tags and both learning and parsing are local and fast, requiring no explicit clustering or global optimization. The parser is evaluated by converting its output into equivalent bracketing and improves on previously published results for unsupervised parsing from plain text."
            },
            "slug": "Fast-Unsupervised-Incremental-Parsing-Seginer",
            "title": {
                "fragments": [],
                "text": "Fast Unsupervised Incremental Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This paper describes an incremental parser and an unsupervised learning algorithm for inducing this parser from plain text, which uses a representation for syntactic structure similar to dependency links which is well-suited for incremental parsing."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145022783"
                        ],
                        "name": "E. Brill",
                        "slug": "E.-Brill",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706729"
                        ],
                        "name": "G. Ngai",
                        "slug": "G.-Ngai",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Ngai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ngai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "Brill and Ngai [10] closely studied the performance of eleven advanced computer science students who were asked to develop rules identifying base noun phrases in text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15260546,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f51938e9b35dfd1fa61e7d6bff2154c01cb65d2f",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "A great deal of work has been done demonstrating the ability of machine learning algorithms to automatically extract linguistic knowledge from annotated corpora. Very little work has gone into quantifying the difference in ability at this task between a person and a machine. This paper is a first step in that direction."
            },
            "slug": "Man*-vs.-Machine:-A-Case-Study-in-Base-Noun-Phrase-Brill-Ngai",
            "title": {
                "fragments": [],
                "text": "Man* vs. Machine: A Case Study in Base Noun Phrase Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper quantifies the difference in ability at this task between a person and a machine and takes a first step in that direction."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740213"
                        ],
                        "name": "Pedro M. Domingos",
                        "slug": "Pedro-M.-Domingos",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Domingos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro M. Domingos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Empirical multistrategy learning refers to the combination of multiple learning approaches using a single algorithm [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 119562134,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "131613664d089f26154d02e174f6cf75748436cd",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Several well-developed approaches to inductive learning now exist, but each has specific limitations that are hard to overcome. Multi-strategy learning attempts to tackle this problem by combining multiple methods in one algorithm. This article describes a unification of two widely-used empirical approaches: rule induction and instance-based learning. In the new algorithm, instances are treated as maximally specific rules, and classification is performed using a best-match strategy. Rules are learned by gradually generalizing instances until no improvement in apparent accuracy is obtained. Theoretical analysis shows this approach to be efficient. It is implemented in the RISE 3.1 system. In an extensive empirical study, RISE consistently achieves higher accuracies than state-of-the-art representatives of both its parent approaches (PEBLS and CN2), as well as a decision tree learner (C4.5). Lesion studies show that each of RISE\u2018s components is essential to this performance. Most significantly, in 14 of the 30 domains studied, RISE is more accurate than the best of PEBLS and CN2, showing that a significant synergy can be obtained by combining multiple empirical methods."
            },
            "slug": "Unifying-Instance-Based-and-Rule-Based-Induction-Domingos",
            "title": {
                "fragments": [],
                "text": "Unifying Instance-Based and Rule-Based Induction"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "In an extensive empirical study, RISE consistently achieves higher accuracies than state-of-the-art representatives of both its parent approaches, as well as a decision tree learner (C4.5)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3139133"
                        ],
                        "name": "Razvan C. Bunescu",
                        "slug": "Razvan-C.-Bunescu",
                        "structuredName": {
                            "firstName": "Razvan",
                            "lastName": "Bunescu",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Razvan C. Bunescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 132
                            }
                        ],
                        "text": "Several approaches have employed support-vector machines tuned with natural language-oriented kernels to classify pairs of entities [13, 21, 99]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "We use a public implementation of Bunescu\u2019s Subsequence Kernel (SSK) algorithm(1) [13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 155
                            }
                        ],
                        "text": "Keeping pace with progress in machine learning, a diverse set of learning algorithms has been applied to the task of IE, including support vector machines [13, 21, 99], hidden Markov models [36], conditional random fields [63, 23] and Markov logic networks [40]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5511339,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5be027109ec592032bfeb05f1e11b45306106bd8",
            "isKey": false,
            "numCitedBy": 529,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new kernel method for extracting semantic relations between entities in natural language text, based on a generalization of subsequence kernels. This kernel uses three types of subsequence patterns that are typically employed in natural language to assert relationships between two entities. Experiments on extracting protein interactions from biomedical corpora and top-level relations from newspaper corpora demonstrate the advantages of this approach."
            },
            "slug": "Subsequence-Kernels-for-Relation-Extraction-Bunescu-Mooney",
            "title": {
                "fragments": [],
                "text": "Subsequence Kernels for Relation Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A new kernel method for extracting semantic relations between entities in natural language text, based on a generalization of subsequence kernels, is presented, which uses three types of subsequent patterns that are typically employed innatural language to assert relationships between two entities."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "One of the most widely-used and accurate statistical parsers, which was developed by Klein and Manning [50], parses newswire text with an accuracy of 91."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52353,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7395e325914b1f5caea18ea8446ef8db05662318",
            "isKey": false,
            "numCitedBy": 900,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel generative model for natural language tree structures in which semantic (lexical dependency) and syntactic (PCFG) structures are scored with separate models. This factorization provides conceptual simplicity, straightforward opportunities for separately improving the component models, and a level of performance comparable to similar, non-factored models. Most importantly, unlike other modern parsing models, the factored model admits an extremely effective A* parsing algorithm, which enables efficient, exact inference."
            },
            "slug": "Fast-Exact-Inference-with-a-Factored-Model-for-Klein-Manning",
            "title": {
                "fragments": [],
                "text": "Fast Exact Inference with a Factored Model for Natural Language Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A novel generative model for natural language tree structures in which semantic and syntactic structures are scored with separate models that admits an extremely effective A* parsing algorithm, which enables efficient, exact inference."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Thus, prior to full-scale relation extraction, the Learner uses a parser [Klein and Manning, 2003] to automatically identify and label a set of trustworthy (and untrustworthy) extractions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11495042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a600850ac0120cb09a0b7de7da80bb6a7a76de06",
            "isKey": false,
            "numCitedBy": 3370,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize."
            },
            "slug": "Accurate-Unlexicalized-Parsing-Klein-Manning",
            "title": {
                "fragments": [],
                "text": "Accurate Unlexicalized Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "It is demonstrated that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144889265"
                        ],
                        "name": "Ivan Titov",
                        "slug": "Ivan-Titov",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Titov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ivan Titov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144915758"
                        ],
                        "name": "James Henderson",
                        "slug": "James-Henderson",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Henderson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5105979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "763ccae1a5a828c6177b91fd7184854f8736bd12",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous results have shown disappointing performance when porting a parser trained on one domain to another domain where only a small amount of data is available. We propose the use of data-defined kernels as a way to exploit statistics from a source domain while still specializing a parser to a target domain. A probabilistic model trained on the source domain (and possibly also the target domain) is used to define a kernel, which is then used in a large margin classifier trained only on the target domain. With a SVM classifier and a neural network probabilistic model, this method achieves improved performance over the probabilistic model alone."
            },
            "slug": "Porting-Statistical-Parsers-with-Data-Defined-Titov-Henderson",
            "title": {
                "fragments": [],
                "text": "Porting Statistical Parsers with Data-Defined Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes the use of data-defined kernels as a way to exploit statistics from a source domain while still specializing a parser to a target domain with improved performance over the probabilistic model alone."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706729"
                        ],
                        "name": "G. Ngai",
                        "slug": "G.-Ngai",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Ngai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ngai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707117"
                        ],
                        "name": "Radu Florian",
                        "slug": "Radu-Florian",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Florian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radu Florian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "part-of-speech tags and noun phrases can be modeled with high accuracy across domains and languages [Brill and Ngai, 1999; Ngai and Florian, 2001]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6665511,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c52f80f056a2de8f503bf912e8025413ec2111ec",
            "isKey": false,
            "numCitedBy": 252,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Transformation-based learning has been successfully employed to solve many natural language processing problems. It achieves state-of-the-art performance on many natural language processing tasks and does not overtrain easily. However, it does have a serious drawback: the training time is often intorelably long, especially on the large corpora which are often used in NLP. In this paper, we present a novel and realistic method for speeding up the training time of a transformation-based learner without sacrificing performance. The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems: a standard transformation-based learner, and the ICA system (Hepple, 2000). The results of these experiments show that our system is able to achieve a significant improvement in training time while still achieving the same performance as a standard transformation-based learner. This is a valuable contribution to systems and algorithms which utilize transformation-based learning at any part of the execution."
            },
            "slug": "Transformation-Based-Learning-in-the-Fast-Lane-Ngai-Florian",
            "title": {
                "fragments": [],
                "text": "Transformation Based Learning in the Fast Lane"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents a novel and realistic method for speeding up the training time of a transformation-based learner without sacrificing performance and shows that this system is able to achieve a significant improvement in training time while still achieving the same performance as a standard transformation- based learner."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3199842"
                        ],
                        "name": "Barbara Rosario",
                        "slug": "Barbara-Rosario",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Rosario",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barbara Rosario"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 270818,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc34d70f8b0a7dd35725f03463221705eb67cefb",
            "isKey": false,
            "numCitedBy": 292,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "A crucial step toward the goal of automatic extraction of propositional information from natural language text is the identification of semantic relations between constituents in sentences. We examine the problem of distinguishing among seven relation types that can occur between the entities \"treatment\" and \"disease\" in bioscience text, and the problem of identifying such entities. We compare five generative graphical models and a neural network, using lexical, syntactic, and semantic features, finding that the latter help achieve high classification accuracy."
            },
            "slug": "Classifying-Semantic-Relations-in-Bioscience-Texts-Rosario-Hearst",
            "title": {
                "fragments": [],
                "text": "Classifying Semantic Relations in Bioscience Texts"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work examines the problem of distinguishing among seven relation types that can occur between the entities \"treatment\" and \"disease\" in bioscience text, and finds that the latter help achieve high classification accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145644673"
                        ],
                        "name": "Feng Jiao",
                        "slug": "Feng-Jiao",
                        "structuredName": {
                            "firstName": "Feng",
                            "lastName": "Jiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Feng Jiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721114"
                        ],
                        "name": "Shaojun Wang",
                        "slug": "Shaojun-Wang",
                        "structuredName": {
                            "firstName": "Shaojun",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaojun Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109465325"
                        ],
                        "name": "Chi-Hoon Lee",
                        "slug": "Chi-Hoon-Lee",
                        "structuredName": {
                            "firstName": "Chi-Hoon",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chi-Hoon Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143686063"
                        ],
                        "name": "R. Greiner",
                        "slug": "R.-Greiner",
                        "structuredName": {
                            "firstName": "Russell",
                            "lastName": "Greiner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Greiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714772"
                        ],
                        "name": "Dale Schuurmans",
                        "slug": "Dale-Schuurmans",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Schuurmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dale Schuurmans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 89684,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5f5110d65eda0d2df7329582a232a86bf9a3a65",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new semi-supervised training procedure for conditional random fields (CRFs) that can be used to train sequence segmentors and labelers from a combination of labeled and unlabeled training data. Our approach is based on extending the minimum entropy regularization framework to the structured prediction case, yielding a training objective that combines unlabeled conditional entropy with labeled conditional likelihood. Although the training objective is no longer concave, it can still be used to improve an initial model (e.g. obtained from supervised training) by iterative ascent. We apply our new training algorithm to the problem of identifying gene and protein mentions in biological texts, and show that incorporating unlabeled data improves the performance of the supervised CRF in this case."
            },
            "slug": "Semi-Supervised-Conditional-Random-Fields-for-and-Jiao-Wang",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Conditional Random Fields for Improved Sequence Segmentation and Labeling"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A new semi-supervised training procedure for conditional random fields (CRFs) that can be used to train sequence segmentors and labelers from a combination of labeled and unlabeled training data is presented, based on extending the minimum entropy regularization framework to the structured prediction case."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211746"
                        ],
                        "name": "David A. Hull",
                        "slug": "David-A.-Hull",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hull",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. Hull"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The increase was found to be statistically significant at the 90% level, according one-tailed paired t-test [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7149948,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6525eded7f93c6890563ea7bf870b513a67e18c",
            "isKey": false,
            "numCitedBy": 569,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The standard strategies for evaluation based on precision and recall are examined and their relative advantages and disadvantages are discussed. In particular, it is suggested that relevance feedback be evaluated from the perspective of the user. A number of different statistical tests are described for determining if differences in performance between retrieval methods are significant. These tests have often been ignored in the past because most are based on an assumption of normality which is not strictly valid for the standard performance measures. However, one can test this assumption using simple diagnostic plots, and if it is a poor approximation, there are a number of non-parametric alternatives."
            },
            "slug": "Using-statistical-testing-in-the-evaluation-of-Hull",
            "title": {
                "fragments": [],
                "text": "Using statistical testing in the evaluation of retrieval experiments"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is suggested that relevance feedback be evaluated from the perspective of the user and a number of different statistical tests are described for determining if differences in performance between retrieval methods are significant."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624000"
                        ],
                        "name": "Zhi-Hua Zhou",
                        "slug": "Zhi-Hua-Zhou",
                        "structuredName": {
                            "firstName": "Zhi-Hua",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi-Hua Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40655383"
                        ],
                        "name": "Ming Li",
                        "slug": "Ming-Li",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8574005,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa4fac1e42c402c093ddcabe9653adaaf2eb2449",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "The traditional setting of supervised learning requires a large amount of labeled training examples in order to achieve good generalization. However, in many practical applications, unlabeled training examples are readily available but labeled ones are fairly expensive to obtain. Therefore, semi-supervised learning has attracted much attention. Previous research on semi-supervised learning mainly focuses on semi-supervised classification. Although regression is almost as important as classification, semisupervised regression is largely understudied. In particular, although co-training is a main paradigm in semi-supervised learning, few works has been devoted to co-training style semi-supervised regression algorithms. In this paper, a co-training style semi-supervised regression algorithm, i.e. COREG, is proposed. This algorithm uses two regressors each labels the unlabeled data for the other regressor, where the confidence in labeling an unlabeled example is estimated through the amount of reduction in mean square error over the labeled neighborhood of that example. Analysis and experiments show that COREG can effectively exploit unlabeled data to improve regression estimates."
            },
            "slug": "Semi-Supervised-Regression-with-Co-Training-Style-Zhou-Li",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Regression with Co-Training Style Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Analysis and experiments show that COREG can effectively exploit unlabeled data to improve regression estimates and is proposed as a co-training style semi-supervised regression algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157221"
                        ],
                        "name": "E. T. K. Sang",
                        "slug": "E.-T.-K.-Sang",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sang",
                            "middleNames": [
                                "Tjong",
                                "Kim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. T. K. Sang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782178"
                        ],
                        "name": "S. Buchholz",
                        "slug": "S.-Buchholz",
                        "structuredName": {
                            "firstName": "Sabine",
                            "lastName": "Buchholz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Buchholz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 119
                            }
                        ],
                        "text": "Data-driven taggers and chunkers for English have been shown to achieve a precision of up to 97% and 94%, respectively [89, 90], and are generally considered to be robust."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8940645,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "9e85832b04cc3700c2c26d6ba93fdeae39cac04a",
            "isKey": false,
            "numCitedBy": 871,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the CoNLL-2000 shared task: dividing text into syntactically related non-overlapping groups of words, so-called text chunking. We give background information on the data sets, present a general overview of the systems that have taken part in the shared task and briefly discuss their performance."
            },
            "slug": "Introduction-to-the-CoNLL-2000-Shared-Task-Chunking-Sang-Buchholz",
            "title": {
                "fragments": [],
                "text": "Introduction to the CoNLL-2000 Shared Task Chunking"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "The CoNLL-2000 shared task: dividing text into syntactically related non-overlapping groups of words, so-called text chunking is described."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL/LLL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "Stacked generalization, or stacking, [94], is an ensemble-based framework in which the goal is learn a meta-classifier from the output of several base-level classifiers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5895004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e1291583873fb890e7922ec0dfefd4846df46c9",
            "isKey": false,
            "numCitedBy": 5480,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stacked-generalization-Wolpert",
            "title": {
                "fragments": [],
                "text": "Stacked generalization"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145914024"
                        ],
                        "name": "R. Lathrop",
                        "slug": "R.-Lathrop",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lathrop",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lathrop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388700951"
                        ],
                        "name": "Tomas Lozano-Perez",
                        "slug": "Tomas-Lozano-Perez",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Lozano-Perez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Lozano-Perez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7398727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c7d38f68fe1150895a186e30b60c02dd89a676a",
            "isKey": false,
            "numCitedBy": 2428,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Solving-the-Multiple-Instance-Problem-with-Dietterich-Lathrop",
            "title": {
                "fragments": [],
                "text": "Solving the Multiple Instance Problem with Axis-Parallel Rectangles"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145702640"
                        ],
                        "name": "Guido Minnen",
                        "slug": "Guido-Minnen",
                        "structuredName": {
                            "firstName": "Guido",
                            "lastName": "Minnen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guido Minnen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35096557"
                        ],
                        "name": "Darren Pearce",
                        "slug": "Darren-Pearce",
                        "structuredName": {
                            "firstName": "Darren",
                            "lastName": "Pearce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Darren Pearce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 34553826,
            "fieldsOfStudy": [
                "Materials Science"
            ],
            "id": "e979925b15861153a0e9ce8ace39a28d319e613d",
            "isKey": false,
            "numCitedBy": 386,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe two newly developed computational tools for morphological processing: a program for analysis of English inflectional morphology, and a morphological generator, automatically derived from the analyser. The tools are fast, being based on finite-state techniques, have wide coverage, incorporating data from various corpora and machine readable dictionaries, and are robust, in that they are able to deal effectively with unknown words. The tools are freely available. We evaluate the accuracy and speed of both tools and discuss a number of practical applications in which they have been put to use."
            },
            "slug": "Applied-morphological-processing-of-English-Minnen-Carroll",
            "title": {
                "fragments": [],
                "text": "Applied morphological processing of English"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "Two newly developed computational tools for morphological processing are described: a program for analysis of English inflectional morphology, and a morphological generator, automatically derived from the analyser, which are fast, being based on finite-state techniques, and robust, in that they are able to deal effectively with unknown words."
            },
            "venue": {
                "fragments": [],
                "text": "Natural Language Engineering"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113414328"
                        ],
                        "name": "Fernando Pereira",
                        "slug": "Fernando-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Conditional Random Fields (CRFs) [52], are undirected graphical models trained to maximize the conditional probability of a finite set of labels Y given a set of input observations X."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 219683473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4ba954b0412773d047dc41231c733de0c1f4926",
            "isKey": false,
            "numCitedBy": 13411,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "slug": "Conditional-Random-Fields:-Probabilistic-Models-for-Lafferty-McCallum",
            "title": {
                "fragments": [],
                "text": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents iterative parameter estimation algorithms for conditional random fields and compares the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780015"
                        ],
                        "name": "V. D. Sa",
                        "slug": "V.-D.-Sa",
                        "structuredName": {
                            "firstName": "Virginia",
                            "lastName": "Sa",
                            "middleNames": [
                                "R.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Sa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9890353,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5bef9f0d1e74409ca0e67f79f9547c5f4e4e4257",
            "isKey": false,
            "numCitedBy": 222,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the advantages of supervised learning is that the final error metric is available during training. For classifiers, the algorithm can directly reduce the number of misclassifications on the training set. Unfortunately, when modeling human learning or constructing classifiers for autonomous robots, supervisory labels are often not available or too expensive. In this paper we show that we can substitute for the labels by making use of structure between the pattern distributions to different sensory modalities. We show that minimizing the disagreement between the outputs of networks processing patterns from these different modalities is a sensible approximation to minimizing the number of misclassifications in each modality, and leads to similar results. Using the Peterson-Barney vowel dataset we show that the algorithm performs well in finding appropriate placement for the codebook vectors particularly when the confuseable classes are different for the two modalities."
            },
            "slug": "Learning-Classification-with-Unlabeled-Data-Sa",
            "title": {
                "fragments": [],
                "text": "Learning Classification with Unlabeled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper shows that minimizing the disagreement between the outputs of networks processing patterns from these different modalities is a sensible approximation to minimizing the number of misclassifications in each modality, and leads to similar results."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2754495"
                        ],
                        "name": "Massimiliano Ciaramita",
                        "slug": "Massimiliano-Ciaramita",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Ciaramita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Massimiliano Ciaramita"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2420171"
                        ],
                        "name": "Aldo Gangemi",
                        "slug": "Aldo-Gangemi",
                        "structuredName": {
                            "firstName": "Aldo",
                            "lastName": "Gangemi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aldo Gangemi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3059056"
                        ],
                        "name": "E. Ratsch",
                        "slug": "E.-Ratsch",
                        "structuredName": {
                            "firstName": "Esther",
                            "lastName": "Ratsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ratsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34669680"
                        ],
                        "name": "Jasmin Saric",
                        "slug": "Jasmin-Saric",
                        "structuredName": {
                            "firstName": "Jasmin",
                            "lastName": "Saric",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jasmin Saric"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145965398"
                        ],
                        "name": "I. Rojas",
                        "slug": "I.-Rojas",
                        "structuredName": {
                            "firstName": "Isabel",
                            "lastName": "Rojas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Rojas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1566451,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "853d1f221e629dd4143a2221e0de5f4bee71bb28",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present an unsupervised model for learning arbitrary relations between concepts of a molecular biology ontology for the purpose of supporting text mining and manual ontology building. Relations between named-entities are learned from the GENIA corpus by means of several standard natural language processing techniques. An in-depth analysis of the output of the system shows that the model is accurate and has good potentials for text mining and ontology building applications."
            },
            "slug": "Unsupervised-Learning-of-Semantic-Relations-between-Ciaramita-Gangemi",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Semantic Relations between Concepts of a Molecular Biology Ontology"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "An in-depth analysis of the output of the system shows that the model is accurate and has good potentials for text mining and ontology building applications."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741396"
                        ],
                        "name": "B. \u017denko",
                        "slug": "B.-\u017denko",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "\u017denko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. \u017denko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693549"
                        ],
                        "name": "S. D\u017eeroski",
                        "slug": "S.-D\u017eeroski",
                        "structuredName": {
                            "firstName": "Sa\u0161o",
                            "lastName": "D\u017eeroski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D\u017eeroski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Previous studies [88, 100, 79] have shown that the probabilities of each class value as estimated by each base-level algorithm are more effective features when training metalearners."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15992248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6014ff14ff42925d80056c27732cf48351c21803",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new set of meta-level features to be used for learning how to combine classifier predictions with stacking. This set includes the probability distributions predicted by the base-level classifiers and a combination of these with the certainty of the predictions. We use these features in conjunction with multi-response linear regression (MLR) at the meta-level. We empirically evaluate the proposed approach in comparison to several state-of-the-art methods for constructing ensembles of heterogeneous classifiers with stacking. Our approach performs better than existing stacking approaches and also better than selecting the best classifier from the ensemble by cross validation (unlike existing stacking approaches, which at best perform comparably to it)."
            },
            "slug": "Stacking-with-an-Extended-Set-of-Meta-level-and-MLR-\u017denko-D\u017eeroski",
            "title": {
                "fragments": [],
                "text": "Stacking with an Extended Set of Meta-level Attributes and MLR"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work proposes a new set of meta-level features to be used for learning how to combine classifier predictions with stacking, which includes the probability distributions predicted by the base-level classifiers and a combination of these with the certainty of the predictions."
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808676"
                        ],
                        "name": "V. Sindhwani",
                        "slug": "V.-Sindhwani",
                        "structuredName": {
                            "firstName": "Vikas",
                            "lastName": "Sindhwani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sindhwani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10331842,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5098221cf78ba60b5afd26c171da50baf2670996",
            "isKey": false,
            "numCitedBy": 381,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The Co-Training algorithm uses unlabeled examples in multiple views to bootstrap classifiers in each view, typically in a greedy manner, and operating under assumptions of view-independence and compatibility. In this paper, we propose a Co-Regularization framework where classifiers are learnt in each view through forms of multi-view regularization. We propose algorithms within this framework that are based on optimizing measures of agreement and smoothness over labeled and unlabeled examples. These algorithms naturally extend standard regularization methods like Support Vector Machines (SVM) and Regularized Least squares (RLS) for multi-view semi-supervised learning, and inherit their benefits and applicability to high-dimensional classification problems. An empirical investigation is presented that confirms the promise of this approach."
            },
            "slug": "A-Co-Regularization-Approach-to-Semi-supervised-Sindhwani-Niyogi",
            "title": {
                "fragments": [],
                "text": "A Co-Regularization Approach to Semi-supervised Learning with Multiple Views"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper proposes a Co-Regularization framework where classifiers are learnt in each view through forms of multi-view regularization, and proposes algorithms within this framework that are based on optimizing measures of agreement and smoothness over labeled and unlabeled examples."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35212893"
                        ],
                        "name": "K. Ting",
                        "slug": "K.-Ting",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Ting",
                            "middleNames": [
                                "Ming"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ting"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Previous studies [88, 100, 79] have shown that the probabilities of each class value as estimated by each base-level algorithm are more effective features when training metalearners."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1616522,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d5a28ea30e03398f7867ac7ef79dd497fb67864",
            "isKey": false,
            "numCitedBy": 639,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Stacked generalization is a general method of using a high-level model to combine lower-level models to achieve greater predictive accuracy. In this paper we address two crucial issues which have been considered to be a 'black art' in classification tasks ever since the introduction of stacked generalization in 1992 by Wolpert: the type of generalizer that is suitable to derive the higher-level model, and the kind of attributes that should be used as its input. We find that best results are obtained when the higher-level model combines the confidence (and not just the predictions) of the lower-level ones. \n \nWe demonstrate the effectiveness of stacked generalization for combining three different types of learning algorithms for classification tasks. We also compare the performance of stacked generalization with majority vote and published results of arcing and bagging."
            },
            "slug": "Issues-in-Stacked-Generalization-Ting-Witten",
            "title": {
                "fragments": [],
                "text": "Issues in Stacked Generalization"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This paper addresses two crucial issues which have been considered to be a 'black art' in classification tasks ever since the introduction of stacked generalization: the type of generalizer that is suitable to derive the higher-level model, and the kind of attributes that should be used as its input."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2731141,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "a007f46b3303bdb50e705b441c367e595666538c",
            "isKey": false,
            "numCitedBy": 3963,
            "numCiting": 324,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Semi-Supervised-Learning-Literature-Survey-Zhu",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning Literature Survey"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143740826"
                        ],
                        "name": "Andrew R. Bailey",
                        "slug": "Andrew-R.-Bailey",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Bailey",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew R. Bailey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 170886736,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "a560fad06fd18076c8c6339f487b27d06e660bd8",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Evaluation de la critique platonicienne de la these relativiste de Protagoras selon laquelle l'homme est la mesure de toutes choses, developpee dans le \u00abTheetete\u00bb (169d-171d). Examinant la conception du jugement, de la connaissance, de la perception et du subjectivisme chez Protagoras, l'A. montre que l'argument de l'autorefutation, developpe par Socrate a partir de l'idee d'inconsistance, ne demontre pas la faussete de la doctrine relativiste: elle montre pour le moins que celle-ci est relative au fait plus qu'a la verite"
            },
            "slug": "Is-man-the-measure-Bailey",
            "title": {
                "fragments": [],
                "text": "Is man the measure"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 59
                            }
                        ],
                        "text": "These observations have sparked several extraction efforts [64, 86, 96] that focus on mining these semi-structured resources as opposed to the unstructured text of Wikipedia articles."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Daniel S. Weld (weld@cs.washington.edu) is the Thomas j. cable/Wrf \nProfessor of computer science and Engineering at the university of Washington, seattle."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "Weld, d., \nWu, f., adar, E., amershi, s., fogarty, j., hoffmann, r., Patel, k. and skinner, m. intelligence in Wikipedia. \nin Proceedings of the 23rd Conference on Artificial Intelligence (2008)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "As noted by Wu and Weld [96], there are several characteristics of Wikipedia that have made it an increasingly popular target for extraction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "Etzioni, o., cafarella, \nm., downey, d., kok, s., Popescu, a., shaked, T., soderland, s., Weld, d. and yates, a. unsupervised \nnamed-entity extraction from the Web: an experimental study."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatically semantifying wikipedia"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 16th Conference on Information and Knowledge Management (CIKM)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34780795"
                        ],
                        "name": "D. Walker",
                        "slug": "D.-Walker",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Walker",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Walker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792160"
                        ],
                        "name": "L. M. Norton",
                        "slug": "L.-M.-Norton",
                        "structuredName": {
                            "firstName": "Lewis",
                            "lastName": "Norton",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. M. Norton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62145080,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "ae405d63528a1be39e9ef71a3b420bed4a3d4f1f",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Proceedings-of-the-International-Joint-Conference-:-Walker-Norton",
            "title": {
                "fragments": [],
                "text": "Proceedings of the International Joint Conference on Artificial Intelligence : IJCAI-69, 7-9 May 1969, Washington, D.C."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3115414"
                        ],
                        "name": "A. Nenkova",
                        "slug": "A.-Nenkova",
                        "structuredName": {
                            "firstName": "Ani",
                            "lastName": "Nenkova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nenkova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31983004"
                        ],
                        "name": "Jason M. Brenier",
                        "slug": "Jason-M.-Brenier",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Brenier",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason M. Brenier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27265935"
                        ],
                        "name": "Anubha Kothari",
                        "slug": "Anubha-Kothari",
                        "structuredName": {
                            "firstName": "Anubha",
                            "lastName": "Kothari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anubha Kothari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143978026"
                        ],
                        "name": "S. Calhoun",
                        "slug": "S.-Calhoun",
                        "structuredName": {
                            "firstName": "Sasha",
                            "lastName": "Calhoun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Calhoun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096098955"
                        ],
                        "name": "L. Whitton",
                        "slug": "L.-Whitton",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Whitton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Whitton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153937601"
                        ],
                        "name": "D. Beaver",
                        "slug": "D.-Beaver",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Beaver",
                            "middleNames": [
                                "Ian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Beaver"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144138733"
                        ],
                        "name": "D. Jurafsky",
                        "slug": "D.-Jurafsky",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Jurafsky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57100246,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "6d4d302c7aeb48ba2b89905ae22ee99a2910bebc",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "NAACL-HLT-2007-Human-Language-Technologies-2007:-of-Nenkova-Brenier",
            "title": {
                "fragments": [],
                "text": "NAACL HLT 2007 - Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, Proceedings of the Main Conference"
            },
            "venue": {
                "fragments": [],
                "text": "HLT-NAACL 2007"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685296"
                        ],
                        "name": "Eugene Agichtein",
                        "slug": "Eugene-Agichtein",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Agichtein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Agichtein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684012"
                        ],
                        "name": "L. Gravano",
                        "slug": "L.-Gravano",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Gravano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gravano"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 56947569,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90e067c6c001a4ec549d5e141736295a77004039",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Extracting-Relations-from-Large-Plain-Text-Agichtein-Gravano",
            "title": {
                "fragments": [],
                "text": "Extracting Relations from Large Plain-Text Collections"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40608958,
            "fieldsOfStudy": [],
            "id": "03dd72487691c918a6b524a2f796ef3eb7b566c9",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Self-supervised relation extraction from the Web"
            },
            "venue": {
                "fragments": [],
                "text": "Knowledge and Information Systems"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "chapter Processing Natural Language without Natural Language Processing"
            },
            "venue": {
                "fragments": [],
                "text": "chapter Processing Natural Language without Natural Language Processing"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "This issue was addressed in a subsequent implementation, KnowItNow [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "KnowItNow: Fast"
            },
            "venue": {
                "fragments": [],
                "text": "scalable information extraction from the web. In Proceedings of EMLNP"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Integrating probabilistic extraction models and relational data mining to discover relations and patterns in text"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of HLT-NAACL"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Integrating probabilistic extraction models and relation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "soderland, s., broadhead, m. and Etzioni, o. open information extraction from the Web"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the International Joint Conference on Artificial Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "While in previous work [5] we reported experiments in which the distinction between abstract and concrete tuples was made by hand, the experiments we report on use part-of-speech tag information about the entities under consideration to automatically characterize TextRunner\u2019s output."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We first treated open extraction as a classification problem [5], using the set of self-labeled examples to learn a Naive Bayes classifier that predicted whether heuristically-chosen tokens surrounding two entities indicated a relationship or not."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Open information extraction from the web"
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI 2007"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Man of Measure, chapter Two Strategies for Text Parsing"
            },
            "venue": {
                "fragments": [],
                "text": "SKY Journal of Linguistics"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IJCAI-07"
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI-07"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proceedings of the 3rd Message Understanding Conference"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 3rd Message Understanding Conference"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Man (and woman) vs. machine: a case study in base noun phrase learning Extracting Patterns and Relations from the World Wide Web"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the ACL WebDB Workshop at 6th International Conference on Extending Database Technology, EDBT'98"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 42
                            }
                        ],
                        "text": "and Mooney, 2005], maximum-entropy models [Kambhatla, 2004], graphical models [Rosario and Hearst, 2004; Culotta et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combining lexical"
            },
            "venue": {
                "fragments": [],
                "text": "syntactic and semantic features with maximum entropy models. In Proceedings of ACL"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Manning . Accurate unlexicalized parsing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "amershi, s., fogarty, j., hoffmann, r., Patel, k. and skinner, m. intelligence in Wikipedia"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 23rd Conference on Artificial Intelligence"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "brin, s. Extracting patterns and relations from the World Wide Web"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Workshop at the 6th International Conference on Extending Database Technology"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "Modern IE, beginning with the works of Soderland [84, 83], Riloff [69] and Moldovan and Kim [48], automatically learned an extractor from a training set in which domain-specific examples were tagged."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 57
                            }
                        ],
                        "text": "Modern IE, beginning with the works of Soder\u00ad land,21, 22 Riloff,17 \nand Kim and Mol\u00addovan,11 automatically learns an ex\u00adtractor from a training set in which domain-specific \nexamples have been tagged."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatically constructing extraction patterns from untagged text"
            },
            "venue": {
                "fragments": [],
                "text": "Procs. of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), pages 1044\u20131049"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 81
                            }
                        ],
                        "text": "In light of the success of CRFs, we developed an extractor, referred to as O-crf [7], which uses a second-order linear chain CRF to learn whether sequences of tokens are part of a salient relation or not."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The tradeoffs between traditional and open relation extraction"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Association for Computational Linguistics (ACL)"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Inducing a conceptual dictionary"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 14 th International Joint Conference on Artificial Intelligence"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Open knowledge extraction using compositional language processing"
            },
            "venue": {
                "fragments": [],
                "text": "Symposium on Semantics in Systems for Text Processing"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "In 2008, Knext was extended to analyze arbitrary inputs using a parser trained from the Penn Treebank [32], and subsequently deployed over a Web corpus of nearly 11."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Open knowledge extraction using compositional language processing"
            },
            "venue": {
                "fragments": [],
                "text": "Symposium on Semantics in Systems for Text Processing "
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Oren Etzioni (etzioni@cs.washington.edu) is a professor of computer science and the founder and director of the Turing center at the university of Washington"
            },
            "venue": {
                "fragments": [],
                "text": "Oren Etzioni (etzioni@cs.washington.edu) is a professor of computer science and the founder and director of the Turing center at the university of Washington"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 100
                            }
                        ],
                        "text": "part-of-speech tags and noun phrases can be modeled with high accuracy across domains and languages [Brill and Ngai, 1999; Ngai and Florian, 2001]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Man (and woman) vs"
            },
            "venue": {
                "fragments": [],
                "text": "machine: a case study in base noun phrase learning. In Proceedings of the ACL, pages 65\u201372"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 130
                            }
                        ],
                        "text": "These include relationship queries, unnamed-item queries , and multiple-attribute queries, each of which is described in detail in[Cafarellaet al., 2006]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Technical Report 06-04-02"
            },
            "venue": {
                "fragments": [],
                "text": "University of Washington,"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "The Alice system [6] demonstrated it could discover high-level concepts and relations among them from the output of TextRunner."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Strategies for lifelong extraction from the web"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings on ACM Conference on Knowledge Capture"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Michele Banko (banko@cs.washington.edu) is a Ph.d. candidate at the university of Washington"
            },
            "venue": {
                "fragments": [],
                "text": "Michele Banko (banko@cs.washington.edu) is a Ph.d. candidate at the university of Washington"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "O-crf was built using the CRF implementation provided by Mallet [55], as well as part-of-speech tagging and phrase-chunking tools available from OpenNLP."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mallet: A machine learning for language toolkit"
            },
            "venue": {
                "fragments": [],
                "text": "http://mallet.cs.umass.edu"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatically constructing extraction patterns from untagged text Classifying semantic relations in bioscience text"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of AAAI Proc. of ACL"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and Pereira, f. conditional random fields: Probabilistic models for segmenting and labeling sequence data Efficiently inducing features of conditional random fields"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 International Conference on Machine Learning. 13. mccallum Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence (acapulco"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and Etzioni, o. The tradeoffs between traditional and open relation extraction"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Association of Computational Linguistics"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stephen Soderland (soderland@cs.washington.edu) is a research scientist in the department of computer science and Engineering at the university of Washington"
            },
            "venue": {
                "fragments": [],
                "text": "Stephen Soderland (soderland@cs.washington.edu) is a research scientist in the department of computer science and Engineering at the university of Washington"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 281
                            }
                        ],
                        "text": "One alternative proposed by the empirical natural language processing community in recent years is to forgo making improvements to deep linguistic analyzers in favor of employing techniques light on natural language understanding over significantly larger datasets such as the Web [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lecture Notes in Computer Science"
            },
            "venue": {
                "fragments": [],
                "text": "chapter Processing Natural Language without Natural Language Processing, pages 179\u2013185. Springer Berlin / Heidelberg"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ontology is not necessary for reasoning 2. Open IE is \" gracefully \" ontologized 3. Open IE is boosting text analysis 4. LOD has distribution & scale (but not text) = opportunity Etzioni"
            },
            "venue": {
                "fragments": [],
                "text": "Ontology is not necessary for reasoning 2. Open IE is \" gracefully \" ontologized 3. Open IE is boosting text analysis 4. LOD has distribution & scale (but not text) = opportunity Etzioni"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extending Propbank with Verbnet semantic predicates"
            },
            "venue": {
                "fragments": [],
                "text": "AMTA Workshop on Applied Interlinguas"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Etzioni, o. and soderland, s. a probabilistic model of redundancy in information"
            },
            "venue": {
                "fragments": [],
                "text": "Etzioni, o. and soderland, s. a probabilistic model of redundancy in information"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 204
                            }
                        ],
                        "text": "While the exact number of relations in the English language is unknown, we can reasonably estimate the number to be tens of thousands according to existing lexical resources such as WordNet [56], VerbNet [46] and PropBank [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "VerbNet [46] and PropBank [3], two verb lexicons for the English language, contain 5000 and 3600 semantically distinct entries, respectively."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extending Propbank with Verbnet semantic predicates"
            },
            "venue": {
                "fragments": [],
                "text": "In AMTA Workshop on Applied Interlinguas,"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "http://en.wikipedia.org/wiki/Wikipedia$:$Size comparisons"
            },
            "venue": {
                "fragments": [],
                "text": "http://en.wikipedia.org/wiki/Wikipedia$:$Size comparisons"
            },
            "year": 2008
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 42,
            "methodology": 42,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 149,
        "totalPages": 15
    },
    "page_url": "https://www.semanticscholar.org/paper/Open-Information-Extraction-from-the-Web-Banko-Cafarella/498bb0efad6ec15dd09d941fb309aa18d6df9f5f?sort=total-citations"
}