{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110551642"
                        ],
                        "name": "Chih-Wei Hsu",
                        "slug": "Chih-Wei-Hsu",
                        "structuredName": {
                            "firstName": "Chih-Wei",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Wei Hsu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15874442,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f755d620b57acf27a16ff95923c5677ff8198bb",
            "isKey": false,
            "numCitedBy": 6346,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Support vector machines (SVMs) were originally designed for binary classification. How to effectively extend it for multiclass classification is still an ongoing research issue. Several methods have been proposed where typically we construct a multiclass classifier by combining several binary classifiers. Some authors also proposed methods that consider all classes at once. As it is computationally more expensive to solve multiclass problems, comparisons of these methods using large-scale problems have not been seriously conducted. Especially for methods solving multiclass SVM in one step, a much larger optimization problem is required so up to now experiments are limited to small data sets. In this paper we give decomposition implementations for two such \"all-together\" methods. We then compare their performance with three methods based on binary classifications: \"one-against-all,\" \"one-against-one,\" and directed acyclic graph SVM (DAGSVM). Our experiments indicate that the \"one-against-one\" and DAG methods are more suitable for practical use than the other methods. Results also show that for large problems methods by considering all data at once in general need fewer support vectors."
            },
            "slug": "A-comparison-of-methods-for-multiclass-support-Hsu-Lin",
            "title": {
                "fragments": [],
                "text": "A comparison of methods for multiclass support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Decomposition implementations for two \"all-together\" multiclass SVM methods are given and it is shown that for large problems methods by considering all data at once in general need fewer support vectors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3242194"
                        ],
                        "name": "Ghulum Bakiri",
                        "slug": "Ghulum-Bakiri",
                        "structuredName": {
                            "firstName": "Ghulum",
                            "lastName": "Bakiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ghulum Bakiri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 70
                            }
                        ],
                        "text": "This generalization is closest to the ECOC approach of Dietterich and Bakiri (1995) but differs in that the coding matrix is taken from the larger set f 1;0;+1gk `."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 15
                            }
                        ],
                        "text": "Dietterich and Bakiri (1995) presented a general framework in which the classes are partitioned into opposing subsets using error-correcting codes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 86
                            }
                        ],
                        "text": "A more general suggestion on handling multiclass problems was given by Dietterich and Bakiri (1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 47109072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d221bbcbd20c7157e4500f942de8ceec490f8936",
            "isKey": false,
            "numCitedBy": 2852,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k > 2 values (i.e., k \"classes\"). The definition is acquired by studying collections of training examples of the form (xi, f(xi)). Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that--like the other methods--the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems."
            },
            "slug": "Solving-Multiclass-Learning-Problems-via-Output-Dietterich-Bakiri",
            "title": {
                "fragments": [],
                "text": "Solving Multiclass Learning Problems via Error-Correcting Output Codes"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is demonstrated that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721077"
                        ],
                        "name": "V. Guruswami",
                        "slug": "V.-Guruswami",
                        "structuredName": {
                            "firstName": "Venkatesan",
                            "lastName": "Guruswami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Guruswami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695851"
                        ],
                        "name": "A. Sahai",
                        "slug": "A.-Sahai",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Sahai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sahai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 93
                            }
                        ],
                        "text": "These results generalize more specialized bounds proved by Schapire and Singer (1999) and by Guruswami and Sahai (1999)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 31
                            }
                        ],
                        "text": "Another approach, suggested by Hastie and Tibshirani (1998), is to use the given binary learning algorithm to distinguish each pair of classes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 53
                            }
                        ],
                        "text": "Also, Corollary 2 generalizes some of the results of Guruswami and Sahai (1999) that bound the multiclass training error in terms of the training (misclassification) error rates of the binary classifiers."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 182
                            }
                        ],
                        "text": "We note in passing that the loss-based decoding method for log-loss is the well known and widely used maximum-likelihood decoding which was studied briefly in the context of ECOC by Guruswami and Sahai (1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1873928,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dd0b6ab9f6d47cbf5a8e3dfd15093e31a357559",
            "isKey": true,
            "numCitedBy": 187,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We focus on methods to solve multiclass learning problems by using only simple and efficient binary learners. We investigate the approach of Dietterich and Bakiri [2] based on error-correcting codes (which we call ECC). We distill error correlation as one of the key parameters influencing the performance of the ECC approach, and prove upper and lower bounds on the training error of the final hypothesis in terms of the error-correlation between the various binary hypotheses. Boosting is a powerful and well-studied learning technique that appears to annul error correlation disadvantages by cleverly weighting training examples and hypotheses. An interesting algorithm called ADABOOST.OC [12] combines boosting with the ECC approach and gives an algorithm that has the performance advantages of boosting and at the same time relies only on simple binary weak learners. We propose a variant of this algorithm, which we call ADABOOST.ECC, that, by using a different weighting of the votes of the weak hypotheses, is able to improve on the performance of ADABOOST.OC, both theoretically and experimentally, and in addition is arguably a more direct reduction of multiclass learning to binary learning problems than previous multiclass boosting algorithms."
            },
            "slug": "Multiclass-learning,-boosting,-and-error-correcting-Guruswami-Sahai",
            "title": {
                "fragments": [],
                "text": "Multiclass learning, boosting, and error-correcting codes"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "ECC, that, by using a different weighting of the votes of the weak hypotheses, is able to improve on the performance of ADABOOST.OC, is arguably a more direct reduction of multiclass learning to binary learning problems than previous multiclass boosting algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2329907,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a9309e056272ff2076f447df8dbc536f46fc466",
            "isKey": false,
            "numCitedBy": 1919,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe several improvements to Freund and Schapire's AdaBoost boosting algorithm, particularly in a setting in which hypotheses may assign confidences to each of their predictions. We give a simplified analysis of AdaBoost in this setting, and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses. We give a specific method for assigning confidences to the predictions of decision trees, a method closely related to one used by Quinlan. This method also suggests a technique for growing decision trees which turns out to be identical to one proposed by Kearns and Mansour. We focus next on how to apply the new boosting algorithms to multiclass classification problems, particularly to the multi-label case in which each example may belong to more than one class. We give two boosting methods for this problem, plus a third method based on output coding. One of these leads to a new method for handling the single-label case which is simpler but as effective as techniques suggested by Freund and Schapire. Finally, we give some experimental results comparing a few of the algorithms discussed in this paper."
            },
            "slug": "Improved-Boosting-Algorithms-Using-Confidence-rated-Schapire-Singer",
            "title": {
                "fragments": [],
                "text": "Improved Boosting Algorithms Using Confidence-rated Predictions"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Several improvements to Freund and Schapire's AdaBoost boosting algorithm are described, particularly in a setting in which hypotheses may assign confidences to each of their predictions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 173
                            }
                        ],
                        "text": "In logistic regression and related methods such as Iterative Scaling (Csisz\u00e1r & Tusn\u00e1dy, 1984; Della Pietra, Della Pietra, & Lafferty, 1997; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 148
                            }
                        ],
                        "text": "\u2026related methods such as Iterative Scaling (Csisza\u0301r & Tusn\u0301ady, 1984; Della Pietra, Della Pietra, & Lafferty, 1997; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label:\nPr y = +1jx = 1\n1+ e 2f(x)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 162
                            }
                        ],
                        "text": "In logistic regression and related methods such as Iterative Scaling (Csisza\u0301r & Tusn\u0301ady, 1984; Della Pietra, Della Pietra, & Lafferty, 1997; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label:\nPr y = +1jx = 1\n1+ e 2f(x) :\nOne then attempts to maximize the likelihood of the labels in the sample, or equivalently, to minimize the log loss log(Pr yjx ) = log(1+ e 2yf(x)): Thus, for logistic regression and related methods, we takeL(z) = log(1+ e 2z)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9913392,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f4493eff2531536a7aeb3fc11d62c30a8f487f6",
            "isKey": false,
            "numCitedBy": 4829,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced. For many classification algorithms, this simple strategy results in dramatic improvements in performance. We show that this seemingly mysterious phenomenon can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally, making it more suitable to large-scale data mining applications."
            },
            "slug": "Special-Invited-Paper-Additive-logistic-regression:-Friedman",
            "title": {
                "fragments": [],
                "text": "Special Invited Paper-Additive logistic regression: A statistical view of boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work shows that this seemingly mysterious phenomenon of boosting can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood, and develops more direct approximations and shows that they exhibit nearly identical results to boosting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 24
                            }
                        ],
                        "text": "It has been observed by Breiman (1997a, 1997b) and other authors (Collins, Schapire, & Singer, 2000; Friedman, Hastie, & Tibshirani, 2000; Mason, Baxter, Bartlett, & Frean, 1999; Ra\u0308tsch, Onoda, & M\u0308uller, to appear; Schapire & Singer, 1999) that theht\u2019s and t\u2019s are effectively being greedily\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2221,
                                "start": 28
                            }
                        ],
                        "text": "By similar reasoning, CART (Breiman et al., 1984), which spl it using the Gini index, can be linked to the square loss function, while Kearns and Mansour \u2019 (1996) splitting rule can be linked to the exponential loss used by AdaBoost. The analysis we present in the next section might also hold fo r other algorithms that tacitly employ a function of the margin. For instance, Freund\u2019s Brow nBoost algorithm (1999) implicitly uses an instance potential function that satisfies the condi ti we impose onL. Therefore, it can also be combined with output coding and used to solve multiclass p roblems. To conclude this section, we plot in Figure 1 some of the loss functions discussed above . 3. Output Coding for Multiclass Problems In the last section, we discussed margin-based algorithms f or learning binary problems. Suppose now that we are faced with a multiclass learning problem in wh ich each label y is chosen from a set Y of cardinalityk > 2. How can a binary margin-based learning algorithm be modifie d to handle a k-class problem? Several solutions have been proposed for this question. Man y involve reducing the multiclass problem, in one way or another, to a set of binary problems. Fo r instance, perhaps the simplest approach is to create one binary problem for each of the k classes. That is, for each r 2 Y, we apply the given margin-based learning algorithm to a binary problem in which all examples labeled y = r are considered positive examples and all other examples are considered negative examples. We then end up withk hypotheses that somehow must be combined. We call this the one-against-all approach. Another approach, suggested by Hastie and Tibshirani (1998 ), is to use the given binary learning algorithm to distinguish each pair of classes. Thus, for eac h distinct pairr1; r2 2 Y, we run the learning algorithm on a binary problem in which examples lab e edy = r1 are considered positive, and those labeledy = r2 are negative. All other examples are simply ignored. Again, the k2 hypotheses that are generated by this process must then be co mbined. We call this theall-pairs approach. A more general suggestion on handling multiclass problems w as given by Dietterich and Bakiri (1995). Their idea is to associate each class r 2 Y with a row of a \u201ccoding matrix\u201dM 2 f 1;+1gk ` for som\u00e8 ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "By similar reasoning, CART (Breiman et al., 1984), which splits using the Gini index, can be linked to the square loss function, while Kearns and Mansour\u2019s (1996) splitting rule can be linked to the exponential loss used by AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "Some machine learning algorithms, such as C4.5 (Quinlan, 1993) and CART (Breiman, Friedman, Olshen, & Stone, 1984), can then be naturally extended to handle the multiclass case."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 28
                            }
                        ],
                        "text": "By similar reasoning, CART (Breiman et al., 1984), which spl it using the Gini index, can be linked to the square loss function, while Kearns and Mansour \u2019 (1996) splitting rule can be linked to the exponential loss used by AdaBoost."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 414,
                                "start": 28
                            }
                        ],
                        "text": "By similar reasoning, CART (Breiman et al., 1984), which spl it using the Gini index, can be linked to the square loss function, while Kearns and Mansour \u2019 (1996) splitting rule can be linked to the exponential loss used by AdaBoost. The analysis we present in the next section might also hold fo r other algorithms that tacitly employ a function of the margin. For instance, Freund\u2019s Brow nBoost algorithm (1999) implicitly uses an instance potential function that satisfies the condi ti we impose onL."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14849468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65b7b1a0d61fd012f10cfce642d4aa4dec9a5829",
            "isKey": true,
            "numCitedBy": 223,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has shown that adaptively reweighting the training set, growing a classifier using the new weights, and combining the classifiers constructed to date can significantly decrease generalization error. Procedures of this type were called arcing by Breiman[1996]. The first successful arcing procedure was introduced by Freund and Schapire[1995,1996] and called Adaboost. In an effort to explain why Adaboost works, Schapire et.al. [1997] derived a bound on the generalization error of a convex combination of classifiers in terms of the margin. We introduce a function called the edge, which differs from the margin only if there are more than two classes. A framework for understanding arcing algorithms is defined. In this framework, we see that the arcing algorithms currently in the literature are optimization algorithms which minimize some function of the edge. A relation is derived between the optimal reduction in the maximum value of the edge and the PAC concept of weak learner. Two algorithms are described which achieve the optimal reduction. Tests on both synthetic and real data cast doubt on the Schapire et.al. explanation."
            },
            "slug": "Arcing-the-edge-Breiman",
            "title": {
                "fragments": [],
                "text": "Arcing the edge"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A framework for understanding arcing algorithms is defined and a relation is derived between the optimal reduction in the maximum value of the edge and the PAC concept of weak learner."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2022386739"
                        ],
                        "name": "Peter Barlett",
                        "slug": "Peter-Barlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Barlett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Barlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740222"
                        ],
                        "name": "Wee Sun Lee",
                        "slug": "Wee-Sun-Lee",
                        "structuredName": {
                            "firstName": "Wee",
                            "lastName": "Lee",
                            "middleNames": [
                                "Sun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wee Sun Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 573509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d19272112b50547614479a0c409fca66e3b05f7",
            "isKey": false,
            "numCitedBy": 2844,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance"
            },
            "slug": "Boosting-the-margin:-A-new-explanation-for-the-of-Schapire-Freund",
            "title": {
                "fragments": [],
                "text": "Boosting the margin: A new explanation for the effectiveness of voting methods"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9094007"
                        ],
                        "name": "Yuh-Jye Lee",
                        "slug": "Yuh-Jye-Lee",
                        "structuredName": {
                            "firstName": "Yuh-Jye",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuh-Jye Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747026"
                        ],
                        "name": "O. Mangasarian",
                        "slug": "O.-Mangasarian",
                        "structuredName": {
                            "firstName": "Olvi",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mangasarian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11436439,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7a7e37b5cfae27dcd9d99765a36ff6fd30e730b",
            "isKey": false,
            "numCitedBy": 557,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Smoothing methods, extensively used for solving important mathematical programming problems and applications, are applied here to generate and solve an unconstrained smooth reformulation of the support vector machine for pattern classification using a completely arbitrary kernel. We term such reformulation a smooth support vector machine (SSVM). A fast Newton\u2013Armijo algorithm for solving the SSVM converges globally and quadratically. Numerical results and comparisons are given to demonstrate the effectiveness and speed of the algorithm. On six publicly available datasets, tenfold cross validation correctness of SSVM was the highest compared with four other methods as well as the fastest. On larger problems, SSVM was comparable or faster than SVMlight (T. Joachims, in Advances in Kernel Methods\u2014Support Vector Learning, MIT Press: Cambridge, MA, 1999), SOR (O.L. Mangasarian and David R. Musicant, IEEE Transactions on Neural Networks, vol. 10, pp. 1032\u20131037, 1999) and SMO (J. Platt, in Advances in Kernel Methods\u2014Support Vector Learning, MIT Press: Cambridge, MA, 1999). SSVM can also generate a highly nonlinear separating surface such as a checkerboard."
            },
            "slug": "SSVM:-A-Smooth-Support-Vector-Machine-for-Lee-Mangasarian",
            "title": {
                "fragments": [],
                "text": "SSVM: A Smooth Support Vector Machine for Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "Smoothing methods are applied here to generate and solve an unconstrained smooth reformulation of the support vector machine for pattern classification using a completely arbitrary kernel, which converges globally and quadratically."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Optim. Appl."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781874"
                        ],
                        "name": "E. Osuna",
                        "slug": "E.-Osuna",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Osuna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Osuna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771659"
                        ],
                        "name": "R. Freund",
                        "slug": "R.-Freund",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Freund",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2845602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9008cdacbdcff8a218a6928e94fe7c6dfc237b24",
            "isKey": false,
            "numCitedBy": 2841,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs., 1985) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points."
            },
            "slug": "Training-support-vector-machines:-an-application-to-Osuna-Freund",
            "title": {
                "fragments": [],
                "text": "Training support vector machines: an application to face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets is presented, and the feasibility of the approach on a face detection problem that involves a data set of 50,000 data points is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 0
                            }
                        ],
                        "text": "Hastie and Tibshirani (1998) suggest a different approach in which all pairs of classes are compared to each other."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 685,
                                "start": 175
                            }
                        ],
                        "text": "In logistic regression and related methods such as Iterative Scaling (Csisz \u00e1r & Tus\u0144ady, 1984; Della Pietra, Della Pietra, & Lafferty, 1997; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label: Pr y = +1jx = 1 1+ e 2f(x) : One then attempts to maximize the likelihood of the labels in the sample, or equivalently, to minimize the log loss log(Pr yjx ) = log(1+ e ): Thus, for logistic regression and related methods, we take L(z) = log(1+ e 2z). Decision trees. The most popular decision tree algorithms can also be naturally linked to loss functions. For instance, Quinlan\u2019s C4.5 (1993), in its simplest form, for binary classification problems, splits decision nodes in a manner to greedily minimize X"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 174
                            }
                        ],
                        "text": "In logistic regression and related methods such as Iterative Scaling (Csisz \u00e1r & Tus\u0144ady, 1984; Della Pietra, Della Pietra, & Lafferty, 1997; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label: Pr y = +1jx = 1 1+ e 2f(x) : One then attempts to maximize the likelihood of the labels in the sample, or equivalently, to minimize the log loss log(Pr yjx ) = log(1+ e ): Thus, for logistic regression and related methods, we take L(z) = log(1+ e 2z)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 162
                            }
                        ],
                        "text": "In logistic regression and related methods such as Iterative Scaling (Csisza\u0301r & Tusn\u0301ady, 1984; Della Pietra, Della Pietra, & Lafferty, 1997; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label:\nPr y = +1jx = 1\n1+ e 2f(x) :\nOne then attempts to maximize the likelihood of the labels in the sample, or equivalently, to minimize the log loss log(Pr yjx ) = log(1+ e 2yf(x)): Thus, for logistic regression and related methods, we takeL(z) = log(1+ e 2z)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 148
                            }
                        ],
                        "text": "\u2026related methods such as Iterative Scaling (Csisza\u0301r & Tusn\u0301ady, 1984; Della Pietra, Della Pietra, & Lafferty, 1997; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label:\nPr y = +1jx = 1\n1+ e 2f(x)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14723701,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc06ba65edba6f2783bda42da82d90589508e6ab",
            "isKey": true,
            "numCitedBy": 180,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Boosting (Freund & Schapire 1996, Schapire & Singer 1998) is one of the most important recent developments in classiication methodology. The performance of many classiication algorithms often can be dramatically improved by sequentially applying them to reweighted versions of the input data, and taking a weighted majority vote of the sequence of classiiers thereby produced. We show that this seemingly mysterious phenomenon can be understood in terms of well known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to that of boosting. Direct multi-class generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multi-class generalizations of boosting in most situations , and far superior in some. We suggest a minor modiication to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-rst truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally making it more suitable to large scale data mining applications."
            },
            "slug": "Additive-Logistic-Regression-:-a-Statistical-View-Friedman-Hastie",
            "title": {
                "fragments": [],
                "text": "Additive Logistic Regression : a Statistical View ofBoostingJerome"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work develops more direct approximations of boosting that exhibit performance comparable to other recently proposed multi-class generalizations of boosting, and suggests a minor modiication to boosting that can reduce computation, often by factors of 10 to 50."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 24
                            }
                        ],
                        "text": "The algorithm AdaBoost (Freund & Schapire, 1997; Schapire & Singer, 1999) builds a hypothesisf that is a linear combination ofweakor base hypothesesht:\nf(x) = X t tht(x):\nThe hypothesisf is built up in a series of rounds on each of which anht is selected by aweak or base learning algorithmand t 2 R\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 10
                            }
                        ],
                        "text": "AdaBoost (Freund & Schapire, 1997; Schapire & Singer, 1999) is one example: it can be shown that AdaBoost is a greedy procedure for minimizing an exponential loss function of the margins."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 40
                            }
                        ],
                        "text": "For other algorithms, such as AdaBoost (Freund & Schapire, 1997;\nc 2000 AT&T Corp.\nSchapire & Singer, 1999) and the support-vector machines (SVM) algorithm (Vapnik, 1995; Cortes & Vapnik, 1995), a direct extension to the multiclass case may be problematic."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 41
                            }
                        ],
                        "text": "In a little studied variant of AdaBoost (Freund & Schapire, 1997), we allow AdaBoost to output randomized predictions in which the predicted label of a new examplex is chosen randomly to be+1 with probability 1=(1+e 2f(x))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6644398,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ccf5208521cb8c35f50ee8873df89294b8ed7292",
            "isKey": false,
            "numCitedBy": 13126,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line."
            },
            "slug": "A-decision-theoretic-generalization-of-on-line-and-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "A decision-theoretic generalization of on-line learning and an application to boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The model studied can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting, and it is shown that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "EuroCOLT"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 136
                            }
                        ],
                        "text": "The scheme and the corresponding bounds apply to many popular classification learning algorithms including support-vector machines, AdaBoost, regression, logistic regression and decision-tree algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5262555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "807c1f19047f96083e13614f7ce20f2ac98c239a",
            "isKey": false,
            "numCitedBy": 21898,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nClassifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. \n \nC4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. \n \nThis book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses."
            },
            "slug": "C4.5:-Programs-for-Machine-Learning-Quinlan",
            "title": {
                "fragments": [],
                "text": "C4.5: Programs for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A complete guide to the C4.5 system as implemented in C for the UNIX environment, which starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144830983"
                        ],
                        "name": "Y. Mansour",
                        "slug": "Y.-Mansour",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Mansour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Mansour"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 93
                            }
                        ],
                        "text": ", 1984), which splits using the Gini index, can be linked to the square loss function, while Kearns and Mansour\u2019s (1996) splitting rule can be linked to the exponential loss used by AdaBoost."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 370,
                                "start": 93
                            }
                        ],
                        "text": ", 1984), which splits using the Gini index, can be linked to the square loss function, while Kearns and Mansour\u2019s (1996) splitting rule can be linked to the exponential loss used by AdaBoost. The analysis we present in the next section might also hold for other algorithms that tacitly employ a function of the margin. For instance, Freund\u2019s BrownBoost algorithm (1999) implicitly uses an instance potential function that satisfies the condition we impose on L."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15982360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "063bf76e0259f5ef9a3eca729c1733e8fc59770b",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze the performance of top?down algorithms for decision tree learning, such as those employed by the widely used C4.5 and CART software packages. Our main result is a proof that such algorithms areboostingalgorithms. By this we mean that if the functions that label the internal nodes of the decision tree can weakly approximate the unknown target function, then the top?down algorithms we study will amplify this weaks advantage to build a tree achieving any desired level of accuracy. The bounds we obtain for this amplification show an interesting dependence on thesplitting criterionused by the top?down algorithm. More precisely, if the functions used to label the internal nodes have error 1/2??as approximations to the target function, then for the splitting criteria used by CART and C4.5, trees of size (1/?)O(1/?2?2)and (1/?)O(log(1/?)/?2)(respectively) suffice to drive the error below?. Thus (for example), a small constant advantage over random guessing is amplified to any larger constant advantage with trees of constant size. For a new splitting criterion suggested by our analysis, the much stronger bound of (1/?)O(1/?2)which is polynomial in 1/?) is obtained, which is provably optimal for decision tree algorithms. The differing bounds have a natured explanation in terms of concavity properties of the splitting criterion. The primary contribution of this work is in proving that some popular and empirically successful heuristics that are base on first principles meet the criteria of an independently motivated theoretical model."
            },
            "slug": "On-the-boosting-ability-of-top-down-decision-tree-Kearns-Mansour",
            "title": {
                "fragments": [],
                "text": "On the boosting ability of top-down decision tree learning algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This work analyzes the performance of top?down algorithms for decision tree learning and proves that some popular and empirically successful heuristics that are base on first principles meet the criteria of an independently motivated theoretical model."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 141
                            }
                        ],
                        "text": "In logistic regression and related methods such as Iterative Scaling (Csisza\u0301r & Tusn\u0301ady, 1984; Della Pietra, Della Pietra, & Lafferty, 1997; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label:\nPr y =\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6336008,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b03f43c5620bfc8993ea25dee20ce52a203ebcf7",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a framework for designing incremental learning algorithms derived from generalized entropy functionals. Our approach is based on the use of Bregman divergences together with the associated class of additive models constructed using the Legendre transform. A particular one-parameter family of Bregman divergences is shown to yield a family of loss functions that includes the log-likelihood criterion of logistic regression as a special case, and that closely approximates the exponential loss criterion used in the AdaBoost algorithms of Schapire et a/., as the natural parameter of the family varies. We also show how the quadratic approximation of the gain in Bregman divergence results in a weighted least-squares criterion. This leads to a family of incremental learning algorithms that builds upon and extends the recent interpretation of boosting in terms of additive models proposed by Friedman, Hastie, and Tibshirani."
            },
            "slug": "Additive-models,-boosting,-and-inference-for-Lafferty",
            "title": {
                "fragments": [],
                "text": "Additive models, boosting, and inference for generalized divergences"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A framework for designing incremental learning algorithms derived from generalized entropy functionals based on the use of Bregman divergences together with the associated class of additive models constructed using the Legendre transform is presented."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1099857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4de39c94e340a108fff01a90a67b0c17c86fb981",
            "isKey": false,
            "numCitedBy": 5910,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter describes a new algorithm for training Support Vector Machines: Sequential Minimal Optimization, or SMO. Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because large matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while a standard projected conjugate gradient (PCG) chunking algorithm scales somewhere between linear and cubic in the training set size. SMO's computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. For the MNIST database, SMO is as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be more than 1000 times faster than the PCG chunking algorithm."
            },
            "slug": "Fast-training-of-support-vector-machines-using-in-Platt",
            "title": {
                "fragments": [],
                "text": "Fast training of support vector machines using sequential minimal optimization, advances in kernel methods"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "SMO breaks this large quadratic programming problem into a series of smallest possible QP problems, which avoids using a time-consuming numerical QP optimization as an inner loop and hence SMO is fastest for linear SVMs and sparse data sets."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744439"
                        ],
                        "name": "J. Suykens",
                        "slug": "J.-Suykens",
                        "structuredName": {
                            "firstName": "Johan",
                            "lastName": "Suykens",
                            "middleNames": [
                                "A.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Suykens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145355963"
                        ],
                        "name": "J. D. Brabanter",
                        "slug": "J.-D.-Brabanter",
                        "structuredName": {
                            "firstName": "Jos",
                            "lastName": "Brabanter",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. D. Brabanter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144019306"
                        ],
                        "name": "L. Lukas",
                        "slug": "L.-Lukas",
                        "structuredName": {
                            "firstName": "Lukas",
                            "lastName": "Lukas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lukas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704135"
                        ],
                        "name": "J. Vandewalle",
                        "slug": "J.-Vandewalle",
                        "structuredName": {
                            "firstName": "Joos",
                            "lastName": "Vandewalle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Vandewalle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17054045,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80fb68a6db94266b6b90a9cd0f887074a9d10068",
            "isKey": false,
            "numCitedBy": 1142,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Weighted-least-squares-support-vector-machines:-and-Suykens-Brabanter",
            "title": {
                "fragments": [],
                "text": "Weighted least squares support vector machines: robustness and sparse approximation"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b951b9f78b98a186ba259027996a48e4189d37e5",
            "isKey": false,
            "numCitedBy": 1305,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing."
            },
            "slug": "Inducing-Features-of-Random-Fields-Pietra-Pietra",
            "title": {
                "fragments": [],
                "text": "Inducing Features of Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 22
                            }
                        ],
                        "text": "Using techniques from Bartlett (1998), Schapire et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 139
                            }
                        ],
                        "text": "It has been observed by Breiman (1997a, 1997b) and other authors (Collins, Schapire, & Singer, 2000; Friedman, Hastie, & Tibshirani, 2000; Mason, Baxter, Bartlett, & Frean, 1999; Ra\u0308tsch, Onoda, & M\u0308uller, to appear; Schapire & Singer, 1999) that theht\u2019s and t\u2019s are effectively being greedily chosen so as to minimize\n1 m mX i=1 e yif(xi):\nThus, AdaBoost is a binary margin-based learning algorithm in which the loss function isL(z) = e z.\nAdaBoost with randomized predictions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2248,
                                "start": 80
                            }
                        ],
                        "text": "1997b) and other authors (Friedman, Hastie, & Tibshirani, 2 000; Mason, Baxter, Bartlett, & Frean, 1999; R\u00e4tsch, Onoda, & M\u00fcller, to appear; Schapire & Singe r, 1999) that the method in which the ht\u2019s and t\u2019s are chosen has the effect of greedily minimizing 1 m m Xi=1 e yif(xi): Thus, AdaBoost is a binary margin-based learning algorithm in which the loss function isL(z) = e z. AdaBoost with randomized predictions. In a little studied variant of AdaBoost (Freund & Schapire, 1997), we allow AdaBoost to output randomized predictions i n which the predicted label of a new examplex is chosen randomly to be +1 with probability 1=(1 + e 2f(x)). The loss suffered then is the probability that the randomly chosen predicted label disagrees with the correct label y. Let p(x) def = 1=(1 + e 2f(x)). Then the loss isp(x) if y = 1 and1 p(x) if y = +1. Using a simple algebraic manipulation, the loss can be shown to be 1=(1+e2yf(x)). So for this variant of AdaBoost, we setL(z) = 1=(1 + e2z). However, in this case, note that the learning algorithm is n ot directly attempting to minimize this loss (it is instead minimizing t he exponential loss described above). Regression. There are various algorithms, such as neural networks and le ast squares regression, that attempt to minimize the squared error loss function (y f(x))2. When they\u2019s are inf 1;+1g, this function can be rewritten as (y f(x))2 = y2(y f(x))2 = (yy yf(x))2 = (1 yf(x))2: Thus, for binary problems, minimizing squared error fits our framework whereL(z) = (1 z)2. Logistic regression. In logistic regression and related methods such as Iterativ Scaling (Csisz\u00e1r & Tusn\u00e1dy, 1984; Della Pietra, Della Pietra, & Lafferty, 19 97; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label: Pr [y = +1jx\u2104 = 1 1 + e 2f(x) : One then attempts to maximize the likelihood of the labels in the sample, or equivalently, to minimize the log loss log(Pr [yjx\u2104) = log(1 + e 2yf(x)): Thus, for logistic regression and related methods, we take L(z) = log(1 + e 2z). Decision trees. The most popular decision tree algorithms can also be natura lly linked to loss functions. For instance, Quinlan\u2019s C4.5 (1993), in its simp lest form, for binary classification problems, splits decision nodes in a manner to greedily minimize X leafj p+j ln p j + p+j p+j !+ p j ln p j + p+j p j !! (2)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 22
                            }
                        ],
                        "text": "Using techniques from Bartlett (1998), Schapire et al. (1998, Theorem 4) give a theorem which states that, for > 0 and > 0, the probability over the random choice of training setS that there exists any functionF 2 F for which\nPrD F (x; y) 0 > PrS F (x; y) +\nis at most\n2N (F ; =2; =8;2m)e 2m=32:\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 685382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "015999a72c70a960e59c51078b09c8f672af0d2c",
            "isKey": true,
            "numCitedBy": 1198,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Sample complexity results from computational learning theory, when applied to neural network learning for pattern classification problems, suggest that for good generalization performance the number of training examples should grow at least linearly with the number of adjustable parameters in the network. Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights. For example, consider a two-layer feedforward network of sigmoid units, in which the sum of the magnitudes of the weights associated with each unit is bounded by A and the input dimension is n. We show that the misclassification probability is no more than a certain error estimate (that is related to squared error on the training set) plus A/sup 3/ /spl radic/((log n)/m) (ignoring log A and log m factors), where m is the number of training patterns. This may explain the generalization performance of neural networks, particularly when the number of training examples is considerably smaller than the number of weights. It also supports heuristics (such as weight decay and early stopping) that attempt to keep the weights small during training. The proof techniques appear to be useful for the analysis of other pattern classifiers: when the input domain is a totally bounded metric space, we use the same approach to give upper bounds on misclassification probability for classifiers with decision boundaries that are far from the training examples."
            },
            "slug": "The-Sample-Complexity-of-Pattern-Classification-The-Bartlett",
            "title": {
                "fragments": [],
                "text": "The Sample Complexity of Pattern Classification with Neural Networks: The Size of the Weights is More Important than the Size of the Network"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2227181"
                        ],
                        "name": "K. H\u00f6ffgen",
                        "slug": "K.-H\u00f6ffgen",
                        "structuredName": {
                            "firstName": "Klaus-Uwe",
                            "lastName": "H\u00f6ffgen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. H\u00f6ffgen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723095"
                        ],
                        "name": "H. Simon",
                        "slug": "H.-Simon",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Simon",
                            "middleNames": [
                                "Ulrich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Simon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1823251"
                        ],
                        "name": "K. S. V. Horn",
                        "slug": "K.-S.-V.-Horn",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Horn",
                            "middleNames": [
                                "S.",
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. S. V. Horn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 31
                            }
                        ],
                        "text": "Typically, in such cases, the multiclass problem is reduced to multiple binary classification problems that can be solved separately."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6707032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fef36189265d90252106cdfd64e0a8d9d5c4c58d",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the problem of learning concepts by presenting labeled and randomly chosen training\u2013examples to single neurons. It is well-known that linear halfspaces are learnable by the method of linear programming. The corresponding (Mc-Culloch-Pitts) neurons are therefore efficiently trainable to learn an unknown halfspace from examples. We want to analyze how fast the learning performance degrades when the representational power of the neuron is overstrained, i.e., if more complex concepts than just halfspaces are allowed. We show that a neuron cannot efficently find its probably almost optimal adjustment (unless RP = NP). If the weights and the threshold of the neuron have a fixed constant bound on their coding length, the situation is even worse: There is in general no polynomial time training method which bounds the resulting prediction error of the neuron by k.opt for a fixed constant k (unless RP = NP). Other variants of learning more complex concepts than halfspaces by single neurons are also investigated. We show that neither heuristical learning nor learning by sigmoidal neurons with a constant reject-rate is efficiently possible (unless RP = NP)."
            },
            "slug": "Robust-trainability-of-single-neurons-H\u00f6ffgen-Simon",
            "title": {
                "fragments": [],
                "text": "Robust trainability of single neurons"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that a neuron cannot efficently find its probably almost optimal adjustment (unless RP = NP) and neither heuristical learning nor learning by sigmoidal neurons with a constant reject-rate is efficiently possible."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207673395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d73c0d0c92446102fdb6cc728b5d69674a1a387",
            "isKey": false,
            "numCitedBy": 2614,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new class of support vector algorithms for regression and classification. In these algorithms, a parameter lets one effectively control the number of support vectors. While this can be useful in its own right, the parameterization has the additional benefit of enabling us to eliminate one of the other free parameters of the algorithm: the accuracy parameter in the regression case, and the regularization constant C in the classification case. We describe the algorithms, give some theoretical results concerning the meaning and the choice of , and report experimental results."
            },
            "slug": "New-Support-Vector-Algorithms-Sch\u00f6lkopf-Smola",
            "title": {
                "fragments": [],
                "text": "New Support Vector Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new class of support vector algorithms for regression and classification that eliminates one of the other free parameters of the algorithm: the accuracy parameter in the regression case, and the regularization constant C in the classification case."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144086182"
                        ],
                        "name": "I. Song",
                        "slug": "I.-Song",
                        "structuredName": {
                            "firstName": "Iickho",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2775410"
                        ],
                        "name": "Hwang-Ki Min",
                        "slug": "Hwang-Ki-Min",
                        "structuredName": {
                            "firstName": "Hwang-Ki",
                            "lastName": "Min",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hwang-Ki Min"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9383401"
                        ],
                        "name": "C. Park",
                        "slug": "C.-Park",
                        "structuredName": {
                            "firstName": "Cheol",
                            "lastName": "Park",
                            "middleNames": [
                                "Hoon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Park"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16237677,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bddad4dc0dfa8efa402aa5d18c29304a5760f12",
            "isKey": false,
            "numCitedBy": 260,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "1 2 (w \u00b7 w) + \u03b3 2 (w * \u00b7 w *) + C n i=1 \u03be * i s.t. y i ((w \u00b7 z i) + b) \u2265 1 \u2212 \u03be The dual optimization problem of (29) is minimize \u03b1 \u2212 n i=1 \u03b1 i + 1 2 n i, j =1 \u03b1 i \u03b1 j y i y j (z i \u00b7 z j) + 1 2\u03b3 n i, j =1 (\u03b1 i + \u03b2 i \u2212 C)(\u03b1 j + \u03b2 j \u2212 C)(z * i \u00b7 z * j) s.t. ACKNOWLEDGMENT The authors would like to thank the anonymous reviewers for their helpful technical feedback. REFERENCES [1] B. Bakker and T. Heskes, \" Task clustering and gating for Bayesian multitask learning, \" A framework for learning predictive structures from multiple tasks and unlabeled data, \" [20] J. Platt, \" Using sparseness and analytic QP to speed training of support vector machines, \" in Multi-task learning for classification with Dirichlet process priors, \" Abstract\u2014 Owing to the singularity of the within-class scatter, linear discriminant analysis (LDA) becomes ill-posed for small sample size (SSS) problems. Null-space-based LDA (NLDA), which is an extension of LDA, provides good discriminant performances for SSS problems. Yet, as the original scheme for the feature extractor (FE) of NLDA suffers from a complexity burden, a few modified schemes have since been proposed for complexity reduction. In this brief, by transforming the problem of finding the FE of NLDA into a linear equation problem, a novel scheme is derived, offering a further reduction of the complexity."
            },
            "slug": "Working-Set-Selection-Using-Second-Order-for-Svm,-\"-Song-Min",
            "title": {
                "fragments": [],
                "text": "Working Set Selection Using Second Order Information for Training Svm, \" Complexity-reduced Scheme for Feature Extraction with Linear Discriminant Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 30,
                "text": "By transforming the problem of finding the FE of NLDA into a linear equation problem, a novel scheme is derived, offering a further reduction of the complexity."
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 31
                            }
                        ],
                        "text": "Another approach, suggested by Hastie and Tibshirani (1998), is to use the given binary learning algorithm to distinguish each pair of classes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Another approach, suggested by  Hastie and Tibshirani (1998) , is to use the given binary learning algorithm to distinguish each pair of classes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 0
                            }
                        ],
                        "text": "Hastie and Tibshirani (1998) suggest a different approach in which all pairs of classes are compared to each other."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Hastie and Tibshirani (1998)  suggest a different approach in which all pairs of classes are compared to each other."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10097148,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f642a692da944604a7df590e9f9fa06089b7991a",
            "isKey": true,
            "numCitedBy": 1574,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss a strategy for polychotomous classification that involves estimating class probabilities for each pair of classes, and then coupling the estimates together. The coupling model is similar to the Bradley-Terry method for paired comparisons. We study the nature of the class probability estimates that arise, and examine the performance of the procedure in simulated datasets. The classifiers used include linear discriminants and nearest neighbors: application to support vector machines is also briefly described."
            },
            "slug": "Classification-by-Pairwise-Coupling-Hastie-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Classification by Pairwise Coupling"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A strategy for polychotomous classification that involves estimating class probabilities for each pair of classes, and then coupling the estimates together is discussed, similar to the Bradley-Terry method for paired comparisons."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747026"
                        ],
                        "name": "O. Mangasarian",
                        "slug": "O.-Mangasarian",
                        "structuredName": {
                            "firstName": "Olvi",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mangasarian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791069"
                        ],
                        "name": "D. Musicant",
                        "slug": "D.-Musicant",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Musicant",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Musicant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7794861,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45204c008c8f3d9e9b66ea5623c032ce0b3089e7",
            "isKey": false,
            "numCitedBy": 548,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "An implicit Lagrangian for the dual of a simple reformulation of the standard quadratic program of a linear support vector machine is proposed. This leads to the minimization of an unconstrained differentiable convex function in a space of dimensionality equal to the number of classified points. This problem is solvable by an extremely simple linearly convergent Lagrangian support vector machine (LSVM) algorithm. LSVM requires the inversion at the outset of a single matrix of the order of the much smaller dimensionality of the original input space plus one. The full algorithm is given in this paper in 11 lines of MATLAB code without any special optimization tools such as linear or quadratic programming solvers. This LSVM code can be used \"as is\" to solve classification problems with millions of points. For example, 2 million points in 10 dimensional input space were classified by a linear surface in 82 minutes on a Pentium III 500 MHz notebook with 384 megabytes of memory (and additional swap space), and in 7 minutes on a 250 MHz UltraSPARC II processor with 2 gigabytes of memory. Other standard classification test problems were also solved. Nonlinear kernel classification can also be solved by LSVM. Although it does not scale up to very large problems, it can handle any positive semidefinite kernel and is guaranteed to converge. A short MATLAB code is also given for nonlinear kernels and tested on a number of problems."
            },
            "slug": "Lagrangian-Support-Vector-Machines-Mangasarian-Musicant",
            "title": {
                "fragments": [],
                "text": "Lagrangian Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "An implicit Lagrangian for the dual of a simple reformulation of the standard quadratic program of a linear support vector machine is proposed, which leads to the minimization of an unconstrained differentiable convex function in a space of dimensionality equal to the number of classified points."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727076"
                        ],
                        "name": "H. Lodhi",
                        "slug": "H.-Lodhi",
                        "structuredName": {
                            "firstName": "Huma",
                            "lastName": "Lodhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Lodhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144884649"
                        ],
                        "name": "C. Saunders",
                        "slug": "C.-Saunders",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Saunders",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Saunders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4562073"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Watkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 669209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f330f1f472f860212b980bb9be81eff884f7f0e1",
            "isKey": false,
            "numCitedBy": 1643,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel kernel for comparing two text documents. The kernel is an inner product in the feature space consisting of all subsequences of length k. A subsequence is any ordered sequence of k characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences which are close to contiguous. A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. A preliminary experimental comparison of the performance of the kernel compared with a standard word feature space kernel [6] is made showing encouraging results."
            },
            "slug": "Text-Classification-using-String-Kernels-Lodhi-Saunders",
            "title": {
                "fragments": [],
                "text": "Text Classification using String Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A novel kernel is introduced for comparing two text documents consisting of an inner product in the feature space consisting of all subsequences of length k, which can be efficiently evaluated by a dynamic programming technique."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6294728,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4609f6bdc3beab00c9beceaa12dd8101fefe6f1c",
            "isKey": false,
            "numCitedBy": 4845,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical learning theory was introduced in the late 1960's. Until the 1990's it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990's new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more detailed overview of the theory (without proofs) can be found in Vapnik (1995). In Vapnik (1998) one can find detailed description of the theory (including proofs)."
            },
            "slug": "An-overview-of-statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "An overview of statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "How the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms are demonstrated and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems are demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 975467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be9a9efa52c1c0cc708ce3dc79c85433ebd08108",
            "isKey": false,
            "numCitedBy": 276,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new boosting algorithm. This boosting algorithm is an adaptive version of the boost by majority algorithm and combines bounded goals of the boost by majority algorithm with the adaptivity of AdaBoost.The method used for making boost-by-majority adaptive is to consider the limit in which each of the boosting iterations makes an infinitesimally small contribution to the process as a whole. This limit can be modeled using the differential equations that govern Brownian motion. The new boosting algorithm, named BrownBoost, is based on finding solutions to these differential equations.The paper describes two methods for finding approximate solutions to the differential equations. The first is a method that results in a provably polynomial time algorithm. The second method, based on the Newton-Raphson minimization procedure, is much more efficient in practice but is not known to be polynomial."
            },
            "slug": "An-Adaptive-Version-of-the-Boost-by-Majority-Freund",
            "title": {
                "fragments": [],
                "text": "An Adaptive Version of the Boost by Majority Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The paper describes two methods for finding approximate solutions to the differential equations and a method that results in a provably polynomial time algorithm based on the Newton-Raphson minimization procedure, which is much more efficient in practice but is not known to bePolynomial."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736284"
                        ],
                        "name": "D. Sebald",
                        "slug": "D.-Sebald",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Sebald",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sebald"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686036"
                        ],
                        "name": "J. Bucklew",
                        "slug": "J.-Bucklew",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bucklew",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bucklew"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2402148,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d592544922d2a8849c4ca14867797ab76d0f0953",
            "isKey": false,
            "numCitedBy": 291,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The emerging machine learning technique called support vector machines is proposed as a method for performing nonlinear equalization in communication systems. The support vector machine has the advantage that a smaller number of parameters for the model can be identified in a manner that does not require the extent of prior information or heuristic assumptions that some previous techniques require. Furthermore, the optimization method of a support vector machine is quadratic programming, which is a well-studied and understood mathematical programming technique. Support vector machine simulations are carried out on nonlinear problems previously studied by other researchers using neural networks. This allows initial comparison against other techniques to determine the feasibility of using the proposed method for nonlinear detection. Results show that support vector machines perform as well as neural networks on the nonlinear problems investigated. A method is then proposed to introduce decision feedback processing to support vector machines to address the fact that intersymbol interference (ISI) data generates input vectors having temporal correlation, whereas a standard support vector machine assumes independent input vectors. Presenting the problem from the viewpoint of the pattern space illustrates the utility of a bank of support vector machines. This approach yields a nonlinear processing method that is somewhat different than the nonlinear decision feedback method whereby the linear feedback filter of the decision feedback equalizer is replaced by a Volterra filter. A simulation using a linear system shows that the proposed method performs equally to a conventional decision feedback equalizer for this problem."
            },
            "slug": "Support-vector-machine-techniques-for-nonlinear-Sebald-Bucklew",
            "title": {
                "fragments": [],
                "text": "Support vector machine techniques for nonlinear equalization"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The emerging machine learning technique called support vector machines is proposed as a method for performing nonlinear equalization in communication systems and yields a nonlinear processing method that is somewhat different than the nonlinear decision feedback method whereby the linear feedback filter of the decision feedback equalizer is replaced by a Volterra filter."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152597562"
                        ],
                        "name": "Gunnar R\u00e4tsch",
                        "slug": "Gunnar-R\u00e4tsch",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "R\u00e4tsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gunnar R\u00e4tsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633352"
                        ],
                        "name": "J. Kohlmorgen",
                        "slug": "J.-Kohlmorgen",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Kohlmorgen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kohlmorgen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5398743,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f43840dc1638a18eb6178f1060dc5f41af1c5ac7",
            "isKey": false,
            "numCitedBy": 975,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Support Vector Machines are used for time series prediction and compared to radial basis function networks. We make use of two different cost functions for Support Vectors: training with (i) an e insensitive loss and (ii) Huber's robust loss function and discuss how to choose the regularization parameters in these models. Two applications are considered: data from (a) a noisy (normal and uniform noise) Mackey Glass equation and (b) the Santa Fe competition (set D). In both cases Support Vector Machines show an excellent performance. In case (b) the Support Vector approach improves the best known result on the benchmark by a factor of 29%."
            },
            "slug": "Predicting-Time-Series-with-Support-Vector-Machines-M\u00fcller-Smola",
            "title": {
                "fragments": [],
                "text": "Predicting Time Series with Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Two different cost functions for Support Vectors are made use: training with an e insensitive loss and Huber's robust loss function and how to choose the regularization parameters in these models are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146245769"
                        ],
                        "name": "Chun-fu Lin",
                        "slug": "Chun-fu-Lin",
                        "structuredName": {
                            "firstName": "Chun-fu",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chun-fu Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9437199"
                        ],
                        "name": "Sheng-de Wang",
                        "slug": "Sheng-de-Wang",
                        "structuredName": {
                            "firstName": "Sheng-de",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sheng-de Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13435674,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec76f55da5c6df30f6e4c9e4945bd3304d508ef7",
            "isKey": false,
            "numCitedBy": 1287,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A support vector machine (SVM) learns the decision surface from two distinct classes of the input points. In many applications, each input point may not be fully assigned to one of these two classes. In this paper, we apply a fuzzy membership to each input point and reformulate the SVMs such that different input points can make different contributions to the learning of decision surface. We call the proposed method fuzzy SVMs (FSVMs)."
            },
            "slug": "Fuzzy-support-vector-machines-Lin-Wang",
            "title": {
                "fragments": [],
                "text": "Fuzzy support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper applies a fuzzy membership to each input point and reformulate the SVMs such that different input points can make different contributions to the learning of decision surface."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39682833"
                        ],
                        "name": "H. Rowley",
                        "slug": "H.-Rowley",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Rowley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rowley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767244"
                        ],
                        "name": "S. Baluja",
                        "slug": "S.-Baluja",
                        "structuredName": {
                            "firstName": "Shumeet",
                            "lastName": "Baluja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baluja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 40120983,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d76ef8e61395a6e9c32627f1f108772d084e2e9",
            "isKey": false,
            "numCitedBy": 4156,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a neural network-based face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We use a bootstrap algorithm for training the networks, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting non-face training examples, which must be chosen to span the entire space of non-face images. Comparisons with other state-of-the-art face detection systems are presented; our system has better performance in terms of detection and false-positive rates."
            },
            "slug": "Neural-network-based-face-detection-Rowley-Baluja",
            "title": {
                "fragments": [],
                "text": "Neural Network-Based Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A neural network-based face detection system that arbitrates between multiple networks to improve performance over a single network using a bootstrap algorithm, which eliminates the difficult task of manually selecting non-face training examples."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": false,
            "numCitedBy": 26320,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144581435"
                        ],
                        "name": "M. Cottrell",
                        "slug": "M.-Cottrell",
                        "structuredName": {
                            "firstName": "Marie",
                            "lastName": "Cottrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Cottrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064239077"
                        ],
                        "name": "B. Girard",
                        "slug": "B.-Girard",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Girard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Girard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34657634"
                        ],
                        "name": "Y. Girard",
                        "slug": "Y.-Girard",
                        "structuredName": {
                            "firstName": "Yvonne",
                            "lastName": "Girard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Girard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2459132"
                        ],
                        "name": "M. Mangeas",
                        "slug": "M.-Mangeas",
                        "structuredName": {
                            "firstName": "Morgan",
                            "lastName": "Mangeas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mangeas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32839252"
                        ],
                        "name": "C. Muller",
                        "slug": "C.-Muller",
                        "structuredName": {
                            "firstName": "Corinne",
                            "lastName": "Muller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Muller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 206458062,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69dab5e010ba0d8ca42156f349741bc03dc273f2",
            "isKey": false,
            "numCitedBy": 228,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Many authors use feedforward neural networks for modeling and forecasting time series. Most of these applications are mainly experimental, and it is often difficult to extract a general methodology from the published studies. In particular, the choice of architecture is a tricky problem. We try to combine the statistical techniques of linear and nonlinear time series with the connectionist approach. The asymptotical properties of the estimators lead us to propose a systematic methodology to determine which weights are nonsignificant and to eliminate them to simplify the architecture. This method (SSM or statistical stepwise method) is compared to other pruning techniques and is applied to some artificial series, to the famous Sunspots benchmark, and to daily electrical consumption data."
            },
            "slug": "Neural-modeling-for-time-series:-A-statistical-for-Cottrell-Girard",
            "title": {
                "fragments": [],
                "text": "Neural modeling for time series: A statistical stepwise method for weight elimination"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The asymptotical properties of the estimators lead us to propose a systematic methodology to determine which weights are nonsignificant and to eliminate them to simplify the architecture."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40004145"
                        ],
                        "name": "G. Zhang",
                        "slug": "G.-Zhang",
                        "structuredName": {
                            "firstName": "Guoqiang",
                            "lastName": "Zhang",
                            "middleNames": [
                                "Peter"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143948126"
                        ],
                        "name": "B. E. Patuwo",
                        "slug": "B.-E.-Patuwo",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Patuwo",
                            "middleNames": [
                                "Eddy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. E. Patuwo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47603264"
                        ],
                        "name": "M. Hu",
                        "slug": "M.-Hu",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Hu",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27276171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab1dd7090c8a02ce5566dd80ad88e5d741a95e77",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-simulation-study-of-artificial-neural-networks-Zhang-Patuwo",
            "title": {
                "fragments": [],
                "text": "A simulation study of artificial neural networks for nonlinear time-series forecasting"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Oper. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2378652"
                        ],
                        "name": "R. Olshen",
                        "slug": "R.-Olshen",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Olshen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Olshen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103556459"
                        ],
                        "name": "C. J. Stone",
                        "slug": "C.-J.-Stone",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Stone",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. J. Stone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Some machine learning algorithms, such as C4.5 (Quinlan, 1993) and CART ( Breiman, Friedman, Olshen, & Stone, 1984 ), can then be naturally extended to handle the multiclass case."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "By similar reasoning, CART ( Breiman et al., 1984 ), which splits using the Gini index, can be linked to the square loss function, while Kearns and Mansour\u2019s (1996) splitting rule can be linked to the exponential loss used by AdaBoost."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29458883,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8017699564136f93af21575810d557dba1ee6fc6",
            "isKey": false,
            "numCitedBy": 16307,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Background. Introduction to Tree Classification. Right Sized Trees and Honest Estimates. Splitting Rules. Strengthening and Interpreting. Medical Diagnosis and Prognosis. Mass Spectra Classification. Regression Trees. Bayes Rules and Partitions. Optimal Pruning. Construction of Trees from a Learning Sample. Consistency. Bibliography. Notation Index. Subject Index."
            },
            "slug": "Classification-and-Regression-Trees-Breiman-Friedman",
            "title": {
                "fragments": [],
                "text": "Classification and Regression Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This chapter discusses tree classification in the context of medicine, where right Sized Trees and Honest Estimates are considered and Bayes Rules and Partitions are used as guides to optimal pruning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725841"
                        ],
                        "name": "Z. Tang",
                        "slug": "Z.-Tang",
                        "structuredName": {
                            "firstName": "Zaiyong",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734138"
                        ],
                        "name": "P. Fishwick",
                        "slug": "P.-Fishwick",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Fishwick",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fishwick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 34891270,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b0804d207bad27d0903f4d6d4d4212f17a8fb98",
            "isKey": false,
            "numCitedBy": 336,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We have studied neural networks as models for time series forecasting, and our research compares the Box-Jenkins method against the neural network method for long and short term memory series. Our work was inspired by previously published works that yielded inconsistent results about comparative performance. We have since experimented with 16 time series of differing complexity using neural networks. The performance of the neural networks is compared with that of the Box-Jenkins method. Our experiments indicate that for time series with long memory, both methods produced comparable results. However, for series with short memory, neural networks outperformed the Box-Jenkins model. Because neural networks can be easily built for multiple-step-ahead forecasting, they may present a better long term forecast model than the Box-Jenkins method. We discussed the representation ability, the model building process and the applicability of the neural net approach. Neural networks appear to provide a promising altern..."
            },
            "slug": "Feedforward-Neural-Nets-as-Models-for-Time-Series-Tang-Fishwick",
            "title": {
                "fragments": [],
                "text": "Feedforward Neural Nets as Models for Time Series Forecasting"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This research compares the Box-Jenkins method against the neural network method for long and short term memory series, and indicates that neural networks appear to provide a promising alternative for time series forecasting."
            },
            "venue": {
                "fragments": [],
                "text": "INFORMS J. Comput."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4362653"
                        ],
                        "name": "N. Fisher",
                        "slug": "N.-Fisher",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Fisher",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Fisher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731053"
                        ],
                        "name": "P. Sen",
                        "slug": "P.-Sen",
                        "structuredName": {
                            "firstName": "Pranab",
                            "lastName": "Sen",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 48
                            }
                        ],
                        "text": "= 2 + 1 ` X\u0300 s=1 zs:\nThis is because, as proved by Hoeffding (1963), for any random variableX with a X b, and for > 0,\nE h e X i exp 2(b a)2\n8 + E [X]\n!"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123205318,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "30ffd4a8e479d04b1dea5749eac4a466dccde64b",
            "isKey": false,
            "numCitedBy": 417,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "If S is a random variable with finite rnean and variance, the Bienayme-Chebyshev inequality states that for x > 0, \n \n$$\\Pr \\left[ {\\left| {S - ES} \\right| \\geqslant x{{{(\\operatorname{var} S)}}^{{1/2}}}} \\right] \\leqslant {{x}^{{ - 2}}}$$ \n \n(1) \n \nIf S is the surn of n independent, identically distributed random variables, then, by the central limit theorem*, as n \u2192 \u221e, the probability on the left approaehes 2\u0424( - x), where \u0424(x) is the standard normal distribution function. For x large, \u0424( - x) behaves as const. x -1 exp( - x2/2)."
            },
            "slug": "Probability-Inequalities-for-Sums-of-Bounded-Random-Fisher-Sen",
            "title": {
                "fragments": [],
                "text": "Probability Inequalities for Sums of Bounded Random Variables"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065168169"
                        ],
                        "name": "G. Zang",
                        "slug": "G.-Zang",
                        "structuredName": {
                            "firstName": "Guangming",
                            "lastName": "Zang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47603264"
                        ],
                        "name": "M. Hu",
                        "slug": "M.-Hu",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Hu",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 150705011,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "15fd49e6806d3c8ccb039f04f44cd423b5128258",
            "isKey": false,
            "numCitedBy": 300,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-network-forecasting-of-the-British-pound/US-Zang-Hu",
            "title": {
                "fragments": [],
                "text": "Neural network forecasting of the British pound/US dol-lar exchange rate"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586148"
                        ],
                        "name": "L. Mason",
                        "slug": "L.-Mason",
                        "structuredName": {
                            "firstName": "Llew",
                            "lastName": "Mason",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mason"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40073871"
                        ],
                        "name": "Marcus Frean",
                        "slug": "Marcus-Frean",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Frean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Frean"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60744708,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f5e23d650853dc7f3dbe4370d4ace6be55f931ae",
            "isKey": false,
            "numCitedBy": 324,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introduction, Optimizing Cost Functions of the Margin, A Gradient Descent View of Voting Methods, Theoretically Motivated Cost Functions, Convergence Results, Experiments, Conclusions, Acknowledgments"
            },
            "slug": "Functional-Gradient-Techniques-for-Combining-Mason-Baxter",
            "title": {
                "fragments": [],
                "text": "Functional Gradient Techniques for Combining Hypotheses"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This chapter contains sections titled: Introduction, Optimizing Cost Functions of the Margin, A Gradient Descent View of Voting Methods, Theoretically Motivated Cost Functions, Convergence Results, Experiments, Conclusions, and Acknowledgments."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298005"
                        ],
                        "name": "Catherine Blake",
                        "slug": "Catherine-Blake",
                        "structuredName": {
                            "firstName": "Catherine",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Catherine Blake"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 89
                            }
                        ],
                        "text": "Next, we describe experiments we performed with multiclass data from the UCI repository (Merz & Murphy, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62622768,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e068be31ded63600aea068eacd12931efd2a1029",
            "isKey": false,
            "numCitedBy": 13446,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "UCI-Repository-of-machine-learning-databases-Blake",
            "title": {
                "fragments": [],
                "text": "UCI Repository of machine learning databases"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51083130"
                        ],
                        "name": "F. Rosenblatt",
                        "slug": "F.-Rosenblatt",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Rosenblatt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rosenblatt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12781225,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "5d11aad09f65431b5d3cb1d85328743c9e53ba96",
            "isKey": false,
            "numCitedBy": 9074,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The first of these questions is in the province of sensory physiology, and is the only one for which appreciable understanding has been achieved. This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory. With regard to the second question, two alternative positions have been maintained. The first suggests that storage of sensory information is in the form of coded representations or images, with some sort of one-to-one mapping between the sensory stimulus"
            },
            "slug": "The-perceptron:-a-probabilistic-model-for-storage-Rosenblatt",
            "title": {
                "fragments": [],
                "text": "The perceptron: a probabilistic model for information storage and organization in the brain."
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1958
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2351850"
                        ],
                        "name": "Fang-Mei Tseng",
                        "slug": "Fang-Mei-Tseng",
                        "structuredName": {
                            "firstName": "Fang-Mei",
                            "lastName": "Tseng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fang-Mei Tseng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144271135"
                        ],
                        "name": "Hsiao-Cheng Yu",
                        "slug": "Hsiao-Cheng-Yu",
                        "structuredName": {
                            "firstName": "Hsiao-Cheng",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsiao-Cheng Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145898925"
                        ],
                        "name": "G. Tzeng",
                        "slug": "G.-Tzeng",
                        "structuredName": {
                            "firstName": "Gwo-Hshiung",
                            "lastName": "Tzeng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tzeng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16479748,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29d09a073df25741c2d98f93fc08903bddf2facd",
            "isKey": false,
            "numCitedBy": 291,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Combining-neural-network-model-with-seasonal-time-Tseng-Yu",
            "title": {
                "fragments": [],
                "text": "Combining neural network model with seasonal time series ARIMA model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5216020"
                        ],
                        "name": "M. Casdagli",
                        "slug": "M.-Casdagli",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Casdagli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Casdagli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122236599,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "be946457d3f880d9ec836aee3d0d231ffa3bcc9a",
            "isKey": false,
            "numCitedBy": 1398,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Nonlinear-prediction-of-chaotic-time-series-Casdagli",
            "title": {
                "fragments": [],
                "text": "Nonlinear prediction of chaotic time series"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 98
                            }
                        ],
                        "text": "For training data that may not be linearly separable, the sup portvector machines (SVM) algorithm (Vapnik, 1995; Cortes & Vap nik, 1995) seeks a linear classifier f : Rn ! R of the formf(x) = w x+ b that minimizes the objective function 12 jjwjj22 + C m Xi=1 i ; for some parameter C, subject to the linear constraints yi((xi w) + b) 1 i; i 0 : 3"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 157
                            }
                        ],
                        "text": "For other algorithms, such as AdaBoost (Freund & Schapire, 1997;\nc 2000 AT&T Corp.\nSchapire & Singer, 1999) and the support-vector machines (SVM) algorithm (Vapnik, 1995; Cortes & Vapnik, 1995), a direct extension to the multiclass case may be problematic."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 74
                            }
                        ],
                        "text": "Schapire & Singer, 1999) and the support-vector machines (S VM) algorithm (Vapnik, 1995; Cortes & Vapnik, 1995), a direct extension to the multiclass case ma y be problematic."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 98
                            }
                        ],
                        "text": "For training data that may not be linearly separable, the supportvector machines (SVM) algorithm (Vapnik, 1995; Cortes & Vapnik, 1995) seeks a linear classifier f : Rn !"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 33
                            }
                        ],
                        "text": "For instance, the SVM algorithm (Vapnik, 1995; Cortes & Vapnik, 1995) attempts to maximize the minimum margin of any training example."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": true,
            "numCitedBy": 38756,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19355,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688083"
                        ],
                        "name": "N. Gershenfeld",
                        "slug": "N.-Gershenfeld",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Gershenfeld",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Gershenfeld"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 26996169,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "85f5b8e3e0b2fb868528349e5032b0c2d20c7a34",
            "isKey": false,
            "numCitedBy": 1882,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Make more knowledge even in less time every day. You may not always spend your time and money to go abroad and get the experience and knowledge by yourself. Reading is a good alternative to do in getting this desirable knowledge and experience. You may gain many things from experiencing directly, but of course it will spend much money. So here, by reading time series prediction forecasting the future and understanding the past, you can take more advantages with limited budget."
            },
            "slug": "Time-Series-Prediction:-Forecasting-the-Future-and-Weigend-Gershenfeld",
            "title": {
                "fragments": [],
                "text": "Time Series Prediction: Forecasting the Future and Understanding the Past"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "By reading time series prediction forecasting the future and understanding the past, you can take more advantages with limited budget."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48107116"
                        ],
                        "name": "M. Mackey",
                        "slug": "M.-Mackey",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mackey",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mackey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145548576"
                        ],
                        "name": "L. Glass",
                        "slug": "L.-Glass",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Glass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Glass"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42039623,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "e39c17da0a3a0e7f709ef3e785c912df5cf386df",
            "isKey": false,
            "numCitedBy": 3643,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "First-order nonlinear differential-delay equations describing physiological control systems are studied. The equations display a broad diversity of dynamical behavior including limit cycle oscillations, with a variety of wave forms, and apparently aperiodic or \"chaotic\" solutions. These results are discussed in relation to dynamical respiratory and hematopoietic diseases."
            },
            "slug": "Oscillation-and-chaos-in-physiological-control-Mackey-Glass",
            "title": {
                "fragments": [],
                "text": "Oscillation and chaos in physiological control systems."
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "First-order nonlinear differential-delay equations describing physiological control systems displaying a broad diversity of dynamical behavior including limit cycle oscillations, with a variety of wave forms, and apparently aperiodic or \"chaotic\" solutions are studied."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2701,
                                "start": 0
                            }
                        ],
                        "text": "Dietterich and Bakiri (1995) presented a general framework in which the classes are partitioned into opposing subsets using error-correcting codes. For all of t hese methods, after the binary classification problems have been solved, the resulting set of binar y cl ssifiers must then be combined in some way. In this paper, we study a general framework, which i s a simple extension of Dietterich and Bakiri\u2019s framework, that unifies all of these methods of r educing a multiclass problem to a binary problem. We pay particular attention to the case in which the binary le arning algorithm is one that is based on themarginof a training example. Roughly speaking, the margin of a trai ning example is a number that is positive if and only if the example is correct ly lassified by a given classifier and whose magnitude is a measure of confidence in the prediction. Several well known algorithms work directly with margins. For instance, the SVM algorithm (Vap nik, 1995; Cortes & Vapnik, 1995) attempts to maximize the minimum margin of any training exampl e. There are many more algorithms that attempt to minimize some loss function of the margin. Ad aBoost (Freund & Schapire, 1997; Schapire & Singer, 1999) is one example: it can be shown that A d Boost is a greedy procedure for minimizing an exponential loss function of the margins. In S ection 2, we catalog many other algorithms that also can be viewed as margin-based learning algo rithms, including regression, logistic regression and decision-tree algorithms. The simplest method of combining the binary classifiers (whi ch we callHamming decoding ) ignores the loss function that was used during training as we ll as the confidences attached to predictions made by the classifier. In Section 3, we give a new and general technique for combining classifiers that does not suffer from either of these defects . We call this methodloss-based decoding . We next prove some of the theoretical properties of these met hods in Section 4. In particular, for both of the decoding methods, we prove general bounds on t he training error on the multiclass problem in terms of the empirical performance on the individ ual binary problems. These bounds indicate that loss-based decoding is superior to Hamming de coding. Also, these bounds depend on the manner in which the multiclass problem has been reduced t o binary problems. For the oneagainst-all approach, our bounds are linear in the number of classes, but for a reduction based on random partitions of the classes, the bounds are ind pendent of the number of classes. These results generalize more specialized bounds proved by Schapire and S inger (1999) and by Guruswami and Sahai (1999). In Section 5, we prove a bound on the generalization error of o ur method when the binary learner is AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 72
                            }
                        ],
                        "text": "A more general suggestion on handling multiclass problems w as given by Dietterich and Bakiri (1995). Their idea is to associate each class r 2 Y with a row of a \u201ccoding matrix\u201dM 2 f 1;+1gk ` for som\u00e8 ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2882,
                                "start": 0
                            }
                        ],
                        "text": "Dietterich and Bakiri (1995) presented a general framework in which the classes are partitioned into opposing subsets using error-correcting codes. For all of t hese methods, after the binary classification problems have been solved, the resulting set of binar y cl ssifiers must then be combined in some way. In this paper, we study a general framework, which i s a simple extension of Dietterich and Bakiri\u2019s framework, that unifies all of these methods of r educing a multiclass problem to a binary problem. We pay particular attention to the case in which the binary le arning algorithm is one that is based on themarginof a training example. Roughly speaking, the margin of a trai ning example is a number that is positive if and only if the example is correct ly lassified by a given classifier and whose magnitude is a measure of confidence in the prediction. Several well known algorithms work directly with margins. For instance, the SVM algorithm (Vap nik, 1995; Cortes & Vapnik, 1995) attempts to maximize the minimum margin of any training exampl e. There are many more algorithms that attempt to minimize some loss function of the margin. Ad aBoost (Freund & Schapire, 1997; Schapire & Singer, 1999) is one example: it can be shown that A d Boost is a greedy procedure for minimizing an exponential loss function of the margins. In S ection 2, we catalog many other algorithms that also can be viewed as margin-based learning algo rithms, including regression, logistic regression and decision-tree algorithms. The simplest method of combining the binary classifiers (whi ch we callHamming decoding ) ignores the loss function that was used during training as we ll as the confidences attached to predictions made by the classifier. In Section 3, we give a new and general technique for combining classifiers that does not suffer from either of these defects . We call this methodloss-based decoding . We next prove some of the theoretical properties of these met hods in Section 4. In particular, for both of the decoding methods, we prove general bounds on t he training error on the multiclass problem in terms of the empirical performance on the individ ual binary problems. These bounds indicate that loss-based decoding is superior to Hamming de coding. Also, these bounds depend on the manner in which the multiclass problem has been reduced t o binary problems. For the oneagainst-all approach, our bounds are linear in the number of classes, but for a reduction based on random partitions of the classes, the bounds are ind pendent of the number of classes. These results generalize more specialized bounds proved by Schapire and S inger (1999) and by Guruswami and Sahai (1999). In Section 5, we prove a bound on the generalization error of o ur method when the binary learner is AdaBoost. In particular, we generalize the analysis of Sc hapire et al. (1998), expressing a bound on the generalization error in terms of the training-set mar gins of the combined multiclass classifier, and showing that boosting, when used in this way, tends to agg ressively increase the margins of the training examples."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 0
                            }
                        ],
                        "text": "Dietterich and Bakiri (1995) presented a general framework in which the classes are partitioned into opposing subsets using error-correcting codes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 55
                            }
                        ],
                        "text": "This generalization is closest to the ECOC approach of Dietterich and Bakiri (1995) but differs in that the coding matrix is taken from the larger set f 1;0;+1gk `."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 71
                            }
                        ],
                        "text": "A more general suggestion on handling multiclass problems was given by Dietterich and Bakiri (1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 56
                            }
                        ],
                        "text": "This generaliz ation is closest to the ECOC approach of Dietterich and Bakiri (1995) but differs in that the codin g matrix is taken from the larger set f 1; 0;+1gk `."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Solving multiclass l"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 17
                            }
                        ],
                        "text": "MO algo rithm of Schapire and Singer (1999) (specifically, what they called \u201cVariant 2\u201d)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 81
                            }
                        ],
                        "text": "Theorem 1 and Corollary 2 are broad generalizations of similar results proved by Schapire and Singer (1999) in a much more specialized setting involving only AdaBoost."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 4
                            }
                        ],
                        "text": "MH (Schapire & Singer, 1999) we were able to devise a simple implementation of the single-call variant that was described in Section 5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 104
                            }
                        ],
                        "text": "To prove that the bound holds simultaneously f or all > 0, we can use exactly the same argument used by Schapire and Singer (1999) in the very l ast part of their Theorem 8."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 59
                            }
                        ],
                        "text": "These results generalize more specialized bounds proved by Schapire and Singer (1999) and by Guruswami and Sahai (1999)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 83
                            }
                        ],
                        "text": "For other algorithms, such as AdaBoost (Freund & Schapire, 1997;\nc 2000 AT&T Corp.\nSchapire & Singer, 1999) and the support-vector machines (SVM) algorithm (Vapnik, 1995; Cortes & Vapnik, 1995), a direct extension to the multiclass case may be problematic."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 82
                            }
                        ],
                        "text": "Theorem 1 and Corollary 2 are broad generalizations of simil ar results proved by Schapire and Singer (1999) in a much more specialized setting involving o ly AdaBoost."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 35
                            }
                        ],
                        "text": "AdaBoost (Freund & Schapire, 1997; Schapire & Singer, 1999) is one example: it can be shown that AdaBoost is a greedy procedure for minimizing an exponential loss function of the margins."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 710,
                                "start": 104
                            }
                        ],
                        "text": "To prove that the bound holds simultaneously f or all > 0, we can use exactly the same argument used by Schapire and Singer (1999) in the very l ast part of their Theorem 8. Thus, we have shown that large training-set margins imply a b etter bound on the generalization error, independent of the number of rounds of boosting. We tu rn now to the second part of our analysis in which we prove that AdaBoost.MO tends to increas e the margins of the training examples, assuming that the binary errors t of the base hypotheses are bounded away from the trivial error rate of1=2 (see the discussion that follows the proof). The theorem tha t we prove below is a direct analog of Theorem 5 of Schapire et al. (1998) for binar y AdaBoost."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 103
                            }
                        ],
                        "text": "To prove that the bound holds simultaneously for all > 0, we can use exactly the same argument used by Schapire and Singer (1999) in the very last part of their Theorem 8."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 49
                            }
                        ],
                        "text": "The algorithm AdaBoost (Freund & Schapire, 1997; Schapire & Singer, 1999) builds a hypothesisf that is a linear combination ofweakor base hypothesesht:\nf(x) = X t tht(x):\nThe hypothesisf is built up in a series of rounds on each of which anht is selected by aweak or base learning algorithmand t 2 R\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 16
                            }
                        ],
                        "text": "MO algorithm of Schapire and Singer (1999) (specifically, what they called \u201cVariant 2\u201d)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 141
                            }
                        ],
                        "text": "\u2026Schapire, & Singer, 2000; Friedman, Hastie, & Tibshirani, 2000; Mason, Baxter, Bartlett, & Frean, 1999; Ra\u0308tsch, Onoda, & M\u0308uller, to appear; Schapire & Singer, 1999) that theht\u2019s and t\u2019s are effectively being greedily chosen so as to minimize\n1 m mX i=1 e yif(xi):\nThus, AdaBoost is a\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improved boosting algo"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 110
                            }
                        ],
                        "text": "It is clear from the plots that loss-based decoding often gives better results than Hamming decoding for both SVM and AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 52
                            }
                        ],
                        "text": "Finally, in Section 6, we present experiments using SVM and AdaBoost with a variety of multiclass-to-binary reductions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 104
                            }
                        ],
                        "text": "In Figures 6 and 7, we plot the test error difference for pairs of codes using loss-based decoding with SVM and AdaBoost as the binary learners."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 38
                            }
                        ],
                        "text": "Experimental results with SVM and AdaBoost show that our scheme provides a viable alternative to the most commonly used multiclass algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12646365,
            "fieldsOfStudy": [],
            "id": "14e53403a0055dbe5faaf9f1f3be96ca0e692a4d",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improved Boosting Algorithms using Confidence-Rated Predictions"
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 97
                            }
                        ],
                        "text": "For training data that may not be linearly separable, the supportvector machines (SVM) algorithm (Vapnik, 1995; Cortes & Vapnik, 1995) seeks a linear classifier f : Rn ! R of the form f(x) = w x+ b that minimizes the objective function 1 2 jjwjj22 + C m Xi=1 i ; for some parameter C , subject to the linear constraints yi((xi w) + b) 1 i; i 0 : 115"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 157
                            }
                        ],
                        "text": "For other algorithms, such as AdaBoost (Freund & Schapire, 1997;\nc 2000 AT&T Corp.\nSchapire & Singer, 1999) and the support-vector machines (SVM) algorithm (Vapnik, 1995; Cortes & Vapnik, 1995), a direct extension to the multiclass case may be problematic."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 73
                            }
                        ],
                        "text": "Schapire & Singer, 1999) and the support-vector machines (SVM) algorithm (Vapnik, 1995; Cortes & Vapnik, 1995), a direct extension to the multiclass case may be problematic."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 98
                            }
                        ],
                        "text": "For training data that may not be linearly separable, the supportvector machines (SVM) algorithm (Vapnik, 1995; Cortes & Vapnik, 1995) seeks a linear classifier f : Rn !"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 33
                            }
                        ],
                        "text": "For instance, the SVM algorithm (Vapnik, 1995; Cortes & Vapnik, 1995) attempts to maximize the minimum margin of any training example."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59752996,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "5451278e1a11cf3f1be28a05f38d36c8641e68f7",
            "isKey": true,
            "numCitedBy": 4580,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Nature-of-Statistical-Learning-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 24
                            }
                        ],
                        "text": "It has been observed by Breiman (1997a, 1997b) and other authors (Collins, Schapire, & Singer, 2000; Friedman, Hastie, & Tibshirani, 2000; Mason, Baxter, Bartlett, & Frean, 1999; Ra\u0308tsch, Onoda, & M\u0308uller, to appear; Schapire & Singer, 1999) that theht\u2019s and t\u2019s are effectively being greedily\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "By similar reasoning, CART (Breiman et al., 1984), which splits using the Gini index, can be linked to the square loss function, while Kearns and Mansour\u2019s (1996) splitting rule can be linked to the exponential loss used by AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "Some machine learning algorithms, such as C4.5 (Quinlan, 1993) and CART (Breiman, Friedman, Olshen, & Stone, 1984), can then be naturally extended to handle the multiclass case."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 28
                            }
                        ],
                        "text": "By similar reasoning, CART (Breiman et al., 1984), which spl it using the Gini index, can be linked to the square loss function, while Kearns and Mansour \u2019 (1996) splitting rule can be linked to the exponential loss used by AdaBoost."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 414,
                                "start": 28
                            }
                        ],
                        "text": "By similar reasoning, CART (Breiman et al., 1984), which spl it using the Gini index, can be linked to the square loss function, while Kearns and Mansour \u2019 (1996) splitting rule can be linked to the exponential loss used by AdaBoost. The analysis we present in the next section might also hold fo r other algorithms that tacitly employ a function of the margin. For instance, Freund\u2019s Brow nBoost algorithm (1999) implicitly uses an instance potential function that satisfies the condi ti we impose onL."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prediction games and arcing classifier"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 0
                            }
                        ],
                        "text": "Hastie and Tibshirani (1998) suggest a different approach in which all pairs o f classes are compared to each other."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 40
                            }
                        ],
                        "text": "For other algorithms, such as AdaBoost (Freund & Schapire, 1997;\nc 2000 AT&T Corp.\nSchapire & Singer, 1999) and the support-vector machines (SVM) algorithm (Vapnik, 1995; Cortes & Vapnik, 1995), a direct extension to the multiclass case may be problematic."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 41
                            }
                        ],
                        "text": "In a little studied variant of AdaBoost (Freund & Schapire, 1997), we allow AdaBoost to output randomized predictions in which the predicted label of a new examplex is chosen randomly to be+1 with probability 1=(1+e 2f(x))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 10
                            }
                        ],
                        "text": "AdaBoost (Freund & Schapire, 1997; Schapire & Singer, 1999) is one example: it can be shown that AdaBoost is a greedy procedure for minimizing an exponential loss function of the margins."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 24
                            }
                        ],
                        "text": "The algorithm AdaBoost (Freund & Schapire, 1997; Schapire & Singer, 1999) builds a hypothesisf that is a linear combination ofweakor base hypothesesht:\nf(x) = X t tht(x):\nThe hypothesisf is built up in a series of rounds on each of which anht is selected by aweak or base learning algorithmand t 2 R\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A decision-theoretic g"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 93
                            }
                        ],
                        "text": "These results generalize more specialized bounds proved by Schapire and Singer (1999) and by Guruswami and Sahai (1999)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 53
                            }
                        ],
                        "text": "Also, Corollary 2 generalizes some of the results of Guruswami and Sahai (1999) that bound the multiclass training error in terms of the training (misclassification) error rates of the binary classifiers."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 183
                            }
                        ],
                        "text": "We note in passing that the loss-based decoding method for lo g-loss is the well known and widely used maximum-likelihood decoding which was studied briefly in the context of ECOC by Guruswami and Sahai (1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 182
                            }
                        ],
                        "text": "We note in passing that the loss-based decoding method for log-loss is the well known and widely used maximum-likelihood decoding which was studied briefly in the context of ECOC by Guruswami and Sahai (1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiclass learning, boo"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49637194"
                        ],
                        "name": "A. Mullin",
                        "slug": "A.-Mullin",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Mullin",
                            "middleNames": [
                                "A."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mullin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51083130"
                        ],
                        "name": "F. Rosenblatt",
                        "slug": "F.-Rosenblatt",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Rosenblatt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rosenblatt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61566132,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cccc0a4817fd5f6d8758c66b4065a23897d49f1d",
            "isKey": false,
            "numCitedBy": 2369,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Principles-of-neurodynamics-Mullin-Rosenblatt",
            "title": {
                "fragments": [],
                "text": "Principles of neurodynamics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095718110"
                        ],
                        "name": "\u91d1\u7530 \u91cd\u90ce",
                        "slug": "\u91d1\u7530-\u91cd\u90ce",
                        "structuredName": {
                            "firstName": "\u91d1\u7530",
                            "lastName": "\u91cd\u90ce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u91d1\u7530 \u91cd\u90ce"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "Thus, C4.5, in this simple form, can be viewed as a margin-based learning algorithm that is naturally linked to the loss function used in logistic regression."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "For instance, Quinlan\u2019s C4.5 (1993), in its simplest form, for binary classification problems, splits decision nodes in a manner to greedily minimize\nX leafj p+j ln p j + p + j p+j\n!"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 48
                            }
                        ],
                        "text": "Some machine learning algorithms, such as C4.5 (Quinlan, 1993) and CART (Breiman, Friedman, Olshen, & Stone, 1984), can then be naturally extended to handle the multiclass case."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59994655,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c16cf47f2b872e7b2ad06facb5d491857650514",
            "isKey": false,
            "numCitedBy": 1401,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "C4.5:-Programs-for-Machine-Learning-(\u66f8\u8a55)-\u91d1\u7530",
            "title": {
                "fragments": [],
                "text": "C4.5: Programs for Machine Learning (\u66f8\u8a55)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49353544"
                        ],
                        "name": "R. Fisher",
                        "slug": "R.-Fisher",
                        "structuredName": {
                            "firstName": "Rory",
                            "lastName": "Fisher",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fisher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 29084021,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "ab21376e43ac90a4eafd14f0f02a0c87502b6bbf",
            "isKey": false,
            "numCitedBy": 13267,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "THE-USE-OF-MULTIPLE-MEASUREMENTS-IN-TAXONOMIC-Fisher",
            "title": {
                "fragments": [],
                "text": "THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1936
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644344103"
                        ],
                        "name": "J. C. BurgesChristopher",
                        "slug": "J.-C.-BurgesChristopher",
                        "structuredName": {
                            "firstName": "J",
                            "lastName": "BurgesChristopher",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. C. BurgesChristopher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 215966761,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6716697767fc601efc7690f40820d9ea7a7bf57c",
            "isKey": false,
            "numCitedBy": 13527,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, w..."
            },
            "slug": "A-Tutorial-on-Support-Vector-Machines-for-Pattern-BurgesChristopher",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Support Vector Machines for Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This tutorial starts with an overview of the concepts of VC dimension and structural risk minimization and describes linear Support Vector Machines (SVMs) for separable and non-separable data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Linear Regression Analysis with Fuzzy Model \""
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans . on Systems , Man and Cybernetics"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "S.Velickov and D.Solomatine, \"Support Vector Machines: Review and Applications in Civil Engineering\", in AI Methods in Civil Engineering Applications"
            },
            "venue": {
                "fragments": [],
                "text": "O.Schleider et al (Eds.), Cottbus,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 80
                            }
                        ],
                        "text": "In logistic regression and related methods such as Iterative Scaling (Csisza\u0301r & Tusn\u0301ady, 1984; Della Pietra, Della Pietra, & Lafferty, 1997; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label:\nPr y =\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information geometry and alternating minimzation procedures"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics and Decisions"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 48
                            }
                        ],
                        "text": "= 2 + 1 ` X\u0300 s=1 zs:\nThis is because, as proved by Hoeffding (1963), for any random variableX with a X b, and for > 0,\nE h e X i exp 2(b a)2\n8 + E [X]\n!"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 30
                            }
                        ],
                        "text": "This is because, as proved by Hoeffding (1963), for any rando m variableX with a X b, and for > 0, E he Xi exp 2(b a)2 8 + E [X\u2104! : On the other hand, by Eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probability inequalities for sums"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 70
                            }
                        ],
                        "text": "In logistic regression and related methods such as Iterative Scaling (Csisza\u0301r & Tusn\u0301ady, 1984; Della Pietra, Della Pietra, & Lafferty, 1997; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label:\nPr y =\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 51
                            }
                        ],
                        "text": "In logistic regression and related methods such as Iterative Scaling (Csisza\u0301r & Tusn\u0301ady, 1984; Della Pietra, Della Pietra, & Lafferty, 1997; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label:\nPr y = +1jx = 1\n1+ e 2f(x) :\nOne then attempts to maximize the likelihood of the labels in the sample, or equivalently, to minimize the log loss log(Pr yjx ) = log(1+ e 2yf(x)): Thus, for logistic regression and related methods, we takeL(z) = log(1+ e 2z)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information geometry and alternating minimization procedures. Statistics and Decisions"
            },
            "venue": {
                "fragments": [],
                "text": "Supplement Issue,"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information geometry and alternating minimization procedures"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics and Decisions , Supplement Issue"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kohlmorgen, \"Using Support Vector Machines for Time Series Prediction"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Kernel Methods- Support Vector Learning,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prediction games and arcing classifiers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 68
                            }
                        ],
                        "text": "In logistic regression and related methods such as Iterativ Scaling (Csisz\u00e1r & Tusn\u00e1dy, 1984; Della Pietra, Della Pietra, & Lafferty, 19 97; Lafferty, 1999), and LogitBoost (Friedman et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 141
                            }
                        ],
                        "text": "In logistic regression and related methods such as Iterative Scaling (Csisza\u0301r & Tusn\u0301ady, 1984; Della Pietra, Della Pietra, & Lafferty, 1997; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label:\nPr y =\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 82
                            }
                        ],
                        "text": "Theorem 1 and Corollary 2 are broad generalizations of simil ar results proved by Schapire and Singer (1999) in a much more specialized setting involving o ly AdaBoost."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Additive models, boosting and inferen"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robust trainabilit"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Golowich and A.Smola, \"Support Vector Method for Function Approximation, Regression Estimation, and Signal Processing\", in Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 70
                            }
                        ],
                        "text": "In logistic regression and related methods such as Iterative Scaling (Csisza\u0301r & Tusn\u0301ady, 1984; Della Pietra, Della Pietra, & Lafferty, 1997; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label:\nPr y =\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 51
                            }
                        ],
                        "text": "In logistic regression and related methods such as Iterative Scaling (Csisza\u0301r & Tusn\u0301ady, 1984; Della Pietra, Della Pietra, & Lafferty, 1997; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label:\nPr y = +1jx = 1\n1+ e 2f(x) :\nOne then attempts to maximize the likelihood of the labels in the sample, or equivalently, to minimize the log loss log(Pr yjx ) = log(1+ e 2yf(x)): Thus, for logistic regression and related methods, we takeL(z) = log(1+ e 2z)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 70
                            }
                        ],
                        "text": "In this section, we describe and discuss experiments we have performed with synthetic data and with natural data from the UCI repository."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information geometry and alternating minimization procedures"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics and Decisions"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 70
                            }
                        ],
                        "text": "In logistic regression and related methods such as Iterative Scaling (Csisza\u0301r & Tusn\u0301ady, 1984; Della Pietra, Della Pietra, & Lafferty, 1997; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label:\nPr y =\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 51
                            }
                        ],
                        "text": "In logistic regression and related methods such as Iterative Scaling (Csisza\u0301r & Tusn\u0301ady, 1984; Della Pietra, Della Pietra, & Lafferty, 1997; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label:\nPr y = +1jx = 1\n1+ e 2f(x) :\nOne then attempts to maximize the likelihood of the labels in the sample, or equivalently, to minimize the log loss log(Pr yjx ) = log(1+ e 2yf(x)): Thus, for logistic regression and related methods, we takeL(z) = log(1+ e 2z)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information geometry and alternating minimization procedures. Statistics and Decisions, Supplement Issue"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 31
                            }
                        ],
                        "text": "Another approach, suggested by Hastie and Tibshirani (1998), is to use the given binary learning algorithm to distinguish each pair of classes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 0
                            }
                        ],
                        "text": "Hastie and Tibshirani (1998) suggest a different approach in which all pairs of classes are compared to each other."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classification by pairw"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 898,
                                "start": 75
                            }
                        ],
                        "text": "Schapire & Singer, 1999) and the support-vector machines (S VM) algorithm (Vapnik, 1995; Cortes & Vapnik, 1995), a direct extension to the multiclass case ma y be problematic. Typically, in such cases, the multiclass problem is reduced to multiple binary cl ssification problems that can be solved separately. Connectionist models (Rumelhart, Hinton, & Wi lliams, 1986), in which each class is represented by an output neuron, are a notable example; each output neuron serves as a discriminator between the class it represents and all of the other class es. Thus, this training algorithm is based on a reduction of the multiclass problem to k binary problems, where k is the number of classes. There are many ways to reduce a multiclass problem to multipl e binary classification problems. In the simple approach mentioned above, each class is compar ed to all others. Hastie and Tibshirani (1998) suggest a different approach in which all pairs o f classes are compared to each another."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the boosting ability"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 148
                            }
                        ],
                        "text": "\u2026related methods such as Iterative Scaling (Csisza\u0301r & Tusn\u0301ady, 1984; Della Pietra, Della Pietra, & Lafferty, 1997; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label:\nPr y = +1jx = 1\n1+ e 2f(x)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 162
                            }
                        ],
                        "text": "In logistic regression and related methods such as Iterative Scaling (Csisza\u0301r & Tusn\u0301ady, 1984; Della Pietra, Della Pietra, & Lafferty, 1997; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label:\nPr y = +1jx = 1\n1+ e 2f(x) :\nOne then attempts to maximize the likelihood of the labels in the sample, or equivalently, to minimize the log loss log(Pr yjx ) = log(1+ e 2yf(x)): Thus, for logistic regression and related methods, we takeL(z) = log(1+ e 2z)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Additivelogistic regression : a statistical view of boosting"
            },
            "venue": {
                "fragments": [],
                "text": "The Annals of Statistics"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 80
                            }
                        ],
                        "text": "In logistic regression and related methods such as Iterative Scaling (Csisza\u0301r & Tusn\u0301ady, 1984; Della Pietra, Della Pietra, & Lafferty, 1997; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label:\nPr y =\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information geometry and alternating minimization procedures"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics and Decisions"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Logistic regression, AdaBoost and Bregman"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 51
                            }
                        ],
                        "text": "Some recent progress in this direction was made by Crammer and Singer (2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the learnability and des"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast Training of Support Vector Machines using Sequential Minimal Optimization\", in B.Scholkopf, C.Burges & A.Smola(Eds.), Advances in Kernel Methods, pp.l85-208"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prediction of Chaotic Time Series Using Support Vector Machines"
            },
            "venue": {
                "fragments": [],
                "text": "in: NNSP'97: Neural Networks for Signal Processing VII: in Proc. of IEEE Signal Processing Society"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prediction games and arcing classifiers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 80
                            }
                        ],
                        "text": "In logistic regression and related methods such as Iterative Scaling (Csisza\u0301r & Tusn\u0301ady, 1984; Della Pietra, Della Pietra, & Lafferty, 1997; Lafferty, 1999), and LogitBoost (Friedman et al., 2000), one posits a logistic model for estimating the conditional probability of a positive label:\nPr y =\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information geometry and lternating minimization procedures"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics and Decisions"
            },
            "year": 1984
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 13,
            "methodology": 31,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 80,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/Reducing-Multiclass-to-Binary:-A-Unifying-Approach-Allwein-Schapire/cd74cc5129c45268d4e766d3619e7cb0ead5c8c8?sort=total-citations"
}