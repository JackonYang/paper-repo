{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9330607"
                        ],
                        "name": "S. Roweis",
                        "slug": "S.-Roweis",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Roweis",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roweis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 188
                            }
                        ],
                        "text": "Although charting is not designed for isometry, after affine transform the forward-projected points disagree with the original points with an RMS error of only 1.0429, lower than the best LLE (3.1423) or best ISOMAP (1.1424, not shown)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 133
                            }
                        ],
                        "text": "They have complementary strengths: ISOMAP handles holes well but can fail if the data hull is nonconvex [12]; and vice versa for LLE [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 101
                            }
                        ],
                        "text": "Figure 2LEFT contrasts the (unique) output of charting and the best outputs obtained from ISOMAP and LLE (considering all neighborhood sizes between 2 and 20 points)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 231
                            }
                        ],
                        "text": "With an order of magnitude fewer points, higher noise levels, no possibility of an isometric mapping, and uneven sampling, this is arguably a much more challenging problem than the \u201c swiss roll\u201d and \u201c s-curve\u201d problems featured in [12, 9, 8, 1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "ISOMAP and LLE show catastrophic folding; we had to change LLE\u2019s\na. data, xy view b. data, yz view c. local charts d. 2D embedding\ntrue manifold arc length\n1D o\nrd in\nat e\ne. 1D embedding\nregularization in order to coax out nondegenerate (>1D) solutions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 45
                            }
                        ],
                        "text": "The locally linear embedding algorithm (LLE) [9] represents each point as a weighted combination of a trusted set of nearest neighbors, then computes a minimally distorting low-dimensional barycentric embedding."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5987139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afcd6da7637ddeef6715109aca248da7a24b1c65",
            "isKey": true,
            "numCitedBy": 13981,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text."
            },
            "slug": "Nonlinear-dimensionality-reduction-by-locally-Roweis-Saul",
            "title": {
                "fragments": [],
                "text": "Nonlinear dimensionality reduction by locally linear embedding."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Locally linear embedding (LLE) is introduced, an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs that learns the global structure of nonlinear manifolds."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9330607"
                        ],
                        "name": "S. Roweis",
                        "slug": "S.-Roweis",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Roweis",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roweis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 49
                            }
                        ],
                        "text": "It bears closest relation to global coordination [8], although by a different construction of the problem, we avoid hill-climbing a spiky posterior and instead develop a closed-form solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 128
                            }
                        ],
                        "text": "In general, there may not be a unique posterior mode and the exact inverse is not solvable in closed form (this is also true of [8])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "As pointed out in [8], if a point has nonzero probabilities in two charts, then there should be affine transforms of those two charts that map the point to the same place in a global coordinate space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "In a return to mapping, Roweis and colleagues [8] proposed global coordination\u2014 learning a mixture of locally linear projections from sample to coordinate space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 246
                            }
                        ],
                        "text": "Even if a local subset of data points are dense in a direction perpendicular to the manifold, the prior encourages the local chart to orient parallel to the manifold as part of a globally optimal solution, protecting against a pathology noted in [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "Unlike global coordination [8], we are not asking that the dominant axes in neighboring charts are aligned\u2014 only that they span nearly the same subspace."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 5
                            }
                        ],
                        "text": "Like [2, 7, 6, 8], the method we develop below is a decomposition of the manifold into locally linear neighborhoods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 231
                            }
                        ],
                        "text": "With an order of magnitude fewer points, higher noise levels, no possibility of an isometric mapping, and uneven sampling, this is arguably a much more challenging problem than the \u201c swiss roll\u201d and \u201c s-curve\u201d problems featured in [12, 9, 8, 1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1258063,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d49080de5eecf0dd756ed6b28743aa837fce881",
            "isKey": true,
            "numCitedBy": 240,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "High dimensional data that lies on or near a low dimensional manifold can be described by a collection of local linear models. Such a description, however, does not provide a global parameterization of the manifold\u2014arguably an important goal of unsupervised learning. In this paper, we show how to learn a collection of local linear models that solves this more difficult problem. Our local linear models are represented by a mixture of factor analyzers, and the \"global coordination\" of these models is achieved by adding a regularizing term to the standard maximum likelihood objective function. The regularizer breaks a degeneracy in the mixture model's parameter space, favoring models whose internal coordinate systems are aligned in a consistent way. As a result, the internal coordinates change smoothly and continuously as one traverses a connected path on the manifold\u2014even when the path crosses the domains of many different local models. The regularizer takes the form of a Kullback-Leibler divergence and illustrates an unexpected application of variational methods: not to perform approximate inference in intractable probabilistic models, but to learn more useful internal representations in tractable ones."
            },
            "slug": "Global-Coordination-of-Local-Linear-Models-Roweis-Saul",
            "title": {
                "fragments": [],
                "text": "Global Coordination of Local Linear Models"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The regularizer takes the form of a Kullback-Leibler divergence and illustrates an unexpected application of variational methods: not to perform approximate inference in intractable probabilistic models, but to learn more useful internal representations in tractable ones."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056191569"
                        ],
                        "name": "Jos\u00e9 Gomes",
                        "slug": "Jos\u00e9-Gomes",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Gomes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jos\u00e9 Gomes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144053687"
                        ],
                        "name": "A. Mojsilovic",
                        "slug": "A.-Mojsilovic",
                        "structuredName": {
                            "firstName": "Aleksandra",
                            "lastName": "Mojsilovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mojsilovic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "More recent advances aim for embeddings: Gomes and Mojsilovic [4] treat manifold completion as an anisotropic diffusion problem, iteratively expanding points until they connect to their neighbors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27671789,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "0b5e2a05b0d95542c769e759b0c4793c58e4973f",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel algorithm for recovering a smooth manifold of unknown dimension and topology from a set of points known to belong to it. Numerous applications in computer vision can be naturally interpreted as instanciations of this fundamental problem. Recently, a non-iterative discrete approach, tensor voting, has been introduced to solve this problem and has been applied successfully to various applications. As an alternative, we propose a variational formulation of this problem in the continuous setting and derive an iterative algorithm which approximates its solutions. This method and tensor voting are somewhat the differential and integral form of one another. Although iterative methods are slower in general, the strength of the suggested method is that it can easily be applied when the ambient space is not Euclidean, which is important in many applications. The algorithm consists in solving a partial differential equation that performs a special anisotropic diffusion on an implicit representation of the known set of points. This results in connecting isolated neighbouring points. This approach is very simple, mathematically sound, robust and powerful since it handles in a homogeneous way manifolds of arbitrary dimension and topology, embedded in Euclidean or non-Euclidean spaces, with or without border. We shall present this approach and demonstrate both its benefits and shortcomings in two different contexts: (i) data visual analysis, (ii) skin detection in color images."
            },
            "slug": "A-Variational-Approach-to-Recovering-a-Manifold-Gomes-Mojsilovic",
            "title": {
                "fragments": [],
                "text": "A Variational Approach to Recovering a Manifold from Sample Points"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A novel algorithm for recovering a smooth manifold of unknown dimension and topology from a set of points known to belong to it is presented and can easily be applied when the ambient space is not Euclidean, which is important in many applications."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094778056"
                        ],
                        "name": "V. De Silva",
                        "slug": "V.-De-Silva",
                        "structuredName": {
                            "firstName": "Vin",
                            "lastName": "De Silva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. De Silva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46657367"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "The ISOMAP algorithm [12] represents remote distances as sums of a trusted set of distances between immediate neighbors, then uses multidimensional scaling to compute a low-dimensional embedding that minimally distorts all distances."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "They have complementary strengths: ISOMAP handles holes well but can fail if the data hull is nonconvex [12]; and vice versa for LLE [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 231
                            }
                        ],
                        "text": "With an order of magnitude fewer points, higher noise levels, no possibility of an isometric mapping, and uneven sampling, this is arguably a much more challenging problem than the \u201c swiss roll\u201d and \u201c s-curve\u201d problems featured in [12, 9, 8, 1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 221338160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3537fcd0ff99a3b3cb3d279012df826358420556",
            "isKey": false,
            "numCitedBy": 12182,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure."
            },
            "slug": "A-global-geometric-framework-for-nonlinear-Tenenbaum-Silva",
            "title": {
                "fragments": [],
                "text": "A global geometric framework for nonlinear dimensionality reduction."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set and efficiently computes a globally optimal solution, and is guaranteed to converge asymptotically to the true structure."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2713302"
                        ],
                        "name": "M. Revow",
                        "slug": "M.-Revow",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Revow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Revow"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 5
                            }
                        ],
                        "text": "Like [2, 7, 6, 8], the method we develop below is a decomposition of the manifold into locally linear neighborhoods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 996158,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "62f4d89a3c1441b47170c7e1380137fb388d0799",
            "isKey": false,
            "numCitedBy": 416,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes two new methods for modeling the manifolds of digitized images of handwritten digits. The models allow a priori information about the structure of the manifolds to be combined with empirical data. Accurate modeling of the manifolds allows digits to be discriminated using the relative probability densities under the alternative models. One of the methods is grounded in principal components analysis, the other in factor analysis. Both methods are based on locally linear low-dimensional approximations to the underlying data manifold. Links with other methods that model the manifold are discussed."
            },
            "slug": "Modeling-the-manifolds-of-images-of-handwritten-Hinton-Dayan",
            "title": {
                "fragments": [],
                "text": "Modeling the manifolds of images of handwritten digits"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "Two new methods for modeling the manifolds of digitized images of handwritten digits of principal components analysis and factor analysis are described, based on locally linear low-dimensional approximations to the underlying data manifold."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 306,
                                "start": 303
                            }
                        ],
                        "text": "It has been noted that trusted-set methods are vulnerable to noise because they consider the subset of point-to-point relationships that has the lowest signal-to-noise ratio; small changes to the trusted set can induce large changes in the set of constraints on the embedding, making solutions unstable [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 231
                            }
                        ],
                        "text": "With an order of magnitude fewer points, higher noise levels, no possibility of an isometric mapping, and uneven sampling, this is arguably a much more challenging problem than the \u201c swiss roll\u201d and \u201c s-curve\u201d problems featured in [12, 9, 8, 1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9969543,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14de052485d4245d5e240d39c62ae0e014ec9e09",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Topological Stability Tenenbaum et al. (1) presented an algorithm, Isomap, for computing a quasi-isometric, low-dimensional embedding of a set of high-dimensional data points. Two issues need to be raised concerning this work. First, the basic approach presented by Tenenbaum et al. is not new, having been described in the context of flattening cortical surfaces using geodesic distances and multidimensional scaling (2, 3) and in later work that used Dijkstra\u2019s algorithm to approximate geodesic distances (4, 5). These ideas generalize to arbitrary dimensionality if the connectivity and metric information of the manifold are correctly supplied. Second, and more important, this approach is topologically unstable and can only be used after careful preprocessing of the data (6). In the application domain of cortical flattening, it is necessary to check manually for connectivity errors, so that points nearby in 3-space (for example, on opposite banks of a cortical sulcus) are not taken to be nearby in the cortical surface. If such care is taken, this method represents the preferred method for quasi-isometric cortical flattening. What is new about the Isomap algorithm is how it defines the connectivity of each data point via its nearest Euclidean neighbors in the high-dimensional space. This step is vulnerable to short-circuit errors if the neighborhood is too large with respect to folds in the manifold on which the data points lie or if noise in the data moves the points slightly off the manifold. Even a single short-circuit error can alter many entries in the geodesic distance matrix, which in turn can lead to a drastically different (and incorrect) low-dimensional embedding. We illustrate this failure in Fig. 1, using the MATLAB code published by Tenenbaum et al. along with their \u201cSwiss roll\u201d data, to which we have added a small amount of noise (7). Clearly, the algorithm is topologically unstable: Small errors in the data connectivity (topology) can lead to large errors in the solution. Choosing a very small neighborhood is not a satisfactory solution, as this can fragment the manifold into a large number of disconnected regions. Choosing the neighborhood \u201cjust right\u201d requires a priori information about the global geometry of the highdimensional data manifold (8), but, presumably, it is exactly in the absence of such information that one would need to use an algorithm to find \u201cmeaningful low-dimensional structures hidden in high-dimensional observations\u201d (1). In summary, the basic idea of Isomap has long been known, and the new component introduced by Tenenbaum et al. provides an unreliable estimate of surface connectivity, which can lead to failure of the algorithm to perform as claimed. Mukund Balasubramanian Eric L. Schwartz Department of Cognitive and Neural Systems and Department of Electrical and Computer Systems Boston University Boston, MA 02215, USA"
            },
            "slug": "The-Isomap-Algorithm-and-Topological-Stability-Tenenbaum",
            "title": {
                "fragments": [],
                "text": "The Isomap Algorithm and Topological Stability"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The basic idea of Isomap has long been known, and the new component introduced by Tenenbaum et al. provides an unreliable estimate of surface connectivity, which can lead to failure of the algorithm to perform as claimed."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144285908"
                        ],
                        "name": "A. G\u00e1mez",
                        "slug": "A.-G\u00e1mez",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "G\u00e1mez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. G\u00e1mez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107699019"
                        ],
                        "name": "C. Zhou",
                        "slug": "C.-Zhou",
                        "structuredName": {
                            "firstName": "C",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34888597"
                        ],
                        "name": "A. Timmermann",
                        "slug": "A.-Timmermann",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Timmermann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Timmermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143842718"
                        ],
                        "name": "J. Kurths",
                        "slug": "J.-Kurths",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Kurths",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kurths"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 67
                            }
                        ],
                        "text": "The field has its roots in mapping algorithms: DeMers and Cottrell [3] proposed using auto-encoding neural networks with a hidden layer \u201c bottleneck,\u201d effectively casting dimensionality reduction as a compression problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16929642,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2792c8ada7ee96e9c9f5bab42ac32522a02c2653",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Linear methods of dimensionality reduction are useful tools for handling and interpreting high dimensional data. However, the cumulative variance explained by each of the subspaces in which the data space is decomposed may show a slow convergence that makes the selection of a proper minimum number of subspaces for successfully representing the variability of the process ambiguous. The use of nonlinear methods can improve the embedding of multivariate data into lower dimensional manifolds. In this article, a nonlinear method for dimensionality reduction, Isomap, is applied to the sea surface temperature and thermocline data in the tropical Pacific Ocean, where the El Nino-Southern Oscillation (ENSO) phenomenon and the annual cycle phenomena interact. Isomap gives a more accurate description of the manifold dimensionality of the physical system. The knowledge of the minimum number of dimensions is expected to improve the development of low dimensional models for understanding and predicting ENSO."
            },
            "slug": "Nonlinear-dimensionality-reduction-in-climate-data-G\u00e1mez-Zhou",
            "title": {
                "fragments": [],
                "text": "Nonlinear dimensionality reduction in climate data"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Isomap is applied to the sea surface temperature and thermocline data in the tropical Pacific Ocean, where the El Nino-Southern Oscillation (ENSO) phenomenon and the annual cycle phenomena interact, and gives a more accurate description of the manifold dimensionality of the physical system."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2302447"
                        ],
                        "name": "N. Kambhatla",
                        "slug": "N.-Kambhatla",
                        "structuredName": {
                            "firstName": "Nanda",
                            "lastName": "Kambhatla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kambhatla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3222903"
                        ],
                        "name": "T. Leen",
                        "slug": "T.-Leen",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Leen",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Leen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 5
                            }
                        ],
                        "text": "Like [2, 7, 6, 8], the method we develop below is a decomposition of the manifold into locally linear neighborhoods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 147780,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb4b0ff68d58d8ef8c1c2ee2b8bdc0a60cbeb4b4",
            "isKey": false,
            "numCitedBy": 664,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Reducing or eliminating statistical redundancy between the components of high-dimensional vector data enables a lower-dimensional representation without significant loss of information. Recognizing the limitations of principal component analysis (PCA), researchers in the statistics and neural network communities have developed nonlinear extensions of PCA. This article develops a local linear approach to dimension reduction that provides accurate representations and is fast to compute. We exercise the algorithms on speech and image data, and compare performance with PCA and with neural network implementations of nonlinear PCA. We find that both nonlinear techniques can provide more accurate representations than PCA and show that the local linear techniques outperform neural network implementations."
            },
            "slug": "Dimension-Reduction-by-Local-Principal-Component-Kambhatla-Leen",
            "title": {
                "fragments": [],
                "text": "Dimension Reduction by Local Principal Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A local linear approach to dimension reduction that provides accurate representations and is fast to compute is developed and it is shown that the local linear techniques outperform neural network implementations."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2428034"
                        ],
                        "name": "C. Bregler",
                        "slug": "C.-Bregler",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Bregler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bregler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 5
                            }
                        ],
                        "text": "Like [2, 7, 6, 8], the method we develop below is a decomposition of the manifold into locally linear neighborhoods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5258377,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72ab6bb230d2ad9a96629480094a45e3d0ef4d8b",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of interpolating between specified images in an image sequence is a simple, but important task in model-based vision. We describe an approach based on the abstract task of \"manifold learning\" and present results on both synthetic and real image sequences. This problem arose in the development of a combined lip-reading and speech recognition system."
            },
            "slug": "Nonlinear-Image-Interpolation-using-Manifold-Bregler-Omohundro",
            "title": {
                "fragments": [],
                "text": "Nonlinear Image Interpolation using Manifold Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work describes an approach based on the abstract task of \"manifold learning\" and presents results on both synthetic and real image sequences to solve the problem of interpolating between specified images in an image sequence."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In fact, [11] is closely related to equation (8) through Weiss\u2019 normalization lemma [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15872360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9d7f589a3d368a3701832e28d90ca09ec9e5577",
            "isKey": false,
            "numCitedBy": 857,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic grouping and segmentation of images remains a challenging problem in computer vision. Recently, a number of authors have demonstrated good performance on this task using methods that are based on eigenvectors of the affinity matrix. These approaches are extremely attractive in that they are based on simple eigendecomposition algorithms whose stability is well understood. Nevertheless, the use of eigendecompositions in the context of segmentation is far from well understood. In this paper we give a unified treatment of these algorithms, and show the close connections between them while highlighting their distinguishing features. We then prove results on eigenvectors of block matrices that allow us to analyze the performance of these algorithms in simple grouping settings. Finally, we use our analysis to motivate a variation on the existing methods that combines aspects from different eigenvector segmentation algorithms. We illustrate our analysis with results on real and synthetic images."
            },
            "slug": "Segmentation-using-eigenvectors:-a-unifying-view-Weiss",
            "title": {
                "fragments": [],
                "text": "Segmentation using eigenvectors: a unifying view"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A unified treatment of eigenvectors of block matrices based on eigendecompositions in the context of segmentation is given, and close connections between them are shown while highlighting their distinguishing features."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145965026"
                        ],
                        "name": "D. DeMers",
                        "slug": "D.-DeMers",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "DeMers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. DeMers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48524582"
                        ],
                        "name": "G. Cottrell",
                        "slug": "G.-Cottrell",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Cottrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cottrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2888517,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eca162dc6b8cea7347090ae92adc07efaf1ec656",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for creating a non-linear encoder-decoder for multidimensional data with compact representations is presented. The commonly used technique of autoassociation is extended to allow non-linear representations, and an objective function which penalizes activations of individual hidden units is shown to result in minimum dimensional encodings with respect to allowable error in reconstruction."
            },
            "slug": "Non-Linear-Dimensionality-Reduction-DeMers-Cottrell",
            "title": {
                "fragments": [],
                "text": "Non-Linear Dimensionality Reduction"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The commonly used technique of autoassociation is extended to allow non-linear representations, and an objective function which penalizes activations of individual hidden units is shown to result in minimum dimensional encodings with respect to allowable error in reconstruction."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 35
                            }
                        ],
                        "text": "Thanks to J. Buhmann, S. Makar, S. Roweis, J. Tenenbaum, and anonymous reviewers for insightful comments and suggested \u201c challenge\u201d problems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 60
                            }
                        ],
                        "text": "Video: We obtained a 1965-frame video sequence (courtesy S. Roweis and B. Frey) of 20\u00d7 28-pixel images in which B.F. strikes a variety of poses and expressions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 24
                            }
                        ],
                        "text": "In a return to mapping, Roweis and colleagues [8] proposed global coordination\u2014 learning a mixture of locally linear projections from sample to coordinate space."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "1We thank reviewers for calling our attention to Teh & Roweis ([11]\u2014 in this volume), which shows how to connect a set of given local dimensionality reducers in a generalized eigenvalue problem that is related to equation (8)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic alignment of hidden representations"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS\u201315"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hastie and W . Stuetzle . Principal curves"
            },
            "venue": {
                "fragments": [],
                "text": "J . Am . Statistical Assoc"
            },
            "year": 1989
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 7,
            "methodology": 7
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 13,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Charting-a-Manifold-Brand/c291edf0bb5ead2e45ee9fefb1683e2438158121?sort=total-citations"
}