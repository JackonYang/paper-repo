{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2570381"
                        ],
                        "name": "Brody Huval",
                        "slug": "Brody-Huval",
                        "structuredName": {
                            "firstName": "Brody",
                            "lastName": "Huval",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brody Huval"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 29
                            }
                        ],
                        "text": "The main idea of the MV-RNN (Socher et al., 2012) is to represent every word and longer phrase in a parse tree as both a vector and a matrix."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 148
                            }
                        ],
                        "text": "We compare to several supervised, compositional models such as standard recursive neural networks (RNN) (Socher et al., 2011b), matrix-vector RNNs (Socher et al., 2012), and baselines such as neural networks that ignore word order, Naive Bayes (NB), bi-gram NB and SVM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 77
                            }
                        ],
                        "text": "For the resulting 131 cases, the RNTN obtains an accuracy of 41% compared to MV-RNN (37), RNN (36) and biNB (27)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 79
                            }
                        ],
                        "text": "For the tree with (vector,matrix) nodes:\n(p2,P2)\n(a,A) (p1,P1)\n(b,B) (c,C) the MV-RNN computes the first parent vector and its matrix via two equations:\np1 = f\n( W [ Cb Bc ]) , P1 = f ( WM [ B C ]) ,\nwhere WM \u2208 Rd\u00d72d and the result is again a d \u00d7 d matrix."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 159
                            }
                        ],
                        "text": "In particular we will describe and experimentally compare our new RNTN model to recursive neural networks (RNN) (Socher et al., 2011b) and matrix-vector RNNs (Socher et al., 2012) both of which have been applied to bag of words sentiment corpora."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 45
                            }
                        ],
                        "text": "The previous state of the art was below 80% (Socher et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 4
                            }
                        ],
                        "text": "The MV-RNN has orders of magnitudes more parameters than any other model due to the word matrices."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 4
                            }
                        ],
                        "text": "The MV-RNN is linguistically motivated in that most of the parameters are associated with words and each composition function that computes vectors for longer phrases depends on the actual words being combined."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 55
                            }
                        ],
                        "text": "The RNTN gets the highest performance, followed by the MV-RNN and RNN."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 247
                            }
                        ],
                        "text": "\u2026they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 21
                            }
                        ],
                        "text": "One problem with the MV-RNN is that the number of parameters becomes very large and depends on the size of the vocabulary."
                    },
                    "intents": []
                }
            ],
            "corpusId": 806709,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27e38351e48fe4b7da2775bf94341738bc4da07e",
            "isKey": false,
            "numCitedBy": 1265,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them."
            },
            "slug": "Semantic-Compositionality-through-Recursive-Spaces-Socher-Huval",
            "title": {
                "fragments": [],
                "text": "Semantic Compositionality through Recursive Matrix-Vector Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A recursive neural network model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length and can learn the meaning of operators in propositional logic and natural language is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 301,
                                "start": 152
                            }
                        ],
                        "text": "Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 89
                            }
                        ],
                        "text": "Hence, \u03b4p2,com = \u03b4p2,s which we can use to obtain the standard backprop derivative for W (Goller and K\u00fcchler, 1996; Socher et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 173
                            }
                        ],
                        "text": "\u2026they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 114
                            }
                        ],
                        "text": "Hence, \u03b4p2,com = \u03b4p2,s which we can use to obtain the standard backprop derivative for W (Goller and Ku\u0308chler, 1996; Socher et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9923502,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81b3b3fe994a9eda6d3f9d2149aa4492d1933975",
            "isKey": false,
            "numCitedBy": 316,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases only partly address the problem at the cost of huge feature spaces and sparseness. To address this, we introduce a recursive neural network architecture for jointly parsing natural language and learning vector space representations for variable-sized inputs. At the core of our architecture are context-sensitive recursive neural networks (CRNN). These networks can induce distributed feature representations for unseen phrases and provide syntactic information to accurately predict phrase structure trees. Most excitingly, the representation of each phrase also captures semantic information: For instance, the phrases \u201cdecline to comment\u201d and \u201cwould not disclose the terms\u201d are close by in the induced embedding space. Our current system achieves an unlabeled bracketing F-measure of 92.1% on the Wall Street Journal dataset for sentences up to length 15."
            },
            "slug": "Learning-Continuous-Phrase-Representations-and-with-Socher-Manning",
            "title": {
                "fragments": [],
                "text": "Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A recursive neural network architecture for jointly parsing natural language and learning vector space representations for variable-sized inputs and captures semantic information: For instance, the phrases \u201cdecline to comment\u201d and \u201cwould not disclose the terms\u201d are close by in the induced embedding space."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143845796"
                        ],
                        "name": "Jeffrey Pennington",
                        "slug": "Jeffrey-Pennington",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Pennington",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey Pennington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40150953"
                        ],
                        "name": "E. Huang",
                        "slug": "E.-Huang",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Huang",
                            "middleNames": [
                                "Hsin-Chun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 105
                            }
                        ],
                        "text": "We compare to several supervised, compositional models such as standard recursive neural networks (RNN) (Socher et al., 2011b), matrix-vector RNNs (Socher et al., 2012), and baselines such as neural networks that ignore word order, Naive Bayes (NB), bi-gram NB and SVM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 112
                            }
                        ],
                        "text": "In particular we will describe and experimentally compare our new RNTN model to recursive neural networks (RNN) (Socher et al., 2011b) and matrix-vector RNNs (Socher et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 81
                            }
                        ],
                        "text": "This model uses the same compositionality function as the recursive autoencoder (Socher et al., 2011b) and recursive auto-associate memories (Pollack, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 94
                            }
                        ],
                        "text": ", 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 231
                            }
                        ],
                        "text": "These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 113
                            }
                        ],
                        "text": "In particular we will describe and experimentally compare our new RNTN model to recursive neural networks (RNN) (Socher et al., 2011b) and matrix-vector RNNs (Socher et al., 2012) both of which have been applied to bag of words sentiment corpora."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 128
                            }
                        ],
                        "text": "The simplest member of this family of neural network models is the standard recursive neural network (Goller and Ku\u0308chler, 1996; Socher et al., 2011a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 104
                            }
                        ],
                        "text": "We compare to several supervised, compositional models such as standard recursive neural networks (RNN) (Socher et al., 2011b), matrix-vector RNNs (Socher et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3116311,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cfa2646776405d50533055ceb1b7f050e9014dcb",
            "isKey": false,
            "numCitedBy": 1244,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model's ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines."
            },
            "slug": "Semi-Supervised-Recursive-Autoencoders-for-Socher-Pennington",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions that outperform other state-of-the-art approaches on commonly used datasets, without using any pre-defined sentiment lexica or polarity shifting rules."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3172106"
                        ],
                        "name": "Ainur Yessenalina",
                        "slug": "Ainur-Yessenalina",
                        "structuredName": {
                            "firstName": "Ainur",
                            "lastName": "Yessenalina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ainur Yessenalina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748501"
                        ],
                        "name": "Claire Cardie",
                        "slug": "Claire-Cardie",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cardie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Cardie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 301,
                                "start": 152
                            }
                        ],
                        "text": "Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6721431,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2063745d08868c928455f422202b72146a1960fb",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a general learning-based approach for phrase-level sentiment analysis that adopts an ordinal sentiment scale and is explicitly compositional in nature. Thus, we can model the compositional effects required for accurate assignment of phrase-level sentiment. For example, combining an adverb (e.g., \"very\") with a positive polar adjective (e.g., \"good\") produces a phrase (\"very good\") with increased polarity over the adjective alone. Inspired by recent work on distributional approaches to compositionality, we model each word as a matrix and combine words using iterated matrix multiplication, which allows for the modeling of both additive and multiplicative semantic effects. Although the multiplication-based matrix-space framework has been shown to be a theoretically elegant way to model composition (Rudolph and Giesbrecht, 2010), training such models has to be done carefully: the optimization is non-convex and requires a good initial starting point. This paper presents the first such algorithm for learning a matrix-space model for semantic composition. In the context of the phrase-level sentiment analysis task, our experimental results show statistically significant improvements in performance over a bag-of-words model."
            },
            "slug": "Compositional-Matrix-Space-Models-for-Sentiment-Yessenalina-Cardie",
            "title": {
                "fragments": [],
                "text": "Compositional Matrix-Space Models for Sentiment Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents the first such algorithm for learning a matrix-space model for semantic composition, and its experimental results show statistically significant improvements in performance over a bag-of-words model."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038285"
                        ],
                        "name": "Alessandro Lenci",
                        "slug": "Alessandro-Lenci",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Lenci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alessandro Lenci"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 89
                            }
                        ],
                        "text": "Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5584134,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "553fb89d5858826c02f26e94262e8958debc777e",
            "isKey": false,
            "numCitedBy": 618,
            "numCiting": 148,
            "paperAbstract": {
                "fragments": [],
                "text": "Research into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus. As an alternative to this \u201cone task, one model\u201d approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems. In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes. Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods. The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature."
            },
            "slug": "Distributional-Memory:-A-General-Framework-for-Baroni-Lenci",
            "title": {
                "fragments": [],
                "text": "Distributional Memory: A General Framework for Corpus-Based Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The Distributional Memory approach is shown to be tenable despite the constraints imposed by its multi-purpose nature, and performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against several state-of-the-art methods."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47340898"
                        ],
                        "name": "Karo Moilanen",
                        "slug": "Karo-Moilanen",
                        "structuredName": {
                            "firstName": "Karo",
                            "lastName": "Moilanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karo Moilanen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50419262"
                        ],
                        "name": "S. Pulman",
                        "slug": "S.-Pulman",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pulman",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pulman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18967545,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d0b27c8bbc5bf4f4304725edd91dc4b236bd58b",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a compositional framework for modelling entity-level sentiment (sub)contexts, and demonstrate how holistic multi-entity polarity scoring emerges as a by-product of compositional sentiment parsing. A data set of five annotators\u2019 multi-entity judgements is presented, and a human ceiling is established for the challenging new task. The accuracy of an initial implementation, which includes both supervised learning and heuristic distance-based scoring methods, is 5.6\u223c6.8 points below the human ceiling amongst sentences and 8.1\u223c8.7 points amongst phrases."
            },
            "slug": "Multi-entity-Sentiment-Scoring-Moilanen-Pulman",
            "title": {
                "fragments": [],
                "text": "Multi-entity Sentiment Scoring"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A compositional framework for modelling entity-level sentiment (sub)contexts is presented, and it is demonstrated how holistic multi-entity polarity scoring emerges as a by-product of compositional sentiment parsing."
            },
            "venue": {
                "fragments": [],
                "text": "RANLP"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40037218"
                        ],
                        "name": "T. Nakagawa",
                        "slug": "T.-Nakagawa",
                        "structuredName": {
                            "firstName": "Tetsuji",
                            "lastName": "Nakagawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nakagawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3040648"
                        ],
                        "name": "Kentaro Inui",
                        "slug": "Kentaro-Inui",
                        "structuredName": {
                            "firstName": "Kentaro",
                            "lastName": "Inui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kentaro Inui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795664"
                        ],
                        "name": "S. Kurohashi",
                        "slug": "S.-Kurohashi",
                        "structuredName": {
                            "firstName": "Sadao",
                            "lastName": "Kurohashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kurohashi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 223
                            }
                        ],
                        "text": "Several works have explored sentiment compositionality through careful engineering of features or polarity shifting rules on syntactic structures (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Rentoumi et al., 2010; Nakagawa et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5935641,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da5cd00115f7ec108de8eebf071c5f3f19807df4",
            "isKey": false,
            "numCitedBy": 362,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a dependency tree-based method for sentiment classification of Japanese and English subjective sentences using conditional random fields with hidden variables. Subjective sentences often contain words which reverse the sentiment polarities of other words. Therefore, interactions between words need to be considered in sentiment classification, which is difficult to be handled with simple bag-of-words approaches, and the syntactic dependency structures of subjective sentences are exploited in our method. In the method, the sentiment polarity of each dependency subtree in a sentence, which is not observable in training data, is represented by a hidden variable. The polarity of the whole sentence is calculated in consideration of interactions between the hidden variables. Sum-product belief propagation is used for inference. Experimental results of sentiment classification for Japanese and English subjective sentences showed that the method performs better than other methods based on bag-of-features."
            },
            "slug": "Dependency-Tree-based-Sentiment-Classification-CRFs-Nakagawa-Inui",
            "title": {
                "fragments": [],
                "text": "Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "Experimental results of sentiment classification of Japanese and English subjective sentences using conditional random fields with hidden variables showed that the method performs better than other methods based on bag-of-features."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47340898"
                        ],
                        "name": "Karo Moilanen",
                        "slug": "Karo-Moilanen",
                        "structuredName": {
                            "firstName": "Karo",
                            "lastName": "Moilanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karo Moilanen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50419262"
                        ],
                        "name": "S. Pulman",
                        "slug": "S.-Pulman",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pulman",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pulman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 146
                            }
                        ],
                        "text": "Several works have explored sentiment compositionality through careful engineering of features or polarity shifting rules on syntactic structures (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Rentoumi et al., 2010; Nakagawa et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32599709,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9b0190b06ac7270e9052895f8592beb4959ccfd",
            "isKey": false,
            "numCitedBy": 194,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Sentiment classification of grammatical constituents can be explained in a quasicompositional way. The classification of a complex constituent is derived via the classification of its component constituents and operations on these that resemble the usual methods of compositional semantic analysis. This claim is illustrated with a description of sentiment propagation, polarity reversal, and polarity conflict resolution within various linguistic constituent types at various grammatical levels. We propose a theoretical composition model, evaluate a lexical dependency parsing post-process implementation, and estimate its impact on general NLP pipelines."
            },
            "slug": "Sentiment-Composition-Moilanen-Pulman",
            "title": {
                "fragments": [],
                "text": "Sentiment Composition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A theoretical composition model is proposed, a lexical dependency parsing post-process implementation is evaluated, and its impact on general NLP pipelines is estimated."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 95
                            }
                        ],
                        "text": "These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2617020,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57458bc1cffe5caa45a885af986d70f723f406b4",
            "isKey": false,
            "numCitedBy": 5024,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance."
            },
            "slug": "A-unified-architecture-for-natural-language-deep-Collobert-Weston",
            "title": {
                "fragments": [],
                "text": "A unified architecture for natural language processing: deep neural networks with multitask learning"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This work describes a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense using a language model."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40150953"
                        ],
                        "name": "E. Huang",
                        "slug": "E.-Huang",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Huang",
                            "middleNames": [
                                "Hsin-Chun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 124
                            }
                        ],
                        "text": "These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 95
                            }
                        ],
                        "text": "These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 372093,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b669398c4cf2ebe04375c8b1beae20f4ac802fa",
            "isKey": false,
            "numCitedBy": 1183,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models."
            },
            "slug": "Improving-Word-Representations-via-Global-Context-Huang-Socher",
            "title": {
                "fragments": [],
                "text": "Improving Word Representations via Global Context and Multiple Word Prototypes"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new neural network architecture is presented which learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and accounts for homonymy and polysemy by learning multiple embedDings per word."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144865353"
                        ],
                        "name": "B. Pang",
                        "slug": "B.-Pang",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Pang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Pang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "com website originally collected and published by Pang and Lee (2005). The original dataset includes 10,662 sennerdy  folks"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3264224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6af58c061f2e4f130c3b795c21ff0c7e3903278f",
            "isKey": false,
            "numCitedBy": 2290,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the rating-inference problem, wherein rather than simply decide whether a review is \"thumbs up\" or \"thumbs down\", as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five \"stars\"). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, \"three stars\" is intuitively closer to \"four stars\" than to \"one star\".We first evaluate human performance at the task. Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem."
            },
            "slug": "Seeing-Stars:-Exploiting-Class-Relationships-for-to-Pang-Lee",
            "title": {
                "fragments": [],
                "text": "Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A meta-algorithm is applied, based on a metric labeling formulation of the rating-inference problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36037226"
                        ],
                        "name": "R\u00e9jean Ducharme",
                        "slug": "R\u00e9jean-Ducharme",
                        "structuredName": {
                            "firstName": "R\u00e9jean",
                            "lastName": "Ducharme",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9jean Ducharme"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120247189"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909943744"
                        ],
                        "name": "Christian Janvin",
                        "slug": "Christian-Janvin",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Janvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Janvin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 62
                            }
                        ],
                        "text": "One possibility to remedy this is to use neural word vectors (Bengio et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 221275765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "isKey": false,
            "numCitedBy": 6009,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts."
            },
            "slug": "A-Neural-Probabilistic-Language-Model-Bengio-Ducharme",
            "title": {
                "fragments": [],
                "text": "A Neural Probabilistic Language Model"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2585821"
                        ],
                        "name": "Cliff Chiung-Yu Lin",
                        "slug": "Cliff-Chiung-Yu-Lin",
                        "structuredName": {
                            "firstName": "Cliff",
                            "lastName": "Lin",
                            "middleNames": [
                                "Chiung-Yu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cliff Chiung-Yu Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 133
                            }
                        ],
                        "text": "1 RNN: Recursive Neural Network The simplest member of this family of neural network models is the standard recursive neural network (Goller and K\u00fcchler, 1996; Socher et al., 2011a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 105
                            }
                        ],
                        "text": "We compare to several supervised, compositional models such as standard recursive neural networks (RNN) (Socher et al., 2011b), matrix-vector RNNs (Socher et al., 2012), and baselines such as neural networks that ignore word order, Naive Bayes (NB), bi-gram NB and SVM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 81
                            }
                        ],
                        "text": "This model uses the same compositionality function as the recursive autoencoder (Socher et al., 2011b) and recursive auto-associate memories (Pollack, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 231
                            }
                        ],
                        "text": "These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 113
                            }
                        ],
                        "text": "In particular we will describe and experimentally compare our new RNTN model to recursive neural networks (RNN) (Socher et al., 2011b) and matrix-vector RNNs (Socher et al., 2012) both of which have been applied to bag of words sentiment corpora."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 128
                            }
                        ],
                        "text": "The simplest member of this family of neural network models is the standard recursive neural network (Goller and Ku\u0308chler, 1996; Socher et al., 2011a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18690358,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c0ddf74f87d154db88d79c640578c1610451eec",
            "isKey": false,
            "numCitedBy": 1320,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%."
            },
            "slug": "Parsing-Natural-Scenes-and-Natural-Language-with-Socher-Lin",
            "title": {
                "fragments": [],
                "text": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1864353"
                        ],
                        "name": "Edward Grefenstette",
                        "slug": "Edward-Grefenstette",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward Grefenstette"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784777"
                        ],
                        "name": "M. Sadrzadeh",
                        "slug": "M.-Sadrzadeh",
                        "structuredName": {
                            "firstName": "Mehrnoosh",
                            "lastName": "Sadrzadeh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sadrzadeh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 326903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c5126da7ce388c64b796c80d15a3c3629d6ad58",
            "isKey": false,
            "numCitedBy": 336,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences. Our model matches the results of its competitors in the first experiment, and betters them in the second. The general improvement in results with increase in syntactic complexity showcases the compositional power of our model."
            },
            "slug": "Experimental-Support-for-a-Categorical-Model-of-Grefenstette-Sadrzadeh",
            "title": {
                "fragments": [],
                "text": "Experimental Support for a Categorical Compositional Distributional Model of Meaning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The abstract categorical model of Coecke et al. (2010) is implemented using data from the BNC and evaluated, with general improvement in results with increase in syntactic complexity showcasing the compositional power of the model."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2337426"
                        ],
                        "name": "Fabio Massimo Zanzotto",
                        "slug": "Fabio-Massimo-Zanzotto",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Zanzotto",
                            "middleNames": [
                                "Massimo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabio Massimo Zanzotto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076561"
                        ],
                        "name": "Ioannis Korkontzelos",
                        "slug": "Ioannis-Korkontzelos",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Korkontzelos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ioannis Korkontzelos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2402028"
                        ],
                        "name": "Francesca Fallucchi",
                        "slug": "Francesca-Fallucchi",
                        "structuredName": {
                            "firstName": "Francesca",
                            "lastName": "Fallucchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francesca Fallucchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756438"
                        ],
                        "name": "S. Manandhar",
                        "slug": "S.-Manandhar",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Manandhar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Manandhar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 301,
                                "start": 152
                            }
                        ],
                        "text": "Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 194
                            }
                        ],
                        "text": "\u2026they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15616495,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d9da542a6aa92fece5dfb7eecfb44ae7de0f664",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In distributional semantics studies, there is a growing attention in compositionally determining the distributional meaning of word sequences. Yet, compositional distributional models depend on a large set of parameters that have not been explored. In this paper we propose a novel approach to estimate parameters for a class of compositional distributional models: the additive models. Our approach leverages on two main ideas. Firstly, a novel idea for extracting compositional distributional semantics examples. Secondly, an estimation method based on regression models for multiple dependent variables. Experiments demonstrate that our approach outperforms existing methods for determining a good model for compositional distributional semantics."
            },
            "slug": "Estimating-Linear-Models-for-Compositional-Zanzotto-Korkontzelos",
            "title": {
                "fragments": [],
                "text": "Estimating Linear Models for Compositional Distributional Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper proposes a novel approach to estimate parameters for a class of compositional distributional models: the additive models, and demonstrates that this approach outperforms existing methods for determining a good model for compositional distributionsal semantics."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2126160"
                        ],
                        "name": "Vassiliki Rentoumi",
                        "slug": "Vassiliki-Rentoumi",
                        "structuredName": {
                            "firstName": "Vassiliki",
                            "lastName": "Rentoumi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vassiliki Rentoumi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2099453"
                        ],
                        "name": "Stefanos Petrakis",
                        "slug": "Stefanos-Petrakis",
                        "structuredName": {
                            "firstName": "Stefanos",
                            "lastName": "Petrakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefanos Petrakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2348993"
                        ],
                        "name": "M. Klenner",
                        "slug": "M.-Klenner",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Klenner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Klenner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680125"
                        ],
                        "name": "G. Vouros",
                        "slug": "G.-Vouros",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Vouros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Vouros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745631"
                        ],
                        "name": "V. Karkaletsis",
                        "slug": "V.-Karkaletsis",
                        "structuredName": {
                            "firstName": "Vangelis",
                            "lastName": "Karkaletsis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Karkaletsis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 200
                            }
                        ],
                        "text": "Several works have explored sentiment compositionality through careful engineering of features or polarity shifting rules on syntactic structures (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Rentoumi et al., 2010; Nakagawa et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14805740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "407f52d900be147410e75c0675105c38e8e039e4",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In the past, we have succesfully used machine learning approaches for sentiment analysis. In the course of those experiments, we observed that our machine learning method, although able to cope well with figurative language could not always reach a certain decision about the polarity orientation of sentences, yielding erroneous evaluations. We support the conjecture that these cases bearing mild figurativeness could be better handled by a rule-based system. These two systems, acting complementarily, could bridge the gap between machine learning and rule-based approaches. Experimental results using the corpus of the Affective Text Task of SemEval \u009207, provide evidence in favor of this direction."
            },
            "slug": "United-we-Stand:-Improving-Sentiment-Analysis-by-Rentoumi-Petrakis",
            "title": {
                "fragments": [],
                "text": "United we Stand: Improving Sentiment Analysis by Joining Machine Learning and Rule Based Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Experimental results using the corpus of the Affective Text Task of SemEval \u009207, provide evidence in favor of the conjecture that cases bearing mild figurativeness could be better handled by a rule-based system."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34902160"
                        ],
                        "name": "Jeff Mitchell",
                        "slug": "Jeff-Mitchell",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 301,
                                "start": 152
                            }
                        ],
                        "text": "Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26901423,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "745d86adca56ec50761591733e157f84cfb19671",
            "isKey": false,
            "numCitedBy": 930,
            "numCiting": 253,
            "paperAbstract": {
                "fragments": [],
                "text": "Vector-based models of word meaning have become increasingly popular in cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar. Despite their widespread use, vector-based models are typically directed at representing words in isolation, and methods for constructing representations for phrases or sentences have received little attention in the literature. This is in marked contrast to experimental evidence (e.g., in sentential priming) suggesting that semantic similarity is more complex than simply a relation between isolated words. This article proposes a framework for representing the meaning of word combinations in vector space. Central to our approach is vector composition, which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models that we evaluate empirically on a phrase similarity task."
            },
            "slug": "Composition-in-Distributional-Models-of-Semantics-Mitchell-Lapata",
            "title": {
                "fragments": [],
                "text": "Composition in Distributional Models of Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This article proposes a framework for representing the meaning of word combinations in vector space in terms of additive and multiplicative functions, and introduces a wide range of composition models that are evaluated empirically on a phrase similarity task."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708114"
                        ],
                        "name": "Katrin Erk",
                        "slug": "Katrin-Erk",
                        "structuredName": {
                            "firstName": "Katrin",
                            "lastName": "Erk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katrin Erk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708581"
                        ],
                        "name": "Sebastian Pad\u00f3",
                        "slug": "Sebastian-Pad\u00f3",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Pad\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Pad\u00f3"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 115
                            }
                        ],
                        "text": "Variants of this idea use more complex frequencies such as how often a word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad\u00f3, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1588782,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb9cc883bdd08d58feee5c7da01acff6fdb4ad78",
            "isKey": false,
            "numCitedBy": 387,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. This task is a crucial step towards a robust, vector-based compositional account of sentence meaning. We argue that existing models for this task do not take syntactic structure sufficiently into account. \n \nWe present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words' argument positions. This makes it possible to integrate syntax into the computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases."
            },
            "slug": "A-Structured-Vector-Space-Model-for-Word-Meaning-in-Erk-Pad\u00f3",
            "title": {
                "fragments": [],
                "text": "A Structured Vector Space Model for Word Meaning in Context"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel structured vector space model is presented that makes it possible to integrate syntax into the computation of word meaning in context and performs at and above the state of the art for modeling the contextual adequacy of paraphrases."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1864353"
                        ],
                        "name": "Edward Grefenstette",
                        "slug": "Edward-Grefenstette",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward Grefenstette"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145505048"
                        ],
                        "name": "Georgiana Dinu",
                        "slug": "Georgiana-Dinu",
                        "structuredName": {
                            "firstName": "Georgiana",
                            "lastName": "Dinu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Georgiana Dinu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144530990"
                        ],
                        "name": "Yao-zhong Zhang",
                        "slug": "Yao-zhong-Zhang",
                        "structuredName": {
                            "firstName": "Yao-zhong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yao-zhong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784777"
                        ],
                        "name": "M. Sadrzadeh",
                        "slug": "M.-Sadrzadeh",
                        "structuredName": {
                            "firstName": "Mehrnoosh",
                            "lastName": "Sadrzadeh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sadrzadeh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 301,
                                "start": 152
                            }
                        ],
                        "text": "Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 268
                            }
                        ],
                        "text": "\u2026they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15659560,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "607cca37c1429b7380df35b3f761ae1499aa84ab",
            "isKey": false,
            "numCitedBy": 147,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a model for compositional distributional semantics related to the framework of Coecke et al. (2010), and emulating formal semantics by representing functions as tensors and arguments as vectors. We introduce a new learning method for tensors, generalising the approach of Baroni and Zamparelli (2010). We evaluate it on two benchmark data sets, and find it to outperform existing leading methods. We argue in our analysis that the nature of this learning method also renders it suitable for solving more subtle problems compositional distributional models might face."
            },
            "slug": "Multi-Step-Regression-Learning-for-Compositional-Grefenstette-Dinu",
            "title": {
                "fragments": [],
                "text": "Multi-Step Regression Learning for Compositional Distributional Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is argued in the analysis that the nature of this learning method also renders it suitable for solving more subtle problems compositional distributional models might face, and is found to outperform existing leading methods."
            },
            "venue": {
                "fragments": [],
                "text": "IWCS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745715"
                        ],
                        "name": "F. Seide",
                        "slug": "F.-Seide",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Seide",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Seide"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 122
                            }
                        ],
                        "text": ", 2012), extending Restricted Boltzmann machines (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 283
                            }
                        ],
                        "text": "\u2026relate inputs through three way interactions, parameterized by a tensor have been proposed for relation classification (Sutskever et al., 2009; Jenatton et al., 2012), extending Restricted Boltzmann machines (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1762679,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a07982a704e117ea9970654d8a2697355d15142d",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, we proposed and developed the context-dependent deep neural network hidden Markov models (CD-DNN-HMMs) for large vocabulary speech recognition and achieved highly promising recognition results including over one third fewer word errors than the discriminatively trained, conventional HMM-based systems on the 300hr Switchboard benchmark task. In this paper, we extend DNNs to deep tensor neural networks (DTNNs) in which one or more layers are double-projection and tensor layers. The basic idea of the DTNN comes from our realization that many factors interact with each other to predict the output. To represent these interactions, we project the input to two nonlinear subspaces through the double-projection layer and model the interactions between these two subspaces and the output neurons through a tensor with three-way connections. Evaluation on 30hr Switchboard task indicates that DTNNs can outperform DNNs with similar number of parameters with 5% relative word error reduction."
            },
            "slug": "Large-Vocabulary-Speech-Recognition-Using-Deep-Yu-Deng",
            "title": {
                "fragments": [],
                "text": "Large Vocabulary Speech Recognition Using Deep Tensor Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Evaluation on 30hr Switchboard task indicates that DTNNs can outperform DNNs with similar number of parameters with 5% relative word error reduction, and is extended to deep tensor neural networks (DTNNs) in which one or more layers are double-projection and tensor layers."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1067591,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb7853c5d609081ea12cd4db3863a87da2d51808",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "A\u00a0plausible definition of \u201creasoning\u201d could be \u201calgebraically manipulating previously acquired knowledge in order to answer a new question\u201d. This definition covers first-order logical inference or probabilistic inference. It also includes much simpler manipulations commonly used to build large learning systems. For instance, we can build an optical character recognition system by first training a character segmenter, an isolated character recognizer, and a language model, using appropriate labelled training sets. Adequately concatenating these modules and fine tuning the resulting system can be viewed as an algebraic operation in a space of models. The resulting model answers a new question, that is, converting the image of a text page into a computer readable text.This observation suggests a conceptual continuity between algebraically rich inference systems, such as logical or probabilistic inference, and simple manipulations, such as the mere concatenation of trainable learning systems. Therefore, instead of trying to bridge the gap between machine learning systems and sophisticated \u201call-purpose\u201d inference mechanisms, we can instead algebraically enrich the set of manipulations applicable to training systems, and build reasoning capabilities from the ground up."
            },
            "slug": "From-machine-learning-to-machine-reasoning-Bottou",
            "title": {
                "fragments": [],
                "text": "From machine learning to machine reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Instead of trying to bridge the gap between machine learning systems and sophisticated \u201call-purpose\u201d inference mechanisms, the set of manipulations applicable to training systems can be algebraically enriched, and reasoning capabilities from the ground up are built."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708581"
                        ],
                        "name": "Sebastian Pad\u00f3",
                        "slug": "Sebastian-Pad\u00f3",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Pad\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Pad\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 115
                            }
                        ],
                        "text": "Variants of this idea use more complex frequencies such as how often a word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad\u00f3, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7747235,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "7441116c5b5a745708a9d7c5aa0ecf04e0c76c93",
            "isKey": false,
            "numCitedBy": 695,
            "numCiting": 103,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditionally, vector-based semantic space models use word co-occurrence counts from large corpora to represent lexical meaning. In this article we present a novel framework for constructing semantic spaces that takes syntactic relations into account. We introduce a formalization for this class of models, which allows linguistic knowledge to guide the construction process. We evaluate our framework on a range of tasks relevant for cognitive science and natural language processing: semantic priming, synonymy detection, and word sense disambiguation. In all cases, our framework obtains results that are comparable or superior to the state of the art."
            },
            "slug": "Dependency-Based-Construction-of-Semantic-Space-Pad\u00f3-Lapata",
            "title": {
                "fragments": [],
                "text": "Dependency-Based Construction of Semantic Space Models"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This article presents a novel framework for constructing semantic spaces that takes syntactic relations into account, and introduces a formalization for this class of models, which allows linguistic knowledge to guide the construction process."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068720"
                        ],
                        "name": "Rodolphe Jenatton",
                        "slug": "Rodolphe-Jenatton",
                        "structuredName": {
                            "firstName": "Rodolphe",
                            "lastName": "Jenatton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rodolphe Jenatton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7245737"
                        ],
                        "name": "Nicolas Le Roux",
                        "slug": "Nicolas-Le-Roux",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Le Roux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Le Roux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713934"
                        ],
                        "name": "Antoine Bordes",
                        "slug": "Antoine-Bordes",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Bordes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoine Bordes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2533906"
                        ],
                        "name": "G. Obozinski",
                        "slug": "G.-Obozinski",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Obozinski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Obozinski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 148
                            }
                        ],
                        "text": "\u2026to relate inputs through three way interactions, parameterized by a tensor have been proposed for relation classification (Sutskever et al., 2009; Jenatton et al., 2012), extending Restricted Boltzmann machines (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et al.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 131
                            }
                        ],
                        "text": "The idea to relate inputs through three way interactions, parameterized by a tensor have been proposed for relation classification (Sutskever et al., 2009; Jenatton et al., 2012), extending Restricted Boltzmann machines (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10854724,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "473b3f2cc2c942c0116d980fe5b36a338f6017de",
            "isKey": false,
            "numCitedBy": 357,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Many data such as social networks, movie preferences or knowledge bases are multi-relational, in that they describe multiple relations between entities. While there is a large body of work focused on modeling these data, modeling these multiple types of relations jointly remains challenging. Further, existing approaches tend to breakdown when the number of these types grows. In this paper, we propose a method for modeling large multi-relational datasets, with possibly thousands of relations. Our model is based on a bilinear structure, which captures various orders of interaction of the data, and also shares sparse latent factors across different relations. We illustrate the performance of our approach on standard tensor-factorization datasets where we attain, or outperform, state-of-the-art results. Finally, a NLP application demonstrates our scalability and the ability of our model to learn efficient and semantically meaningful verb representations."
            },
            "slug": "A-latent-factor-model-for-highly-multi-relational-Jenatton-Roux",
            "title": {
                "fragments": [],
                "text": "A latent factor model for highly multi-relational data"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper proposes a method for modeling large multi-relational datasets, with possibly thousands of relations, based on a bilinear structure, which captures various orders of interaction of the data and also shares sparse latent factors across different relations."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 125
                            }
                        ],
                        "text": "A related field that tackles compositionality from a very different angle is that of trying to map sentences to logical form (Zettlemoyer and Collins, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 449252,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74fe7ec751cd50295b15cfd46389a8fefb37c414",
            "isKey": false,
            "numCitedBy": 874,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of mapping natural language sentences to lambda\u2013calculus encodings of their meaning. We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. The algorithm induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence. We apply the method to the task of learning natural language interfaces to databases and show that the learned parsers outperform previous methods in two benchmark database domains."
            },
            "slug": "Learning-to-Map-Sentences-to-Logical-Form:-with-Zettlemoyer-Collins",
            "title": {
                "fragments": [],
                "text": "Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "A learning algorithm is described that takes as input a training set of sentences labeled with expressions in the lambda calculus and induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802811"
                        ],
                        "name": "D. Widdows",
                        "slug": "D.-Widdows",
                        "structuredName": {
                            "firstName": "Dominic",
                            "lastName": "Widdows",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Widdows"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 93
                            }
                        ],
                        "text": "Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Giesbrecht, 2010) have not been experimentally validated on larger corpora."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12044606,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c69a90236f1c57348de858918c554a9420f1521",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic vector models have proven their worth in a number of natural language applications whose goals can be accomplished by modelling individual semantic concepts and measuring similarities between them. By comparison, the area of semantic compositionality in these models has so far remained underdeveloped. This will be a crucial hurdle for semantic vector models: in order to play a fuller part in the modelling of human language, these models will need some way of modelling the way in which single concepts are put together to form more complex conceptual structures. This paper explores some of the opportunities for using vector product operations to model compositional phenomena in natural language. These vector operations are all well-known and used in mathematics and physics, particularly in quantum mechanics. Instead of designing new vector composition operators, this paper gathers a list of existing operators, and a list of typical composition operations in natural language, and describes two small experiments that begin to investigate the use of certain vector operators to model certain"
            },
            "slug": "Semantic-Vector-Products:-Some-Initial-Widdows",
            "title": {
                "fragments": [],
                "text": "Semantic Vector Products: Some Initial Investigations"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper gathers a list of existing operators, an list of typical composition operations in natural language, and describes two small experiments that begin to investigate the use of certain vector operators to model certain compositional phenomena innatural language."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759124"
                        ],
                        "name": "C. Goller",
                        "slug": "C.-Goller",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Goller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Goller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34940447"
                        ],
                        "name": "A. K\u00fcchler",
                        "slug": "A.-K\u00fcchler",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "K\u00fcchler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. K\u00fcchler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 102
                            }
                        ],
                        "text": "The simplest member of this family of neural network models is the standard recursive neural network (Goller and Ku\u0308chler, 1996; Socher et al., 2011a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 88
                            }
                        ],
                        "text": "Hence, \u03b4p2,com = \u03b4p2,s which we can use to obtain the standard backprop derivative for W (Goller and Ku\u0308chler, 1996; Socher et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6536466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c58dd287a476b4722c5b6b1316629e2874682219",
            "isKey": false,
            "numCitedBy": 578,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "While neural networks are very successfully applied to the processing of fixed-length vectors and variable-length sequences, the current state of the art does not allow the efficient processing of structured objects of arbitrary shape (like logical terms, trees or graphs). We present a connectionist architecture together with a novel supervised learning scheme which is capable of solving inductive inference tasks on complex symbolic structures of arbitrary size. The most general structures that can be handled are labeled directed acyclic graphs. The major difference of our approach compared to others is that the structure-representations are exclusively tuned for the intended inference task. Our method is applied to tasks consisting in the classification of logical terms. These range from the detection of a certain subterm to the satisfaction of a specific unification pattern. Compared to previously known approaches we obtained superior results in that domain."
            },
            "slug": "Learning-task-dependent-distributed-representations-Goller-K\u00fcchler",
            "title": {
                "fragments": [],
                "text": "Learning task-dependent distributed representations by backpropagation through structure"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A connectionist architecture together with a novel supervised learning scheme which is capable of solving inductive inference tasks on complex symbolic structures of arbitrary size is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of International Conference on Neural Networks (ICNN'96)"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143730547"
                        ],
                        "name": "S. Rudolph",
                        "slug": "S.-Rudolph",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Rudolph",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rudolph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48394537"
                        ],
                        "name": "Eugenie Giesbrecht",
                        "slug": "Eugenie-Giesbrecht",
                        "structuredName": {
                            "firstName": "Eugenie",
                            "lastName": "Giesbrecht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugenie Giesbrecht"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 210
                            }
                        ],
                        "text": "Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Giesbrecht, 2010) have not been experimentally validated on larger corpora."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2678583,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9269fd1efeb87000d346cc8514dec3fc6ec01d6",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose CMSMs, a novel type of generic compositional models for syntactic and semantic aspects of natural language, based on matrix multiplication. We argue for the structural and cognitive plausibility of this model and show that it is able to cover and combine various common compositional NLP approaches ranging from statistical word space models to symbolic grammar formalisms."
            },
            "slug": "Compositional-Matrix-Space-Models-of-Language-Rudolph-Giesbrecht",
            "title": {
                "fragments": [],
                "text": "Compositional Matrix-Space Models of Language"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "CMSMs, a novel type of generic compositional models for syntactic and semantic aspects of natural language, based on matrix multiplication is proposed and it is shown that it is able to cover and combine various common compositional NLP approaches."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50419262"
                        ],
                        "name": "S. Pulman",
                        "slug": "S.-Pulman",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pulman",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pulman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 137
                            }
                        ],
                        "text": "Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Giesbrecht, 2010) have not been experimentally validated on larger corpora."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2280191,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73e897104540642698321c106cc9c35af369fe12",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The are two main approaches to the representation of meaning in Computational Linguistics: a symbolic approach and a distributional approach. This paper considers the fundamental question of how these approaches might be combined. The proposal is to adapt a method from the Cognitive Science literature, in which symbolic and connectionist representations are combined using tensor products. Possible applications of this method for language processing are described. Finally, a potentially fruitful link between Quantum Mechanics, Computational Linguistics, and other related areas such as Information Retrieval and Machine Learning, is proposed."
            },
            "slug": "Combining-Symbolic-and-Distributional-Models-of-Clark-Pulman",
            "title": {
                "fragments": [],
                "text": "Combining Symbolic and Distributional Models of Meaning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A method is to be adapted from the Cognitive Science literature, in which symbolic and connectionist representations are combined using tensor products, to adapt a method for language processing."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI Spring Symposium: Quantum Interaction"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144163566"
                        ],
                        "name": "Benjamin Snyder",
                        "slug": "Benjamin-Snyder",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Snyder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Snyder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741283"
                        ],
                        "name": "R. Barzilay",
                        "slug": "R.-Barzilay",
                        "structuredName": {
                            "firstName": "Regina",
                            "lastName": "Barzilay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Barzilay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 129
                            }
                        ],
                        "text": "This work is connected to five different areas of NLP research, each with their own large amount of related work to which we cannot do full justice given space constraints."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2279432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ab8faaa1ee1791e6da911e408dee97681cfa58e",
            "isKey": false,
            "numCitedBy": 329,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of analyzing multiple related opinions in a text. For instance, in a restaurant review such opinions may include food, ambience and service. We formulate this task as a multiple aspect ranking problem, where the goal is to produce a set of numerical scores, one for each aspect. We present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks. This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast. We prove that our agreementbased joint model is more expressive than individual ranking models. Our empirical results further conrm the strength of the model: the algorithm provides signicant improvement over both individual rankers and a state-of-the-art joint ranking model."
            },
            "slug": "Multiple-Aspect-Ranking-Using-the-Good-Grief-Snyder-Barzilay",
            "title": {
                "fragments": [],
                "text": "Multiple Aspect Ranking Using the Good Grief Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An algorithm is presented that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks, and it is proved that the agreementbased joint model is more expressive than individual ranking models."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48619205"
                        ],
                        "name": "M. Israel",
                        "slug": "M.-Israel",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Israel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Israel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 161
                            }
                        ],
                        "text": "For instance, \u2018The movie was terrible\u2019 is negative but the \u2018The movie was not terrible\u2019 says only that it was less bad than a terrible one, not that it was good (Horn, 1989; Israel, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18053023,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6735700adb3e46c0ff6e13d54f2a3c95415d39f2",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper examines the lexicalization patterns of polarity items with a view to understanding the range of possible polarity items and the reasons why such forms should exist in the first place. My starting point is the Scalar Model of Polarity (Israel 1996, 1998), which predicts a reliable correlation between a polarity item's sensitivity and its scalar semantic properties: specifically, it predicts that forms denoting a minimal scalar degree may be emphatic negative polarity items (NPIs), while forms denoting maximal degrees can be emphatic positive polarity items. A variety of anomalous polarity items are discussed which flout this prediction, including both emphatic NPIs denoting maximal degrees (e.g. for all the tea in China, wild horses) and emphatic PPIs denoting minimal degrees (e.g. for a pittance, in a jiffy). The exceptional behaviour of these forms is shown to be a direct function of the participant roles they denote, reflecting the fact that different roles license different kinds of scalar inferences depending on how they contribute to the likelihood of an expressed proposition. In addition to establishing a link between thematic structure and the lexical semantics of polarity sensitivity, this result is shown to have important implications for the nature of scalar reasoning generally, and for the role it plays in structuring rhetorical discourse."
            },
            "slug": "Minimizers,-Maximizers-and-the-Rhetoric-of-Scalar-Israel",
            "title": {
                "fragments": [],
                "text": "Minimizers, Maximizers and the Rhetoric of Scalar Reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The lexicalization patterns of polarity items are examined and the exceptional behaviour of these forms is shown to be a direct function of the participant roles they denote, reflecting the fact that different roles license different kinds of scalar inferences depending on how they contribute to the likelihood of an expressed proposition."
            },
            "venue": {
                "fragments": [],
                "text": "J. Semant."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 89
                            }
                        ],
                        "text": "Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 89
                            }
                        ],
                        "text": "1 Introduction Semantic vector spaces for single words have been widely used as features (Turney and Pantel, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1500900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a0e788268fafb23ab20da0e98bb578b06830f7d",
            "isKey": false,
            "numCitedBy": 2721,
            "numCiting": 208,
            "paperAbstract": {
                "fragments": [],
                "text": "Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field."
            },
            "slug": "From-Frequency-to-Meaning:-Vector-Space-Models-of-Turney-Pantel",
            "title": {
                "fragments": [],
                "text": "From Frequency to Meaning: Vector Space Models of Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs, and to provide pointers into the literature for those who are less familiar with the field."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 49
                            }
                        ],
                        "text": ", 2012), extending Restricted Boltzmann machines (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6584289,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7c64258997838087c9ba4e87225627b015122b2",
            "isKey": false,
            "numCitedBy": 233,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep belief nets have been successful in modeling handwritten characters, but it has proved more difficult to apply them to real images. The problem lies in the restricted Boltzmann machine (RBM) which is used as a module for learning deep belief nets one layer at a time. The Gaussian-Binary RBMs that have been used to model real-valued data are not a good way to model the covariance structure of natural images. We propose a factored 3-way RBM that uses the states of its hidden units to represent abnormalities in the local covariance structure of an image. This provides a probabilistic framework for the widely used simple/complex cell architecture. Our model learns binary features that work very well for object recognition on the \u201ctiny images\u201d data set. Even better features are obtained by then using standard binary RBM\u2019s to learn a deeper model."
            },
            "slug": "Factored-3-Way-Restricted-Boltzmann-Machines-For-Ranzato-Krizhevsky",
            "title": {
                "fragments": [],
                "text": "Factored 3-Way Restricted Boltzmann Machines For Modeling Natural Images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A factored 3-way RBM is proposed that uses the states of its hidden units to represent abnormalities in the local covariance structure of an image to provide a probabilistic framework for the widely used simple/complex cell architecture."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 39
                            }
                        ],
                        "text": "It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 20
                            }
                        ],
                        "text": "The Stanford Parser (Klein and Manning, 2003) is used to parses all 10,662 sentences."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11495042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a600850ac0120cb09a0b7de7da80bb6a7a76de06",
            "isKey": false,
            "numCitedBy": 3370,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize."
            },
            "slug": "Accurate-Unlexicalized-Parsing-Klein-Manning",
            "title": {
                "fragments": [],
                "text": "Accurate Unlexicalized Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "It is demonstrated that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145539951"
                        ],
                        "name": "J. Pollack",
                        "slug": "J.-Pollack",
                        "structuredName": {
                            "firstName": "Jordan",
                            "lastName": "Pollack",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pollack"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 47
                            }
                        ],
                        "text": ", 2011b) and recursive auto-associate memories (Pollack, 1990)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 770011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a835df43fdc2f79126319f6fa033bb42147c6f6",
            "isKey": false,
            "numCitedBy": 948,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recursive-Distributed-Representations-Pollack",
            "title": {
                "fragments": [],
                "text": "Recursive Distributed Representations"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 132
                            }
                        ],
                        "text": "The idea to relate inputs through three way interactions, parameterized by a tensor have been proposed for relation classification (Sutskever et al., 2009; Jenatton et al., 2012), extending Restricted Boltzmann machines (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 131
                            }
                        ],
                        "text": "The idea to relate inputs through three way interactions, parameterized by a tensor have been proposed for relation classification (Sutskever et al., 2009; Jenatton et al., 2012), extending Restricted Boltzmann machines (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14474967,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81bbe42e3ec09c28b8864956148e58f4cb5aa860",
            "isKey": false,
            "numCitedBy": 258,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of learning probabilistic models for complex relational structures between various types of objects. A model can help us \"understand\" a dataset of relational facts in at least two ways, by finding interpretable structure in the data, and by supporting predictions, or inferences about whether particular unobserved relations are likely to be true. Often there is a tradeoff between these two aims: cluster-based models yield more easily interpretable representations, while factorization-based approaches have given better predictive performance on large data sets. We introduce the Bayesian Clustered Tensor Factorization (BCTF) model, which embeds a factorized representation of relations in a nonparametric Bayesian clustering framework. Inference is fully Bayesian but scales well to large data sets. The model simultaneously discovers interpretable clusters and yields predictive performance that matches or beats previous probabilistic models for relational data."
            },
            "slug": "Modelling-Relational-Data-using-Bayesian-Clustered-Sutskever-Salakhutdinov",
            "title": {
                "fragments": [],
                "text": "Modelling Relational Data using Bayesian Clustered Tensor Factorization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The Bayesian Clustered Tensor Factorization (BCTF) model is introduced, which embeds a factorized representation of relations in a nonparametric Bayesian clustering framework that is fully Bayesian but scales well to large data sets."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114187676"
                        ],
                        "name": "Alexis Kalokerinos",
                        "slug": "Alexis-Kalokerinos",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Kalokerinos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexis Kalokerinos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 158
                            }
                        ],
                        "text": "For instance, \u2018The movie was terrible\u2019 is negative but the \u2018The movie was not terrible\u2019 says only that it was less bad than a terrible one, not that it was good (Horn, 1989; Israel, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 141956109,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "44a83a681461b6511f06bc68697edf56255252d5",
            "isKey": false,
            "numCitedBy": 493,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-natural-history-of-negation-Kalokerinos",
            "title": {
                "fragments": [],
                "text": "A natural history of negation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3300549"
                        ],
                        "name": "A. Merin",
                        "slug": "A.-Merin",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Merin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Merin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 143
                            }
                        ],
                        "text": "The conjunction is interpreted as an argument for the second conjunct, with the first functioning concessively (Lakoff, 1971; Blakemore, 1989; Merin, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "junction is interpreted as an argument for the second conjunct, with the first functioning concessively (Lakoff, 1971; Blakemore, 1989; Merin, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16856475,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "6957a4e222d0a2b357e4e095f7e1ebe0dfa5a3e4",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "I propose to treat natural language semantics as a branch of pragmatics, identified in the way of C.S. Peirce, F.P. Ramsey, and R. Carnap as decision-theory. The notion of relevance plays a key role. It is explicated traditionally, distinguished from a recent homophone, and applied in its natural framework of issue-based communication. Empirical emphasis is on implicature and presupposition. Several theorems are stated and made use of. Items analyzed include \u2018or\u2019, \u2018not\u2019, \u2018but\u2019, \u2018even\u2019, and \u2018also\u2019. I conclude on parts of mind. This paper submits an approach to meaning, with a focus on broadly nontruth-conditional aspects of natural language. Semantics is treated as a branch of pragmatics, identified as decision-theory in the way of C.S. Peirce, F.P. Ramsey, and of Rudolf Carnap in his later work. A key theoretical notion, distinguishable from, but intelligibly related to, information is the positive or negative relevance of a proposition or sentence to another. It is explicated in the probabilistic way familiar from Carnap and traditional in the philosophies of science and rational action. This makes it a representation of local epistemic context-change potential that is directional in a precisely specifiable sense and naturally related to utterers\u2019 instrumental intentions. Relevance so defined is proposed as an explicans for Oswald Ducrot\u2019s insightful \u2018valeur argumentative\u2019. In view of possible confusion among some students of language, it is contrasted with a more recent and idiosyncratic pretender to the appellation, due to Dan Sperber and Deirdre Wilson. The latter proposal turns out, at best, to paraphrase H.P. Grice\u2019s non-directional concepts of \u2018informativeness\u2019 and \u2018perspicuity\u2019. (More informative designations are suggested for it, and for the eponymous linguistic doctrine emanating from parts of CNRS Paris and of UC London.)"
            },
            "slug": "Information,-relevance,-and-social-decisionmaking:-Merin",
            "title": {
                "fragments": [],
                "text": "Information, relevance, and social decisionmaking: some principles and results of decision-theoretic semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An approach to meaning is submitted, with a focus on broadly nontruth-conditional aspects of natural language, and a key theoretical notion, distinguishable from, but intelligibly related to, information is the positive or negative relevance of a proposition or sentence to another."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7544770,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71dd4d477ca17b4db3b270d25225822ff3a41fac",
            "isKey": false,
            "numCitedBy": 364,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Mapping-Part-Whole-Hierarchies-into-Connectionist-Hinton",
            "title": {
                "fragments": [],
                "text": "Mapping Part-Whole Hierarchies into Connectionist Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728073"
                        ],
                        "name": "Hao Wang",
                        "slug": "Hao-Wang",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2913858"
                        ],
                        "name": "Dogan Can",
                        "slug": "Dogan-Can",
                        "structuredName": {
                            "firstName": "Dogan",
                            "lastName": "Can",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dogan Can"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764265"
                        ],
                        "name": "Ebrahim (Abe) Kazemzadeh",
                        "slug": "Ebrahim-(Abe)-Kazemzadeh",
                        "structuredName": {
                            "firstName": "Ebrahim (Abe)",
                            "lastName": "Kazemzadeh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ebrahim (Abe) Kazemzadeh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47709855"
                        ],
                        "name": "F. Bar",
                        "slug": "F.-Bar",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Bar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145254843"
                        ],
                        "name": "Shrikanth S. Narayanan",
                        "slug": "Shrikanth-S.-Narayanan",
                        "structuredName": {
                            "firstName": "Shrikanth",
                            "lastName": "Narayanan",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shrikanth S. Narayanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 148
                            }
                        ],
                        "text": "While there are several datasets with document and chunk labels available, there is a need to better capture sentiment from short comments, such as Twitter data, which provide less overall signal per document."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 125
                            }
                        ],
                        "text": "For the more difficult multiclass case including a neutral class, accuracy is often below 60% for short messages on Twitter (Wang et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2091504,
            "fieldsOfStudy": [
                "Political Science"
            ],
            "id": "f798edd26edbc5faa4a9b61c2a269b2662d861da",
            "isKey": false,
            "numCitedBy": 548,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a system for real-time analysis of public sentiment toward presidential candidates in the 2012 U.S. election as expressed on Twitter, a micro-blogging service. Twitter has become a central site where people express their opinions and views on political parties and candidates. Emerging events or news are often followed almost instantly by a burst in Twitter volume, providing a unique opportunity to gauge the relation between expressed public sentiment and electoral events. In addition, sentiment analysis can help explore how these events affect public opinion. While traditional content analysis takes days or weeks to complete, the system demonstrated here analyzes sentiment in the entire Twitter traffic about the election, delivering results instantly and continuously. It offers the public, the media, politicians and scholars a new and timely perspective on the dynamics of the electoral process and public opinion."
            },
            "slug": "A-System-for-Real-time-Twitter-Sentiment-Analysis-Wang-Can",
            "title": {
                "fragments": [],
                "text": "A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A system for real-time analysis of public sentiment toward presidential candidates in the 2012 U.S. election as expressed on Twitter, a micro-blogging service, offers a new and timely perspective on the dynamics of the electoral process and public opinion."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2236517"
                        ],
                        "name": "L. Polanyi",
                        "slug": "L.-Polanyi",
                        "structuredName": {
                            "firstName": "Livia",
                            "lastName": "Polanyi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Polanyi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809816"
                        ],
                        "name": "A. Zaenen",
                        "slug": "A.-Zaenen",
                        "structuredName": {
                            "firstName": "Annie",
                            "lastName": "Zaenen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zaenen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 146
                            }
                        ],
                        "text": "Several works have explored sentiment compositionality through careful engineering of features or polarity shifting rules on syntactic structures (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Rentoumi et al., 2010; Nakagawa et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8975187,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "a8ddcabf410b5691cd73880df2ee4b83a9799a5d",
            "isKey": false,
            "numCitedBy": 599,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In addition to describing facts and events, texts often communicate information about the attitude of the writer or various participants towards material being described. The most salient clues about attitude are provided by the lexical choice of the writer but, as discussed below, the organization of the text also contributes information relevant to assessing attitude. We argue that the current work in this area that concentrates mainly on the negative or positive attitude communicated by individual terms (Edmonds and Hirst, 2002; Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2002; Wiebe et al., 2001) is incomplete and often gives the wrong results when implemented directly. We then describe how the base attitudinal valence of a lexical item is modified by lexical and discourse context and propose a simple, \u201cproof of concept\u201d implementation for some contextual shifters."
            },
            "slug": "Contextual-Valence-Shifters-Polanyi-Zaenen",
            "title": {
                "fragments": [],
                "text": "Contextual Valence Shifters"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is argued that the current work in this area that concentrates mainly on the negative or positive attitude communicated by individual terms is incomplete and often gives the wrong results when implemented directly and proposes a simple, \u201cproof of concept\u201d implementation for some contextual shifters."
            },
            "venue": {
                "fragments": [],
                "text": "Computing Attitude and Affect in Text"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734693"
                        ],
                        "name": "John C. Duchi",
                        "slug": "John-C.-Duchi",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Duchi",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Duchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34840427"
                        ],
                        "name": "Elad Hazan",
                        "slug": "Elad-Hazan",
                        "structuredName": {
                            "firstName": "Elad",
                            "lastName": "Hazan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elad Hazan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 154
                            }
                        ],
                        "text": "For all models, we use the dev set and crossvalidate over regularization of the weights, word vector size as well as learning rate and minibatch size for AdaGrad."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 48
                            }
                        ],
                        "text": "For this nonconvex optimization we use AdaGrad (Duchi et al., 2011) which converges in less than 3 hours to a local optimum."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 538820,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "413c1142de9d91804d6d11c67ff3fed59c9fc279",
            "isKey": false,
            "numCitedBy": 8025,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms."
            },
            "slug": "Adaptive-Subgradient-Methods-for-Online-Learning-Duchi-Hazan",
            "title": {
                "fragments": [],
                "text": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work describes and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal functions that can be chosen in hindsight."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2859277"
                        ],
                        "name": "T. Plate",
                        "slug": "T.-Plate",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Plate",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Plate"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 64
                            }
                        ],
                        "text": "Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Giesbrecht, 2010) have not been experimentally validated on larger corpora."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2352281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "564427596799f7967c91934966cd3c6bd31cb06d",
            "isKey": false,
            "numCitedBy": 542,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Associative memories are conventionally used to represent data with very simple structure: sets of pairs of vectors. This paper describes a method for representing more complex compositional structure in distributed representations. The method uses circular convolution to associate items, which are represented by vectors. Arbitrary variable bindings, short sequences of various lengths, simple frame-like structures, and reduced representations can be represented in a fixed width vector. These representations are items in their own right and can be used in constructing compositional structures. The noisy reconstructions extracted from convolution memories can be cleaned up by using a separate associative memory that has good reconstructive properties."
            },
            "slug": "Holographic-reduced-representations-Plate",
            "title": {
                "fragments": [],
                "text": "Holographic reduced representations"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper describes a method for representing more complex compositional structure in distributed representations that uses circular convolution to associate items, which are represented by vectors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69839035"
                        ],
                        "name": "D. Blakemore",
                        "slug": "D.-Blakemore",
                        "structuredName": {
                            "firstName": "Diane",
                            "lastName": "Blakemore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blakemore"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 111
                            }
                        ],
                        "text": "The conjunction is interpreted as an argument for the second conjunct, with the first functioning concessively (Lakoff, 1971; Blakemore, 1989; Merin, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62602518,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "0f717bda5ae5a37bbafb842492cdb90aae61cf51",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Denial-and-contrast:-A-relevance-theoretic-analysis-Blakemore",
            "title": {
                "fragments": [],
                "text": "Denial and contrast: A relevance theoretic analysis ofbut"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Experiments Borui(Athena) Ye July"
            },
            "venue": {
                "fragments": [],
                "text": "Experiments Borui(Athena) Ye July"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Model Analysis: High Level Negation (Cont.) Set 2: Negating Negative Sentences Experiments Borui(Athena) Ye July 15"
            },
            "venue": {
                "fragments": [],
                "text": "Model Analysis: High Level Negation (Cont.) Set 2: Negating Negative Sentences Experiments Borui(Athena) Ye July 15"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 111
                            }
                        ],
                        "text": "The conjunction is interpreted as an argument for the second conjunct, with the first functioning concessively (Lakoff, 1971; Blakemore, 1989; Merin, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "If\u2019s, and\u2019s, and but\u2019s about conjunction"
            },
            "venue": {
                "fragments": [],
                "text": "Charles J. Fillmore and D. Terence Langendoen, editors, Studies in Linguistic Semantics, pages 114\u2013149. Holt, Rinehart, and Winston, New York."
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Model Analysis: High Level Negation Set 1: Negating Positive Sentences Experiments Borui(Athena) Ye July 15"
            },
            "venue": {
                "fragments": [],
                "text": "Model Analysis: High Level Negation Set 1: Negating Positive Sentences Experiments Borui(Athena) Ye July 15"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2012. Improving Word Representations via Global Context and Multiple Word Prototypes. In ACL"
            },
            "venue": {
                "fragments": [],
                "text": "2012. Improving Word Representations via Global Context and Multiple Word Prototypes. In ACL"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 161
                            }
                        ],
                        "text": "For instance, \u2018The movie was terrible\u2019 is negative but the \u2018The movie was not terrible\u2019 says only that it was less bad than a terrible one, not that it was good (Horn, 1989; Israel, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A natural history of negation, volume 960"
            },
            "venue": {
                "fragments": [],
                "text": "University of Chicago Press Chicago."
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recursive Neural Models BoruiAthena) Ye July"
            },
            "venue": {
                "fragments": [],
                "text": "Recursive Neural Models BoruiAthena) Ye July"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 108
                            }
                        ],
                        "text": "Apart from the abovementioned work, most approaches in sentiment analysis use bag of words representations (Pang and Lee, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "RNN: Recursive Neural Network[ 10] where f = tanh"
            },
            "venue": {
                "fragments": [],
                "text": "RNN: Recursive Neural Network[ 10] where f = tanh"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Athena) Ye July 15, 2015 15 / 26 Sentiment Classification 1 Exp. 1: Fine-grained Sentiment For All Phrases 2 Exp. 2: Full Sentence Binary Sentiment Experiments Borui(Athena) Ye July 15"
            },
            "venue": {
                "fragments": [],
                "text": "Athena) Ye July 15, 2015 15 / 26 Sentiment Classification 1 Exp. 1: Fine-grained Sentiment For All Phrases 2 Exp. 2: Full Sentence Binary Sentiment Experiments Borui(Athena) Ye July 15"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2011a. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. In ICML"
            },
            "venue": {
                "fragments": [],
                "text": "2011a. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. In ICML"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 126
                            }
                        ],
                        "text": "The conjunction is interpreted as an argument for the second conjunct, with the first functioning concessively (Lakoff, 1971; Blakemore, 1989; Merin, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Denial and contrast: A relevance theoretic analysis of 'but'. Linguistics and Philosophy"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 37,
            "methodology": 10
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 55,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/Recursive-Deep-Models-for-Semantic-Compositionality-Socher-Perelygin/687bac2d3320083eb4530bf18bb8f8f721477600?sort=total-citations"
}