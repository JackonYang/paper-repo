{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054613265"
                        ],
                        "name": "D. Esser",
                        "slug": "D.-Esser",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Esser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Esser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145654957"
                        ],
                        "name": "Daniel Schuster",
                        "slug": "Daniel-Schuster",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2956206"
                        ],
                        "name": "Klemens Muthmann",
                        "slug": "Klemens-Muthmann",
                        "structuredName": {
                            "firstName": "Klemens",
                            "lastName": "Muthmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Klemens Muthmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113458452"
                        ],
                        "name": "Michael Berger",
                        "slug": "Michael-Berger",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Berger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145417024"
                        ],
                        "name": "A. Schill",
                        "slug": "A.-Schill",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Schill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "More details about this can be found in [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "We have also presented a similar approach in the past [10]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 1897279,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41db13459f0344a0ca63342302484c4f6e044376",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Archiving official written documents such as invoices, reminders and account statements in business and private area gets more and more important. Creating appropriate index entries for document archives like sender's name, creation date or document number is a tedious manual work. We present a novel approach to handle automatic indexing of documents based on generic positional extraction of index terms. For this purpose we apply the knowledge of document templates stored in a common full text search index to find index positions that were successfully extracted in the past."
            },
            "slug": "Automatic-indexing-of-scanned-documents:-a-approach-Esser-Schuster",
            "title": {
                "fragments": [],
                "text": "Automatic indexing of scanned documents: a layout-based approach"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents a novel approach to handle automatic indexing of documents based on generic positional extraction of index terms based on document templates stored in a common full text search index to find index positions that were successfully extracted in the past."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071723932"
                        ],
                        "name": "M. Hanke",
                        "slug": "M.-Hanke",
                        "structuredName": {
                            "firstName": "Marcel",
                            "lastName": "Hanke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hanke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2956206"
                        ],
                        "name": "Klemens Muthmann",
                        "slug": "Klemens-Muthmann",
                        "structuredName": {
                            "firstName": "Klemens",
                            "lastName": "Muthmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Klemens Muthmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145654957"
                        ],
                        "name": "Daniel Schuster",
                        "slug": "Daniel-Schuster",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145417024"
                        ],
                        "name": "A. Schill",
                        "slug": "A.-Schill",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Schill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2652700"
                        ],
                        "name": "Kamil Aliyev",
                        "slug": "Kamil-Aliyev",
                        "structuredName": {
                            "firstName": "Kamil",
                            "lastName": "Aliyev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kamil Aliyev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113458452"
                        ],
                        "name": "Michael Berger",
                        "slug": "Michael-Berger",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Berger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Berger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "Replacement strategies for training documents and more details about feedback handling in Intellix can be found in [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15632114,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bef26d1f261755e2b39002dee910703d740f13e4",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatically processing production documents requires document type detection as well as data capture to find appropriate index data from a post-OCR representation of the document. While current learning-based methods perform quite well due to many similar documents created with the same template, their machine learning models require intense training and are hard to update frequently. We provide a method for continuously incorporating user feedback in a layout-based extraction process taking care of both immediate learning as well as limiting the size of the model. The method is evaluated on a tagged corpus of more than 5,000 business documents. It allows not only continuous re-training of the model thus adapting it to new document templates, but also starting from scratch with an empty model requiring less than 10% of the corpus as training documents to reach an accuracy measure of more than 80%."
            },
            "slug": "Continuous-User-Feedback-Learning-for-Data-Capture-Hanke-Muthmann",
            "title": {
                "fragments": [],
                "text": "Continuous User Feedback Learning for Data Capture from Business Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work provides a method for continuously incorporating user feedback in a layout-based extraction process taking care of both immediate learning as well as limiting the size of the model."
            },
            "venue": {
                "fragments": [],
                "text": "HAIS"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3104424"
                        ],
                        "name": "Toshiko Matsumoto",
                        "slug": "Toshiko-Matsumoto",
                        "structuredName": {
                            "firstName": "Toshiko",
                            "lastName": "Matsumoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshiko Matsumoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21162206"
                        ],
                        "name": "Mitsuharu Oba",
                        "slug": "Mitsuharu-Oba",
                        "structuredName": {
                            "firstName": "Mitsuharu",
                            "lastName": "Oba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mitsuharu Oba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117415"
                        ],
                        "name": "T. Onoyama",
                        "slug": "T.-Onoyama",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Onoyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Onoyama"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[11] also considers layout characteristics like bold or italic characters for rule generation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26219244,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1352c18eef5be854dd56a8d52c1702a9dfdc1831",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an algorithm for automatically generating metadata extraction parameters. It first enumerates candidates on the basis of metadata occurrence in training documents, and then examines these candidates to avoid side effects and to maximize effectiveness. This two-stage approach enables both avoidance of exponential explosion of computation and detailed optimization. An experiment on Japanese business documents shows that an automatically generated parameter enables metadata extraction as accurately as a manually adjusted one."
            },
            "slug": "Sample-Based-Collection-and-Adjustment-Algorithm-of-Matsumoto-Oba",
            "title": {
                "fragments": [],
                "text": "Sample-Based Collection and Adjustment Algorithm for Metadata Extraction Parameter of Flexible Format Document"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "An algorithm for automatically generating metadata extraction parameters is proposed that first enumerates candidates on the basis of metadata occurrence in training documents, and then examines these candidates to avoid side effects and to maximize effectiveness."
            },
            "venue": {
                "fragments": [],
                "text": "ICAISC"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145654957"
                        ],
                        "name": "Daniel Schuster",
                        "slug": "Daniel-Schuster",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071723932"
                        ],
                        "name": "M. Hanke",
                        "slug": "M.-Hanke",
                        "structuredName": {
                            "firstName": "Marcel",
                            "lastName": "Hanke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hanke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2956206"
                        ],
                        "name": "Klemens Muthmann",
                        "slug": "Klemens-Muthmann",
                        "structuredName": {
                            "firstName": "Klemens",
                            "lastName": "Muthmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Klemens Muthmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054613265"
                        ],
                        "name": "D. Esser",
                        "slug": "D.-Esser",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Esser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Esser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18654464,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d583346c870f72644b98b2a1bba6f0ccde39502",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Current systems for automatic extraction of index terms from business documents either take a rule-based or training-based approach. As both approaches have their advantages and disadvantages it seems natural to combine both methods to get the best of both worlds. We present a combination method with the steps selection, normalization, and combination based on comparable scores produced during extraction. Furthermore, novel evaluation metrics are developed to support the assessment of each step in an existing extraction system. Our methods were evaluated on an example extraction system with three individual extractors and a corpus of 12,000 scanned business documents."
            },
            "slug": "Rule-based-versus-training-based-extraction-of-from-Schuster-Hanke",
            "title": {
                "fragments": [],
                "text": "Rule-based versus training-based extraction of index terms from business documents: how to combine the results"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents a combination method with the steps selection, normalization, and combination based on comparable scores produced during extraction, to get the best of both worlds for automatic extraction of index terms from business documents."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065856167"
                        ],
                        "name": "Gaurav Pandey",
                        "slug": "Gaurav-Pandey",
                        "structuredName": {
                            "firstName": "Gaurav",
                            "lastName": "Pandey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gaurav Pandey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113012823"
                        ],
                        "name": "Rakshit Daga",
                        "slug": "Rakshit-Daga",
                        "structuredName": {
                            "firstName": "Rakshit",
                            "lastName": "Daga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rakshit Daga"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[14] make use of similarity measures for detecting relevant information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15872769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d79dd895912a36670b3477645f361e2fdd73185b",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient management of text data is a major concern of business organizations. In this direction, we propose a novel approach to extract structured knowledge from large corpora of unstructured business documents. This knowledge is represented in the form of object instances, which are common ways of organizing the available information about entities, and are modeled here using document templates. The approach itself is based on the observation that a significant fraction of these documents are created using the cut-copy-paste method, and thus, it is important to factor this observation into business document analysis projects. Correspondingly, our approach solves the problem of object instance extraction in two steps, namely similarity search and then extraction of object instances from the selected documents. Early qualitative results on a couple of carefully selected document corpora indicate the effective applicability of the approach for solving an important component of the efficient text management problem."
            },
            "slug": "On-Extracting-Structured-Knowledge-from-Business-Pandey-Daga",
            "title": {
                "fragments": [],
                "text": "On Extracting Structured Knowledge from Unstructured Business Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "Early qualitative results on a couple of carefully selected document corpora indicate the effective applicability of the novel approach to extract structured knowledge from large corpora of unstructured business documents for solving an important component of the efficient text management problem."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153874783"
                        ],
                        "name": "S. Adali",
                        "slug": "S.-Adali",
                        "structuredName": {
                            "firstName": "Serif",
                            "lastName": "Adali",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Adali"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50691081"
                        ],
                        "name": "A. Sonmez",
                        "slug": "A.-Sonmez",
                        "structuredName": {
                            "firstName": "Ahmet",
                            "lastName": "Sonmez",
                            "middleNames": [
                                "Coskun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sonmez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739667"
                        ],
                        "name": "Mehmet Gokturk",
                        "slug": "Mehmet-Gokturk",
                        "structuredName": {
                            "firstName": "Mehmet",
                            "lastName": "Gokturk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mehmet Gokturk"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "Rules range from simple regular expressions [4] over the identification of relational key words [5] to more complex patterns for identifying addresses [6] or table contents [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 21019505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03dd8cc0c452ef0c9d2f2cee2e65b5954947746a",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper covers the first research activity in the field of automatic processing of business documents in Turkish. In contrast to traditional information extraction systems which process input text as a linear sequence of words and focus on semantic aspects, proposed approach doesn't ignore document layout information and benefits hints provided by layout analysis. In addition, approach not only checks relations of entities across document for verifying its integrity, but also verifies extracted information against real word data (e.g. customer database). This rule-based approach uses a morphological analyzer for Turkish, a lexicon integrated domain ontology, a document layout analyzer, an extraction ontology and a template mining module. Based on extraction ontology, conceptual sentence analysis increases portability which requires only domain concepts when compared to information extraction systems that rely on large set of linguistic patterns."
            },
            "slug": "An-Integrated-Architecture-for-Processing-Business-Adali-Sonmez",
            "title": {
                "fragments": [],
                "text": "An Integrated Architecture for Processing Business Documents in Turkish"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This paper covers the first research activity in the field of automatic processing of business documents in Turkish and proposes a rule-based approach based on extraction ontology which increases portability which requires only domain concepts when compared to information extraction systems that rely on large set of linguistic patterns."
            },
            "venue": {
                "fragments": [],
                "text": "CICLing"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763321"
                        ],
                        "name": "E. Saund",
                        "slug": "E.-Saund",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Saund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Saund"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 124
                            }
                        ],
                        "text": "Thus the research task to be solved is nearly configurationfree information extraction or functional role labeling (compare [3]) of a few commonly used fields in document archiving."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7736275,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7323009ab2cf70fc6e809489825d427faa90e60",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The Field of Document Recognition is bipolar. On one end lies the excellent work of academic institutions engaging in original research on scientifically interesting topics. On the other end lies the document recognition industry which services needs for high-volume data capture for transaction and back-office applications. These realms seldom meet, yet the need is great to address technical hurdles for practical problems using modern approaches from the Document Recognition, Computer Vision, and Machine Learning disciplines. We reflect on three categories of problems we have encountered which are both scientifically challenging and of high practical value. These are Doctype Classification, Functional Role Labeling, and Document Sets. Doctype Classification asks, \"What is the type of page I am looking at?\" Functional Role Labeling asks, \"What is the status of text and graphical elements in a model of document structure?\" Document Sets asks, \"How are pages and their contents related to one another?\" Each of these has ad hoc engineering approaches that provide 40-80% solutions, and each of them begs for a deeply grounded formulation both to provide understanding and to attain the remaining 20-60% of practical value. The practical need is not purely technical but also depends on the user experience in application setup and configuration, and in collection and groundtruthing of sample documents. The challenge therefore extends beyond the science behind document image recognition and into user interface and user experience design."
            },
            "slug": "Scientific-challenges-underlying-production-Saund",
            "title": {
                "fragments": [],
                "text": "Scientific challenges underlying production document processing"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The challenge therefore extends beyond the science behind document image recognition and into user interface and user experience design."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778757"
                        ],
                        "name": "Eric Medvet",
                        "slug": "Eric-Medvet",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Medvet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Medvet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39295971"
                        ],
                        "name": "Alberto Bartoli",
                        "slug": "Alberto-Bartoli",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Bartoli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alberto Bartoli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770449"
                        ],
                        "name": "G. Davanzo",
                        "slug": "G.-Davanzo",
                        "structuredName": {
                            "firstName": "Giorgio",
                            "lastName": "Davanzo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Davanzo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "Matsumoto et al. [11] also considers layout characteristics like bold or italic characters for rule generation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 22854184,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f4e0912f9eb1fd2f87646906dfbf2deabc4b875",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an approach for information extraction for multi-page printed document understanding. The approach is designed for scenarios in which the set of possible document classes, i.e., documents sharing similar content and layout, is large and may evolve over time. Describing a new class is a very simple task: the operator merely provides a few samples and then, by means of a GUI, clicks on the OCR-generated blocks of a document containing the information to be extracted. Our approach is based on probability: we derived a general form for the probability that a sequence of blocks contains the searched information. We estimate the parameters for a new class by applying the maximum likelihood method to the samples of the class. All these parameters depend only on block properties that can be extracted automatically from the operator actions on the GUI. Processing a document of a given class consists in finding the sequence of blocks, which maximizes the corresponding probability for that class. We evaluated experimentally our proposal using 807 multi-page printed documents of different domains (invoices, patents, data-sheets), obtaining very good results\u2014e.g., a success rate often greater than 90% even for classes with just two samples."
            },
            "slug": "A-probabilistic-approach-to-printed-document-Medvet-Bartoli",
            "title": {
                "fragments": [],
                "text": "A probabilistic approach to printed document understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "The approach is designed for scenarios in which the set of possible document classes, i.e., documents sharing similar content and layout, is large and may evolve over time and derived a general form for the probability that a sequence of blocks contains the searched information."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal on Document Analysis and Recognition (IJDAR)"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40265241"
                        ],
                        "name": "Bill Janssen",
                        "slug": "Bill-Janssen",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Janssen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bill Janssen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763321"
                        ],
                        "name": "E. Saund",
                        "slug": "E.-Saund",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Saund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Saund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734223"
                        ],
                        "name": "E. Bier",
                        "slug": "E.-Bier",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Bier",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144445858"
                        ],
                        "name": "Patricia Wall",
                        "slug": "Patricia-Wall",
                        "structuredName": {
                            "firstName": "Patricia",
                            "lastName": "Wall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patricia Wall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7776699"
                        ],
                        "name": "M. Sprague",
                        "slug": "M.-Sprague",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Sprague",
                            "middleNames": [
                                "Ann"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sprague"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 96
                            }
                        ],
                        "text": "Rules range from simple regular expressions [4] over the identification of relational key words [5] to more complex patterns for identifying addresses [6] or table contents [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5985110,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d36185006382dbd3c7fa0079c0c90508c4cbd24e",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The Receipts2Go system is about the world of one-page documents: cash register receipts, book covers, cereal boxes, price tags, train tickets, fire extinguisher tags. In that world, we're exploring techniques for extracting accurate information from documents for which we have no layout descriptions -- indeed no initial idea of what the document's genre is -- using photos taken with cell phone cameras by users who aren't skilled document capture technicians. This paper outlines the system and reports on some initial results, including the algorithms we've found useful for cleaning up those document images, and the techniques used to extract and organize relevant information from thousands of similar-but-different page layouts."
            },
            "slug": "Receipts2Go:-the-big-world-of-small-documents-Janssen-Saund",
            "title": {
                "fragments": [],
                "text": "Receipts2Go: the big world of small documents"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The Receipts2Go system is outlined, including the algorithms the authors've found useful for cleaning up those document images, and the techniques used to extract and organize relevant information from thousands of similar-but-different page layouts."
            },
            "venue": {
                "fragments": [],
                "text": "DocEng '12"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470204"
                        ],
                        "name": "Stefan Klink",
                        "slug": "Stefan-Klink",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Klink",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Klink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3320897"
                        ],
                        "name": "T. Kieninger",
                        "slug": "T.-Kieninger",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Kieninger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kieninger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[8] provide a more sophisticated solution."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5660854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d500522804a04f9a622c74dc9124c2923b31a4c8",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Document image processing is a crucial process in the office automation and begins from the \u2019OCR\u2019 phase with difficulty of the document \u2019analysis\u2019 and \u2019understanding\u2019. This paper presents a hybrid and comprehensive approach to document structure analysis. Hybrid in the sense, that it makes use of layout (geometrical) as well as textual features of a given document. These features are the base for potential conditions which in turn are used to express fuzzy matched rules of an underlying rule base."
            },
            "slug": "Document-Structure-Analysis-Based-on-Layout-and-Klink-Dengel",
            "title": {
                "fragments": [],
                "text": "Document Structure Analysis Based on Layout and Textual Features"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper presents a hybrid and comprehensive approach to document structure analysis that makes use of layout as well as textual features of a given document to express fuzzy matched rules of an underlying rule base."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38128725"
                        ],
                        "name": "Florian Deckert",
                        "slug": "Florian-Deckert",
                        "structuredName": {
                            "firstName": "Florian",
                            "lastName": "Deckert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florian Deckert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080543878"
                        ],
                        "name": "Benjamin Seidler",
                        "slug": "Benjamin-Seidler",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Seidler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Seidler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2198984"
                        ],
                        "name": "Markus Ebbecke",
                        "slug": "Markus-Ebbecke",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Ebbecke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Markus Ebbecke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2336967"
                        ],
                        "name": "M. Gillmann",
                        "slug": "M.-Gillmann",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Gillmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gillmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 35
                            }
                        ],
                        "text": "Commercial solutions like smartFix [1] and the Open Text Capture Center [2] automatically process scanned invoices, medical documents or insurances on a regular basis."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 113
                            }
                        ],
                        "text": "It is important to mention that we do not want to extract more specific information like full table content (see [1]) or handwritten information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 19969469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40ce25812de4896b29f23c4fd386e9410677b696",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The analysis of table structures and the retrieval of table contents is widely agreed to be a difficult challenge in the area of document analysis systems. Instead of extracting the layout of tables, we are interested in understanding their content. In this paper, we present and discuss the smartFIX approach to table recognition and content extraction. Rather than relying on layout features only, we recognize tables by taking into account the presence and semantics of data entities that we expect to find contained in a table. The relationship of a document, including a table, to a specific business process aids in shaping helpful knowledge and expectations about the table's content. smartFIX is a commercial document analysis system complying with the complete bandwidth of industrial requirements. Therefore, smartFIX must locate the tables and extract its business process relevant information with high reliability."
            },
            "slug": "Table-Content-Understanding-in-SmartFIX-Deckert-Seidler",
            "title": {
                "fragments": [],
                "text": "Table Content Understanding in SmartFIX"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents and discusses the smartFIX approach to table recognition and content extraction, which recognizes tables by taking into account the presence and semantics of data entities that the authors expect to find contained in a table."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152180024"
                        ],
                        "name": "Jianying Hu",
                        "slug": "Jianying-Hu",
                        "structuredName": {
                            "firstName": "Jianying",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianying Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32225756"
                        ],
                        "name": "R. Kashi",
                        "slug": "R.-Kashi",
                        "structuredName": {
                            "firstName": "Ramanujan",
                            "lastName": "Kashi",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2859740"
                        ],
                        "name": "G. Wilfong",
                        "slug": "G.-Wilfong",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Wilfong",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wilfong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 171
                            }
                        ],
                        "text": "For our Template Detection component we tested different techniques based on graphical and layout features like the simplified document layout representation described in [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14535685,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8efc295011e191d4830d0f066e0c8d06a9631b15",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes features and methods for document image comparison and classification at the spatial layout level. The methods are useful for visual similarity based document retrieval as well as fast algorithms for initial document type classification without OCR. A novel feature set called interval encoding is introduced to capture elements of spatial layout. This feature set encodes region layout information in fixed-length vectors by capturing structural characteristics of the image. These fixed-length vectors are then compared to each other through a Manhattan distance computation for fast page layout comparison. The paper describes experiments and results to rank-order a set of document pages in terms of their layout similarity to a test document. We also demonstrate the usefulness of the features derived from interval coding in a hidden Markov model based page layout classification system that is trainable and extendible. The methods described in the paper can be used in various document retrieval tasks including visual similarity based retrieval, categorization and information extraction."
            },
            "slug": "Comparison-and-Classification-of-Documents-Based-on-Hu-Kashi",
            "title": {
                "fragments": [],
                "text": "Comparison and Classification of Documents Based on Layout Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The usefulness of the features derived from interval coding in a hidden Markov model based page layout classification system that is trainable and extendible are demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "Information Retrieval"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768769"
                        ],
                        "name": "F. Cesarini",
                        "slug": "F.-Cesarini",
                        "structuredName": {
                            "firstName": "Francesca",
                            "lastName": "Cesarini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Cesarini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751770"
                        ],
                        "name": "E. Francesconi",
                        "slug": "E.-Francesconi",
                        "structuredName": {
                            "firstName": "Enrico",
                            "lastName": "Francesconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Francesconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2540925"
                        ],
                        "name": "G. Soda",
                        "slug": "G.-Soda",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Soda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Soda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[9] identify similar documents within the training set and generate positional rules that can in turn be used for extraction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10190924,
            "fieldsOfStudy": [
                "Computer Science",
                "Sociology"
            ],
            "id": "b0175f2602256e71c4f40b96b6997422db38f39c",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.In this paper a system for processing documents that can be grouped into classes is illustrated. We have considered invoices as a case-study. The system is divided into three phases: document analysis, classification, and understanding. We illustrate the analysis and understanding phases. The system is based on knowledge constructed by means of a learning procedure. The experimental results demonstrate the reliability of our document analysis and understanding procedures. They also present evidence that it is possible to use a small learning set of invoices to obtain reliable knowledge for the understanding phase."
            },
            "slug": "Analysis-and-understanding-of-multi-class-invoices-Cesarini-Francesconi",
            "title": {
                "fragments": [],
                "text": "Analysis and understanding of multi-class invoices"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The experimental results demonstrate the reliability of the document analysis and understanding procedures and present evidence that it is possible to use a small learning set of invoices to obtain reliable knowledge for the understanding phase."
            },
            "venue": {
                "fragments": [],
                "text": "Document Analysis and Recognition"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2448729"
                        ],
                        "name": "Y. Bela\u00efd",
                        "slug": "Y.-Bela\u00efd",
                        "structuredName": {
                            "firstName": "Yolande",
                            "lastName": "Bela\u00efd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Bela\u00efd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2128453"
                        ],
                        "name": "A. Bela\u00efd",
                        "slug": "A.-Bela\u00efd",
                        "structuredName": {
                            "firstName": "Abdel",
                            "lastName": "Bela\u00efd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bela\u00efd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 506926,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39270756426b01e1478d3b6669a16055d4cfdb39",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper a morphological tagging approach for document image invoice analysis is described. Tokens close by their morphology and confirmed in their location within different similar contexts make apparent some parts of speech representative of the structure elements. This bottom up approach avoids the use of an priori knowledge provided that there are redundant and frequent contexts in the text. The approach is applied on the invoice body text roughly recognized by OCR and automatically segmented. The method makes possible the detection of the invoice articles and their different fields. The regularity of the article composition and its redundancy in the invoice is a good help for its structure. The recognition rate of 276 invoices and 1704 articles, is over than 91.02% for articles and 92.56% for fields."
            },
            "slug": "Morphological-Tagging-Approach-in-Document-Analysis-Bela\u00efd-Bela\u00efd",
            "title": {
                "fragments": [],
                "text": "Morphological Tagging Approach in Document Analysis of Invoices"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A morphological tagging approach for document image invoice analysis is described that avoids the use of an priori knowledge provided that there are redundant and frequent contexts in the text."
            },
            "venue": {
                "fragments": [],
                "text": "ICPR"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073807191"
                        ],
                        "name": "Intelligent Systems Lab",
                        "slug": "Intelligent-Systems-Lab",
                        "structuredName": {
                            "firstName": "Intelligent",
                            "lastName": "Lab",
                            "middleNames": [
                                "Systems"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Intelligent Systems Lab"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[13] use a probabilistic model to detect index fields and table content."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 21527326,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7910316af75e01fb634947a6b078b16526c3fe44",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a method for automatically parsing images of tables, focusing in particular on \u2018simple\u2019 matrix-like tables with rectilinear layout. Such tables account for over 50% of tables in business documents. The main novelty of the proposed method is that it combines intrinsic properties of table cells with properties of cell separators, as well as table rows, columns, and layout, in a single global objective function. This is in contrast to previous methods which focused on either separators alone or intrinsic cell properties alone. Our method uses a variety of perceptual cues, such as alignment and saliency, to characterize these properties. Candidate parses are evaluated by comparing their likelihoods, and the parse that optimizes the likelihood is selected. The proposed approach deals successfully with a wide variety of tables, as illustrated on a dataset of over 1,000 images. Keywords-table parsing, document analysis"
            },
            "slug": "Parsing-tables-by-probabilistic-modeling-of-cues-Lab",
            "title": {
                "fragments": [],
                "text": "Parsing tables by probabilistic modeling of perceptual cues"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A method for automatically parsing images of tables, focusing in particular on \u2018simple\u2019 matrix-like tables with rectilinear layout, that combines intrinsic properties of table cells with properties of cell separators, as well as table rows, columns, and layout, in a single global objective function."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34996245"
                        ],
                        "name": "B. Klein",
                        "slug": "B.-Klein",
                        "structuredName": {
                            "firstName": "Bertin",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2582412"
                        ],
                        "name": "S. Agne",
                        "slug": "S.-Agne",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Agne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Agne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4839405,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "88ffc69930d8ee8c4d762ac90978d96d71023bf7",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Companies order, receive, and pay for goods. Hence they continually receive and process invoices. For the most part these are printed on paper and are dealt with manually, so that each invoice after receipt involves processing costs of about 9 Euro on average. Often, human searching and typing of data into computer forms is required to transfer the information from paper into the computer, e.g. into ERP-systems, like SAP, that many companies run. This article presents the main results of our 300-page market survey of 11 suppliers of invoice reading systems (\\(\\mathcal{I}\\)-\\(\\mathcal{R}\\)-\\(\\mathcal{S}\\)), which automate the transfer of invoice data to ERP-systems. For the scientific \\(\\mathcal{I}\\)-\\(\\mathcal{R}\\)-\\(\\mathcal{S}\\) community we hope to provide the service of a better visibility of our discipline to potential investors and users."
            },
            "slug": "Results-of-a-Study-on-Invoice-Reading-Systems-in-Klein-Agne",
            "title": {
                "fragments": [],
                "text": "Results of a Study on Invoice-Reading Systems in Germany"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This article presents the main results of the 300-page market survey of 11 suppliers of invoice reading systems (I-R-S), which automate the transfer of invoice data to ERP-systems, to provide a better visibility of the discipline to potential investors and users."
            },
            "venue": {
                "fragments": [],
                "text": "Document Analysis Systems"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2867844"
                        ],
                        "name": "Nancy A. Chinchor",
                        "slug": "Nancy-A.-Chinchor",
                        "structuredName": {
                            "firstName": "Nancy",
                            "lastName": "Chinchor",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nancy A. Chinchor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2620384"
                        ],
                        "name": "B. Sundheim",
                        "slug": "B.-Sundheim",
                        "structuredName": {
                            "firstName": "Beth",
                            "lastName": "Sundheim",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sundheim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 153
                            }
                        ],
                        "text": "We present numbers for micro and macro averaged precision, recall and F1-measure per field as well as combined scores according to Chinchor and Sundheim [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14454803,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f898e821bbf4157d857dc512a85f49610638f1aa",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The metrics used for the Fifth Message Understanding Conference (MUC-5) evaluation are a major update to those used for MUC-4 in 1992. The official MUC-5 metrics express error rates while the official MUC-4 metrics express performance in terms of recall and precision (used for MUC-5 only as \"unofficial\" metrics). This paper discusses the current metrics and the reasons for their adoption."
            },
            "slug": "MUC-5-evaluation-metrics-Chinchor-Sundheim",
            "title": {
                "fragments": [],
                "text": "MUC-5 evaluation metrics"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "The metrics used for the Fifth Message Understanding Conference (MUC-5) evaluation are a major update to those used for MUC-4 in 1992, and the reasons for their adoption are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "MUC"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2767978"
                        ],
                        "name": "Christophe Salperwyck",
                        "slug": "Christophe-Salperwyck",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Salperwyck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christophe Salperwyck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145359583"
                        ],
                        "name": "V. Lemaire",
                        "slug": "V.-Lemaire",
                        "structuredName": {
                            "firstName": "V.",
                            "lastName": "Lemaire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lemaire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 167
                            }
                        ],
                        "text": "While this problem has not yet been deeply discussed in the information extraction domain, the influence of training on the performance of classifiers is well studied [15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15462931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16a39cf3e6c82278b49ee6bbf7f21d668ec64af8",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning algorithms proved their ability to deal with large amount of data. Most of the statistical approaches use defined size learning sets and produce static models. However in specific situations: active or incremental learning, the learning task starts with only very few data. In that case, looking for algorithms able to produce models with only few examples becomes necessary. The literature's classifiers are generally evaluated with criterion such as: accuracy, ability to order data (ranking)... But this classifiers' taxonomy can dramatically change if the focus is on the ability to learn with just few examples. To our knowledge, just few studies were performed on this problem. The study presented in this paper aims to study a larger panel of both algorithms (9 different kinds) and data sets (17 UCI bases)."
            },
            "slug": "Learning-with-few-examples:-An-empirical-study-on-Salperwyck-Lemaire",
            "title": {
                "fragments": [],
                "text": "Learning with few examples: An empirical study on leading classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The study presented in this paper aims to study a larger panel of both algorithms (9 different kinds) and data sets (17 UCI bases) to study the ability of algorithms to produce models with only few examples."
            },
            "venue": {
                "fragments": [],
                "text": "The 2011 International Joint Conference on Neural Networks"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "89868233"
                        ],
                        "name": "P. Jaccard",
                        "slug": "P.-Jaccard",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Jaccard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Jaccard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This matrix is used for new instances of a user model space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 135345056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c315fcd49f07a0b8e4a381c07332c4f8bea4faab",
            "isKey": false,
            "numCitedBy": 2770,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Etude-comparative-de-la-distribution-florale-dans-Jaccard",
            "title": {
                "fragments": [],
                "text": "Etude comparative de la distribution florale dans une portion des Alpes et des Jura"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1901
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 151
                            }
                        ],
                        "text": "Rules range from simple regular expressions [4] over the identification of relational key words [5] to more complex patterns for identifying addresses [6] or table contents [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Results of a study on invoicereading systems in germany."
            },
            "venue": {
                "fragments": [],
                "text": "Document Analysis Systems,"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "Lucene [16] is used as a kNN Classifier where each known training document is indexed and then ranked according to the words included in the input document."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Apache Software Foundation Apache Lucene"
            },
            "venue": {
                "fragments": [],
                "text": "The Apache Software Foundation Apache Lucene"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Table content understanding in smartfix , \u201d in Document Analysis and Recognition ( ICDAR ) , 2011 International Conference on , 2011 . [ 2 ] Opentext , \u201c Opentext capture center"
            },
            "venue": {
                "fragments": [],
                "text": "Document Recognition and Retrieval XVIII ( DRR )"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "Lucene [16] is used as a kNN Classifier where each known training document is indexed and then ranked according to the words included in the input document."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Apache Lucene"
            },
            "venue": {
                "fragments": [],
                "text": "http://lucene.apache.org, 2013."
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 56
                            }
                        ],
                        "text": "Commercial solutions like smartFix [1] and the Open Text Capture Center [2] automatically process scanned invoices, medical documents or insurances on a regular basis."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Opentext capture center http://www.opentext.de/3/global/products/products-capture-and- imaging/products-opentext-capture-center.htm"
            },
            "venue": {
                "fragments": [],
                "text": "Opentext capture center http://www.opentext.de/3/global/products/products-capture-and- imaging/products-opentext-capture-center.htm"
            },
            "year": 2012
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 13,
            "methodology": 6,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 24,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Intellix-End-User-Trained-Information-Extraction-Schuster-Muthmann/87dee6b4a5fcbab541b45a967c24030df6cee29b?sort=total-citations"
}