{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34192119"
                        ],
                        "name": "Liang-Chieh Chen",
                        "slug": "Liang-Chieh-Chen",
                        "structuredName": {
                            "firstName": "Liang-Chieh",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang-Chieh Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776496"
                        ],
                        "name": "G. Papandreou",
                        "slug": "G.-Papandreou",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Papandreou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Papandreou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2010660"
                        ],
                        "name": "Iasonas Kokkinos",
                        "slug": "Iasonas-Kokkinos",
                        "structuredName": {
                            "firstName": "Iasonas",
                            "lastName": "Kokkinos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iasonas Kokkinos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702318"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 190
                            }
                        ],
                        "text": "Instead, the field of view of filters can also be effectively enlarged and multi-scale contexts can be captured by combining multi-resolution features [11] or by applying atrous convolution [2, 3, 4, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 72
                            }
                        ],
                        "text": "Standard convolution is a special case of atrous convolution with r = 1 [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3429309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cab372bc3824780cce20d9dd1c22d4df39ed081a",
            "isKey": false,
            "numCitedBy": 9408,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or \u2018atrous\u00a0convolution\u2019, as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous\u00a0spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed \u201cDeepLab\u201d system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online."
            },
            "slug": "DeepLab:-Semantic-Image-Segmentation-with-Deep-and-Chen-Papandreou",
            "title": {
                "fragments": [],
                "text": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This work addresses the task of semantic image segmentation with Deep Learning and proposes atrous\u00a0spatial pyramid pooling (ASPP), which is proposed to robustly segment objects at multiple scales, and improves the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145654957"
                        ],
                        "name": "Daniel Schuster",
                        "slug": "Daniel-Schuster",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2956206"
                        ],
                        "name": "Klemens Muthmann",
                        "slug": "Klemens-Muthmann",
                        "structuredName": {
                            "firstName": "Klemens",
                            "lastName": "Muthmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Klemens Muthmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054613265"
                        ],
                        "name": "D. Esser",
                        "slug": "D.-Esser",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Esser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Esser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145417024"
                        ],
                        "name": "A. Schill",
                        "slug": "A.-Schill",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Schill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113458452"
                        ],
                        "name": "Michael Berger",
                        "slug": "Michael-Berger",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Berger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2906706"
                        ],
                        "name": "C. Weidling",
                        "slug": "C.-Weidling",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Weidling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Weidling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2652700"
                        ],
                        "name": "Kamil Aliyev",
                        "slug": "Kamil-Aliyev",
                        "structuredName": {
                            "firstName": "Kamil",
                            "lastName": "Aliyev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kamil Aliyev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1942133"
                        ],
                        "name": "A. Hofmeier",
                        "slug": "A.-Hofmeier",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Hofmeier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hofmeier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 61
                            }
                        ],
                        "text": "Several rule-based invoice analysis systems were proposed in [10, 6, 8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "Intellix by DocuWare requires the a template being annotated with relevant fields [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18934920,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87dee6b4a5fcbab541b45a967c24030df6cee29b",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic information extraction from scanned business documents is especially valuable in the application domain of document archiving. But current systems for automated document processing still require a lot of configuration work that can only be done by experienced users or administrators. We present an approach for information extraction which purely builds on end-user provided training examples and intentionally omits efficient known extraction techniques like rule based extraction that require intense training and/or information extraction expertise. Our evaluation on a large corpus of business documents shows competitive results of above 85% F1-measure on 10 commonly used fields like document type, sender, receiver and date. The system is deployed and used inside the commercial document management system DocuWare."
            },
            "slug": "Intellix-End-User-Trained-Information-Extraction-Schuster-Muthmann",
            "title": {
                "fragments": [],
                "text": "Intellix -- End-User Trained Information Extraction for Document Archiving"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work presents an approach for information extraction which purely builds on end-user provided training examples and intentionally omits efficient known extraction techniques like rule based extraction that require intense training and/or information extraction expertise."
            },
            "venue": {
                "fragments": [],
                "text": "2013 12th International Conference on Document Analysis and Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34192119"
                        ],
                        "name": "Liang-Chieh Chen",
                        "slug": "Liang-Chieh-Chen",
                        "structuredName": {
                            "firstName": "Liang-Chieh",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang-Chieh Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776496"
                        ],
                        "name": "G. Papandreou",
                        "slug": "G.-Papandreou",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Papandreou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Papandreou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3302320"
                        ],
                        "name": "Florian Schroff",
                        "slug": "Florian-Schroff",
                        "structuredName": {
                            "firstName": "Florian",
                            "lastName": "Schroff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florian Schroff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2595180"
                        ],
                        "name": "Hartwig Adam",
                        "slug": "Hartwig-Adam",
                        "structuredName": {
                            "firstName": "Hartwig",
                            "lastName": "Adam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hartwig Adam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 190
                            }
                        ],
                        "text": "Instead, the field of view of filters can also be effectively enlarged and multi-scale contexts can be captured by combining multi-resolution features [11] or by applying atrous convolution [2, 3, 4, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 22655199,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee4a012a4b12d11d7ab8c0e79c61e807927a163c",
            "isKey": false,
            "numCitedBy": 3882,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark."
            },
            "slug": "Rethinking-Atrous-Convolution-for-Semantic-Image-Chen-Papandreou",
            "title": {
                "fragments": [],
                "text": "Rethinking Atrous Convolution for Semantic Image Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed `DeepLabv3' system significantly improves over the previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34192119"
                        ],
                        "name": "Liang-Chieh Chen",
                        "slug": "Liang-Chieh-Chen",
                        "structuredName": {
                            "firstName": "Liang-Chieh",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang-Chieh Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844940337"
                        ],
                        "name": "Yukun Zhu",
                        "slug": "Yukun-Zhu",
                        "structuredName": {
                            "firstName": "Yukun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yukun Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776496"
                        ],
                        "name": "G. Papandreou",
                        "slug": "G.-Papandreou",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Papandreou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Papandreou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3302320"
                        ],
                        "name": "Florian Schroff",
                        "slug": "Florian-Schroff",
                        "structuredName": {
                            "firstName": "Florian",
                            "lastName": "Schroff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florian Schroff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2595180"
                        ],
                        "name": "Hartwig Adam",
                        "slug": "Hartwig-Adam",
                        "structuredName": {
                            "firstName": "Hartwig",
                            "lastName": "Adam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hartwig Adam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 190
                            }
                        ],
                        "text": "Instead, the field of view of filters can also be effectively enlarged and multi-scale contexts can be captured by combining multi-resolution features [11] or by applying atrous convolution [2, 3, 4, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3638670,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9217e28b2273eb3b26e4e9b7b498b4661e6e09f5",
            "isKey": false,
            "numCitedBy": 4914,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0\\% and 82.1\\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at \\url{this https URL}."
            },
            "slug": "Encoder-Decoder-with-Atrous-Separable-Convolution-Chen-Zhu",
            "title": {
                "fragments": [],
                "text": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries and applies the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067170682"
                        ],
                        "name": "Rasmus Berg Palm",
                        "slug": "Rasmus-Berg-Palm",
                        "structuredName": {
                            "firstName": "Rasmus",
                            "lastName": "Palm",
                            "middleNames": [
                                "Berg"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rasmus Berg Palm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724252"
                        ],
                        "name": "O. Winther",
                        "slug": "O.-Winther",
                        "structuredName": {
                            "firstName": "Ole",
                            "lastName": "Winther",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Winther"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805808"
                        ],
                        "name": "Florian Laws",
                        "slug": "Florian-Laws",
                        "structuredName": {
                            "firstName": "Florian",
                            "lastName": "Laws",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florian Laws"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 94
                            }
                        ],
                        "text": "We compare the performance of the proposed method with two state of the art methods CloudScan [9] and BERT [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 95
                            }
                        ],
                        "text": "A single model is trained on the dataset for all these 3 types of document receipts either for CloudScan, BERT, or CUTIE."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "CloudScan is a learning based invoice analysis system [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 167
                            }
                        ],
                        "text": "Since the previous learning based methods treat the key information extraction problem as a NER problem, applying BERT can achieve a better result than the bi-LSTM in CloudScan."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 12
                            }
                        ],
                        "text": "Compared to CloudScan, CUTIE-A and CUTIEB outperforms in every single test case."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 84
                            }
                        ],
                        "text": "We compare the performance of the proposed method with two state of the art methods CloudScan [9] and BERT for NER [7]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 48
                            }
                        ],
                        "text": "Aiming to not rely on invoice layout templates, CloudScan trains a model that could be generalized to unseen invoice layouts, where a model is trained either using Long Short Term Memory (LSTM) or Logistic Regression (LR) with expert designed rules as training features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "CloudScan[9] 82 / - 64 / - 60 / BERT[7]+pp 110M 87."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 129
                            }
                        ],
                        "text": "For comparison, the Cloud Scan model for SROIE is trained from scratch but with several expert designed features as described in [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 65
                            }
                        ],
                        "text": "Moreover, the RNN-based classifier, bi-directional LSTM model in CloudScan, has limited ability to learn the relationship among distant words."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "CloudScan is a work attempting to extract key information with learning based models [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30210556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01a50b9662a59070c4ff53873453d8854c15ade1",
            "isKey": true,
            "numCitedBy": 56,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present CloudScan; an invoice analysis system that requires zero configuration or upfront annotation. In contrast to previous work, CloudScan does not rely on templates of invoice layout, instead it learns a single global model of invoices that naturally generalizes to unseen invoice layouts. The model is trained using data automatically extracted from end-user provided feedback. This automatic training data extraction removes the requirement for users to annotate the data precisely. We describe a recurrent neural network model that can capture long range context and compare it to a baseline logistic regression model corresponding to the current CloudScan production system. We train and evaluate the system on 8 important fields using a dataset of 326,471 invoices. The recurrent neural network and baseline model achieve 0.891 and 0.887 average F1 scores respectively on seen invoice layouts. For the harder task of unseen invoice layouts, the recurrent neural network model outperforms the baseline with 0.840 average F1 compared to 0.788."
            },
            "slug": "CloudScan-A-Configuration-Free-Invoice-Analysis-Palm-Winther",
            "title": {
                "fragments": [],
                "text": "CloudScan - A Configuration-Free Invoice Analysis System Using Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A recurrent neural network model that can capture long range context and compare it to a baseline logistic regression model corresponding to the current CloudScan production system are described."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34192119"
                        ],
                        "name": "Liang-Chieh Chen",
                        "slug": "Liang-Chieh-Chen",
                        "structuredName": {
                            "firstName": "Liang-Chieh",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang-Chieh Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776496"
                        ],
                        "name": "G. Papandreou",
                        "slug": "G.-Papandreou",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Papandreou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Papandreou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2010660"
                        ],
                        "name": "Iasonas Kokkinos",
                        "slug": "Iasonas-Kokkinos",
                        "structuredName": {
                            "firstName": "Iasonas",
                            "lastName": "Kokkinos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iasonas Kokkinos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702318"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 190
                            }
                        ],
                        "text": "Instead, the field of view of filters can also be effectively enlarged and multi-scale contexts can be captured by combining multi-resolution features [11] or by applying atrous convolution [2, 3, 4, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1996665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39ad6c911f3351a3b390130a6e4265355b4d593b",
            "isKey": false,
            "numCitedBy": 3345,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called \"semantic image segmentation\"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our \"DeepLab\" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU."
            },
            "slug": "Semantic-Image-Segmentation-with-Deep-Convolutional-Chen-Papandreou",
            "title": {
                "fragments": [],
                "text": "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF)."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39172707"
                        ],
                        "name": "Jacob Devlin",
                        "slug": "Jacob-Devlin",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Devlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Devlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744179"
                        ],
                        "name": "Ming-Wei Chang",
                        "slug": "Ming-Wei-Chang",
                        "structuredName": {
                            "firstName": "Ming-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Wei Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 316,
                                "start": 309
                            }
                        ],
                        "text": "The CUTIE allows for simultaneously looking into both semantical information and spatial information of the texts in the scanned document image and can reach a new state of the art result for key information extraction, which outperforms BERT model but without demanding of pretraining on a huge text dataset [7, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 107
                            }
                        ],
                        "text": "We compare the performance of the proposed method with two state of the art methods CloudScan [9] and BERT [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 200
                            }
                        ],
                        "text": ", the Bi-directional Encoder Representations from Transformers (BERT) model is a recently proposed state of the art method and has achieved great success in a wide of range of NLP tasks including NER [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 291
                            }
                        ],
                        "text": "Bidirectional Encoder Representations from Transformers (BERT) is a recently proposed model that is pre-trained on a huge dataset and can be fine-tuned for a specific task, including Named Entity Recognition (NER), which outperforms most of the state of the art results in several NLP tasks [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "CloudScan[9] 82 / - 64 / - 60 / BERT[7]+pp 110M 87."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 139
                            }
                        ],
                        "text": "The BERT model for SROIE is transform learned with the Google released base model that is pre-trained on a huge dataset with 3, 300M words [7, 1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52967399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "isKey": true,
            "numCitedBy": 33744,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
            },
            "slug": "BERT:-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang",
            "title": {
                "fragments": [],
                "text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A new language representation model, BERT, designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054613265"
                        ],
                        "name": "D. Esser",
                        "slug": "D.-Esser",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Esser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Esser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145654957"
                        ],
                        "name": "Daniel Schuster",
                        "slug": "Daniel-Schuster",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2956206"
                        ],
                        "name": "Klemens Muthmann",
                        "slug": "Klemens-Muthmann",
                        "structuredName": {
                            "firstName": "Klemens",
                            "lastName": "Muthmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Klemens Muthmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113458452"
                        ],
                        "name": "Michael Berger",
                        "slug": "Michael-Berger",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Berger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145417024"
                        ],
                        "name": "A. Schill",
                        "slug": "A.-Schill",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Schill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 61
                            }
                        ],
                        "text": "Several rule-based invoice analysis systems were proposed in [10, 6, 8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "uses a dataset of fixed key information positions for each template [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1897279,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41db13459f0344a0ca63342302484c4f6e044376",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Archiving official written documents such as invoices, reminders and account statements in business and private area gets more and more important. Creating appropriate index entries for document archives like sender's name, creation date or document number is a tedious manual work. We present a novel approach to handle automatic indexing of documents based on generic positional extraction of index terms. For this purpose we apply the knowledge of document templates stored in a common full text search index to find index positions that were successfully extracted in the past."
            },
            "slug": "Automatic-indexing-of-scanned-documents:-a-approach-Esser-Schuster",
            "title": {
                "fragments": [],
                "text": "Automatic indexing of scanned documents: a layout-based approach"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents a novel approach to handle automatic indexing of documents based on generic positional extraction of index terms based on document templates stored in a common full text search index to find index positions that were successfully extracted in the past."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143819050"
                        ],
                        "name": "Ke Sun",
                        "slug": "Ke-Sun",
                        "structuredName": {
                            "firstName": "Ke",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ke Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144025674"
                        ],
                        "name": "Bin Xiao",
                        "slug": "Bin-Xiao",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bin Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718355"
                        ],
                        "name": "Dong Liu",
                        "slug": "Dong-Liu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688516"
                        ],
                        "name": "Jingdong Wang",
                        "slug": "Jingdong-Wang",
                        "structuredName": {
                            "firstName": "Jingdong",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jingdong Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "Similar to HRNet proposed in [11], a high-resolution network without striding is employed as the backbone network and several high-tolow resolution sub networks are gradually added and connected to the backbone major network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "Instead, the field of view of filters can also be effectively enlarged and multi-scale contexts can be captured by combining multi-resolution features [11] or by applying atrous convolution [2, 3, 4, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 247
                            }
                        ],
                        "text": "However, spatial resolution is reduced in the encoding process and the decoding process exploits only high resolution but low-level features to recover the spatial resolution, the consecutive striding encoding process decimates detail information [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 67856425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6303bac53abd725c3b458190a6abe389a4a1e72d",
            "isKey": false,
            "numCitedBy": 1280,
            "numCiting": 105,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process. We start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks in parallel. We conduct repeated multi-scale fusions such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich high-resolution representations. As a result, the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. In addition, we show the superiority of our network in pose tracking on the PoseTrack dataset. The code and models have been publicly available at https://github.com/leoxiaobin/deep-high-resolution-net.pytorch."
            },
            "slug": "Deep-High-Resolution-Representation-Learning-for-Sun-Xiao",
            "title": {
                "fragments": [],
                "text": "Deep High-Resolution Representation Learning for Human Pose Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper proposes a network that maintains high-resolution representations through the whole process of human pose estimation and empirically demonstrates the effectiveness of the network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34996245"
                        ],
                        "name": "B. Klein",
                        "slug": "B.-Klein",
                        "structuredName": {
                            "firstName": "Bertin",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Klein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 61
                            }
                        ],
                        "text": "Several rule-based invoice analysis systems were proposed in [10, 6, 8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 77
                            }
                        ],
                        "text": "SmartFix employs specifically designed configuration rules for each template [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7257921,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb41491a9c4bcd09d83a29dc10304c9a8b602920",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Although the internet offers a wide-spread platform for information interchange, day-to-day work in large companies still means the processing of tens of thousands of printed documents every day. This paper presents the system smartFIX which is a document analysis and understanding system developed by the DFKI spin-off INSIDERS. It permits the processing of documents ranging from fixed format forms to unstructured letters of any format. Apart from the architecture, the main components and system characteristics, we also show some results when applying smartFIX to medical bills and prescriptions."
            },
            "slug": "smartFIX:-A-Requirements-Driven-System-for-Document-Dengel-Klein",
            "title": {
                "fragments": [],
                "text": "smartFIX: A Requirements-Driven System for Document Analysis and Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The system smartFIX which is a document analysis and understanding system developed by the DFKI spin-off INSIDERS permits the processing of documents ranging from fixed format forms to unstructured letters of any format."
            },
            "venue": {
                "fragments": [],
                "text": "Document Analysis Systems"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 316,
                                "start": 309
                            }
                        ],
                        "text": "The CUTIE allows for simultaneously looking into both semantical information and spatial information of the texts in the scanned document image and can reach a new state of the art result for key information extraction, which outperforms BERT model but without demanding of pretraining on a huge text dataset [7, 12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Attention is all you"
            },
            "venue": {
                "fragments": [],
                "text": "need. CoRR,"
            },
            "year": 2017
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 7,
            "methodology": 7
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 11,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/CUTIE:-Learning-to-Understand-Documents-with-Text-Zhao-Wu/024aea24cc4d0949d6fe1482591d2429a9d8bbeb?sort=total-citations"
}