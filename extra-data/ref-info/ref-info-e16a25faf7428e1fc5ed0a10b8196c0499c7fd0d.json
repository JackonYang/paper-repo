{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34631309"
                        ],
                        "name": "R. Cowell",
                        "slug": "R.-Cowell",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Cowell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cowell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144845491"
                        ],
                        "name": "A. Dawid",
                        "slug": "A.-Dawid",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Dawid",
                            "middleNames": [
                                "Philip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dawid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050895"
                        ],
                        "name": "S. Lauritzen",
                        "slug": "S.-Lauritzen",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Lauritzen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lauritzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48616434"
                        ],
                        "name": "D. Spiegelhalter",
                        "slug": "D.-Spiegelhalter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spiegelhalter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spiegelhalter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 205
                            }
                        ],
                        "text": "However, by providing general machinery for manipulating joint probability distributions and, in particular, by making hierarchical latent variable models easy to represent and manipulate, the formalism has proved to be particularly popular within the Bayesian paradigm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 22
                            }
                        ],
                        "text": "In (10) we have expressed the cumulant generating function variationally\u2014as the solution to an optimization problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 63
                            }
                        ],
                        "text": "Our presentation will be brief; for a fuller presentation, see Cowell et al. (1999) and Jordan (1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32379969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3f680d9c248d396bb3920fcf98ce9a7ba0a9c88",
            "isKey": true,
            "numCitedBy": 1475,
            "numCiting": 314,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nProbabilistic expert systems are graphical networks that support the modelling of uncertainty and decisions in large complex domains, while retaining ease of calculation. Building on original research by the authors over a number of years, this book gives a thorough and rigorous mathematical treatment of the underlying ideas, structures, and algorithms, emphasizing those cases in which exact answers are obtainable. The book will be of interest to researchers and graduate students in artificial intelligence who desire an understanding of the mathematical and statistical basis of probabilistic expert systems, and to students and research workers in statistics wanting an introduction to this fascinating and rapidly developing field. The careful attention to detail will also make this work an important reference source for all those involved in the theory and applications of probabilistic expert systems."
            },
            "slug": "Probabilistic-Networks-and-Expert-Systems-Cowell-Dawid",
            "title": {
                "fragments": [],
                "text": "Probabilistic Networks and Expert Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This book gives a thorough and rigorous mathematical treatment of the underlying ideas, structures, and algorithms of probabilistic expert systems, emphasizing those cases in which exact answers are obtainable."
            },
            "venue": {
                "fragments": [],
                "text": "Information Science and Statistics"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773821"
                        ],
                        "name": "Matthew J. Beal",
                        "slug": "Matthew-J.-Beal",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Beal",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew J. Beal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 51
                            }
                        ],
                        "text": "See, for example, Attias (2000) and Ghahramani and Beal (2001) for recent papers devoted to full Bayesian applications of variational inference."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1011289,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5f22d558526017f130c75ca35fe0a737c01aaee",
            "isKey": false,
            "numCitedBy": 342,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set."
            },
            "slug": "Propagation-Algorithms-for-Variational-Bayesian-Ghahramani-Beal",
            "title": {
                "fragments": [],
                "text": "Propagation Algorithms for Variational Bayesian Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is demonstrated how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2089883"
                        ],
                        "name": "W. Gilks",
                        "slug": "W.-Gilks",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Gilks",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gilks"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107218881"
                        ],
                        "name": "A. Thomas",
                        "slug": "A.-Thomas",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Thomas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Thomas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48616434"
                        ],
                        "name": "D. Spiegelhalter",
                        "slug": "D.-Spiegelhalter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spiegelhalter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spiegelhalter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 41819931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a1e584f9a91472d6e15184f1648f57256216198",
            "isKey": false,
            "numCitedBy": 718,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Gibbs sampling has enormous potential for analysing complex data sets. However, routine use of Gibbs sampling has been hampered by the lack of general purpose software for its implementation. Until now all applications have involved writing one-off computer code in low or intermediate level languages such as C or Fortran. We describe some general purpose software that we are currently developing for implementing Gibbs sampling: BUGS (Bayesian inference using Gibbs sampling). The BUGS system comprises three components: first, a natural language for specifying complex models; second, an 'expert system' for deciding appropriate methods for obtaining samples required by the Gibbs sampler; third, a sampling module containing numerical routines to perform the sampling. S objects are used for data input and output. BUGS is written in Modula-2 and runs under both DOS and UNIX."
            },
            "slug": "A-Language-and-Program-for-Complex-Bayesian-Gilks-Thomas",
            "title": {
                "fragments": [],
                "text": "A Language and Program for Complex Bayesian Modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work describes some general purpose software that is currently developing for implementing Gibbs sampling: BUGS (Bayesian inference using Gibbs sampling), written in Modula-2 and runs under both DOS and UNIX."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786990"
                        ],
                        "name": "H. Attias",
                        "slug": "H.-Attias",
                        "structuredName": {
                            "firstName": "Hagai",
                            "lastName": "Attias",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Attias"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 18
                            }
                        ],
                        "text": "See, for example, Attias (2000) and Ghahramani and Beal (2001) for recent papers devoted to full Bayesian applications of variational inference."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 2
                            }
                        ],
                        "text": ", Attias (2000) and Ghahramani and Beal (2001) for recent papers devoted to full Bayesian applications of variational inference."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14399513,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c8ee0bc40d8158bba48df860622649a12cabd49",
            "isKey": false,
            "numCitedBy": 837,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel practical framework for Bayesian model averaging and model selection in probabilistic graphical models. Our approach approximates full posterior distributions over model parameters and structures, as well as latent variables, in an analytical manner. These posteriors fall out of a free-form optimization procedure, which naturally incorporates conjugate priors. Unlike in large sample approximations, the posteriors are generally non-Gaussian and no Hessian needs to be computed. Predictive quantities are obtained analytically. The resulting algorithm generalizes the standard Expectation Maximization algorithm, and its convergence is guaranteed. We demonstrate that this approach can be applied to a large class of models in several domains, including mixture models and source separation."
            },
            "slug": "A-Variational-Baysian-Framework-for-Graphical-Attias",
            "title": {
                "fragments": [],
                "text": "A Variational Baysian Framework for Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This paper presents a novel practical framework for Bayesian model averaging and model selection in probabilistic graphical models that approximates full posterior distributions over model parameters and structures, as well as latent variables, in an analytical manner."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 81
                            }
                        ],
                        "text": "A more serious departure is thecoupled hidden Markov modelshown in Figure 10(b) (Saul and Jordan, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14500325,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2dc4d6d7d55f9f0f1de53bb7f6816502f8f38892",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a statistical mechanical framework for the modeling of discrete time series. Maximum likelihood estimation is done via Boltzmann learning in one-dimensional networks with tied weights. We call these networks Boltzmann chains and show that they contain hidden Markov models (HMMs) as a special case. Our framework also motivates new architectures that address particular shortcomings of HMMs. We look at two such architectures: parallel chains that model feature sets with disparate time scales, and looped networks that model long-term dependencies between hidden states. For these networks, we show how to implement the Boltzmann learning rule exactly, in polynomial time, without resort to simulated or mean-field annealing. The necessary computations are done by exact decimation procedures from statistical mechanics."
            },
            "slug": "Boltzmann-Chains-and-Hidden-Markov-Models-Saul-Jordan",
            "title": {
                "fragments": [],
                "text": "Boltzmann Chains and Hidden Markov Models"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A statistical mechanical framework for the modeling of discrete time series is proposed, and maximum likelihood estimation is done via Boltzmann learning in one-dimensional networks with tied weights, which motivates new architectures that address particular shortcomings of HMMs."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47559480"
                        ],
                        "name": "C. S. Jensen",
                        "slug": "C.-S.-Jensen",
                        "structuredName": {
                            "firstName": "Claus",
                            "lastName": "Jensen",
                            "middleNames": [
                                "Skaanning"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Jensen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2775921"
                        ],
                        "name": "Uffe Kj\u00e6rulff",
                        "slug": "Uffe-Kj\u00e6rulff",
                        "structuredName": {
                            "firstName": "Uffe",
                            "lastName": "Kj\u00e6rulff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Uffe Kj\u00e6rulff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49504816"
                        ],
                        "name": "A. Kong",
                        "slug": "A.-Kong",
                        "structuredName": {
                            "firstName": "Augustine",
                            "lastName": "Kong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kong"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15392897,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1354660aba6bbcadb569f684d5a309ae57f58a79",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We introduce a methodology for performing approximate computations in very complex probabilistic systems (e.g. huge pedigrees). Our approach, called blocking Gibbs, combines exact local computations with Gibbs sampling in a way that complements the strengths of both. The methodology is illustrated on a real-world problem involving a heavily inbred pedigreee containing 20 000 individuals. We present results showing that blocking-Gibbs sampling converges much faster than plain Gibbs sampling for very complex problems."
            },
            "slug": "Blocking-Gibbs-sampling-in-very-large-probabilistic-Jensen-Kj\u00e6rulff",
            "title": {
                "fragments": [],
                "text": "Blocking Gibbs sampling in very large probabilistic expert systems"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Results are presented showing that blocking-Gibbs sampling converges much faster than plain Gibbs sampling for very complex problems."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Hum. Comput. Stud."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3198578"
                        ],
                        "name": "J. Yedidia",
                        "slug": "J.-Yedidia",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Yedidia",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yedidia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 180
                            }
                        ],
                        "text": "The Bethe approximation involves retaining only those consistency relations that arise from local neighborhood relationships in the graphical model, dropping all other constraints (Yedidia et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 134
                            }
                        ],
                        "text": "Thus the Bethe approximation is equivalent to applying the local message-passing scheme developed for trees to graphs that have loops (Yedidia et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 139
                            }
                        ],
                        "text": "Algorithms known as \u201ccluster variation methods\u201d have been proposed that extend the Bethe approximation to high-order clusters of variables (Yedidia et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15300022,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2799fd1254689eec52f86daf3668a5aac3ea943",
            "isKey": true,
            "numCitedBy": 1127,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Belief propagation (BP) was only supposed to work for treelike networks but works surprisingly well in many applications involving networks with loops, including turbo codes. However, there has been little understanding of the algorithm or the nature of the solutions it finds for general graphs. \n \nWe show that BP can only converge to a stationary point of an approximate free energy, known as the Bethe free energy in statistical physics. This result characterizes BP fixed-points and makes connections with variational approaches to approximate inference. \n \nMore importantly, our analysis lets us build on the progress made in statistical physics since Bethe's approximation was introduced in 1935. Kikuchi and others have shown how to construct more accurate free energy approximations, of which Bethe's approximation is the simplest. Exploiting the insights from our analysis, we derive generalized belief propagation (GBP) versions of these Kikuchi approximations. These new message passing algorithms can be significantly more accurate than ordinary BP, at an adjustable increase in complexity. We illustrate such a new GBP algorithm on a grid Markov network and show that it gives much more accurate marginal probabilities than those found using ordinary BP."
            },
            "slug": "Generalized-Belief-Propagation-Yedidia-Freeman",
            "title": {
                "fragments": [],
                "text": "Generalized Belief Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown that BP can only converge to a stationary point of an approximate free energy, known as the Bethe free energy in statistical physics, and generalized belief propagation (GBP) versions of these Kikuchi approximations are derived."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115806726"
                        ],
                        "name": "Alun Thomas",
                        "slug": "Alun-Thomas",
                        "structuredName": {
                            "firstName": "Alun",
                            "lastName": "Thomas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alun Thomas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774779"
                        ],
                        "name": "A. Gutin",
                        "slug": "A.-Gutin",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Gutin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gutin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3013000"
                        ],
                        "name": "V. Abkevich",
                        "slug": "V.-Abkevich",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Abkevich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Abkevich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49374535"
                        ],
                        "name": "A. Bansal",
                        "slug": "A.-Bansal",
                        "structuredName": {
                            "firstName": "Aruna",
                            "lastName": "Bansal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bansal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14911089,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47591eeaba44936184ee6a7fca30166083b560c2",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of multilocus linkage analysis is expressed as a graphical model, making explicit a previously implicit connection, and recent developments in the field are described in this context. A novel application of blocked Gibbs sampling for Bayesian networks is developed to generate inheritance matrices from an irreducible Markov chain. This is used as the basis for reconstruction of historical meiotic states and approximate calculation of the likelihood function for the location of an unmapped genetic trait. We believe this to be the only approach that currently makes fully informative multilocus linkage analysis possible on large extended pedigrees."
            },
            "slug": "Multilocus-linkage-analysis-by-blocked-Gibbs-Thomas-Gutin",
            "title": {
                "fragments": [],
                "text": "Multilocus linkage analysis by blocked Gibbs sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is believed that the only approach that currently makes fully informative multilocus linkage analysis possible on large extended pedigrees is the novel application of blocked Gibbs sampling for Bayesian networks."
            },
            "venue": {
                "fragments": [],
                "text": "Stat. Comput."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2129428282"
                        ],
                        "name": "Hoon Kim",
                        "slug": "Hoon-Kim",
                        "structuredName": {
                            "firstName": "Hoon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hoon Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 152
                            }
                        ],
                        "text": "Sampling algorithms such asimportance sampling and Markov chain Monte Carlo(MCMC) provide a general methodology for probabilistic inference (Liu, 2001; Robert and Casella, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33807429,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "17a09383cf450da8fe9830b9420914fa47707916",
            "isKey": false,
            "numCitedBy": 3242,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "dents. The first six chapters, the sixth added since the first edition, cover mixing processes, density and regression estimation for discrete time processes, density and regression estimation for continuous time processes, and the local time density estimator. The final chapter, also added since the first edition and the only one not devoted to theoretical results, reviews some aspects of implementation and gives examples. The book opens with a synopsis that defines the object of the study as being the construction of time series alternatives to the usual BoxJenkins SARIMA processes. Following that, it proceeds to highlight and summarize the main ideas of the book, beginning with definitions of kernel density and regression estimators and concluding with a brief list of some advantages of nonparametric over parametric time series methods. Specific advantages listed are that they are robust, that deseasonalization is not necessary, and that parametric convergence rates can, under some circumstances, be achieved. Having provided that overview, the book then proceeds in Chapter 1 to lay the theoretical groundwork for the analysis of a wide class of time series by a review of historical results for mixing processes. Results given include Berbee\u2019s and Bradley\u2019s lemmas for coupling, some results for covariances and joint densities including Rio\u2019s, Davydov\u2019s, and Billingsley\u2019s inequalities, some inequalities for partial sums including Hoeffding\u2019s and Bernstein\u2019s, and some limit theorems (laws of large numbers and central limit theorem) for strongly mixing processes. Chapters 2 and 3 cover the analysis of discrete time processes, Chapter 2 focusing on density estimation for sequences of correlated random variables and Chapter 3 on regression estimation and prediction. Topics include some specific kernels, optimal asymptotic quadratic error, uniform almost sure convergence for some kernels, asymptotic normality, and prediction for some stationary and nonstationary processes. These chapters are mainly review; although several results are from earlier papers by the author, they are not, by and large, new. Chapters 4 and 5 consider estimation for continuous time processes and are mainly new results. Their development is a broad parallel of the \u2019 development of Chapters 2 and 3, with Chapter 4 devoted to density estimation and Chapter 5 covering regression estimation and prediction. Topics and results include optimal and superoptimal asymptotic quadratic error including a minimax bound of Kutoyants (1997) and minimaxity of intermediate rates, optimal and superoptimal uniform convergence rates, asymptotic normality, irregular and admissible sampling, and the convergence rates of continuous-time nonparametric predictors. Some conditions are given under which a nonparametric predictor reaches a parametric convergence rate. Chapter 6 explores the use of local time for unbiased density estimation given a continuous time sample and consists, apart from one result, of new results. A definition is given of local time, followed by two existence criteria for local time, the first due to Geman and Horowitz (1973, 1980) and the second proven by the author. A density estimator based on local time is then defined and shown to be unbiased and consistent. Some results on convergence rates are then given, followed by asymptotic normality, a functional law of the iterated logarithm, and parametric rates for pointwise and uniform convergence. Chapter 7 is a brief summary of some practical aspects of nonparametric time series analysis. These fall into three areas-aspects of implementation. the comparison of nonparametric with parametric methods, and applied examples. The aspects of implementation addressed are variance stabilization via BoxCox transformation, methods for eliminating trend and seasonality, methods for choosing kernel bandwidth, and choosing a suitable order for predicting a Markov process in which the true order of the process is unknown. In comparing nonparametric and parametric methods of time series analysis, the book summarizes the results of Carbon and Delecroix (1993). who considered several simulated autoregressive moving average (ARMA) processes and some real business and engineering datasets, and the results of Rosa (1993), who considered ARMA models with and without generalized autoregressive conditional heteroscedasticity effects. An appendix gives 17 tables summarizing the results of the comparisons. Finally. some examples are given of applying nonparametric methods to finance and economic data. The value of this book is primarily in its theoretical development and, as such, it would be of more interest to researchers in statistical theory and"
            },
            "slug": "Monte-Carlo-Statistical-Methods-Kim",
            "title": {
                "fragments": [],
                "text": "Monte Carlo Statistical Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "In comparing nonparametric and parametric methods of time series analysis, the book summarizes the results of Carbon and Delecroix (1993), who considered several simulated autoregressive moving average (ARMA) processes."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9311320"
                        ],
                        "name": "E. Lander",
                        "slug": "E.-Lander",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Lander",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Lander"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145679216"
                        ],
                        "name": "P. Green",
                        "slug": "P.-Green",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Green",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Green"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 172
                            }
                        ],
                        "text": "Classical algorithms for inference on multilocus pedigrees are variants of the elimination algorithm on this fHMM and correspond to different choices of elimination order (Lander and Green, 1987; Elston and Stewart, 1971)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 19999685,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "9ba9b51edc9272c01d9b2aa0c514b84186c81441",
            "isKey": false,
            "numCitedBy": 1408,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Human genetic linkage maps are most accurately constructed by using information from many loci simultaneously. Traditional methods for such multilocus linkage analysis are computationally prohibitive in general, even with supercomputers. The problem has acquired practical importance because of the current international collaboration aimed at constructing a complete human linkage map of DNA markers through the study of three-generation pedigrees. We describe here several alternative algorithms for constructing human linkage maps given a specified gene order. One method allows maximum-likelihood multilocus linkage maps for dozens of DNA markers in such three-generation pedigrees to be constructed in minutes."
            },
            "slug": "Construction-of-multilocus-genetic-linkage-maps-in-Lander-Green",
            "title": {
                "fragments": [],
                "text": "Construction of multilocus genetic linkage maps in humans."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Several alternative algorithms for constructing human linkage maps given a specified gene order are described, one of which allows maximum-likelihood multilocus linkage maps for dozens of DNA markers in such three-generation pedigrees to be constructed in minutes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707788"
                        ],
                        "name": "P. P. Shenoy",
                        "slug": "P.-P.-Shenoy",
                        "structuredName": {
                            "firstName": "Prakash",
                            "lastName": "Shenoy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. P. Shenoy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500409"
                        ],
                        "name": "G. Shafer",
                        "slug": "G.-Shafer",
                        "structuredName": {
                            "firstName": "Glenn",
                            "lastName": "Shafer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Shafer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 34413970,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "13976cfe3be7e0873fa9814ac5c08cf27bfa91f1",
            "isKey": false,
            "numCitedBy": 675,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe an abstract framework and axioms under which exact local computation of marginals is possible. The primitive objects of the framework are variables and valuations. The primitive operators of the framework are combination and marginalization. These operate on valuations. We state three axioms for these operators and we derive the possibility of local computation from the axioms. Next, we describe a propagation scheme for computing marginals of a valuation when we have a factorization of the valuation on a hypertree. Finally we show how the problem of computing marginals of joint probability distributions and joint belief functions fits the general framework."
            },
            "slug": "Axioms-for-probability-and-belief-function-Shenoy-Shafer",
            "title": {
                "fragments": [],
                "text": "Axioms for probability and belief-function proagation"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper describes an abstract framework and axioms under which exact local computation of marginals is possible and shows how the problem of computing marginals of joint probability distributions and joint belief functions fits the general framework."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759366"
                        ],
                        "name": "S. Arnborg",
                        "slug": "S.-Arnborg",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Arnborg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Arnborg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806664"
                        ],
                        "name": "D. Corneil",
                        "slug": "D.-Corneil",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Corneil",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Corneil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753890"
                        ],
                        "name": "A. Proskurowski",
                        "slug": "A.-Proskurowski",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Proskurowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Proskurowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 101
                            }
                        ],
                        "text": "2 The problem of finding an elimination ordering that achieves the treewidth turns out to be NP-hard (Arnborg et al., 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123254044,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "656f4417b8651f472c6faa370823c307056ffa48",
            "isKey": false,
            "numCitedBy": 1267,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "A k-tree is a graph that can be reduced to the k-complete graph by a sequence of removals of a degree k vertex with completely connected neighbors. We address the problem of determining whether a graph is a partial graph of a k-tree. This problem is motivated by the existence of polynomial time algorithms for many combinatorial problems on graphs when the graph is constrained to be a partial k-tree for fixed k. These algorithms have practical applications in areas such as reliability, concurrent broadcasting and evaluation of queries in a relational database system. We determine the complexity status of two problems related to finding the smallest number k such that a given graph is a partial k-tree. First, the corresponding decision problem is NP-complete. Second, for a fixed (predetermined) value of k, we present an algorithm with polynomially bounded (but exponential in k) worst case time complexity. Previously, this problem had only been solved for $k = 1,2,3$."
            },
            "slug": "Complexity-of-finding-embeddings-in-a-k-tree-Arnborg-Corneil",
            "title": {
                "fragments": [],
                "text": "Complexity of finding embeddings in a k -tree"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work determines the complexity status of two problems related to finding the smallest number k such that a given graph is a partial k-tree and presents an algorithm with polynomially bounded (but exponential in k) worst case time complexity."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 28
                            }
                        ],
                        "text": "If we now consider the problem of computing the marginalsp(xi) for all of the nodes in the graph, it turns out that we already have the solution at hand."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 216
                            }
                        ],
                        "text": "\u2026however, to find good or even optimal orderings for specific graphs, and a variety of inference algorithms in specific fields (e.g., the algorithms for inference on phylogenies and pedigrees in Section 4.1) have been based on specific choices of elimination orderings in problems of interest."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 286
                            }
                        ],
                        "text": "\u2026the functional forms of the kernelsp(xv|x\u03c0v), the factorization in (1) implies a set of conditional independence statements among the variablesXv , and the entire set of conditional independence statements can be obtained from a polynomial timereachability algorithmbased on the graph (Pearl, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 154
                            }
                        ],
                        "text": "Thus, there are families of probability distributions that are captured by a directed graph and are not captured by any undirected graph, and vice versa (Pearl, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32583695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70ef29e6f0ce082bb8a47fd85b9bfb7cc0f20c93",
            "isKey": true,
            "numCitedBy": 18218,
            "numCiting": 230,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nProbabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty\u0097and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition\u0097in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. \nProbabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability."
            },
            "slug": "Probabilistic-reasoning-in-intelligent-systems-of-Pearl",
            "title": {
                "fragments": [],
                "text": "Probabilistic reasoning in intelligent systems - networks of plausible inference"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic."
            },
            "venue": {
                "fragments": [],
                "text": "Morgan Kaufmann series in representation and reasoning"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3355999"
                        ],
                        "name": "R. Elston",
                        "slug": "R.-Elston",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Elston",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Elston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117148775"
                        ],
                        "name": "J. Stewart",
                        "slug": "J.-Stewart",
                        "structuredName": {
                            "firstName": "J",
                            "lastName": "Stewart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stewart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28958804,
            "fieldsOfStudy": [
                "Biology",
                "Medicine"
            ],
            "id": "976215ce65ef67d8d7dace1a0dc62128fc8f0abf",
            "isKey": false,
            "numCitedBy": 1510,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Assuming random mating and random sampling of pedigrees, the likelihood of a set of pedigree data is developed in terms of: (1) the population distribution of the different genotypes; (2) the phenotyp"
            },
            "slug": "A-general-model-for-the-genetic-analysis-of-data.-Elston-Stewart",
            "title": {
                "fragments": [],
                "text": "A general model for the genetic analysis of pedigree data."
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "Assuming random mating and random sampling of pedigrees, the likelihood of a set of pedigree data is developed in terms of the population distribution of the different genotypes."
            },
            "venue": {
                "fragments": [],
                "text": "Human heredity"
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3231485"
                        ],
                        "name": "S. Aji",
                        "slug": "S.-Aji",
                        "structuredName": {
                            "firstName": "Srinivas",
                            "lastName": "Aji",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Aji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723150"
                        ],
                        "name": "R. McEliece",
                        "slug": "R.-McEliece",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "McEliece",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. McEliece"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 227
                            }
                        ],
                        "text": "Second, the algebraic operations that underlie the sum\u2013product algorithm are justified by the fact that sums and products form a commutative semiring, and the algorithm generalizes immediately to any other commutative semiring (Aji and McEliece, 2000; Shenoy and Shafer, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11355291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e8933300a20f3d799dc9f19e352967f41d8efcc",
            "isKey": false,
            "numCitedBy": 773,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss a general message passing algorithm, which we call the generalized distributive law (GDL). The GDL is a synthesis of the work of many authors in information theory, digital communications, signal processing, statistics, and artificial intelligence. It includes as special cases the Baum-Welch algorithm, the fast Fourier transform (FFT) on any finite Abelian group, the Gallager-Tanner-Wiberg decoding algorithm, Viterbi's algorithm, the BCJR algorithm, Pearl's \"belief propagation\" algorithm, the Shafer-Shenoy probability propagation algorithm, and the turbo decoding algorithm. Although this algorithm is guaranteed to give exact answers only in certain cases (the \"junction tree\" condition), unfortunately not including the cases of GTW with cycles or turbo decoding, there is much experimental evidence, and a few theorems, suggesting that it often works approximately even when it is not supposed to."
            },
            "slug": "The-generalized-distributive-law-Aji-McEliece",
            "title": {
                "fragments": [],
                "text": "The generalized distributive law"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Although this algorithm is guaranteed to give exact answers only in certain cases (the \"junction tree\" condition), unfortunately not including the cases of GTW with cycles or turbo decoding, there is much experimental evidence, and a few theorems, suggesting that it often works approximately even when it is not supposed to."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770859"
                        ],
                        "name": "R. Gallager",
                        "slug": "R.-Gallager",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gallager",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gallager"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 13
                            }
                        ],
                        "text": "GALLAGER, R. G. (1963)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 64
                            }
                        ],
                        "text": "This is the \u201cerrors-in-covariates\u201d logistic regression model of Richardson et al. (2002). The core of this model is a logistic regression of Yi on Xi."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12709402,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "206f827fad201506c315d40c1469b41a45141893",
            "isKey": false,
            "numCitedBy": 10568,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "A low-density parity-check code is a code specified by a parity-check matrix with the following properties: each column contains a small fixed number j \\geq 3 of l's and each row contains a small fixed number k > j of l's. The typical minimum distance of these codes increases linearly with block length for a fixed rate and fixed j . When used with maximum likelihood decoding on a sufficiently quiet binary-input symmetric channel, the typical probability of decoding error decreases exponentially with block length for a fixed rate and fixed j . A simple but nonoptimum decoding scheme operating directly from the channel a posteriori probabilities is described. Both the equipment complexity and the data-handling capacity in bits per second of this decoder increase approximately linearly with block length. For j > 3 and a sufficiently low rate, the probability of error using this decoder on a binary symmetric channel is shown to decrease at least exponentially with a root of the block length. Some experimental results show that the actual probability of decoding error is much smaller than this theoretical bound."
            },
            "slug": "Low-density-parity-check-codes-Gallager",
            "title": {
                "fragments": [],
                "text": "Low-density parity-check codes"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A simple but nonoptimum decoding scheme operating directly from the channel a posteriori probabilities is described and the probability of error using this decoder on a binary symmetric channel is shown to decrease at least exponentially with a root of the block length."
            },
            "venue": {
                "fragments": [],
                "text": "IRE Trans. Inf. Theory"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1932728"
                        ],
                        "name": "T. Richardson",
                        "slug": "T.-Richardson",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Richardson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Richardson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688329"
                        ],
                        "name": "A. Shokrollahi",
                        "slug": "A.-Shokrollahi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Shokrollahi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shokrollahi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066538379"
                        ],
                        "name": "R. Urbanke",
                        "slug": "R.-Urbanke",
                        "structuredName": {
                            "firstName": "Ruediger",
                            "lastName": "Urbanke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urbanke"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8875123,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed4848511da75e6db636c7ff4042175c14670327",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We design sequences of low-density parity check codes that provably perform at rates extremely close to the Shannon capacity. These codes are built from highly irregular bipartite graphs with carefully chosen degree patterns on both sides. We further show that under suitable conditions the message densities fulfil a certain symmetry condition which we call the consistency condition and we present a stability condition which is the most powerful tool to date to bound/determine the threshold of a given family of low-density parity check codes."
            },
            "slug": "Design-of-provably-good-low-density-parity-check-Richardson-Shokrollahi",
            "title": {
                "fragments": [],
                "text": "Design of provably good low-density parity check codes"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A stability condition is presented which is the most powerful tool to date to bound/determine the threshold of a given family of low-density parity check codes."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Symposium on Information Theory (Cat. No.00CH37060)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66418228"
                        ],
                        "name": "\u4e38\u5c71 \u5fb9",
                        "slug": "\u4e38\u5c71-\u5fb9",
                        "structuredName": {
                            "firstName": "\u4e38\u5c71",
                            "lastName": "\u5fb9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u4e38\u5c71 \u5fb9"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 221
                            }
                        ],
                        "text": "We now use two important facts: (1) the cumulant\ngenerating functionA(\u03b8) is a convex function on a convex domain (Brown, 1986); (2) any convex function can be expressed variationally in terms of itsconjugate dual function(Rockafellar, 1970)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117573922,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b272701e77ddb860741a193ac1701ca382853680",
            "isKey": false,
            "numCitedBy": 7927,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Convex-Analysis\u306e\u4e8c,\u4e09\u306e\u9032\u5c55\u306b\u3064\u3044\u3066-\u4e38\u5c71",
            "title": {
                "fragments": [],
                "text": "Convex Analysis\u306e\u4e8c,\u4e09\u306e\u9032\u5c55\u306b\u3064\u3044\u3066"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 193
                            }
                        ],
                        "text": "Using these definitions, Gibbs samplers can be set up automatically from the graphical model specification, a fact that is exploited in the BUGS software for Gibbs sampling in graphical models (Gilks et al., 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A language and a program for complex"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 101
                            }
                        ],
                        "text": "2 The problem of finding an elimination ordering that achieves the treewidth turns out to be NP-hard (Arnborg et al., 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complexity of finding embeddings"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 172
                            }
                        ],
                        "text": "Classical algorithms for inference on multilocus pedigrees are variants of the elimination algorithm on this fHMM and correspond to different choices of elimination order (Lander and Green, 1987; Elston and Stewart, 1971)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Construction of multilocus genetic maps in humans"
            },
            "venue": {
                "fragments": [],
                "text": "Pro - ceedings of the National Academy of Sciences"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Construction of multilocus genetic maps in humans"
            },
            "venue": {
                "fragments": [],
                "text": "Pro - ceedings of the National Academy of Sciences"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "eds.)7 435\u2013442"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 113
                            }
                        ],
                        "text": "We now use two important facts: (1) the cumulant\ngenerating functionA(\u03b8) is a convex function on a convex domain (Brown, 1986); (2) any convex function can be expressed variationally in terms of itsconjugate dual function(Rockafellar, 1970)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 116
                            }
                        ],
                        "text": "We now use two important facts: (1) the cumulant generating function A(\u03b8) is a convex function on a convex domain \u0398 (Brown, 1986), and (2) any convex function can be expressed variationally in terms of its conjugate dual function (Rockafellar, 1970)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fundamentals of Statistical Exponential Families"
            },
            "venue": {
                "fragments": [],
                "text": "Institute of Mathematical Statistics,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A language and a program for complex Bayesian modelling. The Statistician43 169\u2013177"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 9,
            "methodology": 8
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 25,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Graphical-Models-Jordan/e16a25faf7428e1fc5ed0a10b8196c0499c7fd0d?sort=total-citations"
}