{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727849"
                        ],
                        "name": "S. Hanson",
                        "slug": "S.-Hanson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Hanson",
                            "middleNames": [
                                "Jose"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hanson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144442133"
                        ],
                        "name": "L. Pratt",
                        "slug": "L.-Pratt",
                        "structuredName": {
                            "firstName": "Lorien",
                            "lastName": "Pratt",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Pratt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9344018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4ea5a6ff3ffcd11ec2e6ed7828a7d41279fb3ad",
            "isKey": false,
            "numCitedBy": 516,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Rumelhart (1987), has proposed a method for choosing minimal or \"simple\" representations during learning in Back-propagation networks. This approach can be used to (a) dynamically select the number of hidden units, (b) construct a representation that is appropriate for the problem and (c) thus improve the generalization ability of Back-propagation networks. The method Rumelhart suggests involves adding penalty terms to the usual error function. In this paper we introduce Rumelhart's minimal networks idea and compare two possible biases on the weight search space. These biases are compared in both simple counting problems and a speech recognition problem. In general, the constrained search does seem to minimize the number of hidden units required with an expected increase in local minima."
            },
            "slug": "Comparing-Biases-for-Minimal-Network-Construction-Hanson-Pratt",
            "title": {
                "fragments": [],
                "text": "Comparing Biases for Minimal Network Construction with Back-Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper introduces Rumelhart's minimal networks idea and compares two possible biases on the weight search space that are compared in both simple counting problems and a speech recognition problem."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952703"
                        ],
                        "name": "Y. Chauvin",
                        "slug": "Y.-Chauvin",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Chauvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Chauvin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 122
                            }
                        ],
                        "text": "In fact, sev-eral network minimization schemes have been implemented using non-proportionalweight decay (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989), or \\gat-ing coe cients\" (Mozer and Smolensky, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 104
                            }
                        ],
                        "text": "In fact, several network minimization schemes have been implemented using non-proportional weight decay (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989), or \\gating coe cients\" (Mozer and Smolensky, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 186
                            }
                        ],
                        "text": "Several such schemes have been proposed in the statistical inference literature [see (Akaike, 1986; Rissanen, 1989; Vapnik, 1989) and references therein] as well as in the NN literature (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989; Mozer and Smolensky, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 202
                            }
                        ],
                        "text": "Several such schemes have been proposed in the statistical inference literature [see(Akaike, 1986; Rissanen, 1989; Vapnik, 1989) and references therein] as well as inthe NN literature (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989; Mozerand Smolensky, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15784283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28b0563fcd3364077dfc39f42c9684ec00dcd249",
            "isKey": true,
            "numCitedBy": 198,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a variation of the back-propagation algorithm that makes optimal use of a network hidden units by decrasing an \"energy\" term written as a function of the squared activations of these hidden units. The algorithm can automatically find optimal or nearly optimal architectures necessary to solve known Boolean functions, facilitate the interpretation of the activation of the remaining hidden units and automatically estimate the complexity of architectures appropriate for phonetic labeling problems. The general principle of the algorithm can also be adapted to different tasks: for example, it can be used to eliminate the [0, 0] local minimum of the [-1. +1] logistic activation function while preserving a much faster convergence and forcing binary activations over the set of hidden units."
            },
            "slug": "A-Back-Propagation-Algorithm-with-Optimal-Use-of-Chauvin",
            "title": {
                "fragments": [],
                "text": "A Back-Propagation Algorithm with Optimal Use of Hidden Units"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A variation of the back-propagation algorithm that makes optimal use of a network hidden units by decrasing an \"energy\" term written as a function of the squared activations of these hidden units is presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15659829,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "25406e6733a698bfc4ac836f8e74f458e75dad4f",
            "isKey": false,
            "numCitedBy": 1696,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 < \u220a 1/8. We show that if m O(W/\u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 \u220a of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than (W/\u220a) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 \u220a fraction of the future test examples."
            },
            "slug": "What-Size-Net-Gives-Valid-Generalization-Baum-Haussler",
            "title": {
                "fragments": [],
                "text": "What Size Net Gives Valid Generalization?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if m O(W/ \u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 2 \u220a of future test examples drawn from the same distribution."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 156
                            }
                        ],
                        "text": "OBD motivated other architectural changes, ascan be seen by comparing the 2600-parameter network in (LeCun et al., 1990) tothe 10,000-parameter network in (LeCun et al., 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 102
                            }
                        ],
                        "text": "The method was validated using ourhandwritten digit recognition network trained with backpropagation (LeCun et al.,1989).2 OPTIMAL BRAIN DAMAGEObjective functions play a central role in this eld; therefore it is more than rea-sonable to de ne the saliency of a parameter to be the change in the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 132
                            }
                        ],
                        "text": "Speci cally, we prepared charts depicting the saliency of the 10,000parameters in the digit recognition network reported last year (LeCun et al., 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 55
                            }
                        ],
                        "text": "More details can be obtained from the companion paper (LeCun et al., 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41312633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27",
            "isKey": false,
            "numCitedBy": 7830,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification."
            },
            "slug": "Backpropagation-Applied-to-Handwritten-Zip-Code-LeCun-Boser",
            "title": {
                "fragments": [],
                "text": "Backpropagation Applied to Handwritten Zip Code Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper demonstrates how constraints from the task domain can be integrated into a backpropagation network through the architecture of the network, successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 156
                            }
                        ],
                        "text": "OBD motivated other architectural changes, ascan be seen by comparing the 2600-parameter network in (LeCun et al., 1990) tothe 10,000-parameter network in (LeCun et al., 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 102
                            }
                        ],
                        "text": "The method was validated using ourhandwritten digit recognition network trained with backpropagation (LeCun et al.,1989).2 OPTIMAL BRAIN DAMAGEObjective functions play a central role in this eld; therefore it is more than rea-sonable to de ne the saliency of a parameter to be the change in the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 132
                            }
                        ],
                        "text": "Speci cally, we prepared charts depicting the saliency of the 10,000parameters in the digit recognition network reported last year (LeCun et al., 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 55
                            }
                        ],
                        "text": "More details can be obtained from the companion paper (LeCun et al., 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2542741,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86ab4cae682fbd49c5a5bedb630e5a40fa7529f6",
            "isKey": false,
            "numCitedBy": 2930,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service."
            },
            "slug": "Handwritten-Digit-Recognition-with-a-Network-LeCun-Boser",
            "title": {
                "fragments": [],
                "text": "Handwritten Digit Recognition with a Back-Propagation Network"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task, and has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144473519"
                        ],
                        "name": "M. Mozer",
                        "slug": "M.-Mozer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mozer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mozer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748557"
                        ],
                        "name": "P. Smolensky",
                        "slug": "P.-Smolensky",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Smolensky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Smolensky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17651092,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a87953825b0bea2a5d52bfccf09d2518295c5053",
            "isKey": false,
            "numCitedBy": 661,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a means of using the knowledge in a network to determine the functionality or relevance of individual units, both for the purpose of understanding the network's behavior and improving its performance. The basic idea is to iteratively train the network to a certain performance criterion, compute a measure of relevance that identifies which input or hidden units are most critical to performance, and automatically trim the least relevant units. This skeletonization technique can be used to simplify networks by eliminating units that convey redundant information; to improve learning performance by first learning with spare hidden units and then trimming the unnecessary ones away, thereby constraining generalization; and to understand the behavior of networks in terms of minimal \"rules.\""
            },
            "slug": "Skeletonization:-A-Technique-for-Trimming-the-Fat-a-Mozer-Smolensky",
            "title": {
                "fragments": [],
                "text": "Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The basic idea is to iteratively train the network to a certain performance criterion, compute a measure of relevance that identifies which input or hidden units are most critical to performance, and automatically trim the least relevant units."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 180
                            }
                        ],
                        "text": "1 INTRODUCTION Most successful applications of neural network learning to real-world problems have been achieved using highly structured networks of rather large size [for example (Waibel, 1989; LeCun et al., 1990)]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 254
                            }
                        ],
                        "text": "Experiments con rm the usefulness of the methods on areal-world application.1 INTRODUCTIONMost successful applications of neural network learning to real-world problems havebeen achieved using highly structured networks of rather large size [for example(Waibel, 1989; LeCun et al., 1990)]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16685841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4a7e54446d52f066ee692fa38d9aa972519c2f5",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown that neural networks for speech recognition can be constructed in a modular fashion by exploiting the hidden structure of previously trained phonetic subcategory networks. The performance of resulting larger phonetic nets was found to be as good as the performance of the subcomponent nets by themselves. This approach avoids the excessive learning times that would be necessary to train larger networks and allows for incremental learning. Large time-delay neural networks constructed incrementally by applying these modular training techniques achieved a recognition performance of 96.0% for all consonants and 94.7% for all phonemes.<<ETX>>"
            },
            "slug": "Consonant-recognition-by-modular-construction-of-Waibel",
            "title": {
                "fragments": [],
                "text": "Consonant recognition by modular construction of large phonemic time-delay neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "It is shown that neural networks for speech recognition can be constructed in a modular fashion by exploiting the hidden structure of previously trained phonetic subcategory networks to avoid the excessive learning times that would be necessary to train larger networks and allow for incremental learning."
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Acoustics, Speech, and Signal Processing,"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145230652"
                        ],
                        "name": "D. B. Schwartz",
                        "slug": "D.-B.-Schwartz",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. B. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726209"
                        ],
                        "name": "B. Wittner",
                        "slug": "B.-Wittner",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Wittner",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Wittner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 24
                            }
                        ],
                        "text": "It is known from theory (Denker et al., 1987; Baum and Haussler, 1989; Solla et al., 1990) and experience (Le Cun, 1989) that, for a fixed amount of training data, networks with too many weights do not generalize well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 25
                            }
                        ],
                        "text": "It is known from theory (Denker et al., 1987; Baum and Haussler, 1989; Sollaet al., 1990) and experience (LeCun, 1989) that, for a xed amount of trainingdata, networks with too many weights do not generalize well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7508740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33fdc91c520b54e097f5e09fae1cfc94793fbfcf",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Since an tiquity, man has dreamed of building a de vice that would \"learn from examples\" 1 \"form generalizations\", and \"discover t he rules\" behind patt ern s in t he data. Recent work has shown that a high ly connected , layered networ k of simple an alog processing element s can be astonishingly successful at this, in some cases . In ord er to be precise about what has been observed, we give defini t ions of memorization, generalization , and rule ex traction. T he most im portant part of this paper proposes a way to measure th e ent ropy or information content of a learni ng task a nd the effi ciency wit h which a network ext racts informat ion from the dat a. We also a rgue that the way in which the ne tworks ca n compactly represent a wid e class of Boolean (an d othe r) functi ons is analogous to t he way in which polynomials or other famili es of functions can be \"curve fit\" to gene ral data; specifically, they ex tend the domain, a nd average noisy data. Alas , findi ng a suitable rep rese ntation is generall y an ill-posed and ill-cond itio ned problem. E ven whe n the problem has bee n \" regularized\", what rem ain s is a difficult combinatoria l opt imizatio n problem. Whe n a network is given mo re resou rces than the mi nimu m needed to solve a given t ask , the symmetric, low-order , local solut ions that hum an s see m to pre fer are not the ones that the network chooses from th e vast number of solut ions avai la ble; ind eed , th e generalized delt a method a nd similar learning procedures do not usually hold t he \"human \" solut ions stable against perturbations. Fortuna tely, the re are \u00a9 1987 Comp lex Systems Publications, Inc. 878 Denker, Schwart z, Wittner, Solla, Howard , J ackel, and Hopfield ways of \"program ming\" into t he networ k a preference for appropriately chosen symmetries . 1. Overview of the contents Section 2 gives seve ral examples that illustra te t he import ance of automatic learning from examples . Section 3 poses a tes t -case problem (\"c l umps\") which will be used t hroughout the paper to illustrate the issues of interest. Section 4 describes the class of networks we are considering and introdu ces t he notation. Section 5 presents a proof by construction t hat a two-layer network can rep resent any Boolean function, and section 6 shows t hat there is an elegant representation for the c lumps tas k, using very few weights and processing units. Sections 7 an d 8 argue that the ob jective function E(W ) has a complicated st ruct ure: good solutions are generally not points in W space, bu t rat her parameteri zed fam ilies of points. Furt hermore, in all but the simplest sit uations, the E su rface is riddled with local minim a, and any automatic lear ning procedure must take firm measures to deal with t his. Section 9 shows that our c l umps tas k is a very simple prob lem, accordin g to the various schemes that have been proposed to quantify the complexity of network tasks and solut ions. Section 10 shows that a general network does no t prefer t he simple solut ions t hat hum ans seem to prefer. Sect ion 11 discusses the crucial effect of changes of representation on the feasibility of aut oma t ic learni ng. We prove that \"automat ic learn ing will always succeed, given t he right preprocessor,\" but we also show t hat t his statement is grossly misleading since there is no automati c procedure for const ruct ing the requ ired preprocessor. Sections 12 and 13 propose definit ions of rule ext ract ion and genera liza t ion and emphas ize th e disti nction between th e two. Sect ion 14 calculates th e entropy budget for ru le ext ract ion and est imates the informat ion available from the t rain ing data and from the \"programming\" or \"architecture\" of t he network. This leads to an ap proximate express ion for t he efficiency with which the learni ng procedu re ext rac ts infor mat ion from t he t ra ining data. Sect ion 16 presents a simple model which allows us to calculate the erro r rate duri ng t he learn ing process. Sect ion 17 discusses the rela t ionship bet ween rule ext ract ion in general and assoc iat ive memo ry in particular . In sect ion 18, we arg ue that when special informat ion is availabl e, such as infor mation about the symmetry, geomet ry, or topology of the task at hand, the netwo rk must be provided this information. We also discuss various ways in which this informat ion can be \"programmed\" into t he net wor k. Section 19 dr aws the analogy between th e family of functions t hat can be implemented by networks with limited amounts of resour ces and other families of funct ions such as polynomials of limited degree. App endix A contains detai ls of th e condit ions under which our data was taken. Large Automa.tic Learning) Rule Extraction, and Generaliza.tion 879 2. Why lea r n from examples? Automa t ic learning from exa mples is a top ic of enormo us importan ce. There are many application s where there is no ot her way to approach the task. For example, consider th e problem of recognizing hand-wri t ten characters. The raw image can be fed to a preprocessor that will detect salient fea tures such as straight line segments, arcs, terminations, et c., in various parts of the field. But what then? Th ere is no mathematical expression t hat will tell you what features correspo nd to a \"7\" or a \"Q\" . The task is defined purely by th e statist ics of what features convent iona lly go with what meaningt here is no ot her definition. T here is no way to prog ram it ; the solut ion must be learned by examp les [6,11]. Another example is the task of producing the correct pronunciation of a segment of written English . There are pattern s and rules of pron unciation , but th ey are so complex that a network th at could \"discover t he rules\" on its own would save an enormous amount of labor [37J. Another example concerns clinical medicine: t he task of mapping a set of symptoms onto a diagnosis. Here t he inputs have physical meaningth ey are not purely convent iona l as in the previous exa mplesbut we are st ill a long way from writing down an equat ion or a computer program that will perform the task a priori. We must learn from the statist ics of past exa mp les (41). Other examples include classifying sonar returns [10], recogni zing speech [5,16,30,23], and predi cting the secondary st ruct ure of proteins from the primary sequence [42]. In th e foregoing examples, t here was rea lly no alte rnat ive to learni ng from exa mples. However, in order to learn more about the power and limit ations of var ious learnin g methods and evaluate new methods as they are prop osed , people have st udied a number of \"test cases\" where t here was an alternativeth at is, where the \"correct\" solut ion was well understood. T hese includ e classifying input pattern s accord ing to th eir parity [33], geometric shape [33,35], or spatial symmetry [36J. 3. Example : tvo-or-more clumps Th e tes t case that we will use throughout t his pap er is a simple geometric task which an adaptive network ought to be able to handle. Th e network's inp ut pattern s will be Nbit binary st rings. Somet imes we will tr ea t the pattern s as numbers, so we can speak of numerical order ; somet imes we will also treat them as one-dimensional images, in which false bits (Fs) repr esent white pixels and true bits (Ts) rep resent black pixels. A cont iguous clump of T s represents a solid black bar . We th en choose the following rule to determine th e desired output of the network, as shown in table 1: if the inpu t pattern is such that all the T s appear in one cont iguous clump , th en th e output should be F , and if there are two or more dumps, th en t he 880 Denker, Schwartz, Wittner, Solla, Howard, Jackel, and Hopfield Input pattern Outpu t Interpretation ffft ttffff F 1 clump fffttftfff T 2 clumps ftt ttttttt F 1 clump tttffttff t T 3 clumps ffffffffff F no clumps Tabl e 1: Exa mples of the t woor-more clumps predicate. output should be T . We call this t he two-or-more clumps predicate.1 We will consider numerous variat ions of t his problem, such as three-versus -two clumps and so for t h. The one-versus-two clumps version is a lso known as t he contiguity predi cate [25]. Questions of connectedness have played an importan t role in the history of network s and automatic learning: Minsky a nd P ap er t devoted a sizable por t ion of t hei r book [27] to this sort of qu est ion. There a re a host of important questions that immedi a tely a rise, some of whi ch are list ed below. In some cases , we give summary answe rs ; the details of t he an swers will be given in following sections . Ca n any network of t he type we are con sid ering actua lly rep resen t such a fu nct ion? (Yes.) This is not a t rivial resu lt , since Minsky and Paper t [27J showed that a Perce ptron (with one layer of adjustable weight s) absolutely could not perform a wide class of functions, and our fun ction is in th is class. Can it perform the funct ion efficient ly? (Yes .) This is in cont ras t, say, to a solut ion of the par ity function usin g a standard programmable logic array (PLA) [26], which is possibl e but requires enormo us numbers of hardware components (O(2N ) gates). Can the net work learn to perform this function , by learn ing from examples? (Yes.) How qui ckly can it learn it ? (It de pen ds; see below.) How many layers are required , an d how many hidden units in eac h layer? How do t he answers to t he prev ious ques t ions de pen d on t he architecture (i.e. size an d shape) of th e network? How sensit ive a re the resul t s to t he num erical me t hods and other details of the implementation , such as t he an alog represe ntation of T and F, \"moment um term s\" , \"weight decay te rms\" , step size, et c.? Does t he solut ion (i. e. the configuration of weights) t hat the net work find s make sense? Is it s imilar to the solut ions t hat human s would choose , given t he task of designing"
            },
            "slug": "Large-Automatic-Learning,-Rule-Extraction,-and-Denker-Schwartz",
            "title": {
                "fragments": [],
                "text": "Large Automatic Learning, Rule Extraction, and Generalization"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 115
                            }
                        ],
                        "text": "Several such schemes have been proposed in the statistical inference literature [see(Akaike, 1986; Rissanen, 1989; Vapnik, 1989) and references therein] as well as inthe NN literature (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989; Mozerand Smolensky, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37495267,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "6af20fc8e3f848e8f4cd5c10dc04316400d6ca85",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Inductive-principles-of-the-search-for-empirical-on-Vapnik",
            "title": {
                "fragments": [],
                "text": "Inductive principles of the search for empirical dependences (methods based on weak convergence of probability measures)"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '89"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737945"
                        ],
                        "name": "H. Akaike",
                        "slug": "H.-Akaike",
                        "structuredName": {
                            "firstName": "Hirotugu",
                            "lastName": "Akaike",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Akaike"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 85
                            }
                        ],
                        "text": "Several such schemes have been proposed in the statistical inference literature [see (Akaike, 1986; Rissanen, 1989; Vapnik, 1989) and references therein] as well as in the NN literature (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989; Mozer and Smolensky, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 85
                            }
                        ],
                        "text": "Several such schemes have been proposed in the statistical inference literature [see(Akaike, 1986; Rissanen, 1989; Vapnik, 1989) and references therein] as well as inthe NN literature (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989; Mozerand Smolensky, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123148751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2dd98542786c90b7c01b6e44381d9b9c4d81430b",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The necessity of using statistical models for the development of efficient statistical signal processing procedures is explained with simple examples of the time series analysis. Recent extensions of the use of the concept of the likelihood of a statistical model are reviewd from the point of view of the entropy maximization principle, Particular emphasis is placed on the proper use of Bayesian models."
            },
            "slug": "Use-of-statistical-models-for-time-series-analysis-Akaike",
            "title": {
                "fragments": [],
                "text": "Use of statistical models for time series analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "The necessity of using statistical models for the development of efficient statistical signal processing procedures is explained with simple examples of the time series analysis, with particular emphasis on the proper use of Bayesian models."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP '86. IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 93
                            }
                        ],
                        "text": "Various complexity measures have been proposed, including Vapnik-Chervonenkisdimensionality (Vapnik and Chervonenkis, 1971) and description length (Rissanen,1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 93
                            }
                        ],
                        "text": "Various complexity measures have been proposed, including Vapnik-Chervonenkis dimensionality (Vapnik and Chervonenkis, 1971) and description length (Rissanen, 1989) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8142232,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a36b028d024bf358c4af1a5e1dc3ca0aed23b553",
            "isKey": false,
            "numCitedBy": 3709,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady. The paper was first published in Russian as \u0412\u0430\u043f\u043d\u0438\u043a \u0412. \u041d. and \u0427\u0435\u0440\u0432\u043e\u043d\u0435\u043d\u043a\u0438\u0441 \u0410. \u042f. \u041e \u0440\u0430\u0432\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0419 \u0441\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u0447\u0430\u0441\u0442\u043e\u0442 \u043f\u043e\u044f\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043e\u0431\u044b\u0442\u0438\u0419 \u043a \u0438\u0445 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044f\u043c. \u0422\u0435\u043e\u0440\u0438\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0419 \u0438 \u0435\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f 16(2), 264\u2013279 (1971)."
            },
            "slug": "Chervonenkis:-On-the-uniform-convergence-of-of-to-Vapnik",
            "title": {
                "fragments": [],
                "text": "Chervonenkis: On the uniform convergence of relative frequencies of events to their probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 137
                            }
                        ],
                        "text": "In fact, sev-eral network minimization schemes have been implemented using non-proportionalweight decay (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989), or \\gat-ing coe cients\" (Mozer and Smolensky, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 104
                            }
                        ],
                        "text": "In fact, several network minimization schemes have been implemented using non-proportional weight decay (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989), or \"gating coefficients\" (Mozer and Smolensky, 1989)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 186
                            }
                        ],
                        "text": "Several such schemes have been proposed in the statistical inference literature [see (Akaike, 1986; Rissanen, 1989; Vapnik, 1989) and references therein] as well as in the NN literature (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989; Mozer and Smolensky, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 217
                            }
                        ],
                        "text": "Several such schemes have been proposed in the statistical inference literature [see(Akaike, 1986; Rissanen, 1989; Vapnik, 1989) and references therein] as well as inthe NN literature (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989; Mozerand Smolensky, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some Comparisons of Constraints for Minimal Network Construction with Back-Propagation"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Information Processing Systems,"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "propagation method used extensively in various applications (Becker and LeCun, 1989; LeCun, 1989; LeCun et al., 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 106
                            }
                        ],
                        "text": "It is known from theory (Denker et al., 1987; Baum and Haussler, 1989; Sollaet al., 1990) and experience (LeCun, 1989) that, for a xed amount of trainingdata, networks with too many weights do not generalize well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", 1990) and experience (LeCun, 1989) that, for a xed amount of training data, networks with too many weights do not generalize well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 151
                            }
                        ],
                        "text": "Such a procedure was derived in (LeCun, 1987), and was the basis of a fast back-\npropagation method used extensively in various applications (Becker and LeCun,1989; LeCun, 1989; LeCun et al., 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59861896,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01b6affe3ea4eae1978aec54e87087feb76d9215",
            "isKey": false,
            "numCitedBy": 863,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalization-and-network-design-strategies-LeCun",
            "title": {
                "fragments": [],
                "text": "Generalization and network design strategies"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067137798"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48873590"
                        ],
                        "name": "F. Souli\u00e9",
                        "slug": "F.-Souli\u00e9",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Souli\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Souli\u00e9"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 149015233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd6b44fde142686ed22836c4fdb4f6684a007028",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modeles-connexionnistes-de-l'apprentissage-LeCun-Souli\u00e9",
            "title": {
                "fragments": [],
                "text": "Modeles connexionnistes de l'apprentissage"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067137798"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 36
                            }
                        ],
                        "text": "One technique to reach this tradeo is to minimize a cost function composed of twoterms: the ordinary training error, plus some measure of the network complexity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 33
                            }
                        ],
                        "text": "Such a procedure was derived in (LeCun, 1987), and was the basis of a fast back-\npropagation method used extensively in various applications (Becker and LeCun,1989; LeCun, 1989; LeCun et al., 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 151887454,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "6f8fbd0873eb98519d7047c13251aef32e769dfe",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "PhD-thesis:-Modeles-connexionnistes-de-learning-LeCun",
            "title": {
                "fragments": [],
                "text": "PhD thesis: Modeles connexionnistes de l'apprentissage (connectionist learning models)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50333420"
                        ],
                        "name": "S. Becker",
                        "slug": "S.-Becker",
                        "structuredName": {
                            "firstName": "Suzanna",
                            "lastName": "Becker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Becker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 143
                            }
                        ],
                        "text": "One technique to reach this tradeo is to minimize a cost function composed of twoterms: the ordinary training error, plus some measure of the network complexity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 140
                            }
                        ],
                        "text": "Such a procedure was derived in (LeCun, 1987), and was the basis of a fast back-\npropagation method used extensively in various applications (Becker and LeCun,1989; LeCun, 1989; LeCun et al., 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59695337,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "589d377b23e2bdae7ad161b36a5d6613bcfccdde",
            "isKey": false,
            "numCitedBy": 411,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Improving-the-convergence-of-back-propagation-with-Becker-LeCun",
            "title": {
                "fragments": [],
                "text": "Improving the convergence of back-propagation learning with second-order methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Handwritten digit recognition with a back"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Handwritten Digit Recognition with a BackPropagation Network"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal Brain Damage"
            },
            "venue": {
                "fragments": [],
                "text": "To appear in Computer and System Sciences"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal Brain Damage. To appear in Computer and System Sciences"
            },
            "venue": {
                "fragments": [],
                "text": "Optimal Brain Damage. To appear in Computer and System Sciences"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Handwritten digit recognition with a backpropagation network"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 233
                            }
                        ],
                        "text": "Optimal Brain DamageYann Le Cun, John S. Denker and Sara A. SollaAT&T Bell Laboratories, Holmdel, N. J. 07733ABSTRACTWe have used information-theoretic ideas to derive a class of prac-tical and nearly optimal schemes for adapting the size of a neuralnetwork."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Back-Propagation Applied to Handwritten Zipcode Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 187
                            }
                        ],
                        "text": "In fact, sev-eral network minimization schemes have been implemented using non-proportionalweight decay (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989), or \\gat-ing coe cients\" (Mozer and Smolensky, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 185
                            }
                        ],
                        "text": "In fact, several network minimization schemes have been implemented using non-proportional weight decay (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989), or \\gating coe cients\" (Mozer and Smolensky, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 186
                            }
                        ],
                        "text": "Several such schemes have been proposed in the statistical inference literature [see (Akaike, 1986; Rissanen, 1989; Vapnik, 1989) and references therein] as well as in the NN literature (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989; Mozer and Smolensky, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Skeletonization: A Technique for Trimming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal brain damage"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural information Processing Systems 2 ,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 24
                            }
                        ],
                        "text": "It is known from theory (Denker et al., 1987; Baum and Haussler, 1989; Solla et al., 1990) and experience (Le Cun, 1989) that, for a fixed amount of training data, networks with too many weights do not generalize well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Supervised Learning: a Theoretical Framework"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Information Processing Systems,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 148
                            }
                        ],
                        "text": "In fact, sev-eral network minimization schemes have been implemented using non-proportionalweight decay (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989), or \\gat-ing coe cients\" (Mozer and Smolensky, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 228
                            }
                        ],
                        "text": "Several such schemes have been proposed in the statistical inference literature [see(Akaike, 1986; Rissanen, 1989; Vapnik, 1989) and references therein] as well as inthe NN literature (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989; Mozerand Smolensky, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some Comparisons of Constraints for Minimal Network Construction with BackPropagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Supervised Learning : a Theoretical Framework"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the Uniform Convergence ofRelative Frequencies of Events to Their Probabilities"
            },
            "venue": {
                "fragments": [],
                "text": "Th . Prob . and its Applications"
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 33
                            }
                        ],
                        "text": "Such a procedure was derived in (LeCun, 1987), and was the basis of a fast back-\npropagation method used extensively in various applications (Becker and LeCun,1989; LeCun, 1989; LeCun et al., 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 32
                            }
                        ],
                        "text": "Such a procedure was derived in (LeCun, 1987), and was the basis of a fast back-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modeles connexionnistes de l'apprentissage (connectionist"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 101
                            }
                        ],
                        "text": "OBD motivated other architectural changes, ascan be seen by comparing the 2600-parameter network in (LeCun et al., 1990) tothe 10,000-parameter network in (LeCun et al., 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 268
                            }
                        ],
                        "text": "Experiments con rm the usefulness of the methods on areal-world application.1 INTRODUCTIONMost successful applications of neural network learning to real-world problems havebeen achieved using highly structured networks of rather large size [for example(Waibel, 1989; LeCun et al., 1990)]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 176
                            }
                        ],
                        "text": "Such a procedure was derived in (LeCun, 1987), and was the basis of a fast back-\npropagation method used extensively in various applications (Becker and LeCun,1989; LeCun, 1989; LeCun et al., 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Handwritten digit recognition with a backpropagation network"
            },
            "venue": {
                "fragments": [],
                "text": "Touretzky, D., editor, Advances in Neural Information Processing Systems 2 (NIPS*89)"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving the Convergence of BackPropagationLearning with Sec ondOrder Methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 13,
            "methodology": 14
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 31,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Optimal-Brain-Damage-LeCun-Denker/e7297db245c3feb1897720b173a59fe7e36babb7?sort=total-citations"
}